{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torchdiffeq import odeint\n",
    "# from torchdiffeq import odeint_adjoint as odeint # Might be more stable according to docs of torchdiffeq\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "opt = {\n",
    "  'data_norm': 'rw',\n",
    "  'self_loop_weight': 1.0,\n",
    "  'hidden_dim': 80,\n",
    "  'input_dropout': 0.5,\n",
    "  'dropout': 0.046878964627763316,\n",
    "  'optimizer': 'adam',\n",
    "  'lr': 0.01,\n",
    "  'decay': 0.00507685443154266,\n",
    "  'epoch': 100,\n",
    "  'alpha': 1.0,\n",
    "  'block': 'constant',\n",
    "  'function': 'laplacian',\n",
    "  'time': 18.294754260552843,\n",
    "  'method': 'euler',\n",
    "  'step_size': 1,\n",
    "  'adjoint': False,\n",
    "  'tol_scale': 821.9773048827274,\n",
    "  'max_nfe': 2000,\n",
    "  'no_early': True,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Set torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Number of validation nodes: 500\n",
      "Number of test nodes: 1000\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = Path('data')\n",
    "dataset_dir = dataset_dir.absolute()\n",
    "if not dataset_dir.exists():\n",
    "    dataset_dir.mkdir(parents=True)\n",
    "\n",
    "dataset = Planetoid(dataset_dir, 'Cora')\n",
    "\n",
    "# Some info\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset.data\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
    "print(f'Number of test nodes: {data.test_mask.sum()}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# # Plot the graph\n",
    "# G = nx.Graph()\n",
    "#\n",
    "# G.add_nodes_from(list(range(data.num_nodes)))\n",
    "# G.add_edges_from([tuple(x) for x in data.edge_index.T.tolist()])\n",
    "# nx.draw(G)\n",
    "# plt.gca().set_facecolor('white')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Train the given model on the given graph for num_epochs\n",
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = data.x\n",
    "    y = data.y.squeeze()\n",
    "\n",
    "    # Set up the loss and the optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    out = model(x)\n",
    "\n",
    "    loss = loss_fn(out[data.train_mask], y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class LaplacianODEFunc(MessagePassing):\n",
    "\n",
    "  # currently requires in_features = out_features\n",
    "  def __init__(self, in_features, out_features, opt, data, device):\n",
    "    super(MessagePassing, self).__init__()\n",
    "    self.opt = opt\n",
    "    self.device = device\n",
    "    self.edge_index = None\n",
    "    self.edge_weight = None\n",
    "    self.attention_weights = None\n",
    "    self.alpha_train = nn.Parameter(torch.tensor(0.0))\n",
    "    self.beta_train = nn.Parameter(torch.tensor(0.0))\n",
    "    self.x0 = None\n",
    "    self.nfe = 0\n",
    "    self.alpha_sc = nn.Parameter(torch.ones(1))\n",
    "    self.beta_sc = nn.Parameter(torch.ones(1))\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "    self.w = nn.Parameter(torch.eye(opt['hidden_dim']))\n",
    "    self.d = nn.Parameter(torch.zeros(opt['hidden_dim']) + 1)\n",
    "    self.alpha_sc = nn.Parameter(torch.ones(1))\n",
    "    self.beta_sc = nn.Parameter(torch.ones(1))\n",
    "\n",
    "  def sparse_multiply(self, x):\n",
    "    ax = torch_sparse.spmm(self.edge_index, self.edge_weight, x.shape[0], x.shape[0], x)\n",
    "    return ax\n",
    "\n",
    "  def forward(self, t, x):  # the t param is needed by the ODE solver.\n",
    "    self.nfe += 1\n",
    "    ax = self.sparse_multiply(x)\n",
    "    alpha = torch.sigmoid(self.alpha_train)\n",
    "    f = alpha * (ax - x)\n",
    "    f = f + self.beta_train * self.x0\n",
    "    return f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def get_rw_adj(edge_index, edge_weight=None, norm_dim=1, fill_value=0., num_nodes=None, dtype=None):\n",
    "  # Not sure what this does yet, but it is necessary\n",
    "  num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "  if edge_weight is None:\n",
    "    edge_weight = torch.ones((edge_index.size(1),), dtype=dtype,\n",
    "                             device=edge_index.device)\n",
    "\n",
    "  if not fill_value == 0:\n",
    "    edge_index, tmp_edge_weight = add_remaining_self_loops(\n",
    "      edge_index, edge_weight, fill_value, num_nodes)\n",
    "    assert tmp_edge_weight is not None\n",
    "    edge_weight = tmp_edge_weight\n",
    "\n",
    "  row, col = edge_index[0], edge_index[1]\n",
    "  indices = row if norm_dim == 0 else col\n",
    "  deg = scatter_add(edge_weight, indices, dim=0, dim_size=num_nodes)\n",
    "  deg_inv_sqrt = deg.pow_(-1)\n",
    "  edge_weight = deg_inv_sqrt[indices] * edge_weight if norm_dim == 0 else edge_weight * deg_inv_sqrt[indices]\n",
    "  return edge_index, edge_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class ConstantODEblock(nn.Module):\n",
    "  def __init__(self, odefunc, opt, data, device, t=torch.tensor([0, 1])):\n",
    "    super(ConstantODEblock, self).__init__()\n",
    "    self.opt = opt\n",
    "    self.t = t\n",
    "\n",
    "    self.odefunc = odefunc(opt['hidden_dim'], opt['hidden_dim'], opt, data, device)\n",
    "    edge_index, edge_weight = get_rw_adj(data.edge_index, edge_weight=data.edge_attr, norm_dim=1,\n",
    "                                                                   fill_value=opt['self_loop_weight'],\n",
    "                                                                   num_nodes=data.num_nodes,\n",
    "                                                                   dtype=data.x.dtype)\n",
    "    self.odefunc.edge_index = edge_index.to(device)\n",
    "    self.odefunc.edge_weight = edge_weight.to(device)\n",
    "\n",
    "    self.train_integrator = odeint\n",
    "    self.test_integrator = odeint\n",
    "    self.set_tol()\n",
    "\n",
    "  def set_x0(self, x0):\n",
    "    self.odefunc.x0 = x0.clone().detach()\n",
    "\n",
    "  def set_tol(self):\n",
    "    self.atol = self.opt['tol_scale'] * 1e-7\n",
    "    self.rtol = self.opt['tol_scale'] * 1e-9\n",
    "\n",
    "  def reset_tol(self):\n",
    "    self.atol = 1e-7\n",
    "    self.rtol = 1e-9\n",
    "\n",
    "  def set_time(self, time):\n",
    "    self.t = torch.tensor([0, time]).to(self.device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    t = self.t.type_as(x)\n",
    "    integrator = self.train_integrator if self.training else self.test_integrator\n",
    "    func = self.odefunc\n",
    "    state = x\n",
    "    state_dt = integrator(\n",
    "      func, state, t,\n",
    "      method='euler',\n",
    "      options=dict(step_size=self.opt['step_size']),\n",
    "      atol=self.atol,\n",
    "      rtol=self.rtol)\n",
    "    z = state_dt[1]\n",
    "    return z\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__class__.__name__ + '( Time Interval ' + str(self.t[0].item()) + ' -> ' + str(self.t[1].item()) \\\n",
    "           + \")\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class GNN(MessagePassing):\n",
    "  def __init__(self, opt, dataset, device=torch.device('cpu')):\n",
    "    super(MessagePassing, self).__init__()\n",
    "    self.opt = opt\n",
    "    self.T = opt['time']\n",
    "    self.num_classes = dataset.num_classes\n",
    "    self.num_features = dataset.data.num_features\n",
    "    self.num_nodes = dataset.data.num_nodes\n",
    "    self.device = device\n",
    "\n",
    "    self.m1 = nn.Linear(self.num_features, opt['hidden_dim'])\n",
    "    self.hidden_dim = opt['hidden_dim']\n",
    "    self.m2 = nn.Linear(opt['hidden_dim'], dataset.num_classes)\n",
    "    self.f = LaplacianODEFunc\n",
    "    block = ConstantODEblock\n",
    "    time_tensor = torch.tensor([0, self.T]).to(device)\n",
    "    self.odeblock = block(self.f, opt, dataset.data, device, t=time_tensor).to(device)\n",
    "\n",
    "  def getNFE(self):\n",
    "    return self.odeblock.odefunc.nfe\n",
    "\n",
    "  def resetNFE(self):\n",
    "    self.odeblock.odefunc.nfe = 0\n",
    "\n",
    "  def reset(self):\n",
    "    self.m1.reset_parameters()\n",
    "    self.m2.reset_parameters()\n",
    "\n",
    "  def forward(self, x, pos_encoding=None):\n",
    "    x = F.dropout(x, self.opt['input_dropout'], training=self.training)\n",
    "    x = self.m1(x)\n",
    "\n",
    "    self.odeblock.set_x0(x)\n",
    "    z = self.odeblock(x)\n",
    "\n",
    "    # Activation.\n",
    "    z = F.relu(z)\n",
    "\n",
    "    # Dropout.\n",
    "    z = F.dropout(z, self.opt['dropout'], training=self.training)\n",
    "\n",
    "    # Decode each node embedding to get node label.\n",
    "    z = self.m2(z)\n",
    "    return z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def print_model_params(model):\n",
    "  print(model)\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "      print(name)\n",
    "      print(param.data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "  rnd_state = np.random.RandomState(seed)\n",
    "  num_nodes = data.y.shape[0]\n",
    "  development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "  test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "  train_idx = []\n",
    "  rnd_state = np.random.RandomState(seed)\n",
    "  for c in range(data.y.max() + 1):\n",
    "    class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "    train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "  val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "  def get_mask(idx):\n",
    "    mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    mask[idx] = 1\n",
    "    return mask\n",
    "\n",
    "  data.train_mask = get_mask(train_idx)\n",
    "  data.val_mask = get_mask(val_idx)\n",
    "  data.test_mask = get_mask(test_idx)\n",
    "\n",
    "  return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, opt=None):  # opt required for runtime polymorphism\n",
    "  model.eval()\n",
    "  logits, accs = model(data.x), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val accuracy 0.802000 with test accuracy 0.819000 at epoch 58 and best time 18.294754\n"
     ]
    }
   ],
   "source": [
    "model = GNN(opt, dataset, device).to(device)\n",
    "data = dataset.data.to(device)\n",
    "\n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "# print_model_params(model)\n",
    "optimizer = torch.optim.Adam(parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "best_time = best_epoch = train_acc = val_acc = test_acc = 0\n",
    "\n",
    "this_test = test\n",
    "\n",
    "for epoch in range(1, opt['epoch']):\n",
    "  start_time = time.time()\n",
    "\n",
    "  loss = train(model, optimizer, data)\n",
    "  tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, opt)\n",
    "\n",
    "  best_time = opt['time']\n",
    "  if tmp_val_acc > val_acc:\n",
    "    best_epoch = epoch\n",
    "    train_acc = tmp_train_acc\n",
    "    val_acc = tmp_val_acc\n",
    "    test_acc = tmp_test_acc\n",
    "    best_time = opt['time']\n",
    "\n",
    "  log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Best time: {:.4f}'\n",
    "\n",
    "  # print(log.format(epoch, time.time() - start_time, loss, train_acc, val_acc, test_acc, best_time))\n",
    "print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d} and best time {:03f}'.format(val_acc, test_acc,\n",
    "                                                                                                   best_epoch,\n",
    "                                                                                                   best_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Train the given model on the given graph for num_epochs\n",
    "def train_standard_GCN(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y = data.y.squeeze()\n",
    "\n",
    "    # Set up the loss and the optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    out = model(data)\n",
    "\n",
    "    loss = loss_fn(out[data.train_mask], y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_standard_GCN(model, data, opt=None):  # opt required for runtime polymorphism\n",
    "  model.eval()\n",
    "  logits, accs = model(data), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val accuracy 0.810000 with test accuracy 0.823000 at epoch 41 and best time 18.294754\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, opt['hidden_dim'])\n",
    "        self.conv2 = GCNConv(opt['hidden_dim'], dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "      x, edge_index = data.x, data.edge_index\n",
    "\n",
    "      x = self.conv1(x, edge_index)\n",
    "      x = F.relu(x)\n",
    "      x = F.dropout(x, training=self.training)\n",
    "      x = self.conv2(x, edge_index)\n",
    "\n",
    "      return F.log_softmax(x, dim=1)\n",
    "# if not opt['planetoid_split'] and opt['dataset'] in ['Cora','Citeseer','Pubmed']:\n",
    "#   dataset.data = set_train_val_test_split(np.random.randint(0, 1000), dataset.data, num_development=5000 if opt[\"dataset\"] == \"CoauthorCS\" else 1500)\n",
    "model = Net().to(device)\n",
    "\n",
    "data = dataset.data.to(device)\n",
    "\n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "# print_model_params(model)\n",
    "optimizer = torch.optim.Adam(parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "best_time = best_epoch = train_acc = val_acc = test_acc = 0\n",
    "\n",
    "this_test = test_standard_GCN\n",
    "\n",
    "for epoch in range(1, opt['epoch']):\n",
    "  start_time = time.time()\n",
    "\n",
    "  loss = train_standard_GCN(model, optimizer, data)\n",
    "\n",
    "  tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, opt)\n",
    "\n",
    "  best_time = opt['time']\n",
    "  if tmp_val_acc > val_acc:\n",
    "    best_epoch = epoch\n",
    "    train_acc = tmp_train_acc\n",
    "    val_acc = tmp_val_acc\n",
    "    test_acc = tmp_test_acc\n",
    "    best_time = opt['time']\n",
    "\n",
    "  log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Best time: {:.4f}'\n",
    "\n",
    "  # print(log.format(epoch, time.time() - start_time, loss, train_acc, val_acc, test_acc, best_time))\n",
    "print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d} and best time {:03f}'.format(val_acc, test_acc,\n",
    "                                                                                                   best_epoch,\n",
    "                                                                                                   best_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}