{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torchdiffeq import odeint\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_scatter import scatter_add\n",
    "# from torchdiffeq import odeint_adjoint as odeint # Might be more stable according to docs of torchdiffeq\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from pathlib import Path\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'self_loop_weight': 1.0, 'use_labels': False,\n",
    "    'geom_gcn_splits': False, 'num_splits': 2, 'label_rate': 0.5, 'planetoid_split': False, 'hidden_dim': 80,\n",
    "    'fc_out': False, 'input_dropout': 0.5, 'dropout': 0.046878964627763316, 'batch_norm': False, 'optimizer': 'adamax',\n",
    "    'lr': 0.022924849756740397, 'decay': 0.00507685443154266, 'epoch': 100, 'alpha': 1.0, 'alpha_dim': 'sc',\n",
    "    'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'use_mlp': False,\n",
    "    'add_source': True, 'cgnn': False, 'time': 18.294754260552843, 'augment': False, 'method': 'euler', 'step_size': 1,\n",
    "    'max_iters': 100, 'adjoint_method': 'adaptive_heun', 'adjoint': False, 'adjoint_step_size': 1,\n",
    "    'tol_scale': 821.9773048827274, 'tol_scale_adjoint': 1.0, 'ode_blocks': 1, 'max_nfe': 2000, 'no_early': True,\n",
    "    'earlystopxT': 3, 'max_test_steps': 100, 'leaky_relu_slope': 0.2, 'attention_dropout': 0.0, 'heads': 8,\n",
    "    'attention_norm_idx': 1, 'attention_dim': 128, 'mix_features': False, 'reweight_attention': False,\n",
    "    'attention_type': 'scaled_dot', 'square_plus': True, 'jacobian_norm2': None, 'total_deriv': None,\n",
    "    'kinetic_energy': None, 'directional_penalty': None, 'not_lcc': True, 'rewiring': None, 'gdc_method': 'ppr',\n",
    "    'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.01, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05,\n",
    "    'heat_time': 3.0, 'att_samp_pct': 1, 'use_flux': False, 'exact': True, 'M_nodes': 64, 'new_edges': 'random',\n",
    "    'sparsify': 'S_hat', 'threshold_type': 'addD_rvR', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False,\n",
    "    'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 10, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False,\n",
    "    'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'GDC',\n",
    "    'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 16, 'edge_sampling': False,\n",
    "    'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64,\n",
    "    'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False,\n",
    "    'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention',\n",
    "    'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False,\n",
    "    'pos_dist_quantile': 0.001, 'adaptive': False, 'attention_rewiring': False, 'baseline': False, 'cpus': 1,\n",
    "    'dt': 0.001, 'dt_min': 1e-05, 'gpus': 0.5, 'grace_period': 20, 'max_epochs': 1000, 'metric': 'accuracy',\n",
    "    'name': 'cora_beltrami_splits', 'num_init': 1, 'num_samples': 1000, 'patience': 100, 'reduction_factor': 10,\n",
    "    'regularise': False, 'use_lcc': True\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [
    "# Set torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Number of validation nodes: 500\n",
      "Number of test nodes: 1000\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = Path('data')\n",
    "dataset_dir = dataset_dir.absolute()\n",
    "if not dataset_dir.exists():\n",
    "    dataset_dir.mkdir(parents=True)\n",
    "\n",
    "dataset = Planetoid(dataset_dir, 'Cora')\n",
    "\n",
    "# Some info\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset.data\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
    "print(f'Number of test nodes: {data.test_mask.sum()}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [],
   "source": [
    "# # Plot the graph\n",
    "# G = nx.Graph()\n",
    "#\n",
    "# G.add_nodes_from(list(range(data.num_nodes)))\n",
    "# G.add_edges_from([tuple(x) for x in data.edge_index.T.tolist()])\n",
    "# nx.draw(G)\n",
    "# plt.gca().set_facecolor('white')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [],
   "source": [
    "# Train the given model on the given graph for num_epochs\n",
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = data.x\n",
    "    y = data.y.squeeze()\n",
    "\n",
    "    # Set up the loss and the optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    out = model(x)\n",
    "\n",
    "    loss = loss_fn(out[data.train_mask], y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [],
   "source": [
    "class LaplacianODEFunc(MessagePassing):\n",
    "\n",
    "  # currently requires in_features = out_features\n",
    "  def __init__(self, in_features, out_features, opt, data, device):\n",
    "    super(MessagePassing, self).__init__()\n",
    "    self.opt = opt\n",
    "    self.device = device\n",
    "    self.edge_index = None\n",
    "    self.edge_weight = None\n",
    "    self.attention_weights = None\n",
    "    self.alpha_train = nn.Parameter(torch.tensor(0.0))\n",
    "    self.beta_train = nn.Parameter(torch.tensor(0.0))\n",
    "    self.x0 = None\n",
    "    self.nfe = 0\n",
    "    self.alpha_sc = nn.Parameter(torch.ones(1))\n",
    "    self.beta_sc = nn.Parameter(torch.ones(1))\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "    self.w = nn.Parameter(torch.eye(opt['hidden_dim']))\n",
    "    self.d = nn.Parameter(torch.zeros(opt['hidden_dim']) + 1)\n",
    "    self.alpha_sc = nn.Parameter(torch.ones(1))\n",
    "    self.beta_sc = nn.Parameter(torch.ones(1))\n",
    "\n",
    "  def sparse_multiply(self, x):\n",
    "    if self.opt['block'] in ['attention']:  # adj is a multihead attention\n",
    "      mean_attention = self.attention_weights.mean(dim=1)\n",
    "      ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)\n",
    "    elif self.opt['block'] in ['mixed', 'hard_attention']:  # adj is a torch sparse matrix\n",
    "      ax = torch_sparse.spmm(self.edge_index, self.attention_weights, x.shape[0], x.shape[0], x)\n",
    "    else:  # adj is a torch sparse matrix\n",
    "      ax = torch_sparse.spmm(self.edge_index, self.edge_weight, x.shape[0], x.shape[0], x)\n",
    "    return ax\n",
    "\n",
    "  def forward(self, t, x):  # the t param is needed by the ODE solver.\n",
    "    self.nfe += 1\n",
    "    ax = self.sparse_multiply(x)\n",
    "    if not self.opt['no_alpha_sigmoid']:\n",
    "      alpha = torch.sigmoid(self.alpha_train)\n",
    "    else:\n",
    "      alpha = self.alpha_train\n",
    "\n",
    "    f = alpha * (ax - x)\n",
    "    if self.opt['add_source']:\n",
    "      f = f + self.beta_train * self.x0\n",
    "    return f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "def get_rw_adj(edge_index, edge_weight=None, norm_dim=1, fill_value=0., num_nodes=None, dtype=None):\n",
    "  num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "  if edge_weight is None:\n",
    "    edge_weight = torch.ones((edge_index.size(1),), dtype=dtype,\n",
    "                             device=edge_index.device)\n",
    "\n",
    "  if not fill_value == 0:\n",
    "    edge_index, tmp_edge_weight = add_remaining_self_loops(\n",
    "      edge_index, edge_weight, fill_value, num_nodes)\n",
    "    assert tmp_edge_weight is not None\n",
    "    edge_weight = tmp_edge_weight\n",
    "\n",
    "  row, col = edge_index[0], edge_index[1]\n",
    "  indices = row if norm_dim == 0 else col\n",
    "  deg = scatter_add(edge_weight, indices, dim=0, dim_size=num_nodes)\n",
    "  deg_inv_sqrt = deg.pow_(-1)\n",
    "  edge_weight = deg_inv_sqrt[indices] * edge_weight if norm_dim == 0 else edge_weight * deg_inv_sqrt[indices]\n",
    "  return edge_index, edge_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "class ConstantODEblock(nn.Module):\n",
    "  def __init__(self, odefunc, opt, data, device, t=torch.tensor([0, 1])):\n",
    "    super(ConstantODEblock, self).__init__()\n",
    "    self.opt = opt\n",
    "    self.t = t\n",
    "\n",
    "    self.aug_dim = 2 if opt['augment'] else 1\n",
    "    self.odefunc = odefunc(self.aug_dim * opt['hidden_dim'], self.aug_dim * opt['hidden_dim'], opt, data, device)\n",
    "\n",
    "\n",
    "    if opt['adjoint']:\n",
    "      from torchdiffeq import odeint_adjoint as odeint\n",
    "    else:\n",
    "      from torchdiffeq import odeint\n",
    "    self.train_integrator = odeint\n",
    "    self.test_integrator = None\n",
    "    self.set_tol()\n",
    "\n",
    "    self.aug_dim = 2 if opt['augment'] else 1\n",
    "    self.odefunc = odefunc(self.aug_dim * opt['hidden_dim'], self.aug_dim * opt['hidden_dim'], opt, data, device)\n",
    "    edge_index, edge_weight = get_rw_adj(data.edge_index, edge_weight=data.edge_attr, norm_dim=1,\n",
    "                                                                   fill_value=opt['self_loop_weight'],\n",
    "                                                                   num_nodes=data.num_nodes,\n",
    "                                                                   dtype=data.x.dtype)\n",
    "    self.odefunc.edge_index = edge_index.to(device)\n",
    "    self.odefunc.edge_weight = edge_weight.to(device)\n",
    "\n",
    "    if opt['adjoint']:\n",
    "      from torchdiffeq import odeint_adjoint as odeint\n",
    "    else:\n",
    "      from torchdiffeq import odeint\n",
    "\n",
    "    self.train_integrator = odeint\n",
    "    self.test_integrator = odeint\n",
    "    self.set_tol()\n",
    "\n",
    "  def set_x0(self, x0):\n",
    "    self.odefunc.x0 = x0.clone().detach()\n",
    "\n",
    "  def set_tol(self):\n",
    "    self.atol = self.opt['tol_scale'] * 1e-7\n",
    "    self.rtol = self.opt['tol_scale'] * 1e-9\n",
    "    if self.opt['adjoint']:\n",
    "      self.atol_adjoint = self.opt['tol_scale_adjoint'] * 1e-7\n",
    "      self.rtol_adjoint = self.opt['tol_scale_adjoint'] * 1e-9\n",
    "\n",
    "  def reset_tol(self):\n",
    "    self.atol = 1e-7\n",
    "    self.rtol = 1e-9\n",
    "    self.atol_adjoint = 1e-7\n",
    "    self.rtol_adjoint = 1e-9\n",
    "\n",
    "  def set_time(self, time):\n",
    "    self.t = torch.tensor([0, time]).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    t = self.t.type_as(x)\n",
    "\n",
    "    integrator = self.train_integrator if self.training else self.test_integrator\n",
    "\n",
    "\n",
    "    func = self.odefunc\n",
    "    state = x\n",
    "\n",
    "    if self.opt[\"adjoint\"] and self.training:\n",
    "      state_dt = integrator(\n",
    "        func, state, t,\n",
    "        method=self.opt['method'],\n",
    "        options=dict(step_size=self.opt['step_size'], max_iters=self.opt['max_iters']),\n",
    "        adjoint_method=self.opt['adjoint_method'],\n",
    "        adjoint_options=dict(step_size = self.opt['adjoint_step_size'], max_iters=self.opt['max_iters']),\n",
    "        atol=self.atol,\n",
    "        rtol=self.rtol,\n",
    "        adjoint_atol=self.atol_adjoint,\n",
    "        adjoint_rtol=self.rtol_adjoint)\n",
    "    else:\n",
    "      state_dt = integrator(\n",
    "        func, state, t,\n",
    "        method=self.opt['method'],\n",
    "        options=dict(step_size=self.opt['step_size'], max_iters=self.opt['max_iters']),\n",
    "        atol=self.atol,\n",
    "        rtol=self.rtol)\n",
    "\n",
    "\n",
    "    z = state_dt[1]\n",
    "    return z\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__class__.__name__ + '( Time Interval ' + str(self.t[0].item()) + ' -> ' + str(self.t[1].item()) \\\n",
    "           + \")\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "class GNN(MessagePassing):\n",
    "  def __init__(self, opt, dataset, device=torch.device('cpu')):\n",
    "    super(MessagePassing, self).__init__()\n",
    "    self.opt = opt\n",
    "    self.T = opt['time']\n",
    "    self.num_classes = dataset.num_classes\n",
    "    self.num_features = dataset.data.num_features\n",
    "    self.num_nodes = dataset.data.num_nodes\n",
    "    self.device = device\n",
    "\n",
    "    if opt['beltrami']:\n",
    "      self.mx = nn.Linear(self.num_features, opt['feat_hidden_dim'])\n",
    "      self.mp = nn.Linear(opt['pos_enc_dim'], opt['pos_enc_hidden_dim'])\n",
    "      opt['hidden_dim'] = opt['feat_hidden_dim'] + opt['pos_enc_hidden_dim']\n",
    "    else:\n",
    "      self.m1 = nn.Linear(self.num_features, opt['hidden_dim'])\n",
    "\n",
    "    if self.opt['use_mlp']:\n",
    "      self.m11 = nn.Linear(opt['hidden_dim'], opt['hidden_dim'])\n",
    "      self.m12 = nn.Linear(opt['hidden_dim'], opt['hidden_dim'])\n",
    "    if opt['use_labels']:\n",
    "      # todo - fastest way to propagate this everywhere, but error prone - refactor later\n",
    "      opt['hidden_dim'] = opt['hidden_dim'] + dataset.num_classes\n",
    "    else:\n",
    "      self.hidden_dim = opt['hidden_dim']\n",
    "    if opt['fc_out']:\n",
    "      self.fc = nn.Linear(opt['hidden_dim'], opt['hidden_dim'])\n",
    "    self.m2 = nn.Linear(opt['hidden_dim'], dataset.num_classes)\n",
    "    if self.opt['batch_norm']:\n",
    "      self.bn_in = torch.nn.BatchNorm1d(opt['hidden_dim'])\n",
    "      self.bn_out = torch.nn.BatchNorm1d(opt['hidden_dim'])\n",
    "\n",
    "    self.f = LaplacianODEFunc\n",
    "    block = ConstantODEblock\n",
    "    time_tensor = torch.tensor([0, self.T]).to(device)\n",
    "    self.odeblock = block(self.f, opt, dataset.data, device, t=time_tensor).to(device)\n",
    "\n",
    "  def getNFE(self):\n",
    "    return self.odeblock.odefunc.nfe + self.odeblock.reg_odefunc.odefunc.nfe\n",
    "\n",
    "  def resetNFE(self):\n",
    "    self.odeblock.odefunc.nfe = 0\n",
    "    self.odeblock.reg_odefunc.odefunc.nfe = 0\n",
    "\n",
    "  def reset(self):\n",
    "    self.m1.reset_parameters()\n",
    "    self.m2.reset_parameters()\n",
    "\n",
    "  def forward(self, x, pos_encoding=None):\n",
    "    # Encode each node based on its feature.\n",
    "    if self.opt['use_labels']:\n",
    "      y = x[:, -self.num_classes:]\n",
    "      x = x[:, :-self.num_classes]\n",
    "\n",
    "    if self.opt['beltrami']:\n",
    "      x = F.dropout(x, self.opt['input_dropout'], training=self.training)\n",
    "      x = self.mx(x)\n",
    "      p = F.dropout(pos_encoding, self.opt['input_dropout'], training=self.training)\n",
    "      p = self.mp(p)\n",
    "      x = torch.cat([x, p], dim=1)\n",
    "    else:\n",
    "      x = F.dropout(x, self.opt['input_dropout'], training=self.training)\n",
    "      x = self.m1(x)\n",
    "\n",
    "    if self.opt['use_mlp']:\n",
    "      x = F.dropout(x, self.opt['dropout'], training=self.training)\n",
    "      x = F.dropout(x + self.m11(F.relu(x)), self.opt['dropout'], training=self.training)\n",
    "      x = F.dropout(x + self.m12(F.relu(x)), self.opt['dropout'], training=self.training)\n",
    "    # todo investigate if some input non-linearity solves the problem with smooth deformations identified in the ANODE paper\n",
    "\n",
    "    if self.opt['use_labels']:\n",
    "      x = torch.cat([x, y], dim=-1)\n",
    "\n",
    "    if self.opt['batch_norm']:\n",
    "      x = self.bn_in(x)\n",
    "\n",
    "    # Solve the initial value problem of the ODE.\n",
    "    if self.opt['augment']:\n",
    "      c_aux = torch.zeros(x.shape).to(self.device)\n",
    "      x = torch.cat([x, c_aux], dim=1)\n",
    "\n",
    "    self.odeblock.set_x0(x)\n",
    "\n",
    "\n",
    "    z = self.odeblock(x)\n",
    "\n",
    "    if self.opt['augment']:\n",
    "      z = torch.split(z, x.shape[1] // 2, dim=1)[0]\n",
    "\n",
    "    # Activation.\n",
    "    z = F.relu(z)\n",
    "\n",
    "    if self.opt['fc_out']:\n",
    "      z = self.fc(z)\n",
    "      z = F.relu(z)\n",
    "\n",
    "    # Dropout.\n",
    "    z = F.dropout(z, self.opt['dropout'], training=self.training)\n",
    "\n",
    "    # Decode each node embedding to get node label.\n",
    "    z = self.m2(z)\n",
    "    return z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "def print_model_params(model):\n",
    "  print(model)\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "      print(name)\n",
    "      print(param.data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "  rnd_state = np.random.RandomState(seed)\n",
    "  num_nodes = data.y.shape[0]\n",
    "  development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "  test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "  train_idx = []\n",
    "  rnd_state = np.random.RandomState(seed)\n",
    "  for c in range(data.y.max() + 1):\n",
    "    class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "    train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "  val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "  def get_mask(idx):\n",
    "    mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    mask[idx] = 1\n",
    "    return mask\n",
    "\n",
    "  data.train_mask = get_mask(train_idx)\n",
    "  data.val_mask = get_mask(val_idx)\n",
    "  data.test_mask = get_mask(test_idx)\n",
    "\n",
    "  return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, opt=None):  # opt required for runtime polymorphism\n",
    "  model.eval()\n",
    "  logits, accs = model(data.x), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "# def get_optimizer(name, parameters, lr, weight_decay=0):\n",
    "#   if name == 'sgd':\n",
    "#     return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay)\n",
    "#   elif name == 'rmsprop':\n",
    "#     return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay)\n",
    "#   elif name == 'adagrad':\n",
    "#     return torch.optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)\n",
    "#   elif name == 'adam':\n",
    "#     return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "#   elif name == 'adamax':\n",
    "#     return torch.optim.Adamax(parameters, lr=lr, weight_decay=weight_decay)\n",
    "#   else:\n",
    "#     raise Exception(\"Unsupported optimizer: {}\".format(name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val accuracy 0.806000 with test accuracy 0.815000 at epoch 33 and best time 18.294754\n"
     ]
    }
   ],
   "source": [
    "# if opt['rewire_KNN'] or opt['fa_layer']:\n",
    "#   model = GNN_KNN(opt, dataset, device).to(device) if opt[\"no_early\"] else GNNKNNEarly(opt, dataset, device).to(device)\n",
    "# else:\n",
    "model = GNN(opt, dataset, device).to(device) # if opt[\"no_early\"] else GNNEarly(opt, dataset, device).to(device)\n",
    "\n",
    "# if not opt['planetoid_split'] and opt['dataset'] in ['Cora','Citeseer','Pubmed']:\n",
    "#   dataset.data = set_train_val_test_split(np.random.randint(0, 1000), dataset.data, num_development=5000 if opt[\"dataset\"] == \"CoauthorCS\" else 1500)\n",
    "\n",
    "data = dataset.data.to(device)\n",
    "\n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "# print_model_params(model)\n",
    "optimizer = torch.optim.Adam(parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "best_time = best_epoch = train_acc = val_acc = test_acc = 0\n",
    "\n",
    "this_test = test\n",
    "\n",
    "for epoch in range(1, opt['epoch']):\n",
    "  start_time = time.time()\n",
    "\n",
    "  loss = train(model, optimizer, data)\n",
    "  tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, opt)\n",
    "\n",
    "  best_time = opt['time']\n",
    "  if tmp_val_acc > val_acc:\n",
    "    best_epoch = epoch\n",
    "    train_acc = tmp_train_acc\n",
    "    val_acc = tmp_val_acc\n",
    "    test_acc = tmp_test_acc\n",
    "    best_time = opt['time']\n",
    "  # if not opt['no_early'] and model.odeblock.test_integrator.solver.best_val > val_acc:\n",
    "  #   best_epoch = epoch\n",
    "  #   val_acc = model.odeblock.test_integrator.solver.best_val\n",
    "  #   test_acc = model.odeblock.test_integrator.solver.best_test\n",
    "  #   train_acc = model.odeblock.test_integrator.solver.best_train\n",
    "  #   best_time = model.odeblock.test_integrator.solver.best_time\n",
    "\n",
    "  log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Best time: {:.4f}'\n",
    "\n",
    "  # print(log.format(epoch, time.time() - start_time, loss, train_acc, val_acc, test_acc, best_time))\n",
    "print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d} and best time {:03f}'.format(val_acc, test_acc,\n",
    "                                                                                                   best_epoch,\n",
    "                                                                                                   best_time))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "# Train the given model on the given graph for num_epochs\n",
    "def train_standard_GCN(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = data.x\n",
    "    y = data.y.squeeze()\n",
    "\n",
    "    # Set up the loss and the optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    out = model(data)\n",
    "\n",
    "    loss = loss_fn(out[data.train_mask], y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_standard_GCN(model, data, opt=None):  # opt required for runtime polymorphism\n",
    "  model.eval()\n",
    "  logits, accs = model(data), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val accuracy 0.812000 with test accuracy 0.823000 at epoch 6 and best time 18.294754\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, opt['hidden_dim'])\n",
    "        self.conv2 = GCNConv(opt['hidden_dim'], dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "      x, edge_index = data.x, data.edge_index\n",
    "\n",
    "      x = self.conv1(x, edge_index)\n",
    "      x = F.relu(x)\n",
    "      x = F.dropout(x, training=self.training)\n",
    "      x = self.conv2(x, edge_index)\n",
    "\n",
    "      return F.log_softmax(x, dim=1)\n",
    "# if not opt['planetoid_split'] and opt['dataset'] in ['Cora','Citeseer','Pubmed']:\n",
    "#   dataset.data = set_train_val_test_split(np.random.randint(0, 1000), dataset.data, num_development=5000 if opt[\"dataset\"] == \"CoauthorCS\" else 1500)\n",
    "model = Net().to(device)\n",
    "\n",
    "data = dataset.data.to(device)\n",
    "\n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "# print_model_params(model)\n",
    "optimizer = torch.optim.Adam(parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "best_time = best_epoch = train_acc = val_acc = test_acc = 0\n",
    "\n",
    "this_test = test_standard_GCN\n",
    "\n",
    "for epoch in range(1, opt['epoch']):\n",
    "  start_time = time.time()\n",
    "\n",
    "  loss = train_standard_GCN(model, optimizer, data)\n",
    "\n",
    "  tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, opt)\n",
    "\n",
    "  best_time = opt['time']\n",
    "  if tmp_val_acc > val_acc:\n",
    "    best_epoch = epoch\n",
    "    train_acc = tmp_train_acc\n",
    "    val_acc = tmp_val_acc\n",
    "    test_acc = tmp_test_acc\n",
    "    best_time = opt['time']\n",
    "  # if not opt['no_early'] and model.odeblock.test_integrator.solver.best_val > val_acc:\n",
    "  #   best_epoch = epoch\n",
    "  #   val_acc = model.odeblock.test_integrator.solver.best_val\n",
    "  #   test_acc = model.odeblock.test_integrator.solver.best_test\n",
    "  #   train_acc = model.odeblock.test_integrator.solver.best_train\n",
    "  #   best_time = model.odeblock.test_integrator.solver.best_time\n",
    "\n",
    "  log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Best time: {:.4f}'\n",
    "\n",
    "  # print(log.format(epoch, time.time() - start_time, loss, train_acc, val_acc, test_acc, best_time))\n",
    "print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d} and best time {:03f}'.format(val_acc, test_acc,\n",
    "                                                                                                   best_epoch,\n",
    "                                                                                                   best_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}