{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "faed1d68-9918-4d47-9a0f-6b7c1394515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor, GNNBenchmarkDataset, Reddit2, Flickr\n",
    "from GNN import GNN\n",
    "from GNN_KNN import GNN_KNN\n",
    "import time\n",
    "from data import get_dataset, Data\n",
    "\n",
    "from graph_rewiring import get_two_hop, apply_gdc\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import to_undirected\n",
    "from graph_rewiring import make_symmetric, apply_pos_dist_rewire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09c1e662-6fcf-4c95-97e4-41f5ce755be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customArgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb7ee566-4b2c-462c-8437-0ec9d73eb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cora_opt(opt):\n",
    "  opt['dataset'] = 'Cora'\n",
    "  opt['data'] = 'Planetoid'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'rmsprop'\n",
    "  opt['lr'] = 0.0047\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 31\n",
    "  opt['augment'] = True\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['adjoint'] = False\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_computers_opt(opt):\n",
    "  opt['dataset'] = 'Computers'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.01\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['epoch'] = 400\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 100\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_flickr_opt(opt):\n",
    "  opt['dataset'] = 'Flickr'\n",
    "  opt['hidden_dim'] = 128\n",
    "  opt['feature_hidden_dim'] = 64\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.005451476553977102\n",
    "  opt['decay'] = 0\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 1.0\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 500\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 89250\n",
    "  opt['epoch'] = 100\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  opt['gdc_avg_degree']= 48\n",
    "  opt['gdc_k'] = 48\n",
    "  opt['gdc_method'] = 'ppr'\n",
    "  opt['gdc_sparsification'] = 'topk'\n",
    "  opt['gdc_threshold'] =  0.01\n",
    "  return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "442b7c4e-3ad2-455a-907f-38f4f25067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name, parameters, lr, weight_decay=0):\n",
    "  if name == 'sgd':\n",
    "    return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'rmsprop':\n",
    "    return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adagrad':\n",
    "    return torch.optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adam':\n",
    "    return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adamax':\n",
    "    return torch.optim.Adamax(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  else:\n",
    "    raise Exception(\"Unsupported optimizer: {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "768697b4-1f65-4c10-a8d8-3744ded132dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x)\n",
    "  lf = torch.nn.CrossEntropyLoss()\n",
    "  loss = lf(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "  # TODO: What is this block about???\n",
    "  if model.odeblock.nreg > 0:  # add regularisation - slower for small data, but faster and better performance for large data\n",
    "    reg_states = tuple(torch.mean(rs) for rs in model.reg_states)\n",
    "    regularization_coeffs = model.regularization_coeffs\n",
    "\n",
    "    reg_loss = sum(\n",
    "      reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
    "    )\n",
    "    loss = loss + reg_loss\n",
    "\n",
    "  # Update count of forward evaluations from ODE solver\n",
    "  # NOTE: fm stands for \"forward meter\"\n",
    "  # TODO: Rename this to be more informative!\n",
    "  model.fm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  # Gradient step\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Update count of backwards evaluations from ODE solver\n",
    "  model.bm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "  model.eval()\n",
    "  logits, accs = model(data.x), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs\n",
    "\n",
    "def print_model_params(model):\n",
    "  print(model)\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "      print(name)\n",
    "      print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05dd71b0-cad8-45a6-ba14-e62cc96a8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_benchmark(opt: dict, data_dir, use_lcc: bool = False) -> InMemoryDataset:\n",
    "    ds = opt['dataset']\n",
    "    dataset = Flickr(root=os.path.join(data_dir,'Flickr'), transform=T.ToSparseTensor())#T.NormalizeFeatures())  \n",
    "\n",
    "    \"\"\"\n",
    "    d_train = GNNBenchmarkDataset(name=ds, root=path, split='train', transform=T.NormalizeFeatures())\n",
    "    d_val = GNNBenchmarkDataset(name=ds, root=path, split='val', transform=T.NormalizeFeatures())\n",
    "    d_test = GNNBenchmarkDataset(name=ds, root=path, split='test', transform=T.NormalizeFeatures())\n",
    "    \n",
    "    print(d_train.data)\n",
    "    print(d_val.data)\n",
    "    print(d_test.data)\n",
    "    print(torch.max(d_train.data.edge_index))\n",
    "    print(torch.max(d_val.data.edge_index))\n",
    "    print(torch.max(d_test.data.edge_index))\n",
    "    \n",
    "    d_temp = Planetoid(root=os.path.join(data_dir,'Cora'), name='Cora')\n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    d_temp = PygNodePropPredDataset(name='ogbn-arxiv', root=os.path.join(data_dir,'ogbn-arxiv'), transform=T.ToSparseTensor())\n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    d_temp = Flickr(root=os.path.join(data_dir,'Flickr'), transform=T.NormalizeFeatures())  \n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    #d_temp = Reddit2(root=os.path.join(data_dir,'Reddit2'), transform=T.NormalizeFeatures())\n",
    "    #print(d_temp.data)\n",
    "    #print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    return\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "\n",
    "        data = Data(\n",
    "          x=x_new,\n",
    "          edge_index=torch.LongTensor(edges),\n",
    "          y=y_new,\n",
    "          train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "          test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "          val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "    if opt['rewiring'] is not None:\n",
    "        dataset.data = rewire(dataset.data, opt, data_dir)\n",
    "    train_mask_exists = True\n",
    "    try:\n",
    "        dataset.data.train_mask\n",
    "    except AttributeError:\n",
    "        train_mask_exists = False\n",
    "\n",
    "    if ds == 'ogbn-arxiv':\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        ei = to_undirected(dataset.data.edge_index)\n",
    "        data = Data(\n",
    "        x=dataset.data.x,\n",
    "        edge_index=ei,\n",
    "        y=dataset.data.y,\n",
    "        train_mask=split_idx['train'],\n",
    "        test_mask=split_idx['test'],\n",
    "        val_mask=split_idx['valid'])\n",
    "        dataset.data = data\n",
    "        train_mask_exists = True\n",
    "\n",
    "    #todo this currently breaks with heterophilic datasets if you don't pass --geom_gcn_splits\n",
    "    if (use_lcc or not train_mask_exists) and not opt['geom_gcn_splits']:\n",
    "        dataset.data = set_train_val_test_split(\n",
    "          12345,\n",
    "          dataset.data,\n",
    "          num_development=5000 if ds == \"CoauthorCS\" else 1500)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "433d8568-8f88-4feb-8e97-c466a24e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def run(opt, run_count):\n",
    "\n",
    "    # Load dataset and create model\n",
    "    if opt['dataset'] == 'Flickr':\n",
    "        dataset = get_dataset_benchmark(opt, '../data', False)\n",
    "    else:\n",
    "        dataset = get_dataset(opt, '../data', False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if opt['rewire_KNN'] or opt['fa_layer']:\n",
    "        model = GNN_KNN(opt, dataset, device).to(device)\n",
    "    else:\n",
    "        model = GNN(opt, dataset, device).to(device)\n",
    "    data = dataset.data.to(device)\n",
    "    #model, data = GNN(opt, dataset, device).to(device), dataset.data.to(device)\n",
    "    print(opt)\n",
    "\n",
    "    # Todo for some reason the submodule parameters inside the attention module don't show up when running on GPU.\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    print_model_params(model)\n",
    "\n",
    "    # Training/test loop\n",
    "    results = {\n",
    "        'time':[],\n",
    "        'loss':[],\n",
    "        'forward_nfe':[],\n",
    "        'backward_nfe':[],\n",
    "        'train_acc':[],\n",
    "        'test_acc':[],\n",
    "        'val_acc':[],\n",
    "        'layers':0,\n",
    "        'best_epoch':0,\n",
    "        'best_val_acc':0.,\n",
    "        'best_test_acc':0.,\n",
    "    }\n",
    "    runtimes = []\n",
    "    losses = []\n",
    "\n",
    "    optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "    best_val_acc = test_acc = train_acc = best_epoch = 0\n",
    "    overall_time = time.time()\n",
    "    for epoch in range(1, opt['epoch']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss = train(model, optimizer, data)\n",
    "        train_acc, val_acc, test_acc = test(model, data)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_train_acc = train_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        results['time'].append(time.time() - start_time)\n",
    "        results['loss'].append(loss)\n",
    "        results['forward_nfe'].append(model.fm.sum)\n",
    "        results['backward_nfe'].append(model.bm.sum)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['val_acc'].append(val_acc)\n",
    "        results['best_epoch'] = best_epoch\n",
    "        results['best_train_acc'] = best_train_acc\n",
    "        results['best_val_acc'] = best_val_acc\n",
    "        results['best_test_acc'] = best_test_acc\n",
    "\n",
    "        log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        print(log.format(epoch, results['time'][-1], results['loss'][-1], results['forward_nfe'][-1], results['backward_nfe'][-1], results['train_acc'][-1], results['val_acc'][-1], results['test_acc'][-1]))\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d}'.format(best_val_acc, best_test_acc, best_epoch))\n",
    "    \n",
    "    results['all_epochs_time'] = time.time() - overall_time\n",
    "    results['layers'] = opt['time']\n",
    "\n",
    "    # TODO: Save results\n",
    "    # cora_epoch_101_adjoint_false_... . pickle\n",
    "    pickle.dump( results, open( f\"../results/Time{opt['dataset']}_{opt['method']}_stepsize_{opt['step_size']}_layers_{opt['time']}_run_{run_count}.pickle\", \"wb\" ) )\n",
    "\n",
    "    return train_acc, best_val_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0f3b2294-ed94-4c12-b2eb-1115d36300a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Doing time 64 ***\n",
      "*** Doing stepsize 1.0 ***\n",
      "*** Doing run 0 ***\n",
      "tensor(200)\n",
      "tensor(1300)\n",
      "tensor(12252)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dslksdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# NOTE: I think setting dt_min may not be necessary, doing it just to be safe!\u001b[39;00m\n\u001b[1;32m    183\u001b[0m opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt_min\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stepsize\n\u001b[0;32m--> 184\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(opt, run_count)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39msum(dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mval_mask))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39msum(dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest_mask))\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdslksdf\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewire_KNN\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfa_layer\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     model \u001b[38;5;241m=\u001b[39m GNN_KNN(opt, dataset, device)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dslksdf' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_cora_defaults', action='store_true',\n",
    "                  help='Whether to run with best params for cora. Overrides the choice of dataset')\n",
    "parser.add_argument('--dataset', type=str, default='Cora',\n",
    "                  help='Cora, Citeseer, Pubmed, Computers, Photo, CoauthorCS')\n",
    "parser.add_argument('--data_norm', type=str, default='rw',\n",
    "                  help='rw for random walk, gcn for symmetric gcn norm')\n",
    "parser.add_argument('--hidden_dim', type=int, default=16, help='Hidden dimension.')\n",
    "parser.add_argument('--input_dropout', type=float, default=0.5, help='Input dropout rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='One from sgd, rmsprop, adam, adagrad, adamax.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Learning rate.')\n",
    "parser.add_argument('--decay', type=float, default=5e-4, help='Weight decay for optimization')\n",
    "parser.add_argument('--self_loop_weight', type=float, default=1.0, help='Weight of self-loops.')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='Number of training epochs per iteration.')\n",
    "parser.add_argument('--alpha', type=float, default=1.0, help='Factor in front matrix A.')\n",
    "parser.add_argument('--time', type=float, default=1.0, help='End time of ODE integrator.')\n",
    "parser.add_argument('--augment', action='store_true',\n",
    "                  help='double the length of the feature vector by appending zeros to stabilist ODE learning')\n",
    "parser.add_argument('--alpha_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) alpha')\n",
    "parser.add_argument('--no_alpha_sigmoid', dest='no_alpha_sigmoid', action='store_true', help='apply sigmoid before multiplying by alpha')\n",
    "parser.add_argument('--beta_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) beta')\n",
    "parser.add_argument('--block', type=str, default='constant', help='constant, mixed, attention, SDE')\n",
    "parser.add_argument('--function', type=str, default='laplacian', help='laplacian, transformer, dorsey, GAT, SDE')\n",
    "parser.add_argument('--geom_gcn_splits', dest='geom_gcn_splits', action='store_true',\n",
    "                      help='use the 10 fixed splits from '\n",
    "                           'https://arxiv.org/abs/2002.05287')\n",
    "# ODE args\n",
    "parser.add_argument('--method', type=str, default='dopri5',\n",
    "                  help=\"set the numerical solver: dopri5, euler, rk4, midpoint\")\n",
    "parser.add_argument('--step_size', type=float, default=1, help='fixed step size when using fixed step solvers e.g. rk4')\n",
    "parser.add_argument(\n",
    "    \"--adjoint_method\", type=str, default=\"adaptive_heun\",\n",
    "    help=\"set the numerical solver for the backward pass: dopri5, euler, rk4, midpoint\"\n",
    ")\n",
    "parser.add_argument('--adjoint_step_size', type=float, default=1, help='fixed step size when using fixed step adjoint solvers e.g. rk4')\n",
    "parser.add_argument('--adjoint', default=False, help='use the adjoint ODE method to reduce memory footprint')\n",
    "parser.add_argument('--tol_scale', type=float, default=1., help='multiplier for atol and rtol')\n",
    "parser.add_argument(\"--tol_scale_adjoint\", type=float, default=1.0,\n",
    "                  help=\"multiplier for adjoint_atol and adjoint_rtol\")\n",
    "parser.add_argument('--ode_blocks', type=int, default=1, help='number of ode blocks to run')\n",
    "parser.add_argument('--add_source', dest='add_source', action='store_true',\n",
    "                  help='If try get rid of alpha param and the beta*x0 source term')\n",
    "# SDE args\n",
    "parser.add_argument('--dt_min', type=float, default=1e-5, help='minimum timestep for the SDE solver')\n",
    "parser.add_argument('--dt', type=float, default=1e-3, help='fixed step size')\n",
    "parser.add_argument('--adaptive', dest='adaptive', action='store_true', help='use adaptive step sizes')\n",
    "# Attention args\n",
    "parser.add_argument('--leaky_relu_slope', type=float, default=0.2,\n",
    "                  help='slope of the negative part of the leaky relu used in attention')\n",
    "parser.add_argument('--attention_dropout', type=float, default=0., help='dropout of attention weights')\n",
    "parser.add_argument('--heads', type=int, default=4, help='number of attention heads')\n",
    "parser.add_argument('--attention_norm_idx', type=int, default=0, help='0 = normalise rows, 1 = normalise cols')\n",
    "parser.add_argument('--attention_dim', type=int, default=64,\n",
    "                  help='the size to project x to before calculating att scores')\n",
    "parser.add_argument('--mix_features', dest='mix_features', action='store_true',\n",
    "                  help='apply a feature transformation xW to the ODE')\n",
    "parser.add_argument(\"--max_nfe\", type=int, default=1000, help=\"Maximum number of function evaluations allowed.\")\n",
    "parser.add_argument('--reweight_attention', dest='reweight_attention', action='store_true', help=\"multiply attention scores by edge weights before softmax\")\n",
    "# regularisation args\n",
    "parser.add_argument('--jacobian_norm2', type=float, default=None, help=\"int_t ||df/dx||_F^2\")\n",
    "parser.add_argument('--total_deriv', type=float, default=None, help=\"int_t ||df/dt||^2\")\n",
    "\n",
    "parser.add_argument('--kinetic_energy', type=float, default=None, help=\"int_t ||f||_2^2\")\n",
    "parser.add_argument('--directional_penalty', type=float, default=None, help=\"int_t ||(df/dx)^T f||^2\")\n",
    "\n",
    "# rewiring args\n",
    "parser.add_argument('--rewiring', type=str, default=None, help=\"two_hop, gdc\")\n",
    "parser.add_argument('--gdc_method', type=str, default='ppr', help=\"ppr, heat, coeff\")\n",
    "parser.add_argument('--gdc_sparsification', type=str, default='topk', help=\"threshold, topk\")\n",
    "parser.add_argument('--gdc_k', type=int, default=64, help=\"number of neighbours to sparsify to when using topk\")\n",
    "parser.add_argument('--gdc_threshold', type=float, default=0.0001, help=\"obove this edge weight, keep edges when using threshold\")\n",
    "parser.add_argument('--gdc_avg_degree', type=int, default=64,\n",
    "                  help=\"if gdc_threshold is not given can be calculated by specifying avg degree\")\n",
    "parser.add_argument('--ppr_alpha', type=float, default=0.05, help=\"teleport probability\")\n",
    "parser.add_argument('--heat_time', type=float, default=3., help=\"time to run gdc heat kernal diffusion for\")\n",
    "\n",
    "parser.add_argument(\"--not_lcc\", action=\"store_false\", help=\"don't use the largest connected component\")\n",
    "parser.add_argument('--att_samp_pct', type=float, default=1,\n",
    "                  help=\"float in [0,1). The percentage of edges to retain based on attention scores\")\n",
    "parser.add_argument('--use_flux', dest='use_flux', action='store_true',\n",
    "                  help='incorporate the feature grad in attention based edge dropout')\n",
    "parser.add_argument(\"--exact\", action=\"store_true\",\n",
    "                  help=\"for small datasets can do exact diffusion. If dataset is too big for matrix inversion then you can't\")\n",
    "parser.add_argument('--M_nodes', type=int, default=64, help=\"new number of nodes to add\")\n",
    "parser.add_argument('--new_edges', type=str, default=\"random\", help=\"random, random_walk, k_hop\")\n",
    "parser.add_argument('--sparsify', type=str, default=\"S_hat\", help=\"S_hat, recalc_att\")\n",
    "parser.add_argument('--threshold_type', type=str, default=\"topk_adj\", help=\"topk_adj, addD_rvR\")\n",
    "parser.add_argument('--rw_addD', type=float, default=0.02, help=\"percentage of new edges to add\")\n",
    "parser.add_argument('--rw_rmvR', type=float, default=0.02, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--rewire_KNN', action='store_true', help='perform KNN rewiring every few epochs')\n",
    "parser.add_argument('--rewire_KNN_T', type=str, default=\"T0\", help=\"T0, TN\")\n",
    "parser.add_argument('--rewire_KNN_epoch', type=int, default=5, help=\"frequency of epochs to rewire\")\n",
    "parser.add_argument('--rewire_KNN_k', type=int, default=64, help=\"target degree for KNN rewire\")\n",
    "parser.add_argument('--rewire_KNN_sym', action='store_true', help='make KNN symmetric')\n",
    "parser.add_argument('--KNN_online', action='store_true', help='perform rewiring online')\n",
    "parser.add_argument('--KNN_online_reps', type=int, default=4, help=\"how many online KNN its\")\n",
    "parser.add_argument('--KNN_space', type=str, default=\"pos_distance\", help=\"Z,P,QKZ,QKp\")\n",
    "\n",
    "# Stefan's experiment args\n",
    "parser.add_argument('--count_runs', type=int, default=10,\n",
    "                  help=\"number of runs to average results over per parameter settings for each experiment\")\n",
    "\n",
    "# beltrami\n",
    "parser.add_argument('--beltrami', action='store_true', help='perform diffusion beltrami style')\n",
    "parser.add_argument('--fa_layer', action='store_true', help='add a bottleneck paper style layer with more edges')\n",
    "parser.add_argument('--pos_enc_type', type=str, default=\"DW64\",\n",
    "                  help='positional encoder either GDC, DW64, DW128, DW256')\n",
    "parser.add_argument('--pos_enc_orientation', type=str, default=\"row\", help=\"row, col\")\n",
    "parser.add_argument('--feat_hidden_dim', type=int, default=64, help=\"dimension of features in beltrami\")\n",
    "parser.add_argument('--pos_enc_hidden_dim', type=int, default=32, help=\"dimension of position in beltrami\")\n",
    "parser.add_argument('--edge_sampling', action='store_true', help='perform edge sampling rewiring')\n",
    "parser.add_argument('--edge_sampling_T', type=str, default=\"T0\", help=\"T0, TN\")\n",
    "parser.add_argument('--edge_sampling_epoch', type=int, default=5, help=\"frequency of epochs to rewire\")\n",
    "parser.add_argument('--edge_sampling_add', type=float, default=0.64, help=\"percentage of new edges to add\")\n",
    "parser.add_argument('--edge_sampling_add_type', type=str, default=\"importance\",\n",
    "                  help=\"random, ,anchored, importance, degree\")\n",
    "parser.add_argument('--edge_sampling_rmv', type=float, default=0.32, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--edge_sampling_sym', action='store_true', help='make KNN symmetric')\n",
    "parser.add_argument('--edge_sampling_online', action='store_true', help='perform rewiring online')\n",
    "parser.add_argument('--edge_sampling_online_reps', type=int, default=4, help=\"how many online KNN its\")\n",
    "parser.add_argument('--edge_sampling_space', type=str, default=\"attention\",\n",
    "                  help=\"attention,pos_distance, z_distance, pos_distance_QK, z_distance_QK\")\n",
    "parser.add_argument('--symmetric_attention', action='store_true',\n",
    "                  help='maks the attention symmetric for rewring in QK space')\n",
    "\n",
    "parser.add_argument('--fa_layer_edge_sampling_rmv', type=float, default=0.8, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--gpu', type=int, default=0, help=\"GPU to run on (default 0)\")\n",
    "parser.add_argument('--pos_enc_csv', action='store_true', help=\"Generate pos encoding as a sparse CSV\")\n",
    "\n",
    "parser.add_argument('--pos_dist_quantile', type=float, default=0.001, help=\"percentage of N**2 edges to keep\")\n",
    "\n",
    "#added\n",
    "parser.add_argument('--use_mlp', dest='use_mlp', action='store_true',\n",
    "                  help='Add a fully connected layer to the encoder.')\n",
    "parser.add_argument('--use_labels', dest='use_labels', action='store_true', help='Also diffuse labels')\n",
    "parser.add_argument('--fc_out', dest='fc_out', action='store_true',\n",
    "                  help='Add a fully connected layer to the decoder.')\n",
    "parser.add_argument(\"--batch_norm\", dest='batch_norm', action='store_true', help='search over reg params')\n",
    "\n",
    "args = parser.parse_args(customArgs)\n",
    "opt = vars(args)\n",
    "\n",
    "#'Cora' #'Flickr' #'Computer'\n",
    "opt['dataset'] = 'Computer' \n",
    "\n",
    "if opt['dataset'] == 'Cora':\n",
    "    opt = get_cora_opt(opt)\n",
    "elif opt['dataset'] == 'Computer':\n",
    "    opt = get_computers_opt(opt)\n",
    "elif opt['dataset'] == 'Flickr':\n",
    "    opt = get_flickr_opt(opt)\n",
    "\n",
    "opt['adjoint'] = True\n",
    "#opt['method'] = 'explicit_adams'\n",
    "#opt['method'] = 'implicit_adams'\n",
    "#opt['method'] = 'dopri5'\n",
    "opt['method'] = 'rk4'\n",
    "opt['adjoint_method'] = opt['method']\n",
    "opt['max_iters'] = 100\n",
    "opt['step_size'] = opt['dt_min'] = 0.01\n",
    "opt['tol_scale'] = 100.0\n",
    "opt['tol_scale_adjoint'] = 100.0\n",
    "#added\n",
    "opt['max_nfe'] = 100000\n",
    "if opt['dataset'] == 'Flickr':    \n",
    "    opt['rewiring'] == 'gdc'\n",
    "\n",
    "# DEBUG\n",
    "#for k in ['dataset', 'epoch', 'adjoint', 'rewiring', 'adaptive', 'dt', 'dt_min', 'method', 'adjoint_method']:\n",
    "#  print(k, opt[k])\n",
    "#main(opt, 0)\n",
    "\n",
    "# Run combination of experiments\n",
    "for timep in [64]:#[1.0,2.0,4.0,8.0,16.0,32.0]:\n",
    "    print(f'*** Doing time {timep} ***')\n",
    "    opt['time'] = timep\n",
    "    for stepsize in [1.0]:#[1.0,0.5,0.1,0.01]: #[0.5, 0.25, 0.1, 0.01]: # 2.0, 1.0\n",
    "        print(f'*** Doing stepsize {stepsize} ***')\n",
    "        for idx in range(opt['count_runs']):\n",
    "            print(f'*** Doing run {idx} ***')\n",
    "            # NOTE: I think setting dt_min may not be necessary, doing it just to be safe!\n",
    "            opt['step_size'] = opt['dt_min'] = stepsize\n",
    "            run(opt, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ff871-6d10-462c-b9f4-c0d2d6dbc161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedf3b1-af5d-49d0-8ddb-834f9e6dea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
