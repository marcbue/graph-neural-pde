{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faed1d68-9918-4d47-9a0f-6b7c1394515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor, GNNBenchmarkDataset, Reddit2, Flickr\n",
    "from GNN import GNN\n",
    "from GNN_KNN import GNN_KNN\n",
    "import time\n",
    "from data import get_dataset, Data\n",
    "\n",
    "from graph_rewiring import get_two_hop, apply_gdc\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import to_undirected\n",
    "from graph_rewiring import make_symmetric, apply_pos_dist_rewire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c1e662-6fcf-4c95-97e4-41f5ce755be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customArgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7ee566-4b2c-462c-8437-0ec9d73eb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cora_opt(opt):\n",
    "  opt['dataset'] = 'Cora'\n",
    "  opt['data'] = 'Planetoid'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'rmsprop'\n",
    "  opt['lr'] = 0.0047\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 31\n",
    "  opt['augment'] = True\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['adjoint'] = False\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_computers_opt(opt):\n",
    "  opt['dataset'] = 'Computers'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.01\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['epoch'] = 400\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 100\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_flickr_opt(opt):\n",
    "  opt['dataset'] = 'Flickr'\n",
    "  opt['hidden_dim'] = 128\n",
    "  opt['feature_hidden_dim'] = 64\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.005451476553977102\n",
    "  opt['decay'] = 0\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 1.0\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 500\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 89250\n",
    "  opt['epoch'] = 100\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  opt['gdc_avg_degree']= 48\n",
    "  opt['gdc_k'] = 48\n",
    "  opt['gdc_method'] = 'ppr'\n",
    "  opt['gdc_sparsification'] = 'topk'\n",
    "  opt['gdc_threshold'] =  0.01\n",
    "  return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "442b7c4e-3ad2-455a-907f-38f4f25067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name, parameters, lr, weight_decay=0):\n",
    "  if name == 'sgd':\n",
    "    return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'rmsprop':\n",
    "    return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adagrad':\n",
    "    return torch.optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adam':\n",
    "    return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adamax':\n",
    "    return torch.optim.Adamax(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  else:\n",
    "    raise Exception(\"Unsupported optimizer: {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "768697b4-1f65-4c10-a8d8-3744ded132dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x)\n",
    "  lf = torch.nn.CrossEntropyLoss()\n",
    "  loss = lf(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "  # TODO: What is this block about???\n",
    "  if model.odeblock.nreg > 0:  # add regularisation - slower for small data, but faster and better performance for large data\n",
    "    reg_states = tuple(torch.mean(rs) for rs in model.reg_states)\n",
    "    regularization_coeffs = model.regularization_coeffs\n",
    "\n",
    "    reg_loss = sum(\n",
    "      reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
    "    )\n",
    "    loss = loss + reg_loss\n",
    "\n",
    "  # Update count of forward evaluations from ODE solver\n",
    "  # NOTE: fm stands for \"forward meter\"\n",
    "  # TODO: Rename this to be more informative!\n",
    "  model.fm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  # Gradient step\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Update count of backwards evaluations from ODE solver\n",
    "  model.bm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "  model.eval()\n",
    "  logits, accs = model(data.x), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs\n",
    "\n",
    "def print_model_params(model):\n",
    "  print(model)\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "      print(name)\n",
    "      print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05dd71b0-cad8-45a6-ba14-e62cc96a8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_benchmark(opt: dict, data_dir, use_lcc: bool = False) -> InMemoryDataset:\n",
    "    ds = opt['dataset']\n",
    "    dataset = Flickr(root=os.path.join(data_dir,'Flickr'), transform=T.ToSparseTensor())#T.NormalizeFeatures())  \n",
    "\n",
    "    \"\"\"\n",
    "    d_train = GNNBenchmarkDataset(name=ds, root=path, split='train', transform=T.NormalizeFeatures())\n",
    "    d_val = GNNBenchmarkDataset(name=ds, root=path, split='val', transform=T.NormalizeFeatures())\n",
    "    d_test = GNNBenchmarkDataset(name=ds, root=path, split='test', transform=T.NormalizeFeatures())\n",
    "    \n",
    "    print(d_train.data)\n",
    "    print(d_val.data)\n",
    "    print(d_test.data)\n",
    "    print(torch.max(d_train.data.edge_index))\n",
    "    print(torch.max(d_val.data.edge_index))\n",
    "    print(torch.max(d_test.data.edge_index))\n",
    "    \n",
    "    d_temp = Planetoid(root=os.path.join(data_dir,'Cora'), name='Cora')\n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    d_temp = PygNodePropPredDataset(name='ogbn-arxiv', root=os.path.join(data_dir,'ogbn-arxiv'), transform=T.ToSparseTensor())\n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    d_temp = Flickr(root=os.path.join(data_dir,'Flickr'), transform=T.NormalizeFeatures())  \n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    #d_temp = Reddit2(root=os.path.join(data_dir,'Reddit2'), transform=T.NormalizeFeatures())\n",
    "    #print(d_temp.data)\n",
    "    #print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    return\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "\n",
    "        data = Data(\n",
    "          x=x_new,\n",
    "          edge_index=torch.LongTensor(edges),\n",
    "          y=y_new,\n",
    "          train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "          test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "          val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "    if opt['rewiring'] is not None:\n",
    "        dataset.data = rewire(dataset.data, opt, data_dir)\n",
    "    train_mask_exists = True\n",
    "    try:\n",
    "        dataset.data.train_mask\n",
    "    except AttributeError:\n",
    "        train_mask_exists = False\n",
    "\n",
    "    if ds == 'ogbn-arxiv':\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        ei = to_undirected(dataset.data.edge_index)\n",
    "        data = Data(\n",
    "        x=dataset.data.x,\n",
    "        edge_index=ei,\n",
    "        y=dataset.data.y,\n",
    "        train_mask=split_idx['train'],\n",
    "        test_mask=split_idx['test'],\n",
    "        val_mask=split_idx['valid'])\n",
    "        dataset.data = data\n",
    "        train_mask_exists = True\n",
    "\n",
    "    #todo this currently breaks with heterophilic datasets if you don't pass --geom_gcn_splits\n",
    "    if (use_lcc or not train_mask_exists) and not opt['geom_gcn_splits']:\n",
    "        dataset.data = set_train_val_test_split(\n",
    "          12345,\n",
    "          dataset.data,\n",
    "          num_development=5000 if ds == \"CoauthorCS\" else 1500)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "433d8568-8f88-4feb-8e97-c466a24e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def run(opt, run_count):\n",
    "\n",
    "    # Load dataset and create model\n",
    "    if opt['dataset'] == 'Flickr':\n",
    "        dataset = get_dataset_benchmark(opt, '../data', False)\n",
    "    else:\n",
    "        dataset = get_dataset(opt, '../data', False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if opt['rewire_KNN'] or opt['fa_layer']:\n",
    "        model = GNN_KNN(opt, dataset, device).to(device)\n",
    "    else:\n",
    "        model = GNN(opt, dataset, device).to(device)\n",
    "    data = dataset.data.to(device)\n",
    "    #model, data = GNN(opt, dataset, device).to(device), dataset.data.to(device)\n",
    "    print(opt)\n",
    "\n",
    "    # Todo for some reason the submodule parameters inside the attention module don't show up when running on GPU.\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    print_model_params(model)\n",
    "\n",
    "    # Training/test loop\n",
    "    results = {\n",
    "        'time':[],\n",
    "        'loss':[],\n",
    "        'forward_nfe':[],\n",
    "        'backward_nfe':[],\n",
    "        'train_acc':[],\n",
    "        'test_acc':[],\n",
    "        'val_acc':[],\n",
    "        'best_epoch':0,\n",
    "        'best_val_acc':0.,\n",
    "        'best_test_acc':0.,\n",
    "    }\n",
    "    runtimes = []\n",
    "    losses = []\n",
    "\n",
    "    optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "    best_val_acc = test_acc = train_acc = best_epoch = 0\n",
    "    overall_time = time.time()\n",
    "    for epoch in range(1, opt['epoch']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss = train(model, optimizer, data)\n",
    "        train_acc, val_acc, test_acc = test(model, data)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_train_acc = train_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        results['time'].append(time.time() - start_time)\n",
    "        results['loss'].append(loss)\n",
    "        results['forward_nfe'].append(model.fm.sum)\n",
    "        results['backward_nfe'].append(model.bm.sum)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['val_acc'].append(val_acc)\n",
    "        results['best_epoch'] = best_epoch\n",
    "        results['best_train_acc'] = best_train_acc\n",
    "        results['best_val_acc'] = best_val_acc\n",
    "        results['best_test_acc'] = best_test_acc\n",
    "\n",
    "        log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        print(log.format(epoch, results['time'][-1], results['loss'][-1], results['forward_nfe'][-1], results['backward_nfe'][-1], results['train_acc'][-1], results['val_acc'][-1], results['test_acc'][-1]))\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d}'.format(best_val_acc, best_test_acc, best_epoch))\n",
    "    \n",
    "    results['all_epochs_time'] = time.time() - overall_time\n",
    "\n",
    "    # TODO: Save results\n",
    "    # cora_epoch_101_adjoint_false_... . pickle\n",
    "    pickle.dump( results, open( f\"../results/{opt['dataset']}_{opt['method']}_stepsize_{opt['step_size']}_run_{run_count}.pickle\", \"wb\" ) )\n",
    "\n",
    "    return train_acc, best_val_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f3b2294-ed94-4c12-b2eb-1115d36300a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Doing stepsize 1.0 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.112499, Loss 2.319046, forward nfe 52, backward nfe 52, Train: 0.1550, Val: 0.0392, Test: 0.0538\n",
      "Epoch: 002, Runtime 0.098900, Loss 2.407678, forward nfe 156, backward nfe 104, Train: 0.1300, Val: 0.0331, Test: 0.0393\n",
      "Epoch: 003, Runtime 0.098219, Loss 2.367339, forward nfe 260, backward nfe 156, Train: 0.1050, Val: 0.0585, Test: 0.0620\n",
      "Epoch: 004, Runtime 0.097110, Loss 2.327698, forward nfe 364, backward nfe 208, Train: 0.1050, Val: 0.2577, Test: 0.2365\n",
      "Epoch: 005, Runtime 0.092662, Loss 2.361603, forward nfe 468, backward nfe 260, Train: 0.1850, Val: 0.2162, Test: 0.2032\n",
      "Epoch: 006, Runtime 0.095298, Loss 2.216209, forward nfe 572, backward nfe 312, Train: 0.1850, Val: 0.1923, Test: 0.1858\n",
      "Epoch: 007, Runtime 0.096589, Loss 2.246177, forward nfe 676, backward nfe 364, Train: 0.1900, Val: 0.1231, Test: 0.1260\n",
      "Epoch: 008, Runtime 0.096850, Loss 2.238165, forward nfe 780, backward nfe 416, Train: 0.1100, Val: 0.0692, Test: 0.0739\n",
      "Epoch: 009, Runtime 0.098777, Loss 2.250066, forward nfe 884, backward nfe 468, Train: 0.1200, Val: 0.0608, Test: 0.0664\n",
      "Epoch: 010, Runtime 0.098508, Loss 2.216723, forward nfe 988, backward nfe 520, Train: 0.2000, Val: 0.0723, Test: 0.0828\n",
      "Epoch: 011, Runtime 0.097110, Loss 2.169536, forward nfe 1092, backward nfe 572, Train: 0.2100, Val: 0.0915, Test: 0.0931\n",
      "Epoch: 012, Runtime 0.096341, Loss 2.149174, forward nfe 1196, backward nfe 624, Train: 0.3150, Val: 0.2300, Test: 0.2334\n",
      "Epoch: 013, Runtime 0.095785, Loss 2.123833, forward nfe 1300, backward nfe 676, Train: 0.2550, Val: 0.2215, Test: 0.2154\n",
      "Epoch: 014, Runtime 0.097616, Loss 2.119404, forward nfe 1404, backward nfe 728, Train: 0.2600, Val: 0.2208, Test: 0.2204\n",
      "Epoch: 015, Runtime 0.095997, Loss 2.084237, forward nfe 1508, backward nfe 780, Train: 0.3800, Val: 0.2015, Test: 0.2076\n",
      "Epoch: 016, Runtime 0.095864, Loss 2.070746, forward nfe 1612, backward nfe 832, Train: 0.2800, Val: 0.0885, Test: 0.1023\n",
      "Epoch: 017, Runtime 0.097329, Loss 2.061513, forward nfe 1716, backward nfe 884, Train: 0.2950, Val: 0.1246, Test: 0.1306\n",
      "Epoch: 018, Runtime 0.099991, Loss 2.000752, forward nfe 1820, backward nfe 936, Train: 0.3400, Val: 0.2446, Test: 0.2441\n",
      "Epoch: 019, Runtime 0.099788, Loss 1.992552, forward nfe 1924, backward nfe 988, Train: 0.4250, Val: 0.2500, Test: 0.2662\n",
      "Epoch: 020, Runtime 0.097762, Loss 1.972327, forward nfe 2028, backward nfe 1040, Train: 0.4200, Val: 0.1654, Test: 0.1941\n",
      "Epoch: 021, Runtime 0.096433, Loss 1.942990, forward nfe 2132, backward nfe 1092, Train: 0.4050, Val: 0.1054, Test: 0.1431\n",
      "Epoch: 022, Runtime 0.097198, Loss 1.913424, forward nfe 2236, backward nfe 1144, Train: 0.4350, Val: 0.1115, Test: 0.1513\n",
      "Epoch: 023, Runtime 0.097850, Loss 1.872777, forward nfe 2340, backward nfe 1196, Train: 0.3050, Val: 0.0900, Test: 0.1090\n",
      "Epoch: 024, Runtime 0.098300, Loss 1.874601, forward nfe 2444, backward nfe 1248, Train: 0.4600, Val: 0.1600, Test: 0.1963\n",
      "Epoch: 025, Runtime 0.098783, Loss 1.803920, forward nfe 2548, backward nfe 1300, Train: 0.4650, Val: 0.1931, Test: 0.2207\n",
      "Epoch: 026, Runtime 0.102105, Loss 1.798033, forward nfe 2652, backward nfe 1352, Train: 0.4800, Val: 0.1854, Test: 0.2234\n",
      "Epoch: 027, Runtime 0.099976, Loss 1.761378, forward nfe 2756, backward nfe 1404, Train: 0.5350, Val: 0.1992, Test: 0.2351\n",
      "Epoch: 028, Runtime 0.098678, Loss 1.719215, forward nfe 2860, backward nfe 1456, Train: 0.5200, Val: 0.1931, Test: 0.2317\n",
      "Epoch: 029, Runtime 0.098260, Loss 1.700829, forward nfe 2964, backward nfe 1508, Train: 0.5300, Val: 0.2015, Test: 0.2376\n",
      "Epoch: 030, Runtime 0.096297, Loss 1.665047, forward nfe 3068, backward nfe 1560, Train: 0.5300, Val: 0.2223, Test: 0.2522\n",
      "Epoch: 031, Runtime 0.096519, Loss 1.619889, forward nfe 3172, backward nfe 1612, Train: 0.5350, Val: 0.2277, Test: 0.2747\n",
      "Epoch: 032, Runtime 0.098506, Loss 1.608161, forward nfe 3276, backward nfe 1664, Train: 0.5450, Val: 0.2462, Test: 0.2902\n",
      "Epoch: 033, Runtime 0.098948, Loss 1.576856, forward nfe 3380, backward nfe 1716, Train: 0.5600, Val: 0.2554, Test: 0.2893\n",
      "Epoch: 034, Runtime 0.097382, Loss 1.531853, forward nfe 3484, backward nfe 1768, Train: 0.5850, Val: 0.2662, Test: 0.2938\n",
      "Epoch: 035, Runtime 0.097235, Loss 1.509759, forward nfe 3588, backward nfe 1820, Train: 0.6000, Val: 0.2708, Test: 0.3018\n",
      "Epoch: 036, Runtime 0.097049, Loss 1.484769, forward nfe 3692, backward nfe 1872, Train: 0.6150, Val: 0.2846, Test: 0.3204\n",
      "Epoch: 037, Runtime 0.096725, Loss 1.452875, forward nfe 3796, backward nfe 1924, Train: 0.6250, Val: 0.3077, Test: 0.3459\n",
      "Epoch: 038, Runtime 0.096957, Loss 1.429679, forward nfe 3900, backward nfe 1976, Train: 0.6250, Val: 0.3300, Test: 0.3568\n",
      "Epoch: 039, Runtime 0.096967, Loss 1.405148, forward nfe 4004, backward nfe 2028, Train: 0.6300, Val: 0.3146, Test: 0.3500\n",
      "Epoch: 040, Runtime 0.096854, Loss 1.359832, forward nfe 4108, backward nfe 2080, Train: 0.6500, Val: 0.2862, Test: 0.3328\n",
      "Epoch: 041, Runtime 0.097374, Loss 1.330502, forward nfe 4212, backward nfe 2132, Train: 0.6550, Val: 0.2669, Test: 0.3095\n",
      "Epoch: 042, Runtime 0.096743, Loss 1.310889, forward nfe 4316, backward nfe 2184, Train: 0.6650, Val: 0.2815, Test: 0.3214\n",
      "Epoch: 043, Runtime 0.098439, Loss 1.281501, forward nfe 4420, backward nfe 2236, Train: 0.6700, Val: 0.3569, Test: 0.3934\n",
      "Epoch: 044, Runtime 0.095930, Loss 1.252598, forward nfe 4524, backward nfe 2288, Train: 0.6450, Val: 0.4285, Test: 0.4507\n",
      "Epoch: 045, Runtime 0.097553, Loss 1.213009, forward nfe 4628, backward nfe 2340, Train: 0.6300, Val: 0.4377, Test: 0.4634\n",
      "Epoch: 046, Runtime 0.095987, Loss 1.192550, forward nfe 4732, backward nfe 2392, Train: 0.6600, Val: 0.4115, Test: 0.4356\n",
      "Epoch: 047, Runtime 0.099540, Loss 1.165615, forward nfe 4836, backward nfe 2444, Train: 0.6850, Val: 0.3900, Test: 0.4141\n",
      "Epoch: 048, Runtime 0.098244, Loss 1.137498, forward nfe 4940, backward nfe 2496, Train: 0.7200, Val: 0.3585, Test: 0.4050\n",
      "Epoch: 049, Runtime 0.099806, Loss 1.120404, forward nfe 5044, backward nfe 2548, Train: 0.7400, Val: 0.3592, Test: 0.4074\n",
      "Epoch: 050, Runtime 0.097243, Loss 1.088845, forward nfe 5148, backward nfe 2600, Train: 0.7400, Val: 0.3954, Test: 0.4388\n",
      "Epoch: 051, Runtime 0.098151, Loss 1.060361, forward nfe 5252, backward nfe 2652, Train: 0.7500, Val: 0.5423, Test: 0.5648\n",
      "Epoch: 052, Runtime 0.096557, Loss 1.037542, forward nfe 5356, backward nfe 2704, Train: 0.7450, Val: 0.5869, Test: 0.6118\n",
      "Epoch: 053, Runtime 0.096419, Loss 1.003333, forward nfe 5460, backward nfe 2756, Train: 0.7450, Val: 0.5862, Test: 0.6109\n",
      "Epoch: 054, Runtime 0.096876, Loss 0.990300, forward nfe 5564, backward nfe 2808, Train: 0.7750, Val: 0.5892, Test: 0.6158\n",
      "Epoch: 055, Runtime 0.096197, Loss 0.961231, forward nfe 5668, backward nfe 2860, Train: 0.7950, Val: 0.5854, Test: 0.6198\n",
      "Epoch: 056, Runtime 0.096992, Loss 0.933531, forward nfe 5772, backward nfe 2912, Train: 0.8000, Val: 0.5892, Test: 0.6235\n",
      "Epoch: 057, Runtime 0.096691, Loss 0.920141, forward nfe 5876, backward nfe 2964, Train: 0.7900, Val: 0.6069, Test: 0.6343\n",
      "Epoch: 058, Runtime 0.096604, Loss 0.891223, forward nfe 5980, backward nfe 3016, Train: 0.8050, Val: 0.6315, Test: 0.6548\n",
      "Epoch: 059, Runtime 0.097092, Loss 0.866376, forward nfe 6084, backward nfe 3068, Train: 0.8200, Val: 0.6485, Test: 0.6704\n",
      "Epoch: 060, Runtime 0.096848, Loss 0.836827, forward nfe 6188, backward nfe 3120, Train: 0.8300, Val: 0.6538, Test: 0.6752\n",
      "Epoch: 061, Runtime 0.097254, Loss 0.807912, forward nfe 6292, backward nfe 3172, Train: 0.8350, Val: 0.6592, Test: 0.6781\n",
      "Epoch: 062, Runtime 0.096517, Loss 0.795309, forward nfe 6396, backward nfe 3224, Train: 0.8250, Val: 0.6638, Test: 0.6823\n",
      "Epoch: 063, Runtime 0.096517, Loss 0.774914, forward nfe 6500, backward nfe 3276, Train: 0.8300, Val: 0.6662, Test: 0.6852\n",
      "Epoch: 064, Runtime 0.096346, Loss 0.755980, forward nfe 6604, backward nfe 3328, Train: 0.8350, Val: 0.6815, Test: 0.6940\n",
      "Epoch: 065, Runtime 0.097323, Loss 0.728574, forward nfe 6708, backward nfe 3380, Train: 0.8350, Val: 0.6885, Test: 0.7035\n",
      "Epoch: 066, Runtime 0.096354, Loss 0.703712, forward nfe 6812, backward nfe 3432, Train: 0.8250, Val: 0.6992, Test: 0.7161\n",
      "Epoch: 067, Runtime 0.099807, Loss 0.705642, forward nfe 6916, backward nfe 3484, Train: 0.8450, Val: 0.7131, Test: 0.7283\n",
      "Epoch: 068, Runtime 0.098813, Loss 0.670678, forward nfe 7020, backward nfe 3536, Train: 0.8400, Val: 0.7246, Test: 0.7360\n",
      "Epoch: 069, Runtime 0.098249, Loss 0.658917, forward nfe 7124, backward nfe 3588, Train: 0.8450, Val: 0.7246, Test: 0.7369\n",
      "Epoch: 070, Runtime 0.097246, Loss 0.637008, forward nfe 7228, backward nfe 3640, Train: 0.8350, Val: 0.7162, Test: 0.7347\n",
      "Epoch: 071, Runtime 0.098202, Loss 0.623241, forward nfe 7332, backward nfe 3692, Train: 0.8400, Val: 0.7223, Test: 0.7356\n",
      "Epoch: 072, Runtime 0.097515, Loss 0.603460, forward nfe 7436, backward nfe 3744, Train: 0.8600, Val: 0.7308, Test: 0.7418\n",
      "Epoch: 073, Runtime 0.097085, Loss 0.589559, forward nfe 7540, backward nfe 3796, Train: 0.8550, Val: 0.7438, Test: 0.7509\n",
      "Epoch: 074, Runtime 0.097366, Loss 0.570491, forward nfe 7644, backward nfe 3848, Train: 0.8550, Val: 0.7408, Test: 0.7537\n",
      "Epoch: 075, Runtime 0.097997, Loss 0.562893, forward nfe 7748, backward nfe 3900, Train: 0.8700, Val: 0.7469, Test: 0.7556\n",
      "Epoch: 076, Runtime 0.099593, Loss 0.547676, forward nfe 7852, backward nfe 3952, Train: 0.8700, Val: 0.7446, Test: 0.7564\n",
      "Epoch: 077, Runtime 0.098579, Loss 0.530375, forward nfe 7956, backward nfe 4004, Train: 0.8700, Val: 0.7454, Test: 0.7596\n",
      "Epoch: 078, Runtime 0.100194, Loss 0.532281, forward nfe 8060, backward nfe 4056, Train: 0.8650, Val: 0.7515, Test: 0.7663\n",
      "Epoch: 079, Runtime 0.099716, Loss 0.501091, forward nfe 8164, backward nfe 4108, Train: 0.8650, Val: 0.7562, Test: 0.7686\n",
      "Epoch: 080, Runtime 0.097125, Loss 0.494749, forward nfe 8268, backward nfe 4160, Train: 0.8650, Val: 0.7546, Test: 0.7673\n",
      "Epoch: 081, Runtime 0.098702, Loss 0.484107, forward nfe 8372, backward nfe 4212, Train: 0.8700, Val: 0.7485, Test: 0.7662\n",
      "Epoch: 082, Runtime 0.097522, Loss 0.472490, forward nfe 8476, backward nfe 4264, Train: 0.8700, Val: 0.7477, Test: 0.7661\n",
      "Epoch: 083, Runtime 0.096788, Loss 0.465389, forward nfe 8580, backward nfe 4316, Train: 0.8750, Val: 0.7546, Test: 0.7728\n",
      "Epoch: 084, Runtime 0.100108, Loss 0.443331, forward nfe 8684, backward nfe 4368, Train: 0.8700, Val: 0.7654, Test: 0.7803\n",
      "Epoch: 085, Runtime 0.097415, Loss 0.441870, forward nfe 8788, backward nfe 4420, Train: 0.8700, Val: 0.7646, Test: 0.7813\n",
      "Epoch: 086, Runtime 0.097916, Loss 0.433710, forward nfe 8892, backward nfe 4472, Train: 0.8800, Val: 0.7669, Test: 0.7813\n",
      "Epoch: 087, Runtime 0.097697, Loss 0.429348, forward nfe 8996, backward nfe 4524, Train: 0.8750, Val: 0.7677, Test: 0.7827\n",
      "Epoch: 088, Runtime 0.095844, Loss 0.423251, forward nfe 9100, backward nfe 4576, Train: 0.8850, Val: 0.7646, Test: 0.7826\n",
      "Epoch: 089, Runtime 0.097746, Loss 0.406264, forward nfe 9204, backward nfe 4628, Train: 0.8900, Val: 0.7669, Test: 0.7853\n",
      "Epoch: 090, Runtime 0.098208, Loss 0.413114, forward nfe 9308, backward nfe 4680, Train: 0.8900, Val: 0.7692, Test: 0.7870\n",
      "Epoch: 091, Runtime 0.098916, Loss 0.398555, forward nfe 9412, backward nfe 4732, Train: 0.8950, Val: 0.7715, Test: 0.7894\n",
      "Epoch: 092, Runtime 0.108015, Loss 0.388078, forward nfe 9516, backward nfe 4784, Train: 0.8900, Val: 0.7746, Test: 0.7899\n",
      "Epoch: 093, Runtime 0.097646, Loss 0.367044, forward nfe 9620, backward nfe 4836, Train: 0.8850, Val: 0.7777, Test: 0.7921\n",
      "Epoch: 094, Runtime 0.102395, Loss 0.367837, forward nfe 9724, backward nfe 4888, Train: 0.8850, Val: 0.7762, Test: 0.7951\n",
      "Epoch: 095, Runtime 0.101568, Loss 0.357848, forward nfe 9828, backward nfe 4940, Train: 0.9000, Val: 0.7808, Test: 0.8015\n",
      "Epoch: 096, Runtime 0.097585, Loss 0.367288, forward nfe 9932, backward nfe 4992, Train: 0.8900, Val: 0.7877, Test: 0.8075\n",
      "Epoch: 097, Runtime 0.096531, Loss 0.365800, forward nfe 10036, backward nfe 5044, Train: 0.8950, Val: 0.7869, Test: 0.8071\n",
      "Epoch: 098, Runtime 0.097366, Loss 0.340333, forward nfe 10140, backward nfe 5096, Train: 0.9050, Val: 0.7854, Test: 0.8036\n",
      "Epoch: 099, Runtime 0.102186, Loss 0.339634, forward nfe 10244, backward nfe 5148, Train: 0.9050, Val: 0.7754, Test: 0.7978\n",
      "best val accuracy 0.787692 with test accuracy 0.807460 at epoch 96\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.100783, Loss 2.313262, forward nfe 52, backward nfe 52, Train: 0.1250, Val: 0.1500, Test: 0.1408\n",
      "Epoch: 002, Runtime 0.102185, Loss 2.484001, forward nfe 156, backward nfe 104, Train: 0.1150, Val: 0.0192, Test: 0.0282\n",
      "Epoch: 003, Runtime 0.102278, Loss 2.361346, forward nfe 260, backward nfe 156, Train: 0.1000, Val: 0.0623, Test: 0.0651\n",
      "Epoch: 004, Runtime 0.100895, Loss 2.298853, forward nfe 364, backward nfe 208, Train: 0.1200, Val: 0.0954, Test: 0.0974\n",
      "Epoch: 005, Runtime 0.101232, Loss 2.260301, forward nfe 468, backward nfe 260, Train: 0.1300, Val: 0.1646, Test: 0.1588\n",
      "Epoch: 006, Runtime 0.102350, Loss 2.265046, forward nfe 572, backward nfe 312, Train: 0.1300, Val: 0.1523, Test: 0.1468\n",
      "Epoch: 007, Runtime 0.102814, Loss 2.258401, forward nfe 676, backward nfe 364, Train: 0.1350, Val: 0.1646, Test: 0.1571\n",
      "Epoch: 008, Runtime 0.096690, Loss 2.249901, forward nfe 780, backward nfe 416, Train: 0.1600, Val: 0.2015, Test: 0.1919\n",
      "Epoch: 009, Runtime 0.099909, Loss 2.238503, forward nfe 884, backward nfe 468, Train: 0.1600, Val: 0.2031, Test: 0.1948\n",
      "Epoch: 010, Runtime 0.097887, Loss 2.227980, forward nfe 988, backward nfe 520, Train: 0.1550, Val: 0.2008, Test: 0.1945\n",
      "Epoch: 011, Runtime 0.099570, Loss 2.215946, forward nfe 1092, backward nfe 572, Train: 0.1700, Val: 0.2046, Test: 0.1992\n",
      "Epoch: 012, Runtime 0.099029, Loss 2.203304, forward nfe 1196, backward nfe 624, Train: 0.1800, Val: 0.2108, Test: 0.2092\n",
      "Epoch: 013, Runtime 0.099138, Loss 2.191877, forward nfe 1300, backward nfe 676, Train: 0.2150, Val: 0.2362, Test: 0.2304\n",
      "Epoch: 014, Runtime 0.100099, Loss 2.170352, forward nfe 1404, backward nfe 728, Train: 0.1450, Val: 0.1392, Test: 0.1401\n",
      "Epoch: 015, Runtime 0.100982, Loss 2.160703, forward nfe 1508, backward nfe 780, Train: 0.2500, Val: 0.2400, Test: 0.2357\n",
      "Epoch: 016, Runtime 0.102752, Loss 2.130548, forward nfe 1612, backward nfe 832, Train: 0.2450, Val: 0.2562, Test: 0.2481\n",
      "Epoch: 017, Runtime 0.097201, Loss 2.127813, forward nfe 1716, backward nfe 884, Train: 0.2500, Val: 0.2508, Test: 0.2418\n",
      "Epoch: 018, Runtime 0.103570, Loss 2.099438, forward nfe 1820, backward nfe 936, Train: 0.1500, Val: 0.2108, Test: 0.2000\n",
      "Epoch: 019, Runtime 0.104005, Loss 2.092312, forward nfe 1924, backward nfe 988, Train: 0.1600, Val: 0.2231, Test: 0.2075\n",
      "Epoch: 020, Runtime 0.101198, Loss 2.078764, forward nfe 2028, backward nfe 1040, Train: 0.2250, Val: 0.1562, Test: 0.1531\n",
      "Epoch: 021, Runtime 0.100291, Loss 2.055616, forward nfe 2132, backward nfe 1092, Train: 0.2400, Val: 0.1615, Test: 0.1520\n",
      "Epoch: 022, Runtime 0.101657, Loss 2.049124, forward nfe 2236, backward nfe 1144, Train: 0.2700, Val: 0.1554, Test: 0.1627\n",
      "Epoch: 023, Runtime 0.101503, Loss 2.027283, forward nfe 2340, backward nfe 1196, Train: 0.2000, Val: 0.2269, Test: 0.2165\n",
      "Epoch: 024, Runtime 0.097185, Loss 2.021869, forward nfe 2444, backward nfe 1248, Train: 0.2550, Val: 0.1954, Test: 0.1956\n",
      "Epoch: 025, Runtime 0.101086, Loss 2.000101, forward nfe 2548, backward nfe 1300, Train: 0.3000, Val: 0.1800, Test: 0.1716\n",
      "Epoch: 026, Runtime 0.098198, Loss 1.993762, forward nfe 2652, backward nfe 1352, Train: 0.3000, Val: 0.1815, Test: 0.1771\n",
      "Epoch: 027, Runtime 0.101576, Loss 1.980698, forward nfe 2756, backward nfe 1404, Train: 0.3050, Val: 0.2169, Test: 0.2153\n",
      "Epoch: 028, Runtime 0.098060, Loss 1.959812, forward nfe 2860, backward nfe 1456, Train: 0.2450, Val: 0.2500, Test: 0.2391\n",
      "Epoch: 029, Runtime 0.098004, Loss 1.954602, forward nfe 2964, backward nfe 1508, Train: 0.3100, Val: 0.2069, Test: 0.2094\n",
      "Epoch: 030, Runtime 0.101310, Loss 1.933755, forward nfe 3068, backward nfe 1560, Train: 0.3100, Val: 0.1854, Test: 0.1922\n",
      "Epoch: 031, Runtime 0.098753, Loss 1.928278, forward nfe 3172, backward nfe 1612, Train: 0.3200, Val: 0.2054, Test: 0.2120\n",
      "Epoch: 032, Runtime 0.099339, Loss 1.909140, forward nfe 3276, backward nfe 1664, Train: 0.3050, Val: 0.2669, Test: 0.2573\n",
      "Epoch: 033, Runtime 0.102464, Loss 1.904550, forward nfe 3380, backward nfe 1716, Train: 0.3100, Val: 0.2346, Test: 0.2363\n",
      "Epoch: 034, Runtime 0.099495, Loss 1.884804, forward nfe 3484, backward nfe 1768, Train: 0.3150, Val: 0.2015, Test: 0.2156\n",
      "Epoch: 035, Runtime 0.099573, Loss 1.875033, forward nfe 3588, backward nfe 1820, Train: 0.3250, Val: 0.2169, Test: 0.2262\n",
      "Epoch: 036, Runtime 0.102765, Loss 1.860224, forward nfe 3692, backward nfe 1872, Train: 0.3350, Val: 0.2631, Test: 0.2675\n",
      "Epoch: 037, Runtime 0.101143, Loss 1.845926, forward nfe 3796, backward nfe 1924, Train: 0.3300, Val: 0.2323, Test: 0.2405\n",
      "Epoch: 038, Runtime 0.100169, Loss 1.831750, forward nfe 3900, backward nfe 1976, Train: 0.3650, Val: 0.2185, Test: 0.2361\n",
      "Epoch: 039, Runtime 0.101911, Loss 1.821278, forward nfe 4004, backward nfe 2028, Train: 0.3750, Val: 0.2369, Test: 0.2517\n",
      "Epoch: 040, Runtime 0.097929, Loss 1.806749, forward nfe 4108, backward nfe 2080, Train: 0.3650, Val: 0.2477, Test: 0.2594\n",
      "Epoch: 041, Runtime 0.103501, Loss 1.797424, forward nfe 4212, backward nfe 2132, Train: 0.3700, Val: 0.2308, Test: 0.2493\n",
      "Epoch: 042, Runtime 0.100642, Loss 1.777349, forward nfe 4316, backward nfe 2184, Train: 0.3600, Val: 0.2046, Test: 0.2282\n",
      "Epoch: 043, Runtime 0.100928, Loss 1.772557, forward nfe 4420, backward nfe 2236, Train: 0.3800, Val: 0.2200, Test: 0.2405\n",
      "Epoch: 044, Runtime 0.100466, Loss 1.756606, forward nfe 4524, backward nfe 2288, Train: 0.3800, Val: 0.2285, Test: 0.2454\n",
      "Epoch: 045, Runtime 0.098949, Loss 1.745771, forward nfe 4628, backward nfe 2340, Train: 0.3800, Val: 0.2208, Test: 0.2416\n",
      "Epoch: 046, Runtime 0.098774, Loss 1.733292, forward nfe 4732, backward nfe 2392, Train: 0.3900, Val: 0.2200, Test: 0.2354\n",
      "Epoch: 047, Runtime 0.098168, Loss 1.723361, forward nfe 4836, backward nfe 2444, Train: 0.3950, Val: 0.2200, Test: 0.2378\n",
      "Epoch: 048, Runtime 0.102542, Loss 1.714051, forward nfe 4940, backward nfe 2496, Train: 0.3900, Val: 0.2300, Test: 0.2469\n",
      "Epoch: 049, Runtime 0.101264, Loss 1.691770, forward nfe 5044, backward nfe 2548, Train: 0.4000, Val: 0.2385, Test: 0.2549\n",
      "Epoch: 050, Runtime 0.100488, Loss 1.686092, forward nfe 5148, backward nfe 2600, Train: 0.4050, Val: 0.2423, Test: 0.2583\n",
      "Epoch: 051, Runtime 0.103045, Loss 1.671651, forward nfe 5252, backward nfe 2652, Train: 0.4200, Val: 0.2546, Test: 0.2672\n",
      "Epoch: 052, Runtime 0.100096, Loss 1.661069, forward nfe 5356, backward nfe 2704, Train: 0.4100, Val: 0.2523, Test: 0.2684\n",
      "Epoch: 053, Runtime 0.101314, Loss 1.654315, forward nfe 5460, backward nfe 2756, Train: 0.4200, Val: 0.2531, Test: 0.2684\n",
      "Epoch: 054, Runtime 0.098651, Loss 1.636200, forward nfe 5564, backward nfe 2808, Train: 0.4200, Val: 0.2538, Test: 0.2702\n",
      "Epoch: 055, Runtime 0.097286, Loss 1.630476, forward nfe 5668, backward nfe 2860, Train: 0.4100, Val: 0.2554, Test: 0.2717\n",
      "Epoch: 056, Runtime 0.098255, Loss 1.619917, forward nfe 5772, backward nfe 2912, Train: 0.4200, Val: 0.2554, Test: 0.2738\n",
      "Epoch: 057, Runtime 0.103122, Loss 1.602355, forward nfe 5876, backward nfe 2964, Train: 0.4150, Val: 0.2562, Test: 0.2746\n",
      "Epoch: 058, Runtime 0.103262, Loss 1.595763, forward nfe 5980, backward nfe 3016, Train: 0.4150, Val: 0.2577, Test: 0.2754\n",
      "Epoch: 059, Runtime 0.099063, Loss 1.588345, forward nfe 6084, backward nfe 3068, Train: 0.4200, Val: 0.2538, Test: 0.2755\n",
      "Epoch: 060, Runtime 0.101332, Loss 1.574539, forward nfe 6188, backward nfe 3120, Train: 0.4200, Val: 0.2546, Test: 0.2773\n",
      "Epoch: 061, Runtime 0.099858, Loss 1.558330, forward nfe 6292, backward nfe 3172, Train: 0.4200, Val: 0.2608, Test: 0.2791\n",
      "Epoch: 062, Runtime 0.099200, Loss 1.551358, forward nfe 6396, backward nfe 3224, Train: 0.4150, Val: 0.2623, Test: 0.2803\n",
      "Epoch: 063, Runtime 0.101013, Loss 1.541676, forward nfe 6500, backward nfe 3276, Train: 0.4250, Val: 0.2608, Test: 0.2800\n",
      "Epoch: 064, Runtime 0.100451, Loss 1.539150, forward nfe 6604, backward nfe 3328, Train: 0.4350, Val: 0.2608, Test: 0.2823\n",
      "Epoch: 065, Runtime 0.100602, Loss 1.526480, forward nfe 6708, backward nfe 3380, Train: 0.4350, Val: 0.2669, Test: 0.2857\n",
      "Epoch: 066, Runtime 0.099502, Loss 1.508173, forward nfe 6812, backward nfe 3432, Train: 0.4300, Val: 0.2654, Test: 0.2858\n",
      "Epoch: 067, Runtime 0.103390, Loss 1.512809, forward nfe 6916, backward nfe 3484, Train: 0.4250, Val: 0.2631, Test: 0.2851\n",
      "Epoch: 068, Runtime 0.100632, Loss 1.493949, forward nfe 7020, backward nfe 3536, Train: 0.4200, Val: 0.2623, Test: 0.2826\n",
      "Epoch: 069, Runtime 0.101364, Loss 1.494988, forward nfe 7124, backward nfe 3588, Train: 0.4250, Val: 0.2469, Test: 0.2707\n",
      "Epoch: 070, Runtime 0.101448, Loss 1.478552, forward nfe 7228, backward nfe 3640, Train: 0.4250, Val: 0.2346, Test: 0.2609\n",
      "Epoch: 071, Runtime 0.102903, Loss 1.477613, forward nfe 7332, backward nfe 3692, Train: 0.4250, Val: 0.2185, Test: 0.2468\n",
      "Epoch: 072, Runtime 0.100026, Loss 1.460593, forward nfe 7436, backward nfe 3744, Train: 0.4300, Val: 0.2046, Test: 0.2354\n",
      "Epoch: 073, Runtime 0.099287, Loss 1.458814, forward nfe 7540, backward nfe 3796, Train: 0.4250, Val: 0.1908, Test: 0.2238\n",
      "Epoch: 074, Runtime 0.102333, Loss 1.455971, forward nfe 7644, backward nfe 3848, Train: 0.4350, Val: 0.1846, Test: 0.2233\n",
      "Epoch: 075, Runtime 0.102576, Loss 1.442250, forward nfe 7748, backward nfe 3900, Train: 0.4400, Val: 0.1862, Test: 0.2263\n",
      "Epoch: 076, Runtime 0.099328, Loss 1.445378, forward nfe 7852, backward nfe 3952, Train: 0.4450, Val: 0.1900, Test: 0.2291\n",
      "Epoch: 077, Runtime 0.103058, Loss 1.430814, forward nfe 7956, backward nfe 4004, Train: 0.4400, Val: 0.1923, Test: 0.2311\n",
      "Epoch: 078, Runtime 0.103243, Loss 1.404797, forward nfe 8060, backward nfe 4056, Train: 0.4650, Val: 0.1915, Test: 0.2320\n",
      "Epoch: 079, Runtime 0.099464, Loss 1.416252, forward nfe 8164, backward nfe 4108, Train: 0.4800, Val: 0.1954, Test: 0.2393\n",
      "Epoch: 080, Runtime 0.099542, Loss 1.378568, forward nfe 8268, backward nfe 4160, Train: 0.4800, Val: 0.2200, Test: 0.2722\n",
      "Epoch: 081, Runtime 0.102076, Loss 1.351110, forward nfe 8372, backward nfe 4212, Train: 0.4850, Val: 0.2631, Test: 0.3011\n",
      "Epoch: 082, Runtime 0.100890, Loss 1.299373, forward nfe 8476, backward nfe 4264, Train: 0.5050, Val: 0.3077, Test: 0.3296\n",
      "Epoch: 083, Runtime 0.099528, Loss 1.270614, forward nfe 8580, backward nfe 4316, Train: 0.5450, Val: 0.2669, Test: 0.2999\n",
      "Epoch: 084, Runtime 0.102967, Loss 1.231842, forward nfe 8684, backward nfe 4368, Train: 0.5700, Val: 0.2908, Test: 0.3257\n",
      "Epoch: 085, Runtime 0.102767, Loss 1.230372, forward nfe 8788, backward nfe 4420, Train: 0.5500, Val: 0.3054, Test: 0.3284\n",
      "Epoch: 086, Runtime 0.099857, Loss 1.217202, forward nfe 8892, backward nfe 4472, Train: 0.5950, Val: 0.3492, Test: 0.3799\n",
      "Epoch: 087, Runtime 0.101556, Loss 1.183674, forward nfe 8996, backward nfe 4524, Train: 0.6150, Val: 0.3508, Test: 0.3851\n",
      "Epoch: 088, Runtime 0.102769, Loss 1.150235, forward nfe 9100, backward nfe 4576, Train: 0.6000, Val: 0.3185, Test: 0.3555\n",
      "Epoch: 089, Runtime 0.103155, Loss 1.133544, forward nfe 9204, backward nfe 4628, Train: 0.6100, Val: 0.3792, Test: 0.4131\n",
      "Epoch: 090, Runtime 0.101155, Loss 1.097881, forward nfe 9308, backward nfe 4680, Train: 0.6500, Val: 0.3592, Test: 0.3970\n",
      "Epoch: 091, Runtime 0.100872, Loss 1.084670, forward nfe 9412, backward nfe 4732, Train: 0.6950, Val: 0.4138, Test: 0.4425\n",
      "Epoch: 092, Runtime 0.098783, Loss 1.047220, forward nfe 9516, backward nfe 4784, Train: 0.7150, Val: 0.4046, Test: 0.4376\n",
      "Epoch: 093, Runtime 0.098596, Loss 1.018427, forward nfe 9620, backward nfe 4836, Train: 0.7100, Val: 0.3738, Test: 0.4039\n",
      "Epoch: 094, Runtime 0.103324, Loss 1.018984, forward nfe 9724, backward nfe 4888, Train: 0.7250, Val: 0.3985, Test: 0.4340\n",
      "Epoch: 095, Runtime 0.102070, Loss 0.984567, forward nfe 9828, backward nfe 4940, Train: 0.7500, Val: 0.4208, Test: 0.4534\n",
      "Epoch: 096, Runtime 0.098042, Loss 0.940101, forward nfe 9932, backward nfe 4992, Train: 0.7400, Val: 0.4231, Test: 0.4603\n",
      "Epoch: 097, Runtime 0.100484, Loss 0.931012, forward nfe 10036, backward nfe 5044, Train: 0.7300, Val: 0.4262, Test: 0.4674\n",
      "Epoch: 098, Runtime 0.102869, Loss 0.899889, forward nfe 10140, backward nfe 5096, Train: 0.7300, Val: 0.4469, Test: 0.4856\n",
      "Epoch: 099, Runtime 0.102621, Loss 0.885548, forward nfe 10244, backward nfe 5148, Train: 0.7550, Val: 0.4500, Test: 0.4845\n",
      "best val accuracy 0.450000 with test accuracy 0.484492 at epoch 99\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.101274, Loss 2.308203, forward nfe 52, backward nfe 52, Train: 0.1000, Val: 0.0562, Test: 0.0589\n",
      "Epoch: 002, Runtime 0.101830, Loss 2.767886, forward nfe 156, backward nfe 104, Train: 0.0850, Val: 0.0146, Test: 0.0214\n",
      "Epoch: 003, Runtime 0.101709, Loss 2.378495, forward nfe 260, backward nfe 156, Train: 0.1400, Val: 0.1092, Test: 0.1091\n",
      "Epoch: 004, Runtime 0.097600, Loss 2.298858, forward nfe 364, backward nfe 208, Train: 0.1950, Val: 0.1823, Test: 0.1743\n",
      "Epoch: 005, Runtime 0.100902, Loss 2.249714, forward nfe 468, backward nfe 260, Train: 0.1250, Val: 0.0954, Test: 0.0953\n",
      "Epoch: 006, Runtime 0.101486, Loss 2.243479, forward nfe 572, backward nfe 312, Train: 0.1350, Val: 0.0723, Test: 0.0807\n",
      "Epoch: 007, Runtime 0.099862, Loss 2.243607, forward nfe 676, backward nfe 364, Train: 0.1700, Val: 0.1038, Test: 0.1035\n",
      "Epoch: 008, Runtime 0.102844, Loss 2.215208, forward nfe 780, backward nfe 416, Train: 0.2150, Val: 0.1385, Test: 0.1450\n",
      "Epoch: 009, Runtime 0.103209, Loss 2.204637, forward nfe 884, backward nfe 468, Train: 0.2750, Val: 0.1631, Test: 0.1663\n",
      "Epoch: 010, Runtime 0.098654, Loss 2.180828, forward nfe 988, backward nfe 520, Train: 0.1900, Val: 0.0931, Test: 0.1015\n",
      "Epoch: 011, Runtime 0.101705, Loss 2.211068, forward nfe 1092, backward nfe 572, Train: 0.1850, Val: 0.1085, Test: 0.1153\n",
      "Epoch: 012, Runtime 0.102842, Loss 2.156744, forward nfe 1196, backward nfe 624, Train: 0.2800, Val: 0.1785, Test: 0.1770\n",
      "Epoch: 013, Runtime 0.098460, Loss 2.146069, forward nfe 1300, backward nfe 676, Train: 0.3000, Val: 0.2154, Test: 0.2118\n",
      "Epoch: 014, Runtime 0.099398, Loss 2.134584, forward nfe 1404, backward nfe 728, Train: 0.3550, Val: 0.2200, Test: 0.2350\n",
      "Epoch: 015, Runtime 0.102921, Loss 2.091260, forward nfe 1508, backward nfe 780, Train: 0.2350, Val: 0.1438, Test: 0.1516\n",
      "Epoch: 016, Runtime 0.100643, Loss 2.084021, forward nfe 1612, backward nfe 832, Train: 0.2500, Val: 0.1369, Test: 0.1501\n",
      "Epoch: 017, Runtime 0.097714, Loss 2.064908, forward nfe 1716, backward nfe 884, Train: 0.3350, Val: 0.2146, Test: 0.2289\n",
      "Epoch: 018, Runtime 0.100153, Loss 2.031520, forward nfe 1820, backward nfe 936, Train: 0.3400, Val: 0.2562, Test: 0.2572\n",
      "Epoch: 019, Runtime 0.098679, Loss 2.012229, forward nfe 1924, backward nfe 988, Train: 0.4050, Val: 0.2846, Test: 0.2875\n",
      "Epoch: 020, Runtime 0.097639, Loss 1.986920, forward nfe 2028, backward nfe 1040, Train: 0.4050, Val: 0.2746, Test: 0.2768\n",
      "Epoch: 021, Runtime 0.103208, Loss 1.962111, forward nfe 2132, backward nfe 1092, Train: 0.3500, Val: 0.2362, Test: 0.2417\n",
      "Epoch: 022, Runtime 0.103437, Loss 1.943580, forward nfe 2236, backward nfe 1144, Train: 0.3550, Val: 0.2392, Test: 0.2427\n",
      "Epoch: 023, Runtime 0.102412, Loss 1.920948, forward nfe 2340, backward nfe 1196, Train: 0.4150, Val: 0.2654, Test: 0.2761\n",
      "Epoch: 024, Runtime 0.098820, Loss 1.886607, forward nfe 2444, backward nfe 1248, Train: 0.4150, Val: 0.2923, Test: 0.2954\n",
      "Epoch: 025, Runtime 0.100106, Loss 1.863499, forward nfe 2548, backward nfe 1300, Train: 0.4400, Val: 0.3269, Test: 0.3254\n",
      "Epoch: 026, Runtime 0.097990, Loss 1.836059, forward nfe 2652, backward nfe 1352, Train: 0.4550, Val: 0.3346, Test: 0.3375\n",
      "Epoch: 027, Runtime 0.099651, Loss 1.802604, forward nfe 2756, backward nfe 1404, Train: 0.4350, Val: 0.2992, Test: 0.3093\n",
      "Epoch: 028, Runtime 0.101251, Loss 1.783554, forward nfe 2860, backward nfe 1456, Train: 0.4350, Val: 0.3015, Test: 0.3110\n",
      "Epoch: 029, Runtime 0.101525, Loss 1.755821, forward nfe 2964, backward nfe 1508, Train: 0.4400, Val: 0.3246, Test: 0.3258\n",
      "Epoch: 030, Runtime 0.097567, Loss 1.725619, forward nfe 3068, backward nfe 1560, Train: 0.4300, Val: 0.3346, Test: 0.3329\n",
      "Epoch: 031, Runtime 0.103208, Loss 1.701953, forward nfe 3172, backward nfe 1612, Train: 0.4850, Val: 0.3338, Test: 0.3390\n",
      "Epoch: 032, Runtime 0.101427, Loss 1.672103, forward nfe 3276, backward nfe 1664, Train: 0.5150, Val: 0.3215, Test: 0.3381\n",
      "Epoch: 033, Runtime 0.097224, Loss 1.652504, forward nfe 3380, backward nfe 1716, Train: 0.5150, Val: 0.3169, Test: 0.3381\n",
      "Epoch: 034, Runtime 0.098267, Loss 1.621478, forward nfe 3484, backward nfe 1768, Train: 0.5050, Val: 0.3231, Test: 0.3395\n",
      "Epoch: 035, Runtime 0.102780, Loss 1.600347, forward nfe 3588, backward nfe 1820, Train: 0.5050, Val: 0.3262, Test: 0.3377\n",
      "Epoch: 036, Runtime 0.101978, Loss 1.577830, forward nfe 3692, backward nfe 1872, Train: 0.5200, Val: 0.3100, Test: 0.3324\n",
      "Epoch: 037, Runtime 0.100067, Loss 1.550554, forward nfe 3796, backward nfe 1924, Train: 0.5450, Val: 0.3054, Test: 0.3326\n",
      "Epoch: 038, Runtime 0.100018, Loss 1.524286, forward nfe 3900, backward nfe 1976, Train: 0.5350, Val: 0.3015, Test: 0.3302\n",
      "Epoch: 039, Runtime 0.102837, Loss 1.500120, forward nfe 4004, backward nfe 2028, Train: 0.5550, Val: 0.3062, Test: 0.3334\n",
      "Epoch: 040, Runtime 0.103220, Loss 1.473383, forward nfe 4108, backward nfe 2080, Train: 0.5600, Val: 0.3146, Test: 0.3376\n",
      "Epoch: 041, Runtime 0.096398, Loss 1.448209, forward nfe 4212, backward nfe 2132, Train: 0.5600, Val: 0.3115, Test: 0.3370\n",
      "Epoch: 042, Runtime 0.099957, Loss 1.434108, forward nfe 4316, backward nfe 2184, Train: 0.5700, Val: 0.3085, Test: 0.3363\n",
      "Epoch: 043, Runtime 0.100070, Loss 1.399615, forward nfe 4420, backward nfe 2236, Train: 0.6050, Val: 0.3085, Test: 0.3413\n",
      "Epoch: 044, Runtime 0.099755, Loss 1.379418, forward nfe 4524, backward nfe 2288, Train: 0.6050, Val: 0.3023, Test: 0.3416\n",
      "Epoch: 045, Runtime 0.102315, Loss 1.359384, forward nfe 4628, backward nfe 2340, Train: 0.5950, Val: 0.3100, Test: 0.3429\n",
      "Epoch: 046, Runtime 0.100089, Loss 1.330932, forward nfe 4732, backward nfe 2392, Train: 0.6050, Val: 0.3169, Test: 0.3473\n",
      "Epoch: 047, Runtime 0.098457, Loss 1.313368, forward nfe 4836, backward nfe 2444, Train: 0.6400, Val: 0.3215, Test: 0.3577\n",
      "Epoch: 048, Runtime 0.099406, Loss 1.288336, forward nfe 4940, backward nfe 2496, Train: 0.6300, Val: 0.3223, Test: 0.3599\n",
      "Epoch: 049, Runtime 0.100178, Loss 1.269544, forward nfe 5044, backward nfe 2548, Train: 0.6250, Val: 0.3215, Test: 0.3599\n",
      "Epoch: 050, Runtime 0.099281, Loss 1.244902, forward nfe 5148, backward nfe 2600, Train: 0.6350, Val: 0.3246, Test: 0.3628\n",
      "Epoch: 051, Runtime 0.099794, Loss 1.225632, forward nfe 5252, backward nfe 2652, Train: 0.6600, Val: 0.3323, Test: 0.3716\n",
      "Epoch: 052, Runtime 0.102244, Loss 1.204541, forward nfe 5356, backward nfe 2704, Train: 0.6700, Val: 0.3408, Test: 0.3782\n",
      "Epoch: 053, Runtime 0.097935, Loss 1.193061, forward nfe 5460, backward nfe 2756, Train: 0.6850, Val: 0.3423, Test: 0.3839\n",
      "Epoch: 054, Runtime 0.098924, Loss 1.163288, forward nfe 5564, backward nfe 2808, Train: 0.6800, Val: 0.3562, Test: 0.3928\n",
      "Epoch: 055, Runtime 0.103017, Loss 1.145997, forward nfe 5668, backward nfe 2860, Train: 0.6950, Val: 0.3708, Test: 0.4021\n",
      "Epoch: 056, Runtime 0.100439, Loss 1.134895, forward nfe 5772, backward nfe 2912, Train: 0.7050, Val: 0.3808, Test: 0.4126\n",
      "Epoch: 057, Runtime 0.098694, Loss 1.114184, forward nfe 5876, backward nfe 2964, Train: 0.6950, Val: 0.3954, Test: 0.4144\n",
      "Epoch: 058, Runtime 0.098717, Loss 1.105769, forward nfe 5980, backward nfe 3016, Train: 0.7000, Val: 0.3962, Test: 0.4208\n",
      "Epoch: 059, Runtime 0.101876, Loss 1.085711, forward nfe 6084, backward nfe 3068, Train: 0.7000, Val: 0.4031, Test: 0.4337\n",
      "Epoch: 060, Runtime 0.097003, Loss 1.069787, forward nfe 6188, backward nfe 3120, Train: 0.7050, Val: 0.4146, Test: 0.4416\n",
      "Epoch: 061, Runtime 0.102682, Loss 1.051148, forward nfe 6292, backward nfe 3172, Train: 0.7150, Val: 0.4162, Test: 0.4487\n",
      "Epoch: 062, Runtime 0.100381, Loss 1.035601, forward nfe 6396, backward nfe 3224, Train: 0.6950, Val: 0.4192, Test: 0.4485\n",
      "Epoch: 063, Runtime 0.099023, Loss 1.030380, forward nfe 6500, backward nfe 3276, Train: 0.7050, Val: 0.4185, Test: 0.4496\n",
      "Epoch: 064, Runtime 0.098583, Loss 1.010603, forward nfe 6604, backward nfe 3328, Train: 0.7200, Val: 0.4262, Test: 0.4544\n",
      "Epoch: 065, Runtime 0.098478, Loss 0.990827, forward nfe 6708, backward nfe 3380, Train: 0.7250, Val: 0.4331, Test: 0.4629\n",
      "Epoch: 066, Runtime 0.100322, Loss 0.979034, forward nfe 6812, backward nfe 3432, Train: 0.7250, Val: 0.4338, Test: 0.4644\n",
      "Epoch: 067, Runtime 0.099616, Loss 0.969384, forward nfe 6916, backward nfe 3484, Train: 0.7250, Val: 0.4323, Test: 0.4673\n",
      "Epoch: 068, Runtime 0.099828, Loss 0.952112, forward nfe 7020, backward nfe 3536, Train: 0.7300, Val: 0.4300, Test: 0.4695\n",
      "Epoch: 069, Runtime 0.102276, Loss 0.942385, forward nfe 7124, backward nfe 3588, Train: 0.7250, Val: 0.4400, Test: 0.4750\n",
      "Epoch: 070, Runtime 0.102873, Loss 0.918568, forward nfe 7228, backward nfe 3640, Train: 0.7300, Val: 0.4515, Test: 0.4841\n",
      "Epoch: 071, Runtime 0.097526, Loss 0.909163, forward nfe 7332, backward nfe 3692, Train: 0.7300, Val: 0.4631, Test: 0.4925\n",
      "Epoch: 072, Runtime 0.100959, Loss 0.900969, forward nfe 7436, backward nfe 3744, Train: 0.7300, Val: 0.4723, Test: 0.5012\n",
      "Epoch: 073, Runtime 0.102590, Loss 0.895245, forward nfe 7540, backward nfe 3796, Train: 0.7200, Val: 0.4792, Test: 0.5060\n",
      "Epoch: 074, Runtime 0.100234, Loss 0.872047, forward nfe 7644, backward nfe 3848, Train: 0.7300, Val: 0.4938, Test: 0.5158\n",
      "Epoch: 075, Runtime 0.097591, Loss 0.859368, forward nfe 7748, backward nfe 3900, Train: 0.7200, Val: 0.4992, Test: 0.5259\n",
      "Epoch: 076, Runtime 0.097375, Loss 0.847756, forward nfe 7852, backward nfe 3952, Train: 0.7300, Val: 0.5085, Test: 0.5320\n",
      "Epoch: 077, Runtime 0.099985, Loss 0.836334, forward nfe 7956, backward nfe 4004, Train: 0.7400, Val: 0.5092, Test: 0.5391\n",
      "Epoch: 078, Runtime 0.098089, Loss 0.834371, forward nfe 8060, backward nfe 4056, Train: 0.7250, Val: 0.5331, Test: 0.5541\n",
      "Epoch: 079, Runtime 0.102157, Loss 0.832623, forward nfe 8164, backward nfe 4108, Train: 0.7300, Val: 0.5531, Test: 0.5761\n",
      "Epoch: 080, Runtime 0.099463, Loss 0.797677, forward nfe 8268, backward nfe 4160, Train: 0.7450, Val: 0.5769, Test: 0.5915\n",
      "Epoch: 081, Runtime 0.103168, Loss 0.788891, forward nfe 8372, backward nfe 4212, Train: 0.7450, Val: 0.5746, Test: 0.5997\n",
      "Epoch: 082, Runtime 0.100842, Loss 0.777668, forward nfe 8476, backward nfe 4264, Train: 0.7650, Val: 0.5815, Test: 0.6120\n",
      "Epoch: 083, Runtime 0.099480, Loss 0.778415, forward nfe 8580, backward nfe 4316, Train: 0.7700, Val: 0.6085, Test: 0.6284\n",
      "Epoch: 084, Runtime 0.099533, Loss 0.764057, forward nfe 8684, backward nfe 4368, Train: 0.7550, Val: 0.6208, Test: 0.6390\n",
      "Epoch: 085, Runtime 0.098572, Loss 0.758116, forward nfe 8788, backward nfe 4420, Train: 0.7650, Val: 0.6315, Test: 0.6502\n",
      "Epoch: 086, Runtime 0.102474, Loss 0.739267, forward nfe 8892, backward nfe 4472, Train: 0.7650, Val: 0.6385, Test: 0.6560\n",
      "Epoch: 087, Runtime 0.100459, Loss 0.723184, forward nfe 8996, backward nfe 4524, Train: 0.7700, Val: 0.6338, Test: 0.6543\n",
      "Epoch: 088, Runtime 0.101172, Loss 0.712386, forward nfe 9100, backward nfe 4576, Train: 0.8100, Val: 0.6392, Test: 0.6562\n",
      "Epoch: 089, Runtime 0.101410, Loss 0.696963, forward nfe 9204, backward nfe 4628, Train: 0.8100, Val: 0.6554, Test: 0.6707\n",
      "Epoch: 090, Runtime 0.097201, Loss 0.679178, forward nfe 9308, backward nfe 4680, Train: 0.8050, Val: 0.6569, Test: 0.6745\n",
      "Epoch: 091, Runtime 0.100921, Loss 0.670212, forward nfe 9412, backward nfe 4732, Train: 0.8000, Val: 0.6692, Test: 0.6810\n",
      "Epoch: 092, Runtime 0.097415, Loss 0.663868, forward nfe 9516, backward nfe 4784, Train: 0.8200, Val: 0.6638, Test: 0.6750\n",
      "Epoch: 093, Runtime 0.101164, Loss 0.650692, forward nfe 9620, backward nfe 4836, Train: 0.8250, Val: 0.6585, Test: 0.6842\n",
      "Epoch: 094, Runtime 0.100598, Loss 0.634706, forward nfe 9724, backward nfe 4888, Train: 0.8250, Val: 0.6738, Test: 0.6979\n",
      "Epoch: 095, Runtime 0.099383, Loss 0.629217, forward nfe 9828, backward nfe 4940, Train: 0.8200, Val: 0.6885, Test: 0.7139\n",
      "Epoch: 096, Runtime 0.099366, Loss 0.620517, forward nfe 9932, backward nfe 4992, Train: 0.8050, Val: 0.7131, Test: 0.7280\n",
      "Epoch: 097, Runtime 0.103194, Loss 0.599532, forward nfe 10036, backward nfe 5044, Train: 0.8250, Val: 0.7069, Test: 0.7296\n",
      "Epoch: 098, Runtime 0.100909, Loss 0.587510, forward nfe 10140, backward nfe 5096, Train: 0.8300, Val: 0.7008, Test: 0.7231\n",
      "Epoch: 099, Runtime 0.097978, Loss 0.584909, forward nfe 10244, backward nfe 5148, Train: 0.8300, Val: 0.6962, Test: 0.7231\n",
      "best val accuracy 0.713077 with test accuracy 0.728044 at epoch 96\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.100987, Loss 2.299140, forward nfe 52, backward nfe 52, Train: 0.1050, Val: 0.0400, Test: 0.0457\n",
      "Epoch: 002, Runtime 0.100643, Loss 2.501889, forward nfe 156, backward nfe 104, Train: 0.1250, Val: 0.0246, Test: 0.0389\n",
      "Epoch: 003, Runtime 0.102561, Loss 2.590731, forward nfe 260, backward nfe 156, Train: 0.1200, Val: 0.0569, Test: 0.0641\n",
      "Epoch: 004, Runtime 0.090252, Loss 2.367446, forward nfe 364, backward nfe 208, Train: 0.1200, Val: 0.1623, Test: 0.1468\n",
      "Epoch: 005, Runtime 0.096422, Loss 2.334546, forward nfe 468, backward nfe 260, Train: 0.1500, Val: 0.1154, Test: 0.1155\n",
      "Epoch: 006, Runtime 0.100096, Loss 2.304529, forward nfe 572, backward nfe 312, Train: 0.1650, Val: 0.0962, Test: 0.1010\n",
      "Epoch: 007, Runtime 0.093750, Loss 2.315267, forward nfe 676, backward nfe 364, Train: 0.1600, Val: 0.1000, Test: 0.1036\n",
      "Epoch: 008, Runtime 0.097358, Loss 2.296507, forward nfe 780, backward nfe 416, Train: 0.1400, Val: 0.1262, Test: 0.1204\n",
      "Epoch: 009, Runtime 0.102588, Loss 2.266961, forward nfe 884, backward nfe 468, Train: 0.2200, Val: 0.1646, Test: 0.1596\n",
      "Epoch: 010, Runtime 0.093603, Loss 2.244269, forward nfe 988, backward nfe 520, Train: 0.1900, Val: 0.0562, Test: 0.0632\n",
      "Epoch: 011, Runtime 0.094075, Loss 2.249910, forward nfe 1092, backward nfe 572, Train: 0.2000, Val: 0.0769, Test: 0.0834\n",
      "Epoch: 012, Runtime 0.097473, Loss 2.237458, forward nfe 1196, backward nfe 624, Train: 0.2350, Val: 0.1023, Test: 0.1072\n",
      "Epoch: 013, Runtime 0.094200, Loss 2.199660, forward nfe 1300, backward nfe 676, Train: 0.2700, Val: 0.1238, Test: 0.1266\n",
      "Epoch: 014, Runtime 0.096427, Loss 2.161072, forward nfe 1404, backward nfe 728, Train: 0.2750, Val: 0.1300, Test: 0.1426\n",
      "Epoch: 015, Runtime 0.097241, Loss 2.148669, forward nfe 1508, backward nfe 780, Train: 0.2900, Val: 0.1238, Test: 0.1350\n",
      "Epoch: 016, Runtime 0.092839, Loss 2.138244, forward nfe 1612, backward nfe 832, Train: 0.3300, Val: 0.1385, Test: 0.1513\n",
      "Epoch: 017, Runtime 0.093945, Loss 2.120702, forward nfe 1716, backward nfe 884, Train: 0.3300, Val: 0.1577, Test: 0.1721\n",
      "Epoch: 018, Runtime 0.096329, Loss 2.099551, forward nfe 1820, backward nfe 936, Train: 0.3750, Val: 0.1538, Test: 0.1709\n",
      "Epoch: 019, Runtime 0.099347, Loss 2.073808, forward nfe 1924, backward nfe 988, Train: 0.3750, Val: 0.1531, Test: 0.1700\n",
      "Epoch: 020, Runtime 0.100947, Loss 2.046977, forward nfe 2028, backward nfe 1040, Train: 0.3950, Val: 0.1854, Test: 0.2023\n",
      "Epoch: 021, Runtime 0.096974, Loss 2.020802, forward nfe 2132, backward nfe 1092, Train: 0.3900, Val: 0.2123, Test: 0.2218\n",
      "Epoch: 022, Runtime 0.091568, Loss 1.995280, forward nfe 2236, backward nfe 1144, Train: 0.4200, Val: 0.2685, Test: 0.2824\n",
      "Epoch: 023, Runtime 0.093834, Loss 1.972747, forward nfe 2340, backward nfe 1196, Train: 0.4250, Val: 0.2592, Test: 0.2667\n",
      "Epoch: 024, Runtime 0.099275, Loss 1.943186, forward nfe 2444, backward nfe 1248, Train: 0.4250, Val: 0.3523, Test: 0.3448\n",
      "Epoch: 025, Runtime 0.101843, Loss 1.914960, forward nfe 2548, backward nfe 1300, Train: 0.4400, Val: 0.5092, Test: 0.5015\n",
      "Epoch: 026, Runtime 0.092118, Loss 1.892583, forward nfe 2652, backward nfe 1352, Train: 0.4500, Val: 0.4808, Test: 0.4700\n",
      "Epoch: 027, Runtime 0.098510, Loss 1.865215, forward nfe 2756, backward nfe 1404, Train: 0.4800, Val: 0.4915, Test: 0.4839\n",
      "Epoch: 028, Runtime 0.098469, Loss 1.834294, forward nfe 2860, backward nfe 1456, Train: 0.5100, Val: 0.5238, Test: 0.5246\n",
      "Epoch: 029, Runtime 0.091068, Loss 1.804988, forward nfe 2964, backward nfe 1508, Train: 0.4850, Val: 0.4785, Test: 0.4822\n",
      "Epoch: 030, Runtime 0.100738, Loss 1.772254, forward nfe 3068, backward nfe 1560, Train: 0.5050, Val: 0.4938, Test: 0.4920\n",
      "Epoch: 031, Runtime 0.103260, Loss 1.738527, forward nfe 3172, backward nfe 1612, Train: 0.5050, Val: 0.5808, Test: 0.5701\n",
      "Epoch: 032, Runtime 0.093198, Loss 1.707386, forward nfe 3276, backward nfe 1664, Train: 0.5800, Val: 0.5869, Test: 0.5839\n",
      "Epoch: 033, Runtime 0.099433, Loss 1.672961, forward nfe 3380, backward nfe 1716, Train: 0.5800, Val: 0.5715, Test: 0.5773\n",
      "Epoch: 034, Runtime 0.091261, Loss 1.646361, forward nfe 3484, backward nfe 1768, Train: 0.5850, Val: 0.5600, Test: 0.5699\n",
      "Epoch: 035, Runtime 0.100513, Loss 1.611477, forward nfe 3588, backward nfe 1820, Train: 0.5950, Val: 0.6238, Test: 0.6223\n",
      "Epoch: 036, Runtime 0.091361, Loss 1.581208, forward nfe 3692, backward nfe 1872, Train: 0.5650, Val: 0.6623, Test: 0.6498\n",
      "Epoch: 037, Runtime 0.093123, Loss 1.548895, forward nfe 3796, backward nfe 1924, Train: 0.5900, Val: 0.6546, Test: 0.6443\n",
      "Epoch: 038, Runtime 0.100176, Loss 1.512721, forward nfe 3900, backward nfe 1976, Train: 0.5850, Val: 0.6800, Test: 0.6637\n",
      "Epoch: 039, Runtime 0.099029, Loss 1.486931, forward nfe 4004, backward nfe 2028, Train: 0.5950, Val: 0.6831, Test: 0.6673\n",
      "Epoch: 040, Runtime 0.092014, Loss 1.460039, forward nfe 4108, backward nfe 2080, Train: 0.6000, Val: 0.6785, Test: 0.6672\n",
      "Epoch: 041, Runtime 0.093455, Loss 1.423748, forward nfe 4212, backward nfe 2132, Train: 0.6300, Val: 0.6800, Test: 0.6649\n",
      "Epoch: 042, Runtime 0.097864, Loss 1.382996, forward nfe 4316, backward nfe 2184, Train: 0.6000, Val: 0.6623, Test: 0.6533\n",
      "Epoch: 043, Runtime 0.091301, Loss 1.374945, forward nfe 4420, backward nfe 2236, Train: 0.6500, Val: 0.6554, Test: 0.6490\n",
      "Epoch: 044, Runtime 0.100415, Loss 1.323262, forward nfe 4524, backward nfe 2288, Train: 0.6750, Val: 0.6500, Test: 0.6499\n",
      "Epoch: 045, Runtime 0.103115, Loss 1.290330, forward nfe 4628, backward nfe 2340, Train: 0.6900, Val: 0.6969, Test: 0.6993\n",
      "Epoch: 046, Runtime 0.089642, Loss 1.265155, forward nfe 4732, backward nfe 2392, Train: 0.6950, Val: 0.7046, Test: 0.7093\n",
      "Epoch: 047, Runtime 0.097823, Loss 1.227094, forward nfe 4836, backward nfe 2444, Train: 0.7000, Val: 0.7015, Test: 0.6968\n",
      "Epoch: 048, Runtime 0.096184, Loss 1.191672, forward nfe 4940, backward nfe 2496, Train: 0.6900, Val: 0.6554, Test: 0.6578\n",
      "Epoch: 049, Runtime 0.092840, Loss 1.170208, forward nfe 5044, backward nfe 2548, Train: 0.7400, Val: 0.6885, Test: 0.6929\n",
      "Epoch: 050, Runtime 0.092356, Loss 1.127656, forward nfe 5148, backward nfe 2600, Train: 0.7400, Val: 0.6946, Test: 0.7096\n",
      "Epoch: 051, Runtime 0.094077, Loss 1.096783, forward nfe 5252, backward nfe 2652, Train: 0.7600, Val: 0.6823, Test: 0.7092\n",
      "Epoch: 052, Runtime 0.091572, Loss 1.066325, forward nfe 5356, backward nfe 2704, Train: 0.7550, Val: 0.6900, Test: 0.7026\n",
      "Epoch: 053, Runtime 0.095798, Loss 1.037327, forward nfe 5460, backward nfe 2756, Train: 0.7350, Val: 0.7146, Test: 0.7292\n",
      "Epoch: 054, Runtime 0.099157, Loss 1.008835, forward nfe 5564, backward nfe 2808, Train: 0.7350, Val: 0.7285, Test: 0.7284\n",
      "Epoch: 055, Runtime 0.102848, Loss 0.981631, forward nfe 5668, backward nfe 2860, Train: 0.7450, Val: 0.7285, Test: 0.7338\n",
      "Epoch: 056, Runtime 0.097994, Loss 0.961938, forward nfe 5772, backward nfe 2912, Train: 0.7700, Val: 0.6877, Test: 0.7071\n",
      "Epoch: 057, Runtime 0.092446, Loss 0.925482, forward nfe 5876, backward nfe 2964, Train: 0.7800, Val: 0.6762, Test: 0.7005\n",
      "Epoch: 058, Runtime 0.097769, Loss 0.905438, forward nfe 5980, backward nfe 3016, Train: 0.8000, Val: 0.7038, Test: 0.7245\n",
      "Epoch: 059, Runtime 0.102805, Loss 0.883946, forward nfe 6084, backward nfe 3068, Train: 0.7850, Val: 0.7277, Test: 0.7454\n",
      "Epoch: 060, Runtime 0.093159, Loss 0.855509, forward nfe 6188, backward nfe 3120, Train: 0.7800, Val: 0.6792, Test: 0.7013\n",
      "Epoch: 061, Runtime 0.094600, Loss 0.838209, forward nfe 6292, backward nfe 3172, Train: 0.7750, Val: 0.7446, Test: 0.7507\n",
      "Epoch: 062, Runtime 0.095506, Loss 0.816861, forward nfe 6396, backward nfe 3224, Train: 0.8000, Val: 0.7454, Test: 0.7537\n",
      "Epoch: 063, Runtime 0.093091, Loss 0.797613, forward nfe 6500, backward nfe 3276, Train: 0.8100, Val: 0.7285, Test: 0.7472\n",
      "Epoch: 064, Runtime 0.098698, Loss 0.768163, forward nfe 6604, backward nfe 3328, Train: 0.8050, Val: 0.7092, Test: 0.7278\n",
      "Epoch: 065, Runtime 0.099205, Loss 0.761990, forward nfe 6708, backward nfe 3380, Train: 0.8100, Val: 0.7031, Test: 0.7275\n",
      "Epoch: 066, Runtime 0.095726, Loss 0.738915, forward nfe 6812, backward nfe 3432, Train: 0.8200, Val: 0.7277, Test: 0.7492\n",
      "Epoch: 067, Runtime 0.098368, Loss 0.718769, forward nfe 6916, backward nfe 3484, Train: 0.8200, Val: 0.7462, Test: 0.7653\n",
      "Epoch: 068, Runtime 0.097913, Loss 0.702199, forward nfe 7020, backward nfe 3536, Train: 0.8150, Val: 0.7438, Test: 0.7628\n",
      "Epoch: 069, Runtime 0.093762, Loss 0.697538, forward nfe 7124, backward nfe 3588, Train: 0.8200, Val: 0.7369, Test: 0.7566\n",
      "Epoch: 070, Runtime 0.099455, Loss 0.674398, forward nfe 7228, backward nfe 3640, Train: 0.8300, Val: 0.7238, Test: 0.7461\n",
      "Epoch: 071, Runtime 0.098927, Loss 0.654983, forward nfe 7332, backward nfe 3692, Train: 0.8350, Val: 0.7308, Test: 0.7479\n",
      "Epoch: 072, Runtime 0.097242, Loss 0.634884, forward nfe 7436, backward nfe 3744, Train: 0.8300, Val: 0.7523, Test: 0.7632\n",
      "Epoch: 073, Runtime 0.100146, Loss 0.625677, forward nfe 7540, backward nfe 3796, Train: 0.8300, Val: 0.7569, Test: 0.7715\n",
      "Epoch: 074, Runtime 0.092957, Loss 0.614522, forward nfe 7644, backward nfe 3848, Train: 0.8300, Val: 0.7531, Test: 0.7692\n",
      "Epoch: 075, Runtime 0.090883, Loss 0.606778, forward nfe 7748, backward nfe 3900, Train: 0.8300, Val: 0.7485, Test: 0.7649\n",
      "Epoch: 076, Runtime 0.099369, Loss 0.586668, forward nfe 7852, backward nfe 3952, Train: 0.8200, Val: 0.7554, Test: 0.7686\n",
      "Epoch: 077, Runtime 0.103276, Loss 0.579093, forward nfe 7956, backward nfe 4004, Train: 0.8300, Val: 0.7508, Test: 0.7652\n",
      "Epoch: 078, Runtime 0.089521, Loss 0.561969, forward nfe 8060, backward nfe 4056, Train: 0.8450, Val: 0.7500, Test: 0.7653\n",
      "Epoch: 079, Runtime 0.098710, Loss 0.567122, forward nfe 8164, backward nfe 4108, Train: 0.8450, Val: 0.7500, Test: 0.7638\n",
      "Epoch: 080, Runtime 0.093391, Loss 0.540462, forward nfe 8268, backward nfe 4160, Train: 0.8400, Val: 0.7569, Test: 0.7771\n",
      "Epoch: 081, Runtime 0.093318, Loss 0.535906, forward nfe 8372, backward nfe 4212, Train: 0.8350, Val: 0.7638, Test: 0.7817\n",
      "Epoch: 082, Runtime 0.098546, Loss 0.534938, forward nfe 8476, backward nfe 4264, Train: 0.8500, Val: 0.7569, Test: 0.7763\n",
      "Epoch: 083, Runtime 0.099364, Loss 0.520274, forward nfe 8580, backward nfe 4316, Train: 0.8600, Val: 0.7554, Test: 0.7685\n",
      "Epoch: 084, Runtime 0.093600, Loss 0.510963, forward nfe 8684, backward nfe 4368, Train: 0.8350, Val: 0.7562, Test: 0.7737\n",
      "Epoch: 085, Runtime 0.096234, Loss 0.504871, forward nfe 8788, backward nfe 4420, Train: 0.8400, Val: 0.7577, Test: 0.7767\n",
      "Epoch: 086, Runtime 0.092733, Loss 0.492366, forward nfe 8892, backward nfe 4472, Train: 0.8650, Val: 0.7546, Test: 0.7756\n",
      "Epoch: 087, Runtime 0.098531, Loss 0.474116, forward nfe 8996, backward nfe 4524, Train: 0.8650, Val: 0.7638, Test: 0.7853\n",
      "Epoch: 088, Runtime 0.096644, Loss 0.469872, forward nfe 9100, backward nfe 4576, Train: 0.8300, Val: 0.7769, Test: 0.7951\n",
      "Epoch: 089, Runtime 0.094315, Loss 0.472385, forward nfe 9204, backward nfe 4628, Train: 0.8550, Val: 0.7685, Test: 0.7909\n",
      "Epoch: 090, Runtime 0.099338, Loss 0.455101, forward nfe 9308, backward nfe 4680, Train: 0.8750, Val: 0.7600, Test: 0.7781\n",
      "Epoch: 091, Runtime 0.100870, Loss 0.462843, forward nfe 9412, backward nfe 4732, Train: 0.8600, Val: 0.7569, Test: 0.7782\n",
      "Epoch: 092, Runtime 0.098742, Loss 0.440440, forward nfe 9516, backward nfe 4784, Train: 0.8650, Val: 0.7700, Test: 0.7851\n",
      "Epoch: 093, Runtime 0.102549, Loss 0.439629, forward nfe 9620, backward nfe 4836, Train: 0.8650, Val: 0.7792, Test: 0.7943\n",
      "Epoch: 094, Runtime 0.102412, Loss 0.436283, forward nfe 9724, backward nfe 4888, Train: 0.8650, Val: 0.7792, Test: 0.7968\n",
      "Epoch: 095, Runtime 0.100753, Loss 0.427920, forward nfe 9828, backward nfe 4940, Train: 0.8600, Val: 0.7823, Test: 0.8014\n",
      "Epoch: 096, Runtime 0.101090, Loss 0.413297, forward nfe 9932, backward nfe 4992, Train: 0.8600, Val: 0.7831, Test: 0.8007\n",
      "Epoch: 097, Runtime 0.103285, Loss 0.413058, forward nfe 10036, backward nfe 5044, Train: 0.8750, Val: 0.7700, Test: 0.7921\n",
      "Epoch: 098, Runtime 0.098477, Loss 0.405270, forward nfe 10140, backward nfe 5096, Train: 0.8750, Val: 0.7692, Test: 0.7903\n",
      "Epoch: 099, Runtime 0.102036, Loss 0.399122, forward nfe 10244, backward nfe 5148, Train: 0.8750, Val: 0.7808, Test: 0.7973\n",
      "best val accuracy 0.783077 with test accuracy 0.800686 at epoch 96\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.099769, Loss 2.339808, forward nfe 52, backward nfe 52, Train: 0.1050, Val: 0.0615, Test: 0.0664\n",
      "Epoch: 002, Runtime 0.100791, Loss 2.411914, forward nfe 156, backward nfe 104, Train: 0.1150, Val: 0.1162, Test: 0.1121\n",
      "Epoch: 003, Runtime 0.101396, Loss 2.414109, forward nfe 260, backward nfe 156, Train: 0.1400, Val: 0.1377, Test: 0.1303\n",
      "Epoch: 004, Runtime 0.098806, Loss 2.304660, forward nfe 364, backward nfe 208, Train: 0.1300, Val: 0.0331, Test: 0.0465\n",
      "Epoch: 005, Runtime 0.100536, Loss 2.248748, forward nfe 468, backward nfe 260, Train: 0.1250, Val: 0.0415, Test: 0.0569\n",
      "Epoch: 006, Runtime 0.100582, Loss 2.285253, forward nfe 572, backward nfe 312, Train: 0.1500, Val: 0.0415, Test: 0.0543\n",
      "Epoch: 007, Runtime 0.102182, Loss 2.245787, forward nfe 676, backward nfe 364, Train: 0.2150, Val: 0.0631, Test: 0.0810\n",
      "Epoch: 008, Runtime 0.098990, Loss 2.208208, forward nfe 780, backward nfe 416, Train: 0.1550, Val: 0.0454, Test: 0.0573\n",
      "Epoch: 009, Runtime 0.098396, Loss 2.215117, forward nfe 884, backward nfe 468, Train: 0.1150, Val: 0.0423, Test: 0.0492\n",
      "Epoch: 010, Runtime 0.098678, Loss 2.215792, forward nfe 988, backward nfe 520, Train: 0.2100, Val: 0.0600, Test: 0.0799\n",
      "Epoch: 011, Runtime 0.101128, Loss 2.184279, forward nfe 1092, backward nfe 572, Train: 0.2050, Val: 0.1223, Test: 0.1292\n",
      "Epoch: 012, Runtime 0.097646, Loss 2.163869, forward nfe 1196, backward nfe 624, Train: 0.1800, Val: 0.1369, Test: 0.1469\n",
      "Epoch: 013, Runtime 0.102083, Loss 2.149367, forward nfe 1300, backward nfe 676, Train: 0.2050, Val: 0.1423, Test: 0.1469\n",
      "Epoch: 014, Runtime 0.097649, Loss 2.150440, forward nfe 1404, backward nfe 728, Train: 0.1950, Val: 0.1692, Test: 0.1746\n",
      "Epoch: 015, Runtime 0.102878, Loss 2.121399, forward nfe 1508, backward nfe 780, Train: 0.2150, Val: 0.1877, Test: 0.1898\n",
      "Epoch: 016, Runtime 0.100979, Loss 2.095487, forward nfe 1612, backward nfe 832, Train: 0.2400, Val: 0.1577, Test: 0.1737\n",
      "Epoch: 017, Runtime 0.096967, Loss 2.074989, forward nfe 1716, backward nfe 884, Train: 0.2450, Val: 0.1669, Test: 0.1749\n",
      "Epoch: 018, Runtime 0.098630, Loss 2.061242, forward nfe 1820, backward nfe 936, Train: 0.2500, Val: 0.2431, Test: 0.2386\n",
      "Epoch: 019, Runtime 0.103107, Loss 2.040936, forward nfe 1924, backward nfe 988, Train: 0.2800, Val: 0.2546, Test: 0.2454\n",
      "Epoch: 020, Runtime 0.100557, Loss 2.009456, forward nfe 2028, backward nfe 1040, Train: 0.3100, Val: 0.2631, Test: 0.2556\n",
      "Epoch: 021, Runtime 0.097846, Loss 1.997254, forward nfe 2132, backward nfe 1092, Train: 0.3050, Val: 0.2600, Test: 0.2524\n",
      "Epoch: 022, Runtime 0.095876, Loss 1.979104, forward nfe 2236, backward nfe 1144, Train: 0.3250, Val: 0.2638, Test: 0.2567\n",
      "Epoch: 023, Runtime 0.102373, Loss 1.947760, forward nfe 2340, backward nfe 1196, Train: 0.3400, Val: 0.2677, Test: 0.2617\n",
      "Epoch: 024, Runtime 0.095604, Loss 1.928673, forward nfe 2444, backward nfe 1248, Train: 0.3400, Val: 0.2654, Test: 0.2675\n",
      "Epoch: 025, Runtime 0.097145, Loss 1.907086, forward nfe 2548, backward nfe 1300, Train: 0.3450, Val: 0.2654, Test: 0.2739\n",
      "Epoch: 026, Runtime 0.097201, Loss 1.886506, forward nfe 2652, backward nfe 1352, Train: 0.3400, Val: 0.2654, Test: 0.2695\n",
      "Epoch: 027, Runtime 0.106889, Loss 1.857411, forward nfe 2756, backward nfe 1404, Train: 0.3200, Val: 0.2685, Test: 0.2656\n",
      "Epoch: 028, Runtime 0.097222, Loss 1.832431, forward nfe 2860, backward nfe 1456, Train: 0.3250, Val: 0.2669, Test: 0.2629\n",
      "Epoch: 029, Runtime 0.103499, Loss 1.815350, forward nfe 2964, backward nfe 1508, Train: 0.3250, Val: 0.2685, Test: 0.2659\n",
      "Epoch: 030, Runtime 0.102163, Loss 1.788311, forward nfe 3068, backward nfe 1560, Train: 0.3500, Val: 0.2823, Test: 0.2755\n",
      "Epoch: 031, Runtime 0.093866, Loss 1.764434, forward nfe 3172, backward nfe 1612, Train: 0.3800, Val: 0.2831, Test: 0.2847\n",
      "Epoch: 032, Runtime 0.090213, Loss 1.735384, forward nfe 3276, backward nfe 1664, Train: 0.4050, Val: 0.2192, Test: 0.2390\n",
      "Epoch: 033, Runtime 0.102066, Loss 1.718587, forward nfe 3380, backward nfe 1716, Train: 0.4100, Val: 0.2169, Test: 0.2412\n",
      "Epoch: 034, Runtime 0.096115, Loss 1.685735, forward nfe 3484, backward nfe 1768, Train: 0.4200, Val: 0.2515, Test: 0.2600\n",
      "Epoch: 035, Runtime 0.094775, Loss 1.671867, forward nfe 3588, backward nfe 1820, Train: 0.4200, Val: 0.2423, Test: 0.2501\n",
      "Epoch: 036, Runtime 0.102386, Loss 1.643029, forward nfe 3692, backward nfe 1872, Train: 0.4500, Val: 0.2408, Test: 0.2491\n",
      "Epoch: 037, Runtime 0.101303, Loss 1.622063, forward nfe 3796, backward nfe 1924, Train: 0.4700, Val: 0.2438, Test: 0.2519\n",
      "Epoch: 038, Runtime 0.098848, Loss 1.590201, forward nfe 3900, backward nfe 1976, Train: 0.4650, Val: 0.2615, Test: 0.2686\n",
      "Epoch: 039, Runtime 0.097357, Loss 1.570718, forward nfe 4004, backward nfe 2028, Train: 0.4950, Val: 0.2762, Test: 0.2887\n",
      "Epoch: 040, Runtime 0.096113, Loss 1.557323, forward nfe 4108, backward nfe 2080, Train: 0.5000, Val: 0.2954, Test: 0.2987\n",
      "Epoch: 041, Runtime 0.099730, Loss 1.530364, forward nfe 4212, backward nfe 2132, Train: 0.5250, Val: 0.3008, Test: 0.3056\n",
      "Epoch: 042, Runtime 0.101061, Loss 1.509963, forward nfe 4316, backward nfe 2184, Train: 0.5300, Val: 0.2985, Test: 0.3049\n",
      "Epoch: 043, Runtime 0.098339, Loss 1.492967, forward nfe 4420, backward nfe 2236, Train: 0.5400, Val: 0.3085, Test: 0.3128\n",
      "Epoch: 044, Runtime 0.098588, Loss 1.468344, forward nfe 4524, backward nfe 2288, Train: 0.5400, Val: 0.3277, Test: 0.3331\n",
      "Epoch: 045, Runtime 0.099900, Loss 1.448902, forward nfe 4628, backward nfe 2340, Train: 0.5400, Val: 0.3431, Test: 0.3495\n",
      "Epoch: 046, Runtime 0.100354, Loss 1.429605, forward nfe 4732, backward nfe 2392, Train: 0.5500, Val: 0.3446, Test: 0.3441\n",
      "Epoch: 047, Runtime 0.098409, Loss 1.408355, forward nfe 4836, backward nfe 2444, Train: 0.5350, Val: 0.3208, Test: 0.3268\n",
      "Epoch: 048, Runtime 0.097161, Loss 1.387638, forward nfe 4940, backward nfe 2496, Train: 0.5400, Val: 0.3031, Test: 0.3170\n",
      "Epoch: 049, Runtime 0.098317, Loss 1.369393, forward nfe 5044, backward nfe 2548, Train: 0.5700, Val: 0.3254, Test: 0.3325\n",
      "Epoch: 050, Runtime 0.102839, Loss 1.361983, forward nfe 5148, backward nfe 2600, Train: 0.5900, Val: 0.3485, Test: 0.3591\n",
      "Epoch: 051, Runtime 0.099500, Loss 1.330671, forward nfe 5252, backward nfe 2652, Train: 0.5950, Val: 0.3715, Test: 0.3819\n",
      "Epoch: 052, Runtime 0.100437, Loss 1.324176, forward nfe 5356, backward nfe 2704, Train: 0.6050, Val: 0.3731, Test: 0.3853\n",
      "Epoch: 053, Runtime 0.100760, Loss 1.301421, forward nfe 5460, backward nfe 2756, Train: 0.6400, Val: 0.3685, Test: 0.3726\n",
      "Epoch: 054, Runtime 0.100490, Loss 1.288380, forward nfe 5564, backward nfe 2808, Train: 0.6550, Val: 0.3685, Test: 0.3777\n",
      "Epoch: 055, Runtime 0.094660, Loss 1.268022, forward nfe 5668, backward nfe 2860, Train: 0.6500, Val: 0.3815, Test: 0.3902\n",
      "Epoch: 056, Runtime 0.098439, Loss 1.249958, forward nfe 5772, backward nfe 2912, Train: 0.6450, Val: 0.4223, Test: 0.4252\n",
      "Epoch: 057, Runtime 0.100196, Loss 1.243403, forward nfe 5876, backward nfe 2964, Train: 0.6550, Val: 0.4631, Test: 0.4625\n",
      "Epoch: 058, Runtime 0.095628, Loss 1.213509, forward nfe 5980, backward nfe 3016, Train: 0.6650, Val: 0.4669, Test: 0.4690\n",
      "Epoch: 059, Runtime 0.097990, Loss 1.201828, forward nfe 6084, backward nfe 3068, Train: 0.6700, Val: 0.4508, Test: 0.4540\n",
      "Epoch: 060, Runtime 0.099701, Loss 1.189397, forward nfe 6188, backward nfe 3120, Train: 0.6750, Val: 0.4485, Test: 0.4540\n",
      "Epoch: 061, Runtime 0.096317, Loss 1.169412, forward nfe 6292, backward nfe 3172, Train: 0.6850, Val: 0.4615, Test: 0.4653\n",
      "Epoch: 062, Runtime 0.102578, Loss 1.149681, forward nfe 6396, backward nfe 3224, Train: 0.6750, Val: 0.5023, Test: 0.4986\n",
      "Epoch: 063, Runtime 0.093609, Loss 1.129874, forward nfe 6500, backward nfe 3276, Train: 0.6900, Val: 0.5223, Test: 0.5245\n",
      "Epoch: 064, Runtime 0.096977, Loss 1.114890, forward nfe 6604, backward nfe 3328, Train: 0.7100, Val: 0.5177, Test: 0.5170\n",
      "Epoch: 065, Runtime 0.096557, Loss 1.111904, forward nfe 6708, backward nfe 3380, Train: 0.7050, Val: 0.5054, Test: 0.5060\n",
      "Epoch: 066, Runtime 0.100757, Loss 1.102963, forward nfe 6812, backward nfe 3432, Train: 0.7100, Val: 0.5208, Test: 0.5296\n",
      "Epoch: 067, Runtime 0.099330, Loss 1.070709, forward nfe 6916, backward nfe 3484, Train: 0.6950, Val: 0.5615, Test: 0.5597\n",
      "Epoch: 068, Runtime 0.101734, Loss 1.057161, forward nfe 7020, backward nfe 3536, Train: 0.7000, Val: 0.5615, Test: 0.5664\n",
      "Epoch: 069, Runtime 0.100458, Loss 1.043481, forward nfe 7124, backward nfe 3588, Train: 0.7200, Val: 0.5546, Test: 0.5626\n",
      "Epoch: 070, Runtime 0.098778, Loss 1.042219, forward nfe 7228, backward nfe 3640, Train: 0.7350, Val: 0.5392, Test: 0.5511\n",
      "Epoch: 071, Runtime 0.103030, Loss 1.019344, forward nfe 7332, backward nfe 3692, Train: 0.7350, Val: 0.5792, Test: 0.5861\n",
      "Epoch: 072, Runtime 0.102703, Loss 0.992831, forward nfe 7436, backward nfe 3744, Train: 0.7550, Val: 0.6054, Test: 0.6112\n",
      "Epoch: 073, Runtime 0.100079, Loss 0.967655, forward nfe 7540, backward nfe 3796, Train: 0.7350, Val: 0.6054, Test: 0.6148\n",
      "Epoch: 074, Runtime 0.099489, Loss 0.970564, forward nfe 7644, backward nfe 3848, Train: 0.7300, Val: 0.5946, Test: 0.6055\n",
      "Epoch: 075, Runtime 0.093492, Loss 0.952184, forward nfe 7748, backward nfe 3900, Train: 0.7450, Val: 0.5962, Test: 0.6059\n",
      "Epoch: 076, Runtime 0.098648, Loss 0.928071, forward nfe 7852, backward nfe 3952, Train: 0.7450, Val: 0.6031, Test: 0.6190\n",
      "Epoch: 077, Runtime 0.103258, Loss 0.914057, forward nfe 7956, backward nfe 4004, Train: 0.7650, Val: 0.6231, Test: 0.6350\n",
      "Epoch: 078, Runtime 0.093075, Loss 0.917637, forward nfe 8060, backward nfe 4056, Train: 0.7750, Val: 0.6362, Test: 0.6491\n",
      "Epoch: 079, Runtime 0.101309, Loss 0.901188, forward nfe 8164, backward nfe 4108, Train: 0.7800, Val: 0.6315, Test: 0.6476\n",
      "Epoch: 080, Runtime 0.097964, Loss 0.876160, forward nfe 8268, backward nfe 4160, Train: 0.7800, Val: 0.6254, Test: 0.6391\n",
      "Epoch: 081, Runtime 0.097083, Loss 0.872995, forward nfe 8372, backward nfe 4212, Train: 0.7800, Val: 0.6300, Test: 0.6485\n",
      "Epoch: 082, Runtime 0.097305, Loss 0.867270, forward nfe 8476, backward nfe 4264, Train: 0.7850, Val: 0.6292, Test: 0.6502\n",
      "Epoch: 083, Runtime 0.101033, Loss 0.822880, forward nfe 8580, backward nfe 4316, Train: 0.7850, Val: 0.6262, Test: 0.6450\n",
      "Epoch: 084, Runtime 0.099094, Loss 0.815467, forward nfe 8684, backward nfe 4368, Train: 0.7850, Val: 0.6262, Test: 0.6463\n",
      "Epoch: 085, Runtime 0.102514, Loss 0.811900, forward nfe 8788, backward nfe 4420, Train: 0.7850, Val: 0.6454, Test: 0.6627\n",
      "Epoch: 086, Runtime 0.102953, Loss 0.813493, forward nfe 8892, backward nfe 4472, Train: 0.7850, Val: 0.6515, Test: 0.6741\n",
      "Epoch: 087, Runtime 0.093029, Loss 0.798320, forward nfe 8996, backward nfe 4524, Train: 0.8050, Val: 0.6508, Test: 0.6718\n",
      "Epoch: 088, Runtime 0.097182, Loss 0.768357, forward nfe 9100, backward nfe 4576, Train: 0.8100, Val: 0.6546, Test: 0.6761\n",
      "Epoch: 089, Runtime 0.098692, Loss 0.777918, forward nfe 9204, backward nfe 4628, Train: 0.8150, Val: 0.6546, Test: 0.6774\n",
      "Epoch: 090, Runtime 0.093465, Loss 0.763291, forward nfe 9308, backward nfe 4680, Train: 0.8150, Val: 0.6592, Test: 0.6816\n",
      "Epoch: 091, Runtime 0.098145, Loss 0.753040, forward nfe 9412, backward nfe 4732, Train: 0.8150, Val: 0.6685, Test: 0.6860\n",
      "Epoch: 092, Runtime 0.096709, Loss 0.730984, forward nfe 9516, backward nfe 4784, Train: 0.8150, Val: 0.6692, Test: 0.6865\n",
      "Epoch: 093, Runtime 0.098853, Loss 0.730887, forward nfe 9620, backward nfe 4836, Train: 0.8150, Val: 0.6708, Test: 0.6903\n",
      "Epoch: 094, Runtime 0.099718, Loss 0.727111, forward nfe 9724, backward nfe 4888, Train: 0.8200, Val: 0.6515, Test: 0.6770\n",
      "Epoch: 095, Runtime 0.096177, Loss 0.701052, forward nfe 9828, backward nfe 4940, Train: 0.8300, Val: 0.6592, Test: 0.6814\n",
      "Epoch: 096, Runtime 0.099969, Loss 0.704452, forward nfe 9932, backward nfe 4992, Train: 0.8200, Val: 0.6777, Test: 0.6990\n",
      "Epoch: 097, Runtime 0.093897, Loss 0.685792, forward nfe 10036, backward nfe 5044, Train: 0.8200, Val: 0.6854, Test: 0.7080\n",
      "Epoch: 098, Runtime 0.095465, Loss 0.696358, forward nfe 10140, backward nfe 5096, Train: 0.8200, Val: 0.6823, Test: 0.7073\n",
      "Epoch: 099, Runtime 0.100760, Loss 0.666094, forward nfe 10244, backward nfe 5148, Train: 0.8250, Val: 0.6608, Test: 0.6917\n",
      "best val accuracy 0.685385 with test accuracy 0.708048 at epoch 97\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.101271, Loss 2.319399, forward nfe 52, backward nfe 52, Train: 0.1050, Val: 0.0454, Test: 0.0521\n",
      "Epoch: 002, Runtime 0.099898, Loss 2.351269, forward nfe 156, backward nfe 104, Train: 0.1750, Val: 0.1631, Test: 0.1581\n",
      "Epoch: 003, Runtime 0.099029, Loss 2.305662, forward nfe 260, backward nfe 156, Train: 0.1850, Val: 0.1585, Test: 0.1605\n",
      "Epoch: 004, Runtime 0.092887, Loss 2.223731, forward nfe 364, backward nfe 208, Train: 0.2450, Val: 0.3038, Test: 0.2934\n",
      "Epoch: 005, Runtime 0.101247, Loss 2.214147, forward nfe 468, backward nfe 260, Train: 0.2200, Val: 0.0785, Test: 0.0928\n",
      "Epoch: 006, Runtime 0.101880, Loss 2.176394, forward nfe 572, backward nfe 312, Train: 0.2150, Val: 0.0738, Test: 0.0866\n",
      "Epoch: 007, Runtime 0.095798, Loss 2.131253, forward nfe 676, backward nfe 364, Train: 0.2600, Val: 0.0938, Test: 0.1052\n",
      "Epoch: 008, Runtime 0.095029, Loss 2.068846, forward nfe 780, backward nfe 416, Train: 0.3050, Val: 0.1269, Test: 0.1432\n",
      "Epoch: 009, Runtime 0.103599, Loss 2.033347, forward nfe 884, backward nfe 468, Train: 0.2750, Val: 0.1292, Test: 0.1434\n",
      "Epoch: 010, Runtime 0.100292, Loss 2.018942, forward nfe 988, backward nfe 520, Train: 0.3250, Val: 0.1738, Test: 0.1894\n",
      "Epoch: 011, Runtime 0.100044, Loss 1.961050, forward nfe 1092, backward nfe 572, Train: 0.3700, Val: 0.1885, Test: 0.1969\n",
      "Epoch: 012, Runtime 0.099897, Loss 1.910094, forward nfe 1196, backward nfe 624, Train: 0.3350, Val: 0.1592, Test: 0.1605\n",
      "Epoch: 013, Runtime 0.098085, Loss 1.893865, forward nfe 1300, backward nfe 676, Train: 0.3500, Val: 0.1815, Test: 0.1870\n",
      "Epoch: 014, Runtime 0.100796, Loss 1.852306, forward nfe 1404, backward nfe 728, Train: 0.5200, Val: 0.2923, Test: 0.3222\n",
      "Epoch: 015, Runtime 0.099268, Loss 1.804457, forward nfe 1508, backward nfe 780, Train: 0.5100, Val: 0.4154, Test: 0.4362\n",
      "Epoch: 016, Runtime 0.099659, Loss 1.762818, forward nfe 1612, backward nfe 832, Train: 0.4900, Val: 0.4323, Test: 0.4376\n",
      "Epoch: 017, Runtime 0.096366, Loss 1.730014, forward nfe 1716, backward nfe 884, Train: 0.5200, Val: 0.4485, Test: 0.4607\n",
      "Epoch: 018, Runtime 0.097083, Loss 1.683995, forward nfe 1820, backward nfe 936, Train: 0.6350, Val: 0.5292, Test: 0.5394\n",
      "Epoch: 019, Runtime 0.100950, Loss 1.638759, forward nfe 1924, backward nfe 988, Train: 0.6550, Val: 0.5523, Test: 0.5674\n",
      "Epoch: 020, Runtime 0.097757, Loss 1.607998, forward nfe 2028, backward nfe 1040, Train: 0.6700, Val: 0.5662, Test: 0.5811\n",
      "Epoch: 021, Runtime 0.092212, Loss 1.565780, forward nfe 2132, backward nfe 1092, Train: 0.6800, Val: 0.5915, Test: 0.6174\n",
      "Epoch: 022, Runtime 0.099591, Loss 1.523743, forward nfe 2236, backward nfe 1144, Train: 0.6850, Val: 0.6069, Test: 0.6294\n",
      "Epoch: 023, Runtime 0.101084, Loss 1.482029, forward nfe 2340, backward nfe 1196, Train: 0.6950, Val: 0.6238, Test: 0.6340\n",
      "Epoch: 024, Runtime 0.097926, Loss 1.440681, forward nfe 2444, backward nfe 1248, Train: 0.6900, Val: 0.6415, Test: 0.6526\n",
      "Epoch: 025, Runtime 0.100528, Loss 1.399212, forward nfe 2548, backward nfe 1300, Train: 0.7000, Val: 0.6392, Test: 0.6562\n",
      "Epoch: 026, Runtime 0.098328, Loss 1.365099, forward nfe 2652, backward nfe 1352, Train: 0.7050, Val: 0.6215, Test: 0.6350\n",
      "Epoch: 027, Runtime 0.090922, Loss 1.328399, forward nfe 2756, backward nfe 1404, Train: 0.7100, Val: 0.6169, Test: 0.6304\n",
      "Epoch: 028, Runtime 0.101673, Loss 1.286638, forward nfe 2860, backward nfe 1456, Train: 0.7150, Val: 0.6177, Test: 0.6330\n",
      "Epoch: 029, Runtime 0.102154, Loss 1.256769, forward nfe 2964, backward nfe 1508, Train: 0.7350, Val: 0.6285, Test: 0.6474\n",
      "Epoch: 030, Runtime 0.097340, Loss 1.226527, forward nfe 3068, backward nfe 1560, Train: 0.7250, Val: 0.6492, Test: 0.6654\n",
      "Epoch: 031, Runtime 0.092106, Loss 1.193949, forward nfe 3172, backward nfe 1612, Train: 0.7450, Val: 0.6485, Test: 0.6675\n",
      "Epoch: 032, Runtime 0.100543, Loss 1.165660, forward nfe 3276, backward nfe 1664, Train: 0.7600, Val: 0.6277, Test: 0.6547\n",
      "Epoch: 033, Runtime 0.099929, Loss 1.129019, forward nfe 3380, backward nfe 1716, Train: 0.7750, Val: 0.6215, Test: 0.6470\n",
      "Epoch: 034, Runtime 0.100416, Loss 1.104452, forward nfe 3484, backward nfe 1768, Train: 0.7650, Val: 0.6385, Test: 0.6614\n",
      "Epoch: 035, Runtime 0.094219, Loss 1.076513, forward nfe 3588, backward nfe 1820, Train: 0.7650, Val: 0.6646, Test: 0.6814\n",
      "Epoch: 036, Runtime 0.098449, Loss 1.050742, forward nfe 3692, backward nfe 1872, Train: 0.7700, Val: 0.6754, Test: 0.6902\n",
      "Epoch: 037, Runtime 0.098328, Loss 1.034562, forward nfe 3796, backward nfe 1924, Train: 0.7800, Val: 0.6715, Test: 0.6853\n",
      "Epoch: 038, Runtime 0.099946, Loss 1.002764, forward nfe 3900, backward nfe 1976, Train: 0.7850, Val: 0.6715, Test: 0.6841\n",
      "Epoch: 039, Runtime 0.100259, Loss 0.985795, forward nfe 4004, backward nfe 2028, Train: 0.7850, Val: 0.6677, Test: 0.6852\n",
      "Epoch: 040, Runtime 0.097749, Loss 0.956887, forward nfe 4108, backward nfe 2080, Train: 0.7700, Val: 0.6754, Test: 0.6882\n",
      "Epoch: 041, Runtime 0.091051, Loss 0.936917, forward nfe 4212, backward nfe 2132, Train: 0.7750, Val: 0.6792, Test: 0.6943\n",
      "Epoch: 042, Runtime 0.103262, Loss 0.918728, forward nfe 4316, backward nfe 2184, Train: 0.7850, Val: 0.6808, Test: 0.6947\n",
      "Epoch: 043, Runtime 0.100732, Loss 0.886954, forward nfe 4420, backward nfe 2236, Train: 0.7950, Val: 0.6869, Test: 0.6992\n",
      "Epoch: 044, Runtime 0.097914, Loss 0.875986, forward nfe 4524, backward nfe 2288, Train: 0.7950, Val: 0.6962, Test: 0.7053\n",
      "Epoch: 045, Runtime 0.093947, Loss 0.851189, forward nfe 4628, backward nfe 2340, Train: 0.8000, Val: 0.6946, Test: 0.7039\n",
      "Epoch: 046, Runtime 0.099613, Loss 0.842644, forward nfe 4732, backward nfe 2392, Train: 0.8100, Val: 0.6846, Test: 0.6961\n",
      "Epoch: 047, Runtime 0.092032, Loss 0.823008, forward nfe 4836, backward nfe 2444, Train: 0.8100, Val: 0.6962, Test: 0.7025\n",
      "Epoch: 048, Runtime 0.099349, Loss 0.813253, forward nfe 4940, backward nfe 2496, Train: 0.8100, Val: 0.7054, Test: 0.7122\n",
      "Epoch: 049, Runtime 0.099817, Loss 0.786045, forward nfe 5044, backward nfe 2548, Train: 0.8100, Val: 0.7092, Test: 0.7200\n",
      "Epoch: 050, Runtime 0.096623, Loss 0.775832, forward nfe 5148, backward nfe 2600, Train: 0.8150, Val: 0.7092, Test: 0.7173\n",
      "Epoch: 051, Runtime 0.100123, Loss 0.751796, forward nfe 5252, backward nfe 2652, Train: 0.8200, Val: 0.7069, Test: 0.7123\n",
      "Epoch: 052, Runtime 0.101530, Loss 0.733139, forward nfe 5356, backward nfe 2704, Train: 0.8300, Val: 0.7131, Test: 0.7164\n",
      "Epoch: 053, Runtime 0.094413, Loss 0.718254, forward nfe 5460, backward nfe 2756, Train: 0.8250, Val: 0.7185, Test: 0.7260\n",
      "Epoch: 054, Runtime 0.100260, Loss 0.714441, forward nfe 5564, backward nfe 2808, Train: 0.8300, Val: 0.7208, Test: 0.7279\n",
      "Epoch: 055, Runtime 0.100260, Loss 0.699163, forward nfe 5668, backward nfe 2860, Train: 0.8300, Val: 0.7192, Test: 0.7273\n",
      "Epoch: 056, Runtime 0.099219, Loss 0.671315, forward nfe 5772, backward nfe 2912, Train: 0.8350, Val: 0.7138, Test: 0.7219\n",
      "Epoch: 057, Runtime 0.096364, Loss 0.660593, forward nfe 5876, backward nfe 2964, Train: 0.8400, Val: 0.7285, Test: 0.7401\n",
      "Epoch: 058, Runtime 0.096683, Loss 0.648145, forward nfe 5980, backward nfe 3016, Train: 0.8450, Val: 0.7377, Test: 0.7498\n",
      "Epoch: 059, Runtime 0.095498, Loss 0.633582, forward nfe 6084, backward nfe 3068, Train: 0.8250, Val: 0.7377, Test: 0.7476\n",
      "Epoch: 060, Runtime 0.092092, Loss 0.617529, forward nfe 6188, backward nfe 3120, Train: 0.8350, Val: 0.7138, Test: 0.7293\n",
      "Epoch: 061, Runtime 0.094002, Loss 0.607440, forward nfe 6292, backward nfe 3172, Train: 0.8400, Val: 0.7354, Test: 0.7490\n",
      "Epoch: 062, Runtime 0.100888, Loss 0.586339, forward nfe 6396, backward nfe 3224, Train: 0.8500, Val: 0.7354, Test: 0.7572\n",
      "Epoch: 063, Runtime 0.092219, Loss 0.571138, forward nfe 6500, backward nfe 3276, Train: 0.8550, Val: 0.7362, Test: 0.7557\n",
      "Epoch: 064, Runtime 0.092602, Loss 0.555022, forward nfe 6604, backward nfe 3328, Train: 0.8550, Val: 0.7346, Test: 0.7558\n",
      "Epoch: 065, Runtime 0.094493, Loss 0.536693, forward nfe 6708, backward nfe 3380, Train: 0.8500, Val: 0.7285, Test: 0.7493\n",
      "Epoch: 066, Runtime 0.099351, Loss 0.538200, forward nfe 6812, backward nfe 3432, Train: 0.8600, Val: 0.7323, Test: 0.7551\n",
      "Epoch: 067, Runtime 0.101898, Loss 0.523119, forward nfe 6916, backward nfe 3484, Train: 0.8600, Val: 0.7515, Test: 0.7651\n",
      "Epoch: 068, Runtime 0.101526, Loss 0.514227, forward nfe 7020, backward nfe 3536, Train: 0.8450, Val: 0.7515, Test: 0.7685\n",
      "Epoch: 069, Runtime 0.098282, Loss 0.501783, forward nfe 7124, backward nfe 3588, Train: 0.8600, Val: 0.7577, Test: 0.7711\n",
      "Epoch: 070, Runtime 0.090709, Loss 0.490559, forward nfe 7228, backward nfe 3640, Train: 0.8750, Val: 0.7523, Test: 0.7672\n",
      "Epoch: 071, Runtime 0.097140, Loss 0.477615, forward nfe 7332, backward nfe 3692, Train: 0.8750, Val: 0.7500, Test: 0.7692\n",
      "Epoch: 072, Runtime 0.093746, Loss 0.470580, forward nfe 7436, backward nfe 3744, Train: 0.8700, Val: 0.7554, Test: 0.7719\n",
      "Epoch: 073, Runtime 0.096834, Loss 0.460962, forward nfe 7540, backward nfe 3796, Train: 0.8700, Val: 0.7585, Test: 0.7743\n",
      "Epoch: 074, Runtime 0.098451, Loss 0.458263, forward nfe 7644, backward nfe 3848, Train: 0.8700, Val: 0.7623, Test: 0.7746\n",
      "Epoch: 075, Runtime 0.093491, Loss 0.438556, forward nfe 7748, backward nfe 3900, Train: 0.8700, Val: 0.7608, Test: 0.7767\n",
      "Epoch: 076, Runtime 0.093733, Loss 0.425059, forward nfe 7852, backward nfe 3952, Train: 0.8750, Val: 0.7546, Test: 0.7770\n",
      "Epoch: 077, Runtime 0.095953, Loss 0.431149, forward nfe 7956, backward nfe 4004, Train: 0.8850, Val: 0.7608, Test: 0.7773\n",
      "Epoch: 078, Runtime 0.097550, Loss 0.419879, forward nfe 8060, backward nfe 4056, Train: 0.8800, Val: 0.7646, Test: 0.7821\n",
      "Epoch: 079, Runtime 0.100951, Loss 0.413686, forward nfe 8164, backward nfe 4108, Train: 0.8850, Val: 0.7554, Test: 0.7797\n",
      "Epoch: 080, Runtime 0.096110, Loss 0.403675, forward nfe 8268, backward nfe 4160, Train: 0.8900, Val: 0.7554, Test: 0.7814\n",
      "Epoch: 081, Runtime 0.099710, Loss 0.389208, forward nfe 8372, backward nfe 4212, Train: 0.8750, Val: 0.7746, Test: 0.7892\n",
      "Epoch: 082, Runtime 0.089969, Loss 0.387412, forward nfe 8476, backward nfe 4264, Train: 0.8700, Val: 0.7762, Test: 0.7915\n",
      "Epoch: 083, Runtime 0.094134, Loss 0.398537, forward nfe 8580, backward nfe 4316, Train: 0.8850, Val: 0.7685, Test: 0.7891\n",
      "Epoch: 084, Runtime 0.092043, Loss 0.367334, forward nfe 8684, backward nfe 4368, Train: 0.8800, Val: 0.7700, Test: 0.7869\n",
      "Epoch: 085, Runtime 0.091187, Loss 0.368716, forward nfe 8788, backward nfe 4420, Train: 0.8800, Val: 0.7754, Test: 0.7940\n",
      "Epoch: 086, Runtime 0.100665, Loss 0.358268, forward nfe 8892, backward nfe 4472, Train: 0.8800, Val: 0.7785, Test: 0.7938\n",
      "Epoch: 087, Runtime 0.102307, Loss 0.353962, forward nfe 8996, backward nfe 4524, Train: 0.8900, Val: 0.7685, Test: 0.7879\n",
      "Epoch: 088, Runtime 0.089699, Loss 0.354181, forward nfe 9100, backward nfe 4576, Train: 0.8950, Val: 0.7608, Test: 0.7841\n",
      "Epoch: 089, Runtime 0.101949, Loss 0.348918, forward nfe 9204, backward nfe 4628, Train: 0.8850, Val: 0.7769, Test: 0.7901\n",
      "Epoch: 090, Runtime 0.093368, Loss 0.334793, forward nfe 9308, backward nfe 4680, Train: 0.8850, Val: 0.7785, Test: 0.7991\n",
      "Epoch: 091, Runtime 0.095513, Loss 0.331903, forward nfe 9412, backward nfe 4732, Train: 0.9000, Val: 0.7723, Test: 0.7977\n",
      "Epoch: 092, Runtime 0.095081, Loss 0.329860, forward nfe 9516, backward nfe 4784, Train: 0.8950, Val: 0.7723, Test: 0.7957\n",
      "Epoch: 093, Runtime 0.096358, Loss 0.310366, forward nfe 9620, backward nfe 4836, Train: 0.8900, Val: 0.7846, Test: 0.7973\n",
      "Epoch: 094, Runtime 0.093938, Loss 0.313233, forward nfe 9724, backward nfe 4888, Train: 0.8900, Val: 0.7792, Test: 0.7990\n",
      "Epoch: 095, Runtime 0.097491, Loss 0.310930, forward nfe 9828, backward nfe 4940, Train: 0.9050, Val: 0.7738, Test: 0.8027\n",
      "Epoch: 096, Runtime 0.091457, Loss 0.301958, forward nfe 9932, backward nfe 4992, Train: 0.9100, Val: 0.7831, Test: 0.8026\n",
      "Epoch: 097, Runtime 0.095329, Loss 0.288217, forward nfe 10036, backward nfe 5044, Train: 0.9050, Val: 0.7831, Test: 0.8064\n",
      "Epoch: 098, Runtime 0.097385, Loss 0.279676, forward nfe 10140, backward nfe 5096, Train: 0.9000, Val: 0.7892, Test: 0.8047\n",
      "Epoch: 099, Runtime 0.100189, Loss 0.291684, forward nfe 10244, backward nfe 5148, Train: 0.9100, Val: 0.7808, Test: 0.8033\n",
      "best val accuracy 0.789231 with test accuracy 0.804685 at epoch 98\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.100166, Loss 2.310960, forward nfe 52, backward nfe 52, Train: 0.1250, Val: 0.1169, Test: 0.1175\n",
      "Epoch: 002, Runtime 0.101285, Loss 2.385213, forward nfe 156, backward nfe 104, Train: 0.1050, Val: 0.1662, Test: 0.1541\n",
      "Epoch: 003, Runtime 0.098079, Loss 2.397390, forward nfe 260, backward nfe 156, Train: 0.1050, Val: 0.0585, Test: 0.0595\n",
      "Epoch: 004, Runtime 0.095072, Loss 2.366533, forward nfe 364, backward nfe 208, Train: 0.0950, Val: 0.0338, Test: 0.0362\n",
      "Epoch: 005, Runtime 0.094187, Loss 2.325234, forward nfe 468, backward nfe 260, Train: 0.1650, Val: 0.0715, Test: 0.0833\n",
      "Epoch: 006, Runtime 0.098580, Loss 2.242916, forward nfe 572, backward nfe 312, Train: 0.1300, Val: 0.0800, Test: 0.0711\n",
      "Epoch: 007, Runtime 0.093477, Loss 2.216667, forward nfe 676, backward nfe 364, Train: 0.1150, Val: 0.0815, Test: 0.0779\n",
      "Epoch: 008, Runtime 0.099764, Loss 2.227386, forward nfe 780, backward nfe 416, Train: 0.2500, Val: 0.2915, Test: 0.2822\n",
      "Epoch: 009, Runtime 0.102781, Loss 2.206525, forward nfe 884, backward nfe 468, Train: 0.1600, Val: 0.1738, Test: 0.1682\n",
      "Epoch: 010, Runtime 0.100428, Loss 2.168819, forward nfe 988, backward nfe 520, Train: 0.2150, Val: 0.1646, Test: 0.1689\n",
      "Epoch: 011, Runtime 0.096174, Loss 2.145241, forward nfe 1092, backward nfe 572, Train: 0.2450, Val: 0.1592, Test: 0.1641\n",
      "Epoch: 012, Runtime 0.099980, Loss 2.119563, forward nfe 1196, backward nfe 624, Train: 0.2450, Val: 0.1654, Test: 0.1700\n",
      "Epoch: 013, Runtime 0.098118, Loss 2.090503, forward nfe 1300, backward nfe 676, Train: 0.2250, Val: 0.1315, Test: 0.1405\n",
      "Epoch: 014, Runtime 0.097856, Loss 2.083167, forward nfe 1404, backward nfe 728, Train: 0.2300, Val: 0.1246, Test: 0.1379\n",
      "Epoch: 015, Runtime 0.099065, Loss 2.051274, forward nfe 1508, backward nfe 780, Train: 0.2500, Val: 0.1408, Test: 0.1530\n",
      "Epoch: 016, Runtime 0.102514, Loss 2.011452, forward nfe 1612, backward nfe 832, Train: 0.3150, Val: 0.1800, Test: 0.1951\n",
      "Epoch: 017, Runtime 0.101689, Loss 1.998225, forward nfe 1716, backward nfe 884, Train: 0.3200, Val: 0.1854, Test: 0.1977\n",
      "Epoch: 018, Runtime 0.099586, Loss 1.972867, forward nfe 1820, backward nfe 936, Train: 0.2950, Val: 0.1569, Test: 0.1730\n",
      "Epoch: 019, Runtime 0.103449, Loss 1.941513, forward nfe 1924, backward nfe 988, Train: 0.3450, Val: 0.1846, Test: 0.1912\n",
      "Epoch: 020, Runtime 0.101462, Loss 1.913675, forward nfe 2028, backward nfe 1040, Train: 0.3350, Val: 0.1631, Test: 0.1794\n",
      "Epoch: 021, Runtime 0.100683, Loss 1.882654, forward nfe 2132, backward nfe 1092, Train: 0.3400, Val: 0.1685, Test: 0.1837\n",
      "Epoch: 022, Runtime 0.100557, Loss 1.857915, forward nfe 2236, backward nfe 1144, Train: 0.3850, Val: 0.2400, Test: 0.2557\n",
      "Epoch: 023, Runtime 0.102179, Loss 1.831084, forward nfe 2340, backward nfe 1196, Train: 0.4150, Val: 0.3231, Test: 0.3293\n",
      "Epoch: 024, Runtime 0.097577, Loss 1.795484, forward nfe 2444, backward nfe 1248, Train: 0.4450, Val: 0.3577, Test: 0.3669\n",
      "Epoch: 025, Runtime 0.102800, Loss 1.761115, forward nfe 2548, backward nfe 1300, Train: 0.4750, Val: 0.3046, Test: 0.3202\n",
      "Epoch: 026, Runtime 0.099461, Loss 1.732604, forward nfe 2652, backward nfe 1352, Train: 0.4800, Val: 0.3023, Test: 0.3173\n",
      "Epoch: 027, Runtime 0.101274, Loss 1.702873, forward nfe 2756, backward nfe 1404, Train: 0.5150, Val: 0.3292, Test: 0.3568\n",
      "Epoch: 028, Runtime 0.096642, Loss 1.677669, forward nfe 2860, backward nfe 1456, Train: 0.5100, Val: 0.3746, Test: 0.3875\n",
      "Epoch: 029, Runtime 0.100384, Loss 1.642303, forward nfe 2964, backward nfe 1508, Train: 0.5200, Val: 0.4169, Test: 0.4176\n",
      "Epoch: 030, Runtime 0.101662, Loss 1.617126, forward nfe 3068, backward nfe 1560, Train: 0.5250, Val: 0.3646, Test: 0.3723\n",
      "Epoch: 031, Runtime 0.100342, Loss 1.584173, forward nfe 3172, backward nfe 1612, Train: 0.5150, Val: 0.3200, Test: 0.3378\n",
      "Epoch: 032, Runtime 0.098883, Loss 1.557766, forward nfe 3276, backward nfe 1664, Train: 0.5200, Val: 0.3246, Test: 0.3506\n",
      "Epoch: 033, Runtime 0.103044, Loss 1.526658, forward nfe 3380, backward nfe 1716, Train: 0.5450, Val: 0.4062, Test: 0.4179\n",
      "Epoch: 034, Runtime 0.100571, Loss 1.496593, forward nfe 3484, backward nfe 1768, Train: 0.5300, Val: 0.5146, Test: 0.5147\n",
      "Epoch: 035, Runtime 0.098675, Loss 1.472990, forward nfe 3588, backward nfe 1820, Train: 0.5250, Val: 0.5346, Test: 0.5468\n",
      "Epoch: 036, Runtime 0.098521, Loss 1.436271, forward nfe 3692, backward nfe 1872, Train: 0.5200, Val: 0.5423, Test: 0.5615\n",
      "Epoch: 037, Runtime 0.100896, Loss 1.411322, forward nfe 3796, backward nfe 1924, Train: 0.5550, Val: 0.5492, Test: 0.5671\n",
      "Epoch: 038, Runtime 0.100305, Loss 1.383016, forward nfe 3900, backward nfe 1976, Train: 0.5900, Val: 0.5446, Test: 0.5674\n",
      "Epoch: 039, Runtime 0.101302, Loss 1.355373, forward nfe 4004, backward nfe 2028, Train: 0.5900, Val: 0.5538, Test: 0.5729\n",
      "Epoch: 040, Runtime 0.101806, Loss 1.329982, forward nfe 4108, backward nfe 2080, Train: 0.5700, Val: 0.5638, Test: 0.5831\n",
      "Epoch: 041, Runtime 0.101060, Loss 1.302355, forward nfe 4212, backward nfe 2132, Train: 0.5750, Val: 0.5615, Test: 0.5833\n",
      "Epoch: 042, Runtime 0.100394, Loss 1.282867, forward nfe 4316, backward nfe 2184, Train: 0.5800, Val: 0.5538, Test: 0.5790\n",
      "Epoch: 043, Runtime 0.102716, Loss 1.260635, forward nfe 4420, backward nfe 2236, Train: 0.6250, Val: 0.5477, Test: 0.5718\n",
      "Epoch: 044, Runtime 0.099159, Loss 1.232787, forward nfe 4524, backward nfe 2288, Train: 0.6300, Val: 0.5523, Test: 0.5749\n",
      "Epoch: 045, Runtime 0.099477, Loss 1.211480, forward nfe 4628, backward nfe 2340, Train: 0.6450, Val: 0.5569, Test: 0.5832\n",
      "Epoch: 046, Runtime 0.099917, Loss 1.184165, forward nfe 4732, backward nfe 2392, Train: 0.6700, Val: 0.5738, Test: 0.5982\n",
      "Epoch: 047, Runtime 0.101992, Loss 1.164353, forward nfe 4836, backward nfe 2444, Train: 0.6850, Val: 0.5877, Test: 0.6108\n",
      "Epoch: 048, Runtime 0.101088, Loss 1.149647, forward nfe 4940, backward nfe 2496, Train: 0.6950, Val: 0.5931, Test: 0.6130\n",
      "Epoch: 049, Runtime 0.098104, Loss 1.120712, forward nfe 5044, backward nfe 2548, Train: 0.6900, Val: 0.5931, Test: 0.6131\n",
      "Epoch: 050, Runtime 0.100069, Loss 1.104998, forward nfe 5148, backward nfe 2600, Train: 0.7100, Val: 0.5892, Test: 0.6115\n",
      "Epoch: 051, Runtime 0.101335, Loss 1.085587, forward nfe 5252, backward nfe 2652, Train: 0.7000, Val: 0.5892, Test: 0.6088\n",
      "Epoch: 052, Runtime 0.100259, Loss 1.070713, forward nfe 5356, backward nfe 2704, Train: 0.7150, Val: 0.5938, Test: 0.6138\n",
      "Epoch: 053, Runtime 0.101418, Loss 1.055719, forward nfe 5460, backward nfe 2756, Train: 0.7350, Val: 0.5962, Test: 0.6205\n",
      "Epoch: 054, Runtime 0.098814, Loss 1.038033, forward nfe 5564, backward nfe 2808, Train: 0.7450, Val: 0.6015, Test: 0.6250\n",
      "Epoch: 055, Runtime 0.100963, Loss 1.023438, forward nfe 5668, backward nfe 2860, Train: 0.7300, Val: 0.6015, Test: 0.6251\n",
      "Epoch: 056, Runtime 0.099485, Loss 1.002154, forward nfe 5772, backward nfe 2912, Train: 0.7350, Val: 0.5931, Test: 0.6217\n",
      "Epoch: 057, Runtime 0.102885, Loss 0.989720, forward nfe 5876, backward nfe 2964, Train: 0.7400, Val: 0.6023, Test: 0.6242\n",
      "Epoch: 058, Runtime 0.098500, Loss 0.979245, forward nfe 5980, backward nfe 3016, Train: 0.7500, Val: 0.6062, Test: 0.6277\n",
      "Epoch: 059, Runtime 0.099427, Loss 0.953734, forward nfe 6084, backward nfe 3068, Train: 0.7500, Val: 0.6108, Test: 0.6322\n",
      "Epoch: 060, Runtime 0.095575, Loss 0.939342, forward nfe 6188, backward nfe 3120, Train: 0.7400, Val: 0.6177, Test: 0.6425\n",
      "Epoch: 061, Runtime 0.097506, Loss 0.926674, forward nfe 6292, backward nfe 3172, Train: 0.7400, Val: 0.6192, Test: 0.6466\n",
      "Epoch: 062, Runtime 0.100389, Loss 0.924107, forward nfe 6396, backward nfe 3224, Train: 0.7550, Val: 0.6200, Test: 0.6448\n",
      "Epoch: 063, Runtime 0.101979, Loss 0.914175, forward nfe 6500, backward nfe 3276, Train: 0.7700, Val: 0.6177, Test: 0.6459\n",
      "Epoch: 064, Runtime 0.097379, Loss 0.902032, forward nfe 6604, backward nfe 3328, Train: 0.7700, Val: 0.6192, Test: 0.6472\n",
      "Epoch: 065, Runtime 0.096580, Loss 0.886076, forward nfe 6708, backward nfe 3380, Train: 0.7750, Val: 0.6277, Test: 0.6527\n",
      "Epoch: 066, Runtime 0.098447, Loss 0.873848, forward nfe 6812, backward nfe 3432, Train: 0.7750, Val: 0.6323, Test: 0.6614\n",
      "Epoch: 067, Runtime 0.103337, Loss 0.859689, forward nfe 6916, backward nfe 3484, Train: 0.7750, Val: 0.6423, Test: 0.6703\n",
      "Epoch: 068, Runtime 0.099952, Loss 0.845241, forward nfe 7020, backward nfe 3536, Train: 0.7700, Val: 0.6454, Test: 0.6734\n",
      "Epoch: 069, Runtime 0.098149, Loss 0.833859, forward nfe 7124, backward nfe 3588, Train: 0.7700, Val: 0.6500, Test: 0.6763\n",
      "Epoch: 070, Runtime 0.100245, Loss 0.827559, forward nfe 7228, backward nfe 3640, Train: 0.8050, Val: 0.6400, Test: 0.6688\n",
      "Epoch: 071, Runtime 0.101433, Loss 0.812491, forward nfe 7332, backward nfe 3692, Train: 0.8100, Val: 0.6331, Test: 0.6641\n",
      "Epoch: 072, Runtime 0.099847, Loss 0.804968, forward nfe 7436, backward nfe 3744, Train: 0.7950, Val: 0.6454, Test: 0.6785\n",
      "Epoch: 073, Runtime 0.099172, Loss 0.793805, forward nfe 7540, backward nfe 3796, Train: 0.7700, Val: 0.6662, Test: 0.6907\n",
      "Epoch: 074, Runtime 0.100792, Loss 0.793917, forward nfe 7644, backward nfe 3848, Train: 0.7850, Val: 0.6692, Test: 0.6951\n",
      "Epoch: 075, Runtime 0.097579, Loss 0.763010, forward nfe 7748, backward nfe 3900, Train: 0.8050, Val: 0.6600, Test: 0.6927\n",
      "Epoch: 076, Runtime 0.100949, Loss 0.762959, forward nfe 7852, backward nfe 3952, Train: 0.8150, Val: 0.6546, Test: 0.6843\n",
      "Epoch: 077, Runtime 0.098946, Loss 0.748699, forward nfe 7956, backward nfe 4004, Train: 0.8100, Val: 0.6615, Test: 0.6951\n",
      "Epoch: 078, Runtime 0.099533, Loss 0.733452, forward nfe 8060, backward nfe 4056, Train: 0.7850, Val: 0.6846, Test: 0.7029\n",
      "Epoch: 079, Runtime 0.097145, Loss 0.727388, forward nfe 8164, backward nfe 4108, Train: 0.7850, Val: 0.6808, Test: 0.7032\n",
      "Epoch: 080, Runtime 0.100566, Loss 0.722099, forward nfe 8268, backward nfe 4160, Train: 0.8150, Val: 0.6562, Test: 0.6950\n",
      "Epoch: 081, Runtime 0.100915, Loss 0.700136, forward nfe 8372, backward nfe 4212, Train: 0.8150, Val: 0.6154, Test: 0.6518\n",
      "Epoch: 082, Runtime 0.099361, Loss 0.715040, forward nfe 8476, backward nfe 4264, Train: 0.7850, Val: 0.7046, Test: 0.7209\n",
      "Epoch: 083, Runtime 0.102777, Loss 0.710559, forward nfe 8580, backward nfe 4316, Train: 0.7900, Val: 0.7362, Test: 0.7381\n",
      "Epoch: 084, Runtime 0.100452, Loss 0.699044, forward nfe 8684, backward nfe 4368, Train: 0.8100, Val: 0.7077, Test: 0.7290\n",
      "Epoch: 085, Runtime 0.098111, Loss 0.680136, forward nfe 8788, backward nfe 4420, Train: 0.8100, Val: 0.6762, Test: 0.7068\n",
      "Epoch: 086, Runtime 0.099094, Loss 0.660357, forward nfe 8892, backward nfe 4472, Train: 0.8250, Val: 0.6585, Test: 0.6896\n",
      "Epoch: 087, Runtime 0.103094, Loss 0.642118, forward nfe 8996, backward nfe 4524, Train: 0.8300, Val: 0.6615, Test: 0.6868\n",
      "Epoch: 088, Runtime 0.098824, Loss 0.655381, forward nfe 9100, backward nfe 4576, Train: 0.8250, Val: 0.6977, Test: 0.7208\n",
      "Epoch: 089, Runtime 0.099185, Loss 0.617472, forward nfe 9204, backward nfe 4628, Train: 0.8150, Val: 0.7100, Test: 0.7272\n",
      "Epoch: 090, Runtime 0.097425, Loss 0.653046, forward nfe 9308, backward nfe 4680, Train: 0.8200, Val: 0.7192, Test: 0.7443\n",
      "Epoch: 091, Runtime 0.097509, Loss 0.610026, forward nfe 9412, backward nfe 4732, Train: 0.8200, Val: 0.7185, Test: 0.7361\n",
      "Epoch: 092, Runtime 0.097729, Loss 0.624558, forward nfe 9516, backward nfe 4784, Train: 0.8450, Val: 0.7123, Test: 0.7339\n",
      "Epoch: 093, Runtime 0.102660, Loss 0.600580, forward nfe 9620, backward nfe 4836, Train: 0.8600, Val: 0.7000, Test: 0.7312\n",
      "Epoch: 094, Runtime 0.103075, Loss 0.596039, forward nfe 9724, backward nfe 4888, Train: 0.8400, Val: 0.7000, Test: 0.7236\n",
      "Epoch: 095, Runtime 0.100172, Loss 0.590639, forward nfe 9828, backward nfe 4940, Train: 0.8500, Val: 0.7031, Test: 0.7282\n",
      "Epoch: 096, Runtime 0.097159, Loss 0.575979, forward nfe 9932, backward nfe 4992, Train: 0.8500, Val: 0.7108, Test: 0.7312\n",
      "Epoch: 097, Runtime 0.100158, Loss 0.576698, forward nfe 10036, backward nfe 5044, Train: 0.8500, Val: 0.7200, Test: 0.7423\n",
      "Epoch: 098, Runtime 0.100534, Loss 0.566457, forward nfe 10140, backward nfe 5096, Train: 0.8700, Val: 0.7223, Test: 0.7453\n",
      "Epoch: 099, Runtime 0.099601, Loss 0.550394, forward nfe 10244, backward nfe 5148, Train: 0.8650, Val: 0.7269, Test: 0.7490\n",
      "best val accuracy 0.736154 with test accuracy 0.738084 at epoch 83\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.101203, Loss 2.311810, forward nfe 52, backward nfe 52, Train: 0.1050, Val: 0.3485, Test: 0.3210\n",
      "Epoch: 002, Runtime 0.099034, Loss 2.889232, forward nfe 156, backward nfe 104, Train: 0.1000, Val: 0.0162, Test: 0.0218\n",
      "Epoch: 003, Runtime 0.101846, Loss 2.525089, forward nfe 260, backward nfe 156, Train: 0.1000, Val: 0.0169, Test: 0.0220\n",
      "Epoch: 004, Runtime 0.097031, Loss 2.481005, forward nfe 364, backward nfe 208, Train: 0.1350, Val: 0.0238, Test: 0.0349\n",
      "Epoch: 005, Runtime 0.097717, Loss 2.368390, forward nfe 468, backward nfe 260, Train: 0.1800, Val: 0.0615, Test: 0.0699\n",
      "Epoch: 006, Runtime 0.102279, Loss 2.240697, forward nfe 572, backward nfe 312, Train: 0.1700, Val: 0.0508, Test: 0.0611\n",
      "Epoch: 007, Runtime 0.100050, Loss 2.227862, forward nfe 676, backward nfe 364, Train: 0.1700, Val: 0.2677, Test: 0.2514\n",
      "Epoch: 008, Runtime 0.101990, Loss 2.279558, forward nfe 780, backward nfe 416, Train: 0.1400, Val: 0.2785, Test: 0.2582\n",
      "Epoch: 009, Runtime 0.100480, Loss 2.268469, forward nfe 884, backward nfe 468, Train: 0.1650, Val: 0.2315, Test: 0.2136\n",
      "Epoch: 010, Runtime 0.100980, Loss 2.233813, forward nfe 988, backward nfe 520, Train: 0.1400, Val: 0.0546, Test: 0.0607\n",
      "Epoch: 011, Runtime 0.100529, Loss 2.187289, forward nfe 1092, backward nfe 572, Train: 0.2400, Val: 0.0831, Test: 0.0925\n",
      "Epoch: 012, Runtime 0.097374, Loss 2.149594, forward nfe 1196, backward nfe 624, Train: 0.3150, Val: 0.1338, Test: 0.1456\n",
      "Epoch: 013, Runtime 0.100023, Loss 2.122253, forward nfe 1300, backward nfe 676, Train: 0.3450, Val: 0.1331, Test: 0.1466\n",
      "Epoch: 014, Runtime 0.101079, Loss 2.124206, forward nfe 1404, backward nfe 728, Train: 0.3100, Val: 0.0900, Test: 0.1168\n",
      "Epoch: 015, Runtime 0.099901, Loss 2.129122, forward nfe 1508, backward nfe 780, Train: 0.3350, Val: 0.1315, Test: 0.1512\n",
      "Epoch: 016, Runtime 0.102141, Loss 2.104205, forward nfe 1612, backward nfe 832, Train: 0.2750, Val: 0.1462, Test: 0.1539\n",
      "Epoch: 017, Runtime 0.100490, Loss 2.079625, forward nfe 1716, backward nfe 884, Train: 0.2650, Val: 0.1446, Test: 0.1532\n",
      "Epoch: 018, Runtime 0.100051, Loss 2.061285, forward nfe 1820, backward nfe 936, Train: 0.2450, Val: 0.1438, Test: 0.1499\n",
      "Epoch: 019, Runtime 0.101164, Loss 2.050672, forward nfe 1924, backward nfe 988, Train: 0.2550, Val: 0.1469, Test: 0.1596\n",
      "Epoch: 020, Runtime 0.100978, Loss 2.037084, forward nfe 2028, backward nfe 1040, Train: 0.2650, Val: 0.1538, Test: 0.1640\n",
      "Epoch: 021, Runtime 0.099957, Loss 2.011948, forward nfe 2132, backward nfe 1092, Train: 0.2900, Val: 0.1538, Test: 0.1670\n",
      "Epoch: 022, Runtime 0.097827, Loss 1.975120, forward nfe 2236, backward nfe 1144, Train: 0.3300, Val: 0.1577, Test: 0.1683\n",
      "Epoch: 023, Runtime 0.100437, Loss 1.949741, forward nfe 2340, backward nfe 1196, Train: 0.3450, Val: 0.1469, Test: 0.1631\n",
      "Epoch: 024, Runtime 0.100414, Loss 1.931399, forward nfe 2444, backward nfe 1248, Train: 0.3800, Val: 0.1531, Test: 0.1731\n",
      "Epoch: 025, Runtime 0.099119, Loss 1.912603, forward nfe 2548, backward nfe 1300, Train: 0.4100, Val: 0.1631, Test: 0.1858\n",
      "Epoch: 026, Runtime 0.100037, Loss 1.891827, forward nfe 2652, backward nfe 1352, Train: 0.3950, Val: 0.1577, Test: 0.1827\n",
      "Epoch: 027, Runtime 0.102777, Loss 1.870021, forward nfe 2756, backward nfe 1404, Train: 0.4200, Val: 0.1600, Test: 0.1875\n",
      "Epoch: 028, Runtime 0.097683, Loss 1.849880, forward nfe 2860, backward nfe 1456, Train: 0.4200, Val: 0.1931, Test: 0.2116\n",
      "Epoch: 029, Runtime 0.102895, Loss 1.819395, forward nfe 2964, backward nfe 1508, Train: 0.4200, Val: 0.1992, Test: 0.2154\n",
      "Epoch: 030, Runtime 0.098053, Loss 1.801840, forward nfe 3068, backward nfe 1560, Train: 0.4950, Val: 0.2185, Test: 0.2409\n",
      "Epoch: 031, Runtime 0.096699, Loss 1.779819, forward nfe 3172, backward nfe 1612, Train: 0.5100, Val: 0.2308, Test: 0.2595\n",
      "Epoch: 032, Runtime 0.102366, Loss 1.744448, forward nfe 3276, backward nfe 1664, Train: 0.5700, Val: 0.2062, Test: 0.2475\n",
      "Epoch: 033, Runtime 0.096220, Loss 1.718985, forward nfe 3380, backward nfe 1716, Train: 0.5450, Val: 0.1915, Test: 0.2267\n",
      "Epoch: 034, Runtime 0.101256, Loss 1.696456, forward nfe 3484, backward nfe 1768, Train: 0.5350, Val: 0.1854, Test: 0.2204\n",
      "Epoch: 035, Runtime 0.098861, Loss 1.666161, forward nfe 3588, backward nfe 1820, Train: 0.5150, Val: 0.1962, Test: 0.2333\n",
      "Epoch: 036, Runtime 0.102199, Loss 1.640280, forward nfe 3692, backward nfe 1872, Train: 0.5700, Val: 0.2277, Test: 0.2698\n",
      "Epoch: 037, Runtime 0.096148, Loss 1.608908, forward nfe 3796, backward nfe 1924, Train: 0.6100, Val: 0.2754, Test: 0.3164\n",
      "Epoch: 038, Runtime 0.100781, Loss 1.580231, forward nfe 3900, backward nfe 1976, Train: 0.6200, Val: 0.2992, Test: 0.3396\n",
      "Epoch: 039, Runtime 0.099147, Loss 1.560104, forward nfe 4004, backward nfe 2028, Train: 0.6200, Val: 0.3108, Test: 0.3522\n",
      "Epoch: 040, Runtime 0.103386, Loss 1.531716, forward nfe 4108, backward nfe 2080, Train: 0.6500, Val: 0.3054, Test: 0.3479\n",
      "Epoch: 041, Runtime 0.098576, Loss 1.500304, forward nfe 4212, backward nfe 2132, Train: 0.6600, Val: 0.2992, Test: 0.3437\n",
      "Epoch: 042, Runtime 0.098392, Loss 1.479490, forward nfe 4316, backward nfe 2184, Train: 0.6550, Val: 0.3231, Test: 0.3615\n",
      "Epoch: 043, Runtime 0.099436, Loss 1.448670, forward nfe 4420, backward nfe 2236, Train: 0.6850, Val: 0.3462, Test: 0.3909\n",
      "Epoch: 044, Runtime 0.101279, Loss 1.423943, forward nfe 4524, backward nfe 2288, Train: 0.7000, Val: 0.3708, Test: 0.4084\n",
      "Epoch: 045, Runtime 0.096322, Loss 1.396592, forward nfe 4628, backward nfe 2340, Train: 0.7100, Val: 0.3777, Test: 0.4194\n",
      "Epoch: 046, Runtime 0.098264, Loss 1.365235, forward nfe 4732, backward nfe 2392, Train: 0.7200, Val: 0.3808, Test: 0.4197\n",
      "Epoch: 047, Runtime 0.102978, Loss 1.345122, forward nfe 4836, backward nfe 2444, Train: 0.7200, Val: 0.3792, Test: 0.4159\n",
      "Epoch: 048, Runtime 0.102571, Loss 1.301811, forward nfe 4940, backward nfe 2496, Train: 0.5850, Val: 0.3200, Test: 0.3510\n",
      "Epoch: 049, Runtime 0.094323, Loss 1.309768, forward nfe 5044, backward nfe 2548, Train: 0.7500, Val: 0.4362, Test: 0.4701\n",
      "Epoch: 050, Runtime 0.099583, Loss 1.239638, forward nfe 5148, backward nfe 2600, Train: 0.7050, Val: 0.4700, Test: 0.5064\n",
      "Epoch: 051, Runtime 0.093732, Loss 1.242797, forward nfe 5252, backward nfe 2652, Train: 0.7300, Val: 0.4377, Test: 0.4796\n",
      "Epoch: 052, Runtime 0.101246, Loss 1.211731, forward nfe 5356, backward nfe 2704, Train: 0.7250, Val: 0.3969, Test: 0.4444\n",
      "Epoch: 053, Runtime 0.098048, Loss 1.169427, forward nfe 5460, backward nfe 2756, Train: 0.7600, Val: 0.3800, Test: 0.4207\n",
      "Epoch: 054, Runtime 0.098725, Loss 1.140226, forward nfe 5564, backward nfe 2808, Train: 0.7500, Val: 0.3685, Test: 0.4122\n",
      "Epoch: 055, Runtime 0.090901, Loss 1.128240, forward nfe 5668, backward nfe 2860, Train: 0.7550, Val: 0.3877, Test: 0.4370\n",
      "Epoch: 056, Runtime 0.101475, Loss 1.100896, forward nfe 5772, backward nfe 2912, Train: 0.7800, Val: 0.4331, Test: 0.4737\n",
      "Epoch: 057, Runtime 0.091009, Loss 1.070011, forward nfe 5876, backward nfe 2964, Train: 0.7550, Val: 0.4969, Test: 0.5300\n",
      "Epoch: 058, Runtime 0.102797, Loss 1.050908, forward nfe 5980, backward nfe 3016, Train: 0.7650, Val: 0.5477, Test: 0.5804\n",
      "Epoch: 059, Runtime 0.095738, Loss 1.035492, forward nfe 6084, backward nfe 3068, Train: 0.7600, Val: 0.5762, Test: 0.6121\n",
      "Epoch: 060, Runtime 0.099385, Loss 1.007656, forward nfe 6188, backward nfe 3120, Train: 0.7950, Val: 0.5831, Test: 0.6191\n",
      "Epoch: 061, Runtime 0.093618, Loss 0.986284, forward nfe 6292, backward nfe 3172, Train: 0.8050, Val: 0.5569, Test: 0.5945\n",
      "Epoch: 062, Runtime 0.101288, Loss 0.956423, forward nfe 6396, backward nfe 3224, Train: 0.8000, Val: 0.5469, Test: 0.5806\n",
      "Epoch: 063, Runtime 0.102659, Loss 0.945143, forward nfe 6500, backward nfe 3276, Train: 0.7950, Val: 0.5538, Test: 0.5914\n",
      "Epoch: 064, Runtime 0.100307, Loss 0.925357, forward nfe 6604, backward nfe 3328, Train: 0.8000, Val: 0.6215, Test: 0.6469\n",
      "Epoch: 065, Runtime 0.096483, Loss 0.898598, forward nfe 6708, backward nfe 3380, Train: 0.8150, Val: 0.6631, Test: 0.6903\n",
      "Epoch: 066, Runtime 0.099734, Loss 0.866301, forward nfe 6812, backward nfe 3432, Train: 0.8150, Val: 0.6800, Test: 0.7036\n",
      "Epoch: 067, Runtime 0.092458, Loss 0.857204, forward nfe 6916, backward nfe 3484, Train: 0.8350, Val: 0.6815, Test: 0.7042\n",
      "Epoch: 068, Runtime 0.097545, Loss 0.835716, forward nfe 7020, backward nfe 3536, Train: 0.8450, Val: 0.6777, Test: 0.7033\n",
      "Epoch: 069, Runtime 0.090088, Loss 0.814514, forward nfe 7124, backward nfe 3588, Train: 0.8450, Val: 0.6654, Test: 0.6975\n",
      "Epoch: 070, Runtime 0.102184, Loss 0.810511, forward nfe 7228, backward nfe 3640, Train: 0.8200, Val: 0.6577, Test: 0.6867\n",
      "Epoch: 071, Runtime 0.092522, Loss 0.782330, forward nfe 7332, backward nfe 3692, Train: 0.8100, Val: 0.6492, Test: 0.6827\n",
      "Epoch: 072, Runtime 0.098906, Loss 0.769755, forward nfe 7436, backward nfe 3744, Train: 0.8250, Val: 0.6677, Test: 0.6952\n",
      "Epoch: 073, Runtime 0.096052, Loss 0.764420, forward nfe 7540, backward nfe 3796, Train: 0.8300, Val: 0.6831, Test: 0.7089\n",
      "Epoch: 074, Runtime 0.099704, Loss 0.736918, forward nfe 7644, backward nfe 3848, Train: 0.8450, Val: 0.6915, Test: 0.7183\n",
      "Epoch: 075, Runtime 0.090601, Loss 0.720727, forward nfe 7748, backward nfe 3900, Train: 0.8400, Val: 0.7008, Test: 0.7229\n",
      "Epoch: 076, Runtime 0.098749, Loss 0.714444, forward nfe 7852, backward nfe 3952, Train: 0.8450, Val: 0.7046, Test: 0.7282\n",
      "Epoch: 077, Runtime 0.097818, Loss 0.699783, forward nfe 7956, backward nfe 4004, Train: 0.8400, Val: 0.7123, Test: 0.7314\n",
      "Epoch: 078, Runtime 0.100216, Loss 0.679076, forward nfe 8060, backward nfe 4056, Train: 0.8350, Val: 0.7131, Test: 0.7328\n",
      "Epoch: 079, Runtime 0.101784, Loss 0.668869, forward nfe 8164, backward nfe 4108, Train: 0.8400, Val: 0.7100, Test: 0.7328\n",
      "Epoch: 080, Runtime 0.102701, Loss 0.663674, forward nfe 8268, backward nfe 4160, Train: 0.8450, Val: 0.7100, Test: 0.7297\n",
      "Epoch: 081, Runtime 0.093510, Loss 0.636321, forward nfe 8372, backward nfe 4212, Train: 0.8500, Val: 0.7023, Test: 0.7289\n",
      "Epoch: 082, Runtime 0.097802, Loss 0.630629, forward nfe 8476, backward nfe 4264, Train: 0.8550, Val: 0.7092, Test: 0.7299\n",
      "Epoch: 083, Runtime 0.094029, Loss 0.618952, forward nfe 8580, backward nfe 4316, Train: 0.8500, Val: 0.7177, Test: 0.7338\n",
      "Epoch: 084, Runtime 0.101185, Loss 0.601750, forward nfe 8684, backward nfe 4368, Train: 0.8450, Val: 0.7223, Test: 0.7397\n",
      "Epoch: 085, Runtime 0.099611, Loss 0.599817, forward nfe 8788, backward nfe 4420, Train: 0.8400, Val: 0.7246, Test: 0.7449\n",
      "Epoch: 086, Runtime 0.102700, Loss 0.574225, forward nfe 8892, backward nfe 4472, Train: 0.8450, Val: 0.7277, Test: 0.7439\n",
      "Epoch: 087, Runtime 0.092026, Loss 0.572933, forward nfe 8996, backward nfe 4524, Train: 0.8550, Val: 0.7292, Test: 0.7425\n",
      "Epoch: 088, Runtime 0.101351, Loss 0.551559, forward nfe 9100, backward nfe 4576, Train: 0.8500, Val: 0.7277, Test: 0.7409\n",
      "Epoch: 089, Runtime 0.093246, Loss 0.540522, forward nfe 9204, backward nfe 4628, Train: 0.8500, Val: 0.7269, Test: 0.7427\n",
      "Epoch: 090, Runtime 0.099340, Loss 0.541300, forward nfe 9308, backward nfe 4680, Train: 0.8550, Val: 0.7354, Test: 0.7502\n",
      "Epoch: 091, Runtime 0.093441, Loss 0.532535, forward nfe 9412, backward nfe 4732, Train: 0.8500, Val: 0.7377, Test: 0.7529\n",
      "Epoch: 092, Runtime 0.102721, Loss 0.526231, forward nfe 9516, backward nfe 4784, Train: 0.8500, Val: 0.7369, Test: 0.7514\n",
      "Epoch: 093, Runtime 0.098084, Loss 0.503733, forward nfe 9620, backward nfe 4836, Train: 0.8700, Val: 0.7331, Test: 0.7500\n",
      "Epoch: 094, Runtime 0.102735, Loss 0.499824, forward nfe 9724, backward nfe 4888, Train: 0.8700, Val: 0.7377, Test: 0.7523\n",
      "Epoch: 095, Runtime 0.090789, Loss 0.492510, forward nfe 9828, backward nfe 4940, Train: 0.8650, Val: 0.7392, Test: 0.7540\n",
      "Epoch: 096, Runtime 0.102276, Loss 0.489599, forward nfe 9932, backward nfe 4992, Train: 0.8700, Val: 0.7408, Test: 0.7534\n",
      "Epoch: 097, Runtime 0.098763, Loss 0.472661, forward nfe 10036, backward nfe 5044, Train: 0.8600, Val: 0.7377, Test: 0.7527\n",
      "Epoch: 098, Runtime 0.096993, Loss 0.475169, forward nfe 10140, backward nfe 5096, Train: 0.8650, Val: 0.7469, Test: 0.7589\n",
      "Epoch: 099, Runtime 0.095912, Loss 0.457399, forward nfe 10244, backward nfe 5148, Train: 0.8550, Val: 0.7508, Test: 0.7606\n",
      "best val accuracy 0.750769 with test accuracy 0.760611 at epoch 99\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.101710, Loss 2.318587, forward nfe 52, backward nfe 52, Train: 0.1250, Val: 0.1315, Test: 0.1157\n",
      "Epoch: 002, Runtime 0.102415, Loss 2.575062, forward nfe 156, backward nfe 104, Train: 0.1050, Val: 0.0223, Test: 0.0311\n",
      "Epoch: 003, Runtime 0.093561, Loss 2.312470, forward nfe 260, backward nfe 156, Train: 0.1200, Val: 0.0238, Test: 0.0315\n",
      "Epoch: 004, Runtime 0.101208, Loss 2.326165, forward nfe 364, backward nfe 208, Train: 0.1250, Val: 0.0338, Test: 0.0380\n",
      "Epoch: 005, Runtime 0.092784, Loss 2.293021, forward nfe 468, backward nfe 260, Train: 0.1300, Val: 0.0669, Test: 0.0709\n",
      "Epoch: 006, Runtime 0.101736, Loss 2.262289, forward nfe 572, backward nfe 312, Train: 0.1150, Val: 0.1538, Test: 0.1517\n",
      "Epoch: 007, Runtime 0.091699, Loss 2.265223, forward nfe 676, backward nfe 364, Train: 0.1400, Val: 0.2646, Test: 0.2591\n",
      "Epoch: 008, Runtime 0.097623, Loss 2.265178, forward nfe 780, backward nfe 416, Train: 0.1400, Val: 0.3285, Test: 0.3235\n",
      "Epoch: 009, Runtime 0.102935, Loss 2.256235, forward nfe 884, backward nfe 468, Train: 0.1400, Val: 0.3815, Test: 0.3591\n",
      "Epoch: 010, Runtime 0.098932, Loss 2.249021, forward nfe 988, backward nfe 520, Train: 0.1450, Val: 0.3992, Test: 0.3757\n",
      "Epoch: 011, Runtime 0.091254, Loss 2.241103, forward nfe 1092, backward nfe 572, Train: 0.1500, Val: 0.3700, Test: 0.3553\n",
      "Epoch: 012, Runtime 0.097766, Loss 2.231762, forward nfe 1196, backward nfe 624, Train: 0.1600, Val: 0.3400, Test: 0.3317\n",
      "Epoch: 013, Runtime 0.093714, Loss 2.217176, forward nfe 1300, backward nfe 676, Train: 0.1550, Val: 0.3000, Test: 0.3015\n",
      "Epoch: 014, Runtime 0.100075, Loss 2.184250, forward nfe 1404, backward nfe 728, Train: 0.1650, Val: 0.3631, Test: 0.3603\n",
      "Epoch: 015, Runtime 0.102237, Loss 2.166718, forward nfe 1508, backward nfe 780, Train: 0.1650, Val: 0.3892, Test: 0.3799\n",
      "Epoch: 016, Runtime 0.101283, Loss 2.153226, forward nfe 1612, backward nfe 832, Train: 0.1800, Val: 0.3862, Test: 0.3722\n",
      "Epoch: 017, Runtime 0.091279, Loss 2.140348, forward nfe 1716, backward nfe 884, Train: 0.2150, Val: 0.4115, Test: 0.4010\n",
      "Epoch: 018, Runtime 0.101260, Loss 2.130376, forward nfe 1820, backward nfe 936, Train: 0.2200, Val: 0.4454, Test: 0.4243\n",
      "Epoch: 019, Runtime 0.089795, Loss 2.111633, forward nfe 1924, backward nfe 988, Train: 0.1700, Val: 0.3592, Test: 0.3521\n",
      "Epoch: 020, Runtime 0.099600, Loss 2.100872, forward nfe 2028, backward nfe 1040, Train: 0.2350, Val: 0.4931, Test: 0.4792\n",
      "Epoch: 021, Runtime 0.090324, Loss 2.087202, forward nfe 2132, backward nfe 1092, Train: 0.2600, Val: 0.3362, Test: 0.3437\n",
      "Epoch: 022, Runtime 0.099308, Loss 2.070884, forward nfe 2236, backward nfe 1144, Train: 0.2600, Val: 0.3646, Test: 0.3696\n",
      "Epoch: 023, Runtime 0.097797, Loss 2.051648, forward nfe 2340, backward nfe 1196, Train: 0.1700, Val: 0.2000, Test: 0.2064\n",
      "Epoch: 024, Runtime 0.102441, Loss 2.044267, forward nfe 2444, backward nfe 1248, Train: 0.2850, Val: 0.1992, Test: 0.2132\n",
      "Epoch: 025, Runtime 0.093814, Loss 2.025635, forward nfe 2548, backward nfe 1300, Train: 0.2800, Val: 0.1877, Test: 0.2049\n",
      "Epoch: 026, Runtime 0.098096, Loss 2.018932, forward nfe 2652, backward nfe 1352, Train: 0.2750, Val: 0.1900, Test: 0.2091\n",
      "Epoch: 027, Runtime 0.091416, Loss 1.993603, forward nfe 2756, backward nfe 1404, Train: 0.2750, Val: 0.1892, Test: 0.2072\n",
      "Epoch: 028, Runtime 0.099485, Loss 1.975679, forward nfe 2860, backward nfe 1456, Train: 0.2600, Val: 0.1331, Test: 0.1530\n",
      "Epoch: 029, Runtime 0.098669, Loss 1.952107, forward nfe 2964, backward nfe 1508, Train: 0.2650, Val: 0.1246, Test: 0.1403\n",
      "Epoch: 030, Runtime 0.099412, Loss 1.945989, forward nfe 3068, backward nfe 1560, Train: 0.3200, Val: 0.1677, Test: 0.1933\n",
      "Epoch: 031, Runtime 0.102623, Loss 1.910819, forward nfe 3172, backward nfe 1612, Train: 0.3200, Val: 0.1838, Test: 0.2080\n",
      "Epoch: 032, Runtime 0.101595, Loss 1.885594, forward nfe 3276, backward nfe 1664, Train: 0.2750, Val: 0.1869, Test: 0.2107\n",
      "Epoch: 033, Runtime 0.090779, Loss 1.878672, forward nfe 3380, backward nfe 1716, Train: 0.2950, Val: 0.1808, Test: 0.2020\n",
      "Epoch: 034, Runtime 0.098961, Loss 1.859697, forward nfe 3484, backward nfe 1768, Train: 0.3300, Val: 0.1715, Test: 0.1957\n",
      "Epoch: 035, Runtime 0.094315, Loss 1.817401, forward nfe 3588, backward nfe 1820, Train: 0.4050, Val: 0.0938, Test: 0.1303\n",
      "Epoch: 036, Runtime 0.098157, Loss 1.821007, forward nfe 3692, backward nfe 1872, Train: 0.4100, Val: 0.1023, Test: 0.1437\n",
      "Epoch: 037, Runtime 0.090330, Loss 1.782299, forward nfe 3796, backward nfe 1924, Train: 0.4250, Val: 0.1546, Test: 0.1972\n",
      "Epoch: 038, Runtime 0.100102, Loss 1.738611, forward nfe 3900, backward nfe 1976, Train: 0.3600, Val: 0.1915, Test: 0.2207\n",
      "Epoch: 039, Runtime 0.090678, Loss 1.732781, forward nfe 4004, backward nfe 2028, Train: 0.4300, Val: 0.2008, Test: 0.2440\n",
      "Epoch: 040, Runtime 0.099774, Loss 1.705576, forward nfe 4108, backward nfe 2080, Train: 0.3800, Val: 0.1515, Test: 0.1920\n",
      "Epoch: 041, Runtime 0.097642, Loss 1.669307, forward nfe 4212, backward nfe 2132, Train: 0.3400, Val: 0.1192, Test: 0.1544\n",
      "Epoch: 042, Runtime 0.099915, Loss 1.662421, forward nfe 4316, backward nfe 2184, Train: 0.3500, Val: 0.1377, Test: 0.1689\n",
      "Epoch: 043, Runtime 0.090516, Loss 1.626231, forward nfe 4420, backward nfe 2236, Train: 0.3950, Val: 0.1815, Test: 0.2196\n",
      "Epoch: 044, Runtime 0.100170, Loss 1.604217, forward nfe 4524, backward nfe 2288, Train: 0.4100, Val: 0.2100, Test: 0.2503\n",
      "Epoch: 045, Runtime 0.094799, Loss 1.592584, forward nfe 4628, backward nfe 2340, Train: 0.4000, Val: 0.2231, Test: 0.2598\n",
      "Epoch: 046, Runtime 0.102049, Loss 1.564686, forward nfe 4732, backward nfe 2392, Train: 0.3600, Val: 0.2162, Test: 0.2556\n",
      "Epoch: 047, Runtime 0.093494, Loss 1.548867, forward nfe 4836, backward nfe 2444, Train: 0.3750, Val: 0.2462, Test: 0.2760\n",
      "Epoch: 048, Runtime 0.097803, Loss 1.528218, forward nfe 4940, backward nfe 2496, Train: 0.3950, Val: 0.3015, Test: 0.3270\n",
      "Epoch: 049, Runtime 0.092865, Loss 1.505953, forward nfe 5044, backward nfe 2548, Train: 0.4850, Val: 0.4315, Test: 0.4520\n",
      "Epoch: 050, Runtime 0.100898, Loss 1.489692, forward nfe 5148, backward nfe 2600, Train: 0.4950, Val: 0.4692, Test: 0.4827\n",
      "Epoch: 051, Runtime 0.092088, Loss 1.467152, forward nfe 5252, backward nfe 2652, Train: 0.4750, Val: 0.4577, Test: 0.4743\n",
      "Epoch: 052, Runtime 0.100680, Loss 1.444244, forward nfe 5356, backward nfe 2704, Train: 0.4450, Val: 0.4046, Test: 0.4240\n",
      "Epoch: 053, Runtime 0.092038, Loss 1.431381, forward nfe 5460, backward nfe 2756, Train: 0.4750, Val: 0.4023, Test: 0.4360\n",
      "Epoch: 054, Runtime 0.098379, Loss 1.405950, forward nfe 5564, backward nfe 2808, Train: 0.5500, Val: 0.4823, Test: 0.5077\n",
      "Epoch: 055, Runtime 0.089453, Loss 1.396568, forward nfe 5668, backward nfe 2860, Train: 0.5900, Val: 0.5077, Test: 0.5291\n",
      "Epoch: 056, Runtime 0.101682, Loss 1.372581, forward nfe 5772, backward nfe 2912, Train: 0.5700, Val: 0.5254, Test: 0.5384\n",
      "Epoch: 057, Runtime 0.096469, Loss 1.349115, forward nfe 5876, backward nfe 2964, Train: 0.5600, Val: 0.5208, Test: 0.5353\n",
      "Epoch: 058, Runtime 0.098159, Loss 1.333338, forward nfe 5980, backward nfe 3016, Train: 0.5650, Val: 0.5146, Test: 0.5309\n",
      "Epoch: 059, Runtime 0.090302, Loss 1.317226, forward nfe 6084, backward nfe 3068, Train: 0.5950, Val: 0.5223, Test: 0.5421\n",
      "Epoch: 060, Runtime 0.098849, Loss 1.300580, forward nfe 6188, backward nfe 3120, Train: 0.5950, Val: 0.5346, Test: 0.5522\n",
      "Epoch: 061, Runtime 0.090317, Loss 1.278026, forward nfe 6292, backward nfe 3172, Train: 0.6300, Val: 0.5423, Test: 0.5669\n",
      "Epoch: 062, Runtime 0.098400, Loss 1.239518, forward nfe 6396, backward nfe 3224, Train: 0.6300, Val: 0.5746, Test: 0.5885\n",
      "Epoch: 063, Runtime 0.091770, Loss 1.223208, forward nfe 6500, backward nfe 3276, Train: 0.6450, Val: 0.5692, Test: 0.6028\n",
      "Epoch: 064, Runtime 0.098024, Loss 1.194785, forward nfe 6604, backward nfe 3328, Train: 0.5900, Val: 0.5546, Test: 0.5753\n",
      "Epoch: 065, Runtime 0.089662, Loss 1.158356, forward nfe 6708, backward nfe 3380, Train: 0.6050, Val: 0.5577, Test: 0.5842\n",
      "Epoch: 066, Runtime 0.100191, Loss 1.157743, forward nfe 6812, backward nfe 3432, Train: 0.6050, Val: 0.5523, Test: 0.5777\n",
      "Epoch: 067, Runtime 0.090200, Loss 1.136425, forward nfe 6916, backward nfe 3484, Train: 0.6300, Val: 0.5631, Test: 0.5853\n",
      "Epoch: 068, Runtime 0.099956, Loss 1.111913, forward nfe 7020, backward nfe 3536, Train: 0.6300, Val: 0.5623, Test: 0.5862\n",
      "Epoch: 069, Runtime 0.089657, Loss 1.076541, forward nfe 7124, backward nfe 3588, Train: 0.6600, Val: 0.5738, Test: 0.6046\n",
      "Epoch: 070, Runtime 0.099210, Loss 1.051474, forward nfe 7228, backward nfe 3640, Train: 0.6500, Val: 0.5731, Test: 0.6087\n",
      "Epoch: 071, Runtime 0.096769, Loss 1.036775, forward nfe 7332, backward nfe 3692, Train: 0.6900, Val: 0.5769, Test: 0.6005\n",
      "Epoch: 072, Runtime 0.102700, Loss 1.011151, forward nfe 7436, backward nfe 3744, Train: 0.6650, Val: 0.5692, Test: 0.5975\n",
      "Epoch: 073, Runtime 0.092900, Loss 0.994097, forward nfe 7540, backward nfe 3796, Train: 0.6850, Val: 0.5854, Test: 0.6071\n",
      "Epoch: 074, Runtime 0.098352, Loss 0.956626, forward nfe 7644, backward nfe 3848, Train: 0.6650, Val: 0.5908, Test: 0.6237\n",
      "Epoch: 075, Runtime 0.091237, Loss 0.954534, forward nfe 7748, backward nfe 3900, Train: 0.7300, Val: 0.6054, Test: 0.6343\n",
      "Epoch: 076, Runtime 0.097449, Loss 0.910410, forward nfe 7852, backward nfe 3952, Train: 0.7450, Val: 0.6015, Test: 0.6267\n",
      "Epoch: 077, Runtime 0.090313, Loss 0.909552, forward nfe 7956, backward nfe 4004, Train: 0.7600, Val: 0.6054, Test: 0.6316\n",
      "Epoch: 078, Runtime 0.100949, Loss 0.866846, forward nfe 8060, backward nfe 4056, Train: 0.7650, Val: 0.6138, Test: 0.6447\n",
      "Epoch: 079, Runtime 0.096921, Loss 0.848757, forward nfe 8164, backward nfe 4108, Train: 0.7850, Val: 0.6354, Test: 0.6643\n",
      "Epoch: 080, Runtime 0.099174, Loss 0.836151, forward nfe 8268, backward nfe 4160, Train: 0.8000, Val: 0.6369, Test: 0.6690\n",
      "Epoch: 081, Runtime 0.089993, Loss 0.803248, forward nfe 8372, backward nfe 4212, Train: 0.8050, Val: 0.6485, Test: 0.6796\n",
      "Epoch: 082, Runtime 0.098613, Loss 0.774973, forward nfe 8476, backward nfe 4264, Train: 0.8050, Val: 0.6331, Test: 0.6636\n",
      "Epoch: 083, Runtime 0.090379, Loss 0.763226, forward nfe 8580, backward nfe 4316, Train: 0.8100, Val: 0.6369, Test: 0.6668\n",
      "Epoch: 084, Runtime 0.101297, Loss 0.717967, forward nfe 8684, backward nfe 4368, Train: 0.8050, Val: 0.6585, Test: 0.6893\n",
      "Epoch: 085, Runtime 0.098402, Loss 0.689801, forward nfe 8788, backward nfe 4420, Train: 0.8350, Val: 0.6600, Test: 0.6905\n",
      "Epoch: 086, Runtime 0.099957, Loss 0.683198, forward nfe 8892, backward nfe 4472, Train: 0.8300, Val: 0.6646, Test: 0.6892\n",
      "Epoch: 087, Runtime 0.091329, Loss 0.655711, forward nfe 8996, backward nfe 4524, Train: 0.8300, Val: 0.6808, Test: 0.6996\n",
      "Epoch: 088, Runtime 0.099397, Loss 0.654154, forward nfe 9100, backward nfe 4576, Train: 0.8400, Val: 0.6785, Test: 0.7060\n",
      "Epoch: 089, Runtime 0.090791, Loss 0.617027, forward nfe 9204, backward nfe 4628, Train: 0.8400, Val: 0.6777, Test: 0.6995\n",
      "Epoch: 090, Runtime 0.099527, Loss 0.591525, forward nfe 9308, backward nfe 4680, Train: 0.8400, Val: 0.6954, Test: 0.7169\n",
      "Epoch: 091, Runtime 0.097319, Loss 0.583311, forward nfe 9412, backward nfe 4732, Train: 0.8400, Val: 0.6938, Test: 0.7129\n",
      "Epoch: 092, Runtime 0.102439, Loss 0.587985, forward nfe 9516, backward nfe 4784, Train: 0.8500, Val: 0.6738, Test: 0.7015\n",
      "Epoch: 093, Runtime 0.092177, Loss 0.549739, forward nfe 9620, backward nfe 4836, Train: 0.8550, Val: 0.6662, Test: 0.6912\n",
      "Epoch: 094, Runtime 0.098320, Loss 0.543105, forward nfe 9724, backward nfe 4888, Train: 0.8400, Val: 0.6877, Test: 0.7167\n",
      "Epoch: 095, Runtime 0.091086, Loss 0.524747, forward nfe 9828, backward nfe 4940, Train: 0.8450, Val: 0.7069, Test: 0.7274\n",
      "Epoch: 096, Runtime 0.099973, Loss 0.492276, forward nfe 9932, backward nfe 4992, Train: 0.8600, Val: 0.7092, Test: 0.7307\n",
      "Epoch: 097, Runtime 0.089784, Loss 0.498396, forward nfe 10036, backward nfe 5044, Train: 0.8600, Val: 0.6962, Test: 0.7274\n",
      "Epoch: 098, Runtime 0.098611, Loss 0.478231, forward nfe 10140, backward nfe 5096, Train: 0.8700, Val: 0.7115, Test: 0.7301\n",
      "Epoch: 099, Runtime 0.090471, Loss 0.474387, forward nfe 10244, backward nfe 5148, Train: 0.8600, Val: 0.7085, Test: 0.7298\n",
      "best val accuracy 0.711538 with test accuracy 0.730085 at epoch 98\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'rk4', 'step_size': 1.0, 'adjoint_method': 'rk4', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.096400, Loss 2.317057, forward nfe 52, backward nfe 52, Train: 0.1000, Val: 0.0131, Test: 0.0288\n",
      "Epoch: 002, Runtime 0.099488, Loss 2.402653, forward nfe 156, backward nfe 104, Train: 0.1100, Val: 0.0169, Test: 0.0220\n",
      "Epoch: 003, Runtime 0.091461, Loss 2.494648, forward nfe 260, backward nfe 156, Train: 0.1850, Val: 0.1162, Test: 0.1179\n",
      "Epoch: 004, Runtime 0.098618, Loss 2.355863, forward nfe 364, backward nfe 208, Train: 0.1750, Val: 0.0915, Test: 0.1090\n",
      "Epoch: 005, Runtime 0.101515, Loss 2.250841, forward nfe 468, backward nfe 260, Train: 0.0850, Val: 0.0192, Test: 0.0318\n",
      "Epoch: 006, Runtime 0.098371, Loss 2.280982, forward nfe 572, backward nfe 312, Train: 0.0800, Val: 0.0177, Test: 0.0306\n",
      "Epoch: 007, Runtime 0.090382, Loss 2.263446, forward nfe 676, backward nfe 364, Train: 0.2350, Val: 0.0400, Test: 0.0735\n",
      "Epoch: 008, Runtime 0.100801, Loss 2.217016, forward nfe 780, backward nfe 416, Train: 0.2800, Val: 0.0569, Test: 0.0904\n",
      "Epoch: 009, Runtime 0.099638, Loss 2.205923, forward nfe 884, backward nfe 468, Train: 0.2450, Val: 0.0469, Test: 0.0802\n",
      "Epoch: 010, Runtime 0.099934, Loss 2.191885, forward nfe 988, backward nfe 520, Train: 0.2900, Val: 0.0500, Test: 0.0879\n",
      "Epoch: 011, Runtime 0.100576, Loss 2.154743, forward nfe 1092, backward nfe 572, Train: 0.2900, Val: 0.0631, Test: 0.0985\n",
      "Epoch: 012, Runtime 0.098297, Loss 2.130292, forward nfe 1196, backward nfe 624, Train: 0.2750, Val: 0.0862, Test: 0.1206\n",
      "Epoch: 013, Runtime 0.101533, Loss 2.113238, forward nfe 1300, backward nfe 676, Train: 0.2700, Val: 0.0931, Test: 0.1288\n",
      "Epoch: 014, Runtime 0.101174, Loss 2.087128, forward nfe 1404, backward nfe 728, Train: 0.3050, Val: 0.0915, Test: 0.1290\n",
      "Epoch: 015, Runtime 0.092410, Loss 2.055247, forward nfe 1508, backward nfe 780, Train: 0.3150, Val: 0.0723, Test: 0.1135\n",
      "Epoch: 016, Runtime 0.101149, Loss 2.023854, forward nfe 1612, backward nfe 832, Train: 0.3250, Val: 0.0808, Test: 0.1216\n",
      "Epoch: 017, Runtime 0.095482, Loss 2.007123, forward nfe 1716, backward nfe 884, Train: 0.3200, Val: 0.0738, Test: 0.1119\n",
      "Epoch: 018, Runtime 0.100509, Loss 1.973970, forward nfe 1820, backward nfe 936, Train: 0.3650, Val: 0.1092, Test: 0.1418\n",
      "Epoch: 019, Runtime 0.092418, Loss 1.939458, forward nfe 1924, backward nfe 988, Train: 0.4200, Val: 0.1408, Test: 0.1872\n",
      "Epoch: 020, Runtime 0.101660, Loss 1.913351, forward nfe 2028, backward nfe 1040, Train: 0.4400, Val: 0.1769, Test: 0.2178\n",
      "Epoch: 021, Runtime 0.102450, Loss 1.883015, forward nfe 2132, backward nfe 1092, Train: 0.4150, Val: 0.1731, Test: 0.2148\n",
      "Epoch: 022, Runtime 0.099189, Loss 1.846774, forward nfe 2236, backward nfe 1144, Train: 0.4150, Val: 0.1446, Test: 0.1879\n",
      "Epoch: 023, Runtime 0.092085, Loss 1.810248, forward nfe 2340, backward nfe 1196, Train: 0.4650, Val: 0.1646, Test: 0.2107\n",
      "Epoch: 024, Runtime 0.102367, Loss 1.775811, forward nfe 2444, backward nfe 1248, Train: 0.4900, Val: 0.1823, Test: 0.2225\n",
      "Epoch: 025, Runtime 0.100415, Loss 1.743547, forward nfe 2548, backward nfe 1300, Train: 0.4750, Val: 0.1923, Test: 0.2330\n",
      "Epoch: 026, Runtime 0.098615, Loss 1.713050, forward nfe 2652, backward nfe 1352, Train: 0.4650, Val: 0.2085, Test: 0.2462\n",
      "Epoch: 027, Runtime 0.098953, Loss 1.675909, forward nfe 2756, backward nfe 1404, Train: 0.4750, Val: 0.2146, Test: 0.2542\n",
      "Epoch: 028, Runtime 0.102424, Loss 1.647157, forward nfe 2860, backward nfe 1456, Train: 0.4800, Val: 0.2246, Test: 0.2600\n",
      "Epoch: 029, Runtime 0.090960, Loss 1.605237, forward nfe 2964, backward nfe 1508, Train: 0.4800, Val: 0.2408, Test: 0.2714\n",
      "Epoch: 030, Runtime 0.099525, Loss 1.567287, forward nfe 3068, backward nfe 1560, Train: 0.4900, Val: 0.2408, Test: 0.2785\n",
      "Epoch: 031, Runtime 0.094409, Loss 1.528136, forward nfe 3172, backward nfe 1612, Train: 0.4900, Val: 0.2477, Test: 0.2770\n",
      "Epoch: 032, Runtime 0.100444, Loss 1.484579, forward nfe 3276, backward nfe 1664, Train: 0.4800, Val: 0.2646, Test: 0.2933\n",
      "Epoch: 033, Runtime 0.089594, Loss 1.458625, forward nfe 3380, backward nfe 1716, Train: 0.5400, Val: 0.3508, Test: 0.3716\n",
      "Epoch: 034, Runtime 0.098712, Loss 1.419872, forward nfe 3484, backward nfe 1768, Train: 0.5800, Val: 0.3731, Test: 0.3981\n",
      "Epoch: 035, Runtime 0.090812, Loss 1.383771, forward nfe 3588, backward nfe 1820, Train: 0.6100, Val: 0.4185, Test: 0.4491\n",
      "Epoch: 036, Runtime 0.098891, Loss 1.354242, forward nfe 3692, backward nfe 1872, Train: 0.6100, Val: 0.4800, Test: 0.5126\n",
      "Epoch: 037, Runtime 0.092325, Loss 1.332349, forward nfe 3796, backward nfe 1924, Train: 0.6300, Val: 0.4931, Test: 0.5283\n",
      "Epoch: 038, Runtime 0.101548, Loss 1.296793, forward nfe 3900, backward nfe 1976, Train: 0.6500, Val: 0.4754, Test: 0.5044\n",
      "Epoch: 039, Runtime 0.090511, Loss 1.271040, forward nfe 4004, backward nfe 2028, Train: 0.6750, Val: 0.4715, Test: 0.4960\n",
      "Epoch: 040, Runtime 0.099293, Loss 1.235582, forward nfe 4108, backward nfe 2080, Train: 0.6900, Val: 0.4685, Test: 0.4981\n",
      "Epoch: 041, Runtime 0.089842, Loss 1.208148, forward nfe 4212, backward nfe 2132, Train: 0.6950, Val: 0.4677, Test: 0.4929\n",
      "Epoch: 042, Runtime 0.100716, Loss 1.184372, forward nfe 4316, backward nfe 2184, Train: 0.6900, Val: 0.4746, Test: 0.5047\n",
      "Epoch: 043, Runtime 0.090917, Loss 1.150723, forward nfe 4420, backward nfe 2236, Train: 0.6750, Val: 0.4985, Test: 0.5232\n",
      "Epoch: 044, Runtime 0.099847, Loss 1.131939, forward nfe 4524, backward nfe 2288, Train: 0.6950, Val: 0.5254, Test: 0.5512\n",
      "Epoch: 045, Runtime 0.100273, Loss 1.100717, forward nfe 4628, backward nfe 2340, Train: 0.7000, Val: 0.5415, Test: 0.5738\n",
      "Epoch: 046, Runtime 0.100717, Loss 1.072910, forward nfe 4732, backward nfe 2392, Train: 0.7150, Val: 0.5823, Test: 0.6042\n",
      "Epoch: 047, Runtime 0.092939, Loss 1.048388, forward nfe 4836, backward nfe 2444, Train: 0.7450, Val: 0.5869, Test: 0.6183\n",
      "Epoch: 048, Runtime 0.099001, Loss 1.034781, forward nfe 4940, backward nfe 2496, Train: 0.7450, Val: 0.5831, Test: 0.6105\n",
      "Epoch: 049, Runtime 0.097724, Loss 1.003198, forward nfe 5044, backward nfe 2548, Train: 0.7400, Val: 0.5923, Test: 0.6217\n",
      "Epoch: 050, Runtime 0.102009, Loss 0.971703, forward nfe 5148, backward nfe 2600, Train: 0.7450, Val: 0.6177, Test: 0.6418\n",
      "Epoch: 051, Runtime 0.096960, Loss 0.958421, forward nfe 5252, backward nfe 2652, Train: 0.7500, Val: 0.6200, Test: 0.6437\n",
      "Epoch: 052, Runtime 0.098023, Loss 0.937968, forward nfe 5356, backward nfe 2704, Train: 0.7750, Val: 0.6269, Test: 0.6523\n",
      "Epoch: 053, Runtime 0.090632, Loss 0.908308, forward nfe 5460, backward nfe 2756, Train: 0.7650, Val: 0.6392, Test: 0.6594\n",
      "Epoch: 054, Runtime 0.097741, Loss 0.892607, forward nfe 5564, backward nfe 2808, Train: 0.7800, Val: 0.6354, Test: 0.6593\n",
      "Epoch: 055, Runtime 0.099277, Loss 0.856253, forward nfe 5668, backward nfe 2860, Train: 0.7850, Val: 0.6454, Test: 0.6654\n",
      "Epoch: 056, Runtime 0.098107, Loss 0.845301, forward nfe 5772, backward nfe 2912, Train: 0.8150, Val: 0.6346, Test: 0.6595\n",
      "Epoch: 057, Runtime 0.099981, Loss 0.823326, forward nfe 5876, backward nfe 2964, Train: 0.8150, Val: 0.6362, Test: 0.6619\n",
      "Epoch: 058, Runtime 0.099654, Loss 0.794434, forward nfe 5980, backward nfe 3016, Train: 0.8200, Val: 0.6438, Test: 0.6721\n",
      "Epoch: 059, Runtime 0.100402, Loss 0.794212, forward nfe 6084, backward nfe 3068, Train: 0.8100, Val: 0.6508, Test: 0.6805\n",
      "Epoch: 060, Runtime 0.099851, Loss 0.750381, forward nfe 6188, backward nfe 3120, Train: 0.8100, Val: 0.6638, Test: 0.6892\n",
      "Epoch: 061, Runtime 0.092088, Loss 0.741377, forward nfe 6292, backward nfe 3172, Train: 0.8200, Val: 0.6662, Test: 0.6897\n",
      "Epoch: 062, Runtime 0.099818, Loss 0.730537, forward nfe 6396, backward nfe 3224, Train: 0.8150, Val: 0.6600, Test: 0.6888\n",
      "Epoch: 063, Runtime 0.099664, Loss 0.720006, forward nfe 6500, backward nfe 3276, Train: 0.8100, Val: 0.6700, Test: 0.6938\n",
      "Epoch: 064, Runtime 0.099749, Loss 0.697245, forward nfe 6604, backward nfe 3328, Train: 0.8250, Val: 0.6746, Test: 0.6998\n",
      "Epoch: 065, Runtime 0.090679, Loss 0.675269, forward nfe 6708, backward nfe 3380, Train: 0.8400, Val: 0.6746, Test: 0.6998\n",
      "Epoch: 066, Runtime 0.097655, Loss 0.670380, forward nfe 6812, backward nfe 3432, Train: 0.8400, Val: 0.6723, Test: 0.6976\n",
      "Epoch: 067, Runtime 0.092349, Loss 0.641443, forward nfe 6916, backward nfe 3484, Train: 0.8300, Val: 0.6669, Test: 0.6924\n",
      "Epoch: 068, Runtime 0.101235, Loss 0.625279, forward nfe 7020, backward nfe 3536, Train: 0.8300, Val: 0.6723, Test: 0.7000\n",
      "Epoch: 069, Runtime 0.099458, Loss 0.624361, forward nfe 7124, backward nfe 3588, Train: 0.8400, Val: 0.6769, Test: 0.7044\n",
      "Epoch: 070, Runtime 0.099258, Loss 0.596236, forward nfe 7228, backward nfe 3640, Train: 0.8450, Val: 0.6785, Test: 0.7103\n",
      "Epoch: 071, Runtime 0.094255, Loss 0.596477, forward nfe 7332, backward nfe 3692, Train: 0.8350, Val: 0.6869, Test: 0.7115\n",
      "Epoch: 072, Runtime 0.102498, Loss 0.569439, forward nfe 7436, backward nfe 3744, Train: 0.8400, Val: 0.6777, Test: 0.7049\n",
      "Epoch: 073, Runtime 0.090607, Loss 0.566713, forward nfe 7540, backward nfe 3796, Train: 0.8400, Val: 0.6754, Test: 0.7067\n",
      "Epoch: 074, Runtime 0.100379, Loss 0.556831, forward nfe 7644, backward nfe 3848, Train: 0.8500, Val: 0.6862, Test: 0.7169\n",
      "Epoch: 075, Runtime 0.093851, Loss 0.545133, forward nfe 7748, backward nfe 3900, Train: 0.8400, Val: 0.6977, Test: 0.7251\n",
      "Epoch: 076, Runtime 0.100591, Loss 0.534387, forward nfe 7852, backward nfe 3952, Train: 0.8500, Val: 0.6908, Test: 0.7207\n",
      "Epoch: 077, Runtime 0.090942, Loss 0.520685, forward nfe 7956, backward nfe 4004, Train: 0.8550, Val: 0.6854, Test: 0.7166\n",
      "Epoch: 078, Runtime 0.101781, Loss 0.507292, forward nfe 8060, backward nfe 4056, Train: 0.8550, Val: 0.6962, Test: 0.7253\n",
      "Epoch: 079, Runtime 0.090560, Loss 0.510297, forward nfe 8164, backward nfe 4108, Train: 0.8650, Val: 0.6985, Test: 0.7299\n",
      "Epoch: 080, Runtime 0.097994, Loss 0.496748, forward nfe 8268, backward nfe 4160, Train: 0.8450, Val: 0.6992, Test: 0.7280\n",
      "Epoch: 081, Runtime 0.097705, Loss 0.483990, forward nfe 8372, backward nfe 4212, Train: 0.8700, Val: 0.6808, Test: 0.7140\n",
      "Epoch: 082, Runtime 0.100914, Loss 0.482886, forward nfe 8476, backward nfe 4264, Train: 0.8750, Val: 0.6854, Test: 0.7183\n",
      "Epoch: 083, Runtime 0.091091, Loss 0.479256, forward nfe 8580, backward nfe 4316, Train: 0.8600, Val: 0.7023, Test: 0.7304\n",
      "Epoch: 084, Runtime 0.099093, Loss 0.464737, forward nfe 8684, backward nfe 4368, Train: 0.8650, Val: 0.7046, Test: 0.7358\n",
      "Epoch: 085, Runtime 0.089755, Loss 0.440918, forward nfe 8788, backward nfe 4420, Train: 0.8700, Val: 0.7031, Test: 0.7346\n",
      "Epoch: 086, Runtime 0.102682, Loss 0.440541, forward nfe 8892, backward nfe 4472, Train: 0.8550, Val: 0.7000, Test: 0.7338\n",
      "Epoch: 087, Runtime 0.090684, Loss 0.442475, forward nfe 8996, backward nfe 4524, Train: 0.8600, Val: 0.7008, Test: 0.7335\n",
      "Epoch: 088, Runtime 0.098488, Loss 0.432765, forward nfe 9100, backward nfe 4576, Train: 0.8800, Val: 0.7015, Test: 0.7319\n",
      "Epoch: 089, Runtime 0.098778, Loss 0.429171, forward nfe 9204, backward nfe 4628, Train: 0.8600, Val: 0.7131, Test: 0.7457\n",
      "Epoch: 090, Runtime 0.100277, Loss 0.425157, forward nfe 9308, backward nfe 4680, Train: 0.8550, Val: 0.7231, Test: 0.7507\n",
      "Epoch: 091, Runtime 0.089411, Loss 0.411690, forward nfe 9412, backward nfe 4732, Train: 0.8650, Val: 0.7154, Test: 0.7458\n",
      "Epoch: 092, Runtime 0.099347, Loss 0.388356, forward nfe 9516, backward nfe 4784, Train: 0.8850, Val: 0.7062, Test: 0.7380\n",
      "Epoch: 093, Runtime 0.090064, Loss 0.400364, forward nfe 9620, backward nfe 4836, Train: 0.8650, Val: 0.7077, Test: 0.7409\n",
      "Epoch: 094, Runtime 0.101250, Loss 0.396379, forward nfe 9724, backward nfe 4888, Train: 0.8700, Val: 0.7092, Test: 0.7413\n",
      "Epoch: 095, Runtime 0.090787, Loss 0.396030, forward nfe 9828, backward nfe 4940, Train: 0.8750, Val: 0.7154, Test: 0.7480\n",
      "Epoch: 096, Runtime 0.100292, Loss 0.386097, forward nfe 9932, backward nfe 4992, Train: 0.8850, Val: 0.7177, Test: 0.7506\n",
      "Epoch: 097, Runtime 0.090729, Loss 0.378330, forward nfe 10036, backward nfe 5044, Train: 0.8700, Val: 0.7238, Test: 0.7568\n",
      "Epoch: 098, Runtime 0.098633, Loss 0.382444, forward nfe 10140, backward nfe 5096, Train: 0.8750, Val: 0.7262, Test: 0.7547\n",
      "Epoch: 099, Runtime 0.089738, Loss 0.362166, forward nfe 10244, backward nfe 5148, Train: 0.8800, Val: 0.7238, Test: 0.7538\n",
      "best val accuracy 0.726154 with test accuracy 0.754734 at epoch 98\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_cora_defaults', action='store_true',\n",
    "                  help='Whether to run with best params for cora. Overrides the choice of dataset')\n",
    "parser.add_argument('--dataset', type=str, default='Cora',\n",
    "                  help='Cora, Citeseer, Pubmed, Computers, Photo, CoauthorCS')\n",
    "parser.add_argument('--data_norm', type=str, default='rw',\n",
    "                  help='rw for random walk, gcn for symmetric gcn norm')\n",
    "parser.add_argument('--hidden_dim', type=int, default=16, help='Hidden dimension.')\n",
    "parser.add_argument('--input_dropout', type=float, default=0.5, help='Input dropout rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='One from sgd, rmsprop, adam, adagrad, adamax.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Learning rate.')\n",
    "parser.add_argument('--decay', type=float, default=5e-4, help='Weight decay for optimization')\n",
    "parser.add_argument('--self_loop_weight', type=float, default=1.0, help='Weight of self-loops.')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='Number of training epochs per iteration.')\n",
    "parser.add_argument('--alpha', type=float, default=1.0, help='Factor in front matrix A.')\n",
    "parser.add_argument('--time', type=float, default=1.0, help='End time of ODE integrator.')\n",
    "parser.add_argument('--augment', action='store_true',\n",
    "                  help='double the length of the feature vector by appending zeros to stabilist ODE learning')\n",
    "parser.add_argument('--alpha_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) alpha')\n",
    "parser.add_argument('--no_alpha_sigmoid', dest='no_alpha_sigmoid', action='store_true', help='apply sigmoid before multiplying by alpha')\n",
    "parser.add_argument('--beta_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) beta')\n",
    "parser.add_argument('--block', type=str, default='constant', help='constant, mixed, attention, SDE')\n",
    "parser.add_argument('--function', type=str, default='laplacian', help='laplacian, transformer, dorsey, GAT, SDE')\n",
    "parser.add_argument('--geom_gcn_splits', dest='geom_gcn_splits', action='store_true',\n",
    "                      help='use the 10 fixed splits from '\n",
    "                           'https://arxiv.org/abs/2002.05287')\n",
    "# ODE args\n",
    "parser.add_argument('--method', type=str, default='dopri5',\n",
    "                  help=\"set the numerical solver: dopri5, euler, rk4, midpoint\")\n",
    "parser.add_argument('--step_size', type=float, default=1, help='fixed step size when using fixed step solvers e.g. rk4')\n",
    "parser.add_argument(\n",
    "    \"--adjoint_method\", type=str, default=\"adaptive_heun\",\n",
    "    help=\"set the numerical solver for the backward pass: dopri5, euler, rk4, midpoint\"\n",
    ")\n",
    "parser.add_argument('--adjoint_step_size', type=float, default=1, help='fixed step size when using fixed step adjoint solvers e.g. rk4')\n",
    "parser.add_argument('--adjoint', default=False, help='use the adjoint ODE method to reduce memory footprint')\n",
    "parser.add_argument('--tol_scale', type=float, default=1., help='multiplier for atol and rtol')\n",
    "parser.add_argument(\"--tol_scale_adjoint\", type=float, default=1.0,\n",
    "                  help=\"multiplier for adjoint_atol and adjoint_rtol\")\n",
    "parser.add_argument('--ode_blocks', type=int, default=1, help='number of ode blocks to run')\n",
    "parser.add_argument('--add_source', dest='add_source', action='store_true',\n",
    "                  help='If try get rid of alpha param and the beta*x0 source term')\n",
    "# SDE args\n",
    "parser.add_argument('--dt_min', type=float, default=1e-5, help='minimum timestep for the SDE solver')\n",
    "parser.add_argument('--dt', type=float, default=1e-3, help='fixed step size')\n",
    "parser.add_argument('--adaptive', dest='adaptive', action='store_true', help='use adaptive step sizes')\n",
    "# Attention args\n",
    "parser.add_argument('--leaky_relu_slope', type=float, default=0.2,\n",
    "                  help='slope of the negative part of the leaky relu used in attention')\n",
    "parser.add_argument('--attention_dropout', type=float, default=0., help='dropout of attention weights')\n",
    "parser.add_argument('--heads', type=int, default=4, help='number of attention heads')\n",
    "parser.add_argument('--attention_norm_idx', type=int, default=0, help='0 = normalise rows, 1 = normalise cols')\n",
    "parser.add_argument('--attention_dim', type=int, default=64,\n",
    "                  help='the size to project x to before calculating att scores')\n",
    "parser.add_argument('--mix_features', dest='mix_features', action='store_true',\n",
    "                  help='apply a feature transformation xW to the ODE')\n",
    "parser.add_argument(\"--max_nfe\", type=int, default=1000, help=\"Maximum number of function evaluations allowed.\")\n",
    "parser.add_argument('--reweight_attention', dest='reweight_attention', action='store_true', help=\"multiply attention scores by edge weights before softmax\")\n",
    "# regularisation args\n",
    "parser.add_argument('--jacobian_norm2', type=float, default=None, help=\"int_t ||df/dx||_F^2\")\n",
    "parser.add_argument('--total_deriv', type=float, default=None, help=\"int_t ||df/dt||^2\")\n",
    "\n",
    "parser.add_argument('--kinetic_energy', type=float, default=None, help=\"int_t ||f||_2^2\")\n",
    "parser.add_argument('--directional_penalty', type=float, default=None, help=\"int_t ||(df/dx)^T f||^2\")\n",
    "\n",
    "# rewiring args\n",
    "parser.add_argument('--rewiring', type=str, default=None, help=\"two_hop, gdc\")\n",
    "parser.add_argument('--gdc_method', type=str, default='ppr', help=\"ppr, heat, coeff\")\n",
    "parser.add_argument('--gdc_sparsification', type=str, default='topk', help=\"threshold, topk\")\n",
    "parser.add_argument('--gdc_k', type=int, default=64, help=\"number of neighbours to sparsify to when using topk\")\n",
    "parser.add_argument('--gdc_threshold', type=float, default=0.0001, help=\"obove this edge weight, keep edges when using threshold\")\n",
    "parser.add_argument('--gdc_avg_degree', type=int, default=64,\n",
    "                  help=\"if gdc_threshold is not given can be calculated by specifying avg degree\")\n",
    "parser.add_argument('--ppr_alpha', type=float, default=0.05, help=\"teleport probability\")\n",
    "parser.add_argument('--heat_time', type=float, default=3., help=\"time to run gdc heat kernal diffusion for\")\n",
    "\n",
    "parser.add_argument(\"--not_lcc\", action=\"store_false\", help=\"don't use the largest connected component\")\n",
    "parser.add_argument('--att_samp_pct', type=float, default=1,\n",
    "                  help=\"float in [0,1). The percentage of edges to retain based on attention scores\")\n",
    "parser.add_argument('--use_flux', dest='use_flux', action='store_true',\n",
    "                  help='incorporate the feature grad in attention based edge dropout')\n",
    "parser.add_argument(\"--exact\", action=\"store_true\",\n",
    "                  help=\"for small datasets can do exact diffusion. If dataset is too big for matrix inversion then you can't\")\n",
    "parser.add_argument('--M_nodes', type=int, default=64, help=\"new number of nodes to add\")\n",
    "parser.add_argument('--new_edges', type=str, default=\"random\", help=\"random, random_walk, k_hop\")\n",
    "parser.add_argument('--sparsify', type=str, default=\"S_hat\", help=\"S_hat, recalc_att\")\n",
    "parser.add_argument('--threshold_type', type=str, default=\"topk_adj\", help=\"topk_adj, addD_rvR\")\n",
    "parser.add_argument('--rw_addD', type=float, default=0.02, help=\"percentage of new edges to add\")\n",
    "parser.add_argument('--rw_rmvR', type=float, default=0.02, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--rewire_KNN', action='store_true', help='perform KNN rewiring every few epochs')\n",
    "parser.add_argument('--rewire_KNN_T', type=str, default=\"T0\", help=\"T0, TN\")\n",
    "parser.add_argument('--rewire_KNN_epoch', type=int, default=5, help=\"frequency of epochs to rewire\")\n",
    "parser.add_argument('--rewire_KNN_k', type=int, default=64, help=\"target degree for KNN rewire\")\n",
    "parser.add_argument('--rewire_KNN_sym', action='store_true', help='make KNN symmetric')\n",
    "parser.add_argument('--KNN_online', action='store_true', help='perform rewiring online')\n",
    "parser.add_argument('--KNN_online_reps', type=int, default=4, help=\"how many online KNN its\")\n",
    "parser.add_argument('--KNN_space', type=str, default=\"pos_distance\", help=\"Z,P,QKZ,QKp\")\n",
    "\n",
    "# Stefan's experiment args\n",
    "parser.add_argument('--count_runs', type=int, default=10,\n",
    "                  help=\"number of runs to average results over per parameter settings for each experiment\")\n",
    "\n",
    "# beltrami\n",
    "parser.add_argument('--beltrami', action='store_true', help='perform diffusion beltrami style')\n",
    "parser.add_argument('--fa_layer', action='store_true', help='add a bottleneck paper style layer with more edges')\n",
    "parser.add_argument('--pos_enc_type', type=str, default=\"DW64\",\n",
    "                  help='positional encoder either GDC, DW64, DW128, DW256')\n",
    "parser.add_argument('--pos_enc_orientation', type=str, default=\"row\", help=\"row, col\")\n",
    "parser.add_argument('--feat_hidden_dim', type=int, default=64, help=\"dimension of features in beltrami\")\n",
    "parser.add_argument('--pos_enc_hidden_dim', type=int, default=32, help=\"dimension of position in beltrami\")\n",
    "parser.add_argument('--edge_sampling', action='store_true', help='perform edge sampling rewiring')\n",
    "parser.add_argument('--edge_sampling_T', type=str, default=\"T0\", help=\"T0, TN\")\n",
    "parser.add_argument('--edge_sampling_epoch', type=int, default=5, help=\"frequency of epochs to rewire\")\n",
    "parser.add_argument('--edge_sampling_add', type=float, default=0.64, help=\"percentage of new edges to add\")\n",
    "parser.add_argument('--edge_sampling_add_type', type=str, default=\"importance\",\n",
    "                  help=\"random, ,anchored, importance, degree\")\n",
    "parser.add_argument('--edge_sampling_rmv', type=float, default=0.32, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--edge_sampling_sym', action='store_true', help='make KNN symmetric')\n",
    "parser.add_argument('--edge_sampling_online', action='store_true', help='perform rewiring online')\n",
    "parser.add_argument('--edge_sampling_online_reps', type=int, default=4, help=\"how many online KNN its\")\n",
    "parser.add_argument('--edge_sampling_space', type=str, default=\"attention\",\n",
    "                  help=\"attention,pos_distance, z_distance, pos_distance_QK, z_distance_QK\")\n",
    "parser.add_argument('--symmetric_attention', action='store_true',\n",
    "                  help='maks the attention symmetric for rewring in QK space')\n",
    "\n",
    "parser.add_argument('--fa_layer_edge_sampling_rmv', type=float, default=0.8, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--gpu', type=int, default=0, help=\"GPU to run on (default 0)\")\n",
    "parser.add_argument('--pos_enc_csv', action='store_true', help=\"Generate pos encoding as a sparse CSV\")\n",
    "\n",
    "parser.add_argument('--pos_dist_quantile', type=float, default=0.001, help=\"percentage of N**2 edges to keep\")\n",
    "\n",
    "#added\n",
    "parser.add_argument('--use_mlp', dest='use_mlp', action='store_true',\n",
    "                  help='Add a fully connected layer to the encoder.')\n",
    "parser.add_argument('--use_labels', dest='use_labels', action='store_true', help='Also diffuse labels')\n",
    "parser.add_argument('--fc_out', dest='fc_out', action='store_true',\n",
    "                  help='Add a fully connected layer to the decoder.')\n",
    "parser.add_argument(\"--batch_norm\", dest='batch_norm', action='store_true', help='search over reg params')\n",
    "\n",
    "args = parser.parse_args(customArgs)\n",
    "opt = vars(args)\n",
    "\n",
    "#'Cora' #'Flickr' #'Computer'\n",
    "opt['dataset'] = 'Computer' \n",
    "\n",
    "if opt['dataset'] == 'Cora':\n",
    "    opt = get_cora_opt(opt)\n",
    "elif opt['dataset'] == 'Computer':\n",
    "    opt = get_computers_opt(opt)\n",
    "elif opt['dataset'] == 'Flickr':\n",
    "    opt = get_flickr_opt(opt)\n",
    "\n",
    "opt['adjoint'] = True\n",
    "#opt['method'] = 'explicit_adams'\n",
    "#opt['method'] = 'implicit_adams'\n",
    "#opt['method'] = 'dopri5'\n",
    "opt['method'] = 'rk4'\n",
    "opt['adjoint_method'] = opt['method']\n",
    "opt['max_iters'] = 100\n",
    "opt['step_size'] = opt['dt_min'] = 0.01\n",
    "opt['tol_scale'] = 100.0\n",
    "opt['tol_scale_adjoint'] = 100.0\n",
    "#added\n",
    "opt['max_nfe'] = 100000\n",
    "if opt['dataset'] == 'Flickr':    \n",
    "    opt['rewiring'] == 'gdc'\n",
    "\n",
    "# DEBUG\n",
    "#for k in ['dataset', 'epoch', 'adjoint', 'rewiring', 'adaptive', 'dt', 'dt_min', 'method', 'adjoint_method']:\n",
    "#  print(k, opt[k])\n",
    "#main(opt, 0)\n",
    "\n",
    "# Run combination of experiments\n",
    "for stepsize in [1.0]:#[1.0,0.5,0.1,0.01]: #[0.5, 0.25, 0.1, 0.01]: # 2.0, 1.0\n",
    "    print(f'*** Doing stepsize {stepsize} ***')\n",
    "    for idx in range(opt['count_runs']):\n",
    "        print(f'*** Doing run {idx} ***')\n",
    "        # NOTE: I think setting dt_min may not be necessary, doing it just to be safe!\n",
    "        opt['step_size'] = opt['dt_min'] = stepsize\n",
    "        run(opt, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ff871-6d10-462c-b9f4-c0d2d6dbc161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedf3b1-af5d-49d0-8ddb-834f9e6dea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
