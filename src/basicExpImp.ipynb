{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faed1d68-9918-4d47-9a0f-6b7c1394515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeljon00/miniconda3/envs/grandtn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa\n",
    "from GNN import GNN\n",
    "import time\n",
    "from data import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c1e662-6fcf-4c95-97e4-41f5ce755be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customArgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7ee566-4b2c-462c-8437-0ec9d73eb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cora_opt(opt):\n",
    "  opt['dataset'] = 'Cora'\n",
    "  opt['data'] = 'Planetoid'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'rmsprop'\n",
    "  opt['lr'] = 0.0047\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 31\n",
    "  opt['augment'] = True\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['adjoint'] = False\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_computers_opt(opt):\n",
    "  opt['dataset'] = 'Computers'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.01\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['epoch'] = 400\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 50\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442b7c4e-3ad2-455a-907f-38f4f25067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name, parameters, lr, weight_decay=0):\n",
    "  if name == 'sgd':\n",
    "    return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'rmsprop':\n",
    "    return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adagrad':\n",
    "    return torch.optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adam':\n",
    "    return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adamax':\n",
    "    return torch.optim.Adamax(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  else:\n",
    "    raise Exception(\"Unsupported optimizer: {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768697b4-1f65-4c10-a8d8-3744ded132dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x)\n",
    "  lf = torch.nn.CrossEntropyLoss()\n",
    "  loss = lf(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "  # TODO: What is this block about???\n",
    "  if model.odeblock.nreg > 0:  # add regularisation - slower for small data, but faster and better performance for large data\n",
    "    reg_states = tuple(torch.mean(rs) for rs in model.reg_states)\n",
    "    regularization_coeffs = model.regularization_coeffs\n",
    "\n",
    "    reg_loss = sum(\n",
    "      reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
    "    )\n",
    "    loss = loss + reg_loss\n",
    "\n",
    "  # Update count of forward evaluations from ODE solver\n",
    "  # NOTE: fm stands for \"forward meter\"\n",
    "  # TODO: Rename this to be more informative!\n",
    "  model.fm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  # Gradient step\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Update count of backwards evaluations from ODE solver\n",
    "  model.bm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "  model.eval()\n",
    "  logits, accs = model(data.x), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs\n",
    "\n",
    "def print_model_params(model):\n",
    "  print(model)\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "      print(name)\n",
    "      print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433d8568-8f88-4feb-8e97-c466a24e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(opt, run_count):\n",
    "\n",
    "    # Load dataset and create model\n",
    "    dataset = get_dataset(opt, '../data', False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, data = GNN(opt, dataset, device).to(device), dataset.data.to(device)\n",
    "    print(opt)\n",
    "\n",
    "    # Todo for some reason the submodule parameters inside the attention module don't show up when running on GPU.\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    print_model_params(model)\n",
    "\n",
    "    # Training/test loop\n",
    "    results = {\n",
    "        'time':[],\n",
    "        'loss':[],\n",
    "        'forward_nfe':[],\n",
    "        'backward_nfe':[],\n",
    "        'train_acc':[],\n",
    "        'test_acc':[],\n",
    "        'val_acc':[],\n",
    "        'best_epoch':0,\n",
    "        'best_val_acc':0.,\n",
    "        'best_test_acc':0.,\n",
    "    }\n",
    "    runtimes = []\n",
    "    losses = []\n",
    "\n",
    "    optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "    best_val_acc = test_acc = train_acc = best_epoch = 0\n",
    "    for epoch in range(1, opt['epoch']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss = train(model, optimizer, data)\n",
    "        train_acc, val_acc, test_acc = test(model, data)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        results['time'].append(time.time() - start_time)\n",
    "        results['loss'].append(loss)\n",
    "        results['forward_nfe'].append(model.fm.sum)\n",
    "        results['backward_nfe'].append(model.bm.sum)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['val_acc'].append(val_acc)\n",
    "        results['best_epoch'] = best_epoch\n",
    "        results['best_val_acc'] = best_val_acc\n",
    "        results['best_test_acc'] = best_test_acc\n",
    "\n",
    "        log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        print(log.format(epoch, results['time'][-1], results['loss'][-1], results['forward_nfe'][-1], results['backward_nfe'][-1], results['train_acc'][-1], results['val_acc'][-1], results['test_acc'][-1]))\n",
    "\n",
    "    print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d}'.format(best_val_acc, best_test_acc, best_epoch))\n",
    "\n",
    "    # TODO: Save results\n",
    "    # cora_epoch_101_adjoint_false_... . pickle\n",
    "    pickle.dump( results, open( f\"../results/{opt['dataset']}_{opt['method']}_stepsize_{opt['dt']}_run_{run_count}.pickle\", \"wb\" ) )\n",
    "\n",
    "    return train_acc, best_val_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ee298-1c23-42ab-9be1-cec3ccae4b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Doing stepsize 0.5 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 1000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.558052, Loss 1.954478, forward nfe 143, backward nfe 95, Train: 0.3143, Val: 0.2020, Test: 0.2140\n",
      "Epoch: 002, Runtime 0.622598, Loss 1.815107, forward nfe 488, backward nfe 217, Train: 0.4071, Val: 0.3040, Test: 0.2960\n",
      "Epoch: 003, Runtime 0.601640, Loss 1.630790, forward nfe 829, backward nfe 333, Train: 0.4929, Val: 0.4660, Test: 0.4810\n",
      "Epoch: 004, Runtime 0.585404, Loss 1.553144, forward nfe 1171, backward nfe 445, Train: 0.4143, Val: 0.2500, Test: 0.2520\n",
      "Epoch: 005, Runtime 0.591916, Loss 1.467792, forward nfe 1504, backward nfe 553, Train: 0.8500, Val: 0.6240, Test: 0.6420\n",
      "Epoch: 006, Runtime 0.576831, Loss 1.182024, forward nfe 1837, backward nfe 657, Train: 0.9000, Val: 0.7680, Test: 0.8040\n",
      "Epoch: 007, Runtime 0.559471, Loss 1.021733, forward nfe 2167, backward nfe 756, Train: 0.9214, Val: 0.7740, Test: 0.7870\n",
      "Epoch: 008, Runtime 0.561189, Loss 0.917719, forward nfe 2493, backward nfe 854, Train: 0.9429, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 009, Runtime 0.551077, Loss 0.808005, forward nfe 2819, backward nfe 951, Train: 0.9286, Val: 0.7800, Test: 0.7980\n",
      "Epoch: 010, Runtime 0.543084, Loss 0.740846, forward nfe 3145, backward nfe 1047, Train: 0.9429, Val: 0.7840, Test: 0.8090\n",
      "Epoch: 011, Runtime 0.561973, Loss 0.666142, forward nfe 3468, backward nfe 1141, Train: 0.9429, Val: 0.7860, Test: 0.7910\n",
      "Epoch: 012, Runtime 0.604149, Loss 0.606626, forward nfe 3789, backward nfe 1233, Train: 0.9286, Val: 0.7680, Test: 0.7750\n",
      "Epoch: 013, Runtime 0.574517, Loss 0.579601, forward nfe 4107, backward nfe 1324, Train: 0.9429, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 014, Runtime 0.580348, Loss 0.504521, forward nfe 4425, backward nfe 1413, Train: 0.9643, Val: 0.7960, Test: 0.8180\n",
      "Epoch: 015, Runtime 0.552100, Loss 0.472706, forward nfe 4740, backward nfe 1502, Train: 0.9643, Val: 0.8000, Test: 0.8240\n",
      "Epoch: 016, Runtime 0.552375, Loss 0.420106, forward nfe 5055, backward nfe 1590, Train: 0.9643, Val: 0.8020, Test: 0.8220\n",
      "Epoch: 017, Runtime 0.749131, Loss 0.379777, forward nfe 5370, backward nfe 1676, Train: 0.9643, Val: 0.8020, Test: 0.8180\n",
      "Epoch: 018, Runtime 0.810973, Loss 0.350328, forward nfe 5681, backward nfe 1760, Train: 0.9643, Val: 0.7960, Test: 0.8170\n",
      "Epoch: 019, Runtime 0.774584, Loss 0.341043, forward nfe 5991, backward nfe 1843, Train: 0.9643, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 020, Runtime 0.790898, Loss 0.318415, forward nfe 6297, backward nfe 1925, Train: 0.9643, Val: 0.7880, Test: 0.8110\n",
      "Epoch: 021, Runtime 0.798817, Loss 0.291564, forward nfe 6604, backward nfe 2007, Train: 0.9786, Val: 0.8040, Test: 0.8180\n",
      "Epoch: 022, Runtime 2.499593, Loss 0.288616, forward nfe 6910, backward nfe 2089, Train: 0.9714, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 023, Runtime 3.916855, Loss 0.267946, forward nfe 7215, backward nfe 2168, Train: 0.9857, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 024, Runtime 3.686404, Loss 0.260980, forward nfe 7518, backward nfe 2246, Train: 0.9857, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 025, Runtime 3.588563, Loss 0.233948, forward nfe 7820, backward nfe 2324, Train: 0.9929, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 026, Runtime 3.843163, Loss 0.225820, forward nfe 8122, backward nfe 2402, Train: 0.9857, Val: 0.7720, Test: 0.7980\n",
      "Epoch: 027, Runtime 3.539752, Loss 0.234524, forward nfe 8424, backward nfe 2479, Train: 0.9929, Val: 0.7900, Test: 0.8090\n",
      "Epoch: 028, Runtime 3.686301, Loss 0.229085, forward nfe 8724, backward nfe 2556, Train: 0.9929, Val: 0.7900, Test: 0.8080\n",
      "Epoch: 029, Runtime 4.670963, Loss 0.204844, forward nfe 9022, backward nfe 2632, Train: 0.9857, Val: 0.7900, Test: 0.8090\n",
      "Epoch: 030, Runtime 3.785980, Loss 0.200459, forward nfe 9319, backward nfe 2706, Train: 0.9929, Val: 0.7900, Test: 0.8040\n",
      "best val accuracy 0.804000 with test accuracy 0.818000 at epoch 21\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 1000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 2.688308, Loss 1.955517, forward nfe 139, backward nfe 94, Train: 0.2714, Val: 0.1700, Test: 0.1600\n",
      "Epoch: 002, Runtime 0.588453, Loss 1.837142, forward nfe 477, backward nfe 210, Train: 0.5429, Val: 0.5120, Test: 0.5420\n",
      "Epoch: 003, Runtime 0.570766, Loss 1.638862, forward nfe 821, backward nfe 321, Train: 0.7071, Val: 0.4240, Test: 0.4560\n",
      "Epoch: 004, Runtime 0.556929, Loss 1.383581, forward nfe 1161, backward nfe 427, Train: 0.8500, Val: 0.7380, Test: 0.7520\n",
      "Epoch: 005, Runtime 0.560902, Loss 1.191217, forward nfe 1496, backward nfe 534, Train: 0.8714, Val: 0.6000, Test: 0.6320\n",
      "Epoch: 006, Runtime 0.559740, Loss 1.043009, forward nfe 1832, backward nfe 637, Train: 0.8714, Val: 0.6840, Test: 0.7100\n",
      "Epoch: 007, Runtime 0.545489, Loss 0.957241, forward nfe 2163, backward nfe 737, Train: 0.9000, Val: 0.6700, Test: 0.6900\n",
      "Epoch: 008, Runtime 0.537009, Loss 0.827951, forward nfe 2496, backward nfe 836, Train: 0.9214, Val: 0.7320, Test: 0.7420\n",
      "Epoch: 009, Runtime 0.537121, Loss 0.732690, forward nfe 2822, backward nfe 932, Train: 0.9500, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 010, Runtime 0.523403, Loss 0.603960, forward nfe 3148, backward nfe 1028, Train: 0.9571, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 011, Runtime 0.519985, Loss 0.551971, forward nfe 3473, backward nfe 1123, Train: 0.9571, Val: 0.8000, Test: 0.8210\n",
      "Epoch: 012, Runtime 0.510323, Loss 0.496667, forward nfe 3794, backward nfe 1216, Train: 0.9571, Val: 0.8000, Test: 0.8210\n",
      "Epoch: 013, Runtime 0.516595, Loss 0.448828, forward nfe 4115, backward nfe 1308, Train: 0.9571, Val: 0.8040, Test: 0.8240\n",
      "Epoch: 014, Runtime 0.526863, Loss 0.425156, forward nfe 4433, backward nfe 1399, Train: 0.9571, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 015, Runtime 0.504380, Loss 0.394286, forward nfe 4750, backward nfe 1487, Train: 0.9643, Val: 0.8020, Test: 0.8200\n",
      "Epoch: 016, Runtime 0.500356, Loss 0.363668, forward nfe 5067, backward nfe 1575, Train: 0.9786, Val: 0.8020, Test: 0.8190\n",
      "Epoch: 017, Runtime 0.503860, Loss 0.344114, forward nfe 5381, backward nfe 1663, Train: 0.9643, Val: 0.7920, Test: 0.8200\n",
      "Epoch: 018, Runtime 0.498420, Loss 0.327001, forward nfe 5695, backward nfe 1749, Train: 0.9786, Val: 0.7900, Test: 0.8150\n",
      "Epoch: 019, Runtime 0.505083, Loss 0.311557, forward nfe 6009, backward nfe 1834, Train: 0.9786, Val: 0.8000, Test: 0.8230\n",
      "Epoch: 020, Runtime 0.499031, Loss 0.295482, forward nfe 6323, backward nfe 1919, Train: 0.9857, Val: 0.7840, Test: 0.8050\n",
      "Epoch: 021, Runtime 0.495328, Loss 0.273466, forward nfe 6634, backward nfe 2001, Train: 0.9643, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 022, Runtime 0.492389, Loss 0.251147, forward nfe 6942, backward nfe 2083, Train: 0.9929, Val: 0.7900, Test: 0.8080\n",
      "Epoch: 023, Runtime 0.483086, Loss 0.240808, forward nfe 7248, backward nfe 2165, Train: 0.9786, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 024, Runtime 0.486147, Loss 0.227256, forward nfe 7554, backward nfe 2247, Train: 0.9929, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 025, Runtime 0.476226, Loss 0.214663, forward nfe 7860, backward nfe 2327, Train: 0.9857, Val: 0.7980, Test: 0.8220\n",
      "Epoch: 026, Runtime 0.480653, Loss 0.219179, forward nfe 8165, backward nfe 2406, Train: 0.9929, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 027, Runtime 0.473235, Loss 0.205003, forward nfe 8467, backward nfe 2484, Train: 0.9929, Val: 0.7800, Test: 0.8100\n",
      "Epoch: 028, Runtime 0.470322, Loss 0.191754, forward nfe 8769, backward nfe 2561, Train: 0.9929, Val: 0.7940, Test: 0.8170\n",
      "Epoch: 029, Runtime 0.471109, Loss 0.187619, forward nfe 9071, backward nfe 2638, Train: 0.9929, Val: 0.7940, Test: 0.8200\n",
      "Epoch: 030, Runtime 0.467445, Loss 0.172380, forward nfe 9373, backward nfe 2714, Train: 0.9929, Val: 0.7880, Test: 0.8130\n",
      "best val accuracy 0.804000 with test accuracy 0.824000 at epoch 13\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 1000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.559721, Loss 1.949069, forward nfe 140, backward nfe 95, Train: 0.3214, Val: 0.1840, Test: 0.2000\n",
      "Epoch: 002, Runtime 0.608851, Loss 1.777704, forward nfe 472, backward nfe 200, Train: 0.5571, Val: 0.4280, Test: 0.4130\n",
      "Epoch: 003, Runtime 0.595178, Loss 1.524313, forward nfe 801, backward nfe 303, Train: 0.7786, Val: 0.6660, Test: 0.6700\n",
      "Epoch: 004, Runtime 0.602360, Loss 1.301904, forward nfe 1133, backward nfe 403, Train: 0.8786, Val: 0.6700, Test: 0.6780\n",
      "Epoch: 005, Runtime 0.576024, Loss 1.114999, forward nfe 1457, backward nfe 500, Train: 0.8929, Val: 0.7160, Test: 0.7590\n",
      "Epoch: 006, Runtime 0.590623, Loss 0.943761, forward nfe 1783, backward nfe 596, Train: 0.9143, Val: 0.7620, Test: 0.7780\n",
      "Epoch: 007, Runtime 0.570517, Loss 0.812613, forward nfe 2104, backward nfe 688, Train: 0.9143, Val: 0.7520, Test: 0.7620\n",
      "Epoch: 008, Runtime 0.569076, Loss 0.724437, forward nfe 2421, backward nfe 779, Train: 0.9429, Val: 0.8060, Test: 0.8200\n",
      "Epoch: 009, Runtime 0.553024, Loss 0.628215, forward nfe 2738, backward nfe 868, Train: 0.9500, Val: 0.7840, Test: 0.7930\n",
      "Epoch: 010, Runtime 0.557756, Loss 0.574672, forward nfe 3052, backward nfe 956, Train: 0.9571, Val: 0.8040, Test: 0.8270\n",
      "Epoch: 011, Runtime 0.574163, Loss 0.490516, forward nfe 3366, backward nfe 1043, Train: 0.9571, Val: 0.8080, Test: 0.8170\n",
      "Epoch: 012, Runtime 0.541131, Loss 0.468364, forward nfe 3677, backward nfe 1129, Train: 0.9571, Val: 0.8120, Test: 0.8210\n",
      "Epoch: 013, Runtime 0.543705, Loss 0.432195, forward nfe 3984, backward nfe 1213, Train: 0.9643, Val: 0.8060, Test: 0.8190\n",
      "Epoch: 014, Runtime 0.543039, Loss 0.387282, forward nfe 4293, backward nfe 1295, Train: 0.9643, Val: 0.8120, Test: 0.8160\n",
      "Epoch: 015, Runtime 0.538176, Loss 0.347745, forward nfe 4599, backward nfe 1377, Train: 0.9643, Val: 0.8100, Test: 0.8140\n",
      "Epoch: 016, Runtime 0.533014, Loss 0.338739, forward nfe 4905, backward nfe 1458, Train: 0.9643, Val: 0.8120, Test: 0.8210\n",
      "Epoch: 017, Runtime 0.529319, Loss 0.325060, forward nfe 5208, backward nfe 1537, Train: 0.9714, Val: 0.8080, Test: 0.8170\n",
      "Epoch: 018, Runtime 0.510533, Loss 0.302317, forward nfe 5510, backward nfe 1615, Train: 0.9643, Val: 0.8100, Test: 0.8140\n",
      "Epoch: 019, Runtime 0.521702, Loss 0.288988, forward nfe 5810, backward nfe 1692, Train: 0.9786, Val: 0.8060, Test: 0.8130\n",
      "Epoch: 020, Runtime 0.521483, Loss 0.266673, forward nfe 6110, backward nfe 1769, Train: 0.9857, Val: 0.7780, Test: 0.8000\n",
      "Epoch: 021, Runtime 0.492760, Loss 0.271399, forward nfe 6409, backward nfe 1844, Train: 0.9786, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 022, Runtime 0.512577, Loss 0.245065, forward nfe 6704, backward nfe 1918, Train: 0.9786, Val: 0.8020, Test: 0.8100\n",
      "Epoch: 023, Runtime 0.502714, Loss 0.222878, forward nfe 6999, backward nfe 1992, Train: 0.9929, Val: 0.8060, Test: 0.8050\n",
      "Epoch: 024, Runtime 0.498096, Loss 0.228525, forward nfe 7294, backward nfe 2065, Train: 1.0000, Val: 0.7980, Test: 0.8010\n",
      "Epoch: 025, Runtime 0.491970, Loss 0.213616, forward nfe 7586, backward nfe 2136, Train: 0.9857, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 026, Runtime 0.492788, Loss 0.216901, forward nfe 7878, backward nfe 2207, Train: 0.9929, Val: 0.7740, Test: 0.7970\n",
      "Epoch: 027, Runtime 0.497407, Loss 0.198617, forward nfe 8169, backward nfe 2278, Train: 0.9857, Val: 0.7900, Test: 0.8180\n",
      "Epoch: 028, Runtime 0.510019, Loss 0.182179, forward nfe 8459, backward nfe 2349, Train: 1.0000, Val: 0.8020, Test: 0.8100\n",
      "Epoch: 029, Runtime 0.501120, Loss 0.174397, forward nfe 8749, backward nfe 2420, Train: 0.9929, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 030, Runtime 0.492088, Loss 0.174189, forward nfe 9036, backward nfe 2490, Train: 1.0000, Val: 0.8040, Test: 0.8070\n",
      "best val accuracy 0.812000 with test accuracy 0.821000 at epoch 12\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 1000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.550536, Loss 1.948284, forward nfe 144, backward nfe 95, Train: 0.4571, Val: 0.4260, Test: 0.4700\n",
      "Epoch: 002, Runtime 0.664942, Loss 1.800158, forward nfe 490, backward nfe 219, Train: 0.4500, Val: 0.2500, Test: 0.2710\n",
      "Epoch: 003, Runtime 0.671141, Loss 1.648728, forward nfe 836, backward nfe 350, Train: 0.4571, Val: 0.4060, Test: 0.4100\n",
      "Epoch: 004, Runtime 0.629109, Loss 1.572227, forward nfe 1178, backward nfe 464, Train: 0.6929, Val: 0.5620, Test: 0.5980\n",
      "Epoch: 005, Runtime 0.608556, Loss 1.334902, forward nfe 1518, backward nfe 570, Train: 0.9000, Val: 0.7300, Test: 0.7200\n",
      "Epoch: 006, Runtime 0.583024, Loss 1.107902, forward nfe 1846, backward nfe 671, Train: 0.9071, Val: 0.7540, Test: 0.7930\n",
      "Epoch: 007, Runtime 0.577522, Loss 0.959056, forward nfe 2178, backward nfe 771, Train: 0.9143, Val: 0.7460, Test: 0.7380\n",
      "Epoch: 008, Runtime 0.573348, Loss 0.836803, forward nfe 2503, backward nfe 868, Train: 0.9143, Val: 0.7820, Test: 0.8180\n",
      "Epoch: 009, Runtime 0.560603, Loss 0.735941, forward nfe 2828, backward nfe 964, Train: 0.9429, Val: 0.7720, Test: 0.7760\n",
      "Epoch: 010, Runtime 0.562720, Loss 0.660421, forward nfe 3148, backward nfe 1058, Train: 0.9500, Val: 0.7800, Test: 0.8190\n",
      "Epoch: 011, Runtime 0.580317, Loss 0.603413, forward nfe 3465, backward nfe 1151, Train: 0.9714, Val: 0.7780, Test: 0.7890\n",
      "Epoch: 012, Runtime 0.554208, Loss 0.564309, forward nfe 3783, backward nfe 1242, Train: 0.9643, Val: 0.7860, Test: 0.8130\n",
      "Epoch: 013, Runtime 0.558032, Loss 0.503264, forward nfe 4099, backward nfe 1331, Train: 0.9643, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 014, Runtime 0.554730, Loss 0.470827, forward nfe 4413, backward nfe 1420, Train: 0.9643, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 015, Runtime 0.545401, Loss 0.427876, forward nfe 4727, backward nfe 1508, Train: 0.9786, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 016, Runtime 0.545916, Loss 0.392507, forward nfe 5041, backward nfe 1595, Train: 0.9643, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 017, Runtime 0.549839, Loss 0.377450, forward nfe 5351, backward nfe 1682, Train: 0.9786, Val: 0.7760, Test: 0.7890\n",
      "Epoch: 018, Runtime 0.545027, Loss 0.367736, forward nfe 5658, backward nfe 1766, Train: 0.9714, Val: 0.7940, Test: 0.8170\n",
      "Epoch: 019, Runtime 0.526890, Loss 0.330979, forward nfe 5964, backward nfe 1848, Train: 0.9857, Val: 0.7800, Test: 0.7980\n",
      "Epoch: 020, Runtime 0.522961, Loss 0.320390, forward nfe 6270, backward nfe 1930, Train: 0.9786, Val: 0.7980, Test: 0.8110\n",
      "Epoch: 021, Runtime 0.522489, Loss 0.301777, forward nfe 6574, backward nfe 2011, Train: 0.9929, Val: 0.7820, Test: 0.7970\n",
      "Epoch: 022, Runtime 0.517816, Loss 0.276136, forward nfe 6877, backward nfe 2091, Train: 0.9857, Val: 0.7900, Test: 0.8040\n",
      "Epoch: 023, Runtime 0.513013, Loss 0.261703, forward nfe 7179, backward nfe 2170, Train: 0.9929, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 024, Runtime 0.510201, Loss 0.248053, forward nfe 7480, backward nfe 2248, Train: 0.9929, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 025, Runtime 0.496366, Loss 0.253151, forward nfe 7780, backward nfe 2325, Train: 0.9929, Val: 0.7820, Test: 0.7980\n",
      "Epoch: 026, Runtime 0.494161, Loss 0.233554, forward nfe 8077, backward nfe 2402, Train: 0.9929, Val: 0.7980, Test: 0.8090\n",
      "Epoch: 027, Runtime 0.492646, Loss 0.210858, forward nfe 8374, backward nfe 2478, Train: 0.9929, Val: 0.7920, Test: 0.8040\n",
      "Epoch: 028, Runtime 0.484783, Loss 0.218788, forward nfe 8669, backward nfe 2553, Train: 1.0000, Val: 0.7920, Test: 0.8030\n",
      "Epoch: 029, Runtime 0.508279, Loss 0.199894, forward nfe 8963, backward nfe 2628, Train: 0.9929, Val: 0.7860, Test: 0.7950\n",
      "Epoch: 030, Runtime 0.485822, Loss 0.193904, forward nfe 9258, backward nfe 2701, Train: 1.0000, Val: 0.7900, Test: 0.8020\n",
      "best val accuracy 0.798000 with test accuracy 0.811000 at epoch 20\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 1000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.561448, Loss 1.953025, forward nfe 140, backward nfe 95, Train: 0.1857, Val: 0.0860, Test: 0.0850\n",
      "Epoch: 002, Runtime 0.628175, Loss 1.869838, forward nfe 472, backward nfe 200, Train: 0.3643, Val: 0.1760, Test: 0.1780\n",
      "Epoch: 003, Runtime 0.600626, Loss 1.624532, forward nfe 803, backward nfe 303, Train: 0.4500, Val: 0.2400, Test: 0.2590\n",
      "Epoch: 004, Runtime 0.599465, Loss 1.500068, forward nfe 1129, backward nfe 401, Train: 0.7643, Val: 0.5560, Test: 0.5650\n",
      "Epoch: 005, Runtime 0.592328, Loss 1.213063, forward nfe 1455, backward nfe 499, Train: 0.8571, Val: 0.6680, Test: 0.6690\n",
      "Epoch: 006, Runtime 0.591653, Loss 1.050244, forward nfe 1772, backward nfe 592, Train: 0.9000, Val: 0.7340, Test: 0.7390\n",
      "Epoch: 007, Runtime 0.567719, Loss 0.872153, forward nfe 2088, backward nfe 684, Train: 0.9500, Val: 0.7900, Test: 0.7870\n",
      "Epoch: 008, Runtime 0.556777, Loss 0.752311, forward nfe 2402, backward nfe 773, Train: 0.9643, Val: 0.8000, Test: 0.8010\n",
      "Epoch: 009, Runtime 0.556987, Loss 0.666986, forward nfe 2716, backward nfe 861, Train: 0.9571, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 010, Runtime 0.554519, Loss 0.621233, forward nfe 3026, backward nfe 948, Train: 0.9571, Val: 0.8100, Test: 0.8130\n",
      "Epoch: 011, Runtime 0.557779, Loss 0.546003, forward nfe 3336, backward nfe 1034, Train: 0.9571, Val: 0.7980, Test: 0.8010\n",
      "Epoch: 012, Runtime 0.549394, Loss 0.500762, forward nfe 3642, backward nfe 1117, Train: 0.9571, Val: 0.8120, Test: 0.8180\n",
      "Epoch: 013, Runtime 0.532773, Loss 0.459602, forward nfe 3948, backward nfe 1199, Train: 0.9571, Val: 0.8140, Test: 0.8110\n",
      "Epoch: 014, Runtime 0.534832, Loss 0.408052, forward nfe 4253, backward nfe 1281, Train: 0.9571, Val: 0.8040, Test: 0.8040\n",
      "Epoch: 015, Runtime 0.530871, Loss 0.379287, forward nfe 4555, backward nfe 1360, Train: 0.9643, Val: 0.8100, Test: 0.8090\n",
      "Epoch: 016, Runtime 0.519252, Loss 0.356834, forward nfe 4857, backward nfe 1438, Train: 0.9571, Val: 0.8120, Test: 0.8060\n",
      "Epoch: 017, Runtime 0.531397, Loss 0.338566, forward nfe 5158, backward nfe 1515, Train: 0.9643, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 018, Runtime 0.524302, Loss 0.320964, forward nfe 5456, backward nfe 1592, Train: 0.9786, Val: 0.7960, Test: 0.8070\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_cora_defaults', action='store_true',\n",
    "                  help='Whether to run with best params for cora. Overrides the choice of dataset')\n",
    "parser.add_argument('--dataset', type=str, default='Cora',\n",
    "                  help='Cora, Citeseer, Pubmed, Computers, Photo, CoauthorCS')\n",
    "parser.add_argument('--data_norm', type=str, default='rw',\n",
    "                  help='rw for random walk, gcn for symmetric gcn norm')\n",
    "parser.add_argument('--hidden_dim', type=int, default=16, help='Hidden dimension.')\n",
    "parser.add_argument('--input_dropout', type=float, default=0.5, help='Input dropout rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='One from sgd, rmsprop, adam, adagrad, adamax.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Learning rate.')\n",
    "parser.add_argument('--decay', type=float, default=5e-4, help='Weight decay for optimization')\n",
    "parser.add_argument('--self_loop_weight', type=float, default=1.0, help='Weight of self-loops.')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='Number of training epochs per iteration.')\n",
    "parser.add_argument('--alpha', type=float, default=1.0, help='Factor in front matrix A.')\n",
    "parser.add_argument('--time', type=float, default=1.0, help='End time of ODE integrator.')\n",
    "parser.add_argument('--augment', action='store_true',\n",
    "                  help='double the length of the feature vector by appending zeros to stabilist ODE learning')\n",
    "parser.add_argument('--alpha_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) alpha')\n",
    "parser.add_argument('--no_alpha_sigmoid', dest='no_alpha_sigmoid', action='store_true', help='apply sigmoid before multiplying by alpha')\n",
    "parser.add_argument('--beta_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) beta')\n",
    "parser.add_argument('--block', type=str, default='constant', help='constant, mixed, attention, SDE')\n",
    "parser.add_argument('--function', type=str, default='laplacian', help='laplacian, transformer, dorsey, GAT, SDE')\n",
    "# ODE args\n",
    "parser.add_argument('--method', type=str, default='dopri5',\n",
    "                  help=\"set the numerical solver: dopri5, euler, rk4, midpoint\")\n",
    "parser.add_argument('--step_size', type=float, default=1, help='fixed step size when using fixed step solvers e.g. rk4')\n",
    "parser.add_argument(\n",
    "    \"--adjoint_method\", type=str, default=\"adaptive_heun\",\n",
    "    help=\"set the numerical solver for the backward pass: dopri5, euler, rk4, midpoint\"\n",
    ")\n",
    "parser.add_argument('--adjoint_step_size', type=float, default=1, help='fixed step size when using fixed step adjoint solvers e.g. rk4')\n",
    "parser.add_argument('--adjoint', default=False, help='use the adjoint ODE method to reduce memory footprint')\n",
    "parser.add_argument('--tol_scale', type=float, default=1., help='multiplier for atol and rtol')\n",
    "parser.add_argument(\"--tol_scale_adjoint\", type=float, default=1.0,\n",
    "                  help=\"multiplier for adjoint_atol and adjoint_rtol\")\n",
    "parser.add_argument('--ode_blocks', type=int, default=1, help='number of ode blocks to run')\n",
    "parser.add_argument('--add_source', dest='add_source', action='store_true',\n",
    "                  help='If try get rid of alpha param and the beta*x0 source term')\n",
    "# SDE args\n",
    "parser.add_argument('--dt_min', type=float, default=1e-5, help='minimum timestep for the SDE solver')\n",
    "parser.add_argument('--dt', type=float, default=1e-3, help='fixed step size')\n",
    "parser.add_argument('--adaptive', dest='adaptive', action='store_true', help='use adaptive step sizes')\n",
    "# Attention args\n",
    "parser.add_argument('--leaky_relu_slope', type=float, default=0.2,\n",
    "                  help='slope of the negative part of the leaky relu used in attention')\n",
    "parser.add_argument('--attention_dropout', type=float, default=0., help='dropout of attention weights')\n",
    "parser.add_argument('--heads', type=int, default=4, help='number of attention heads')\n",
    "parser.add_argument('--attention_norm_idx', type=int, default=0, help='0 = normalise rows, 1 = normalise cols')\n",
    "parser.add_argument('--attention_dim', type=int, default=64,\n",
    "                  help='the size to project x to before calculating att scores')\n",
    "parser.add_argument('--mix_features', dest='mix_features', action='store_true',\n",
    "                  help='apply a feature transformation xW to the ODE')\n",
    "parser.add_argument(\"--max_nfe\", type=int, default=1000, help=\"Maximum number of function evaluations allowed.\")\n",
    "parser.add_argument('--reweight_attention', dest='reweight_attention', action='store_true', help=\"multiply attention scores by edge weights before softmax\")\n",
    "# regularisation args\n",
    "parser.add_argument('--jacobian_norm2', type=float, default=None, help=\"int_t ||df/dx||_F^2\")\n",
    "parser.add_argument('--total_deriv', type=float, default=None, help=\"int_t ||df/dt||^2\")\n",
    "\n",
    "parser.add_argument('--kinetic_energy', type=float, default=None, help=\"int_t ||f||_2^2\")\n",
    "parser.add_argument('--directional_penalty', type=float, default=None, help=\"int_t ||(df/dx)^T f||^2\")\n",
    "\n",
    "# rewiring args\n",
    "parser.add_argument('--rewiring', type=str, default=None, help=\"two_hop, gdc\")\n",
    "parser.add_argument('--gdc_method', type=str, default='ppr', help=\"ppr, heat, coeff\")\n",
    "parser.add_argument('--gdc_sparsification', type=str, default='topk', help=\"threshold, topk\")\n",
    "parser.add_argument('--gdc_k', type=int, default=64, help=\"number of neighbours to sparsify to when using topk\")\n",
    "parser.add_argument('--gdc_threshold', type=float, default=0.0001, help=\"obove this edge weight, keep edges when using threshold\")\n",
    "parser.add_argument('--gdc_avg_degree', type=int, default=64,\n",
    "                  help=\"if gdc_threshold is not given can be calculated by specifying avg degree\")\n",
    "parser.add_argument('--ppr_alpha', type=float, default=0.05, help=\"teleport probability\")\n",
    "parser.add_argument('--heat_time', type=float, default=3., help=\"time to run gdc heat kernal diffusion for\")\n",
    "\n",
    "# Stefan's experiment args\n",
    "parser.add_argument('--count_runs', type=int, default=10,\n",
    "                  help=\"number of runs to average results over per parameter settings for each experiment\")\n",
    "\n",
    "#added\n",
    "parser.add_argument('--beltrami', action='store_true', help='perform diffusion beltrami style')\n",
    "parser.add_argument('--use_mlp', dest='use_mlp', action='store_true',\n",
    "                  help='Add a fully connected layer to the encoder.')\n",
    "parser.add_argument('--use_labels', dest='use_labels', action='store_true', help='Also diffuse labels')\n",
    "parser.add_argument('--fc_out', dest='fc_out', action='store_true',\n",
    "                  help='Add a fully connected layer to the decoder.')\n",
    "parser.add_argument(\"--batch_norm\", dest='batch_norm', action='store_true', help='search over reg params')\n",
    "\n",
    "args = parser.parse_args(customArgs)\n",
    "opt = vars(args)\n",
    "opt = get_cora_opt(opt)\n",
    "\n",
    "opt['epoch'] = 31\n",
    "opt['adjoint'] = True\n",
    "#opt['method'] = 'explicit_adams'\n",
    "opt['method'] = 'implicit_adams'\n",
    "#opt['method'] = 'dopri5'\n",
    "opt['adjoint_method'] = opt['method']\n",
    "opt['max_iters'] = 10000\n",
    "opt['step_size'] = opt['dt_min'] = 0.01\n",
    "opt['tol_scale'] = 100.0\n",
    "opt['tol_scale_adjoint'] = 100.0\n",
    "\n",
    "# DEBUG\n",
    "#for k in ['dataset', 'epoch', 'adjoint', 'rewiring', 'adaptive', 'dt', 'dt_min', 'method', 'adjoint_method']:\n",
    "#  print(k, opt[k])\n",
    "#main(opt, 0)\n",
    "\n",
    "# Run combination of experiments\n",
    "for stepsize in [0.5, 0.25, 0.1, 0.01]: # 2.0, 1.0\n",
    "    print(f'*** Doing stepsize {stepsize} ***')\n",
    "    for idx in range(opt['count_runs']):\n",
    "        print(f'*** Doing run {idx} ***')\n",
    "        # NOTE: I think setting dt_min may not be necessary, doing it just to be safe!\n",
    "        opt['step_size'] = opt['dt_min'] = stepsize\n",
    "        run(opt, idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
