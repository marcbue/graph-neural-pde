{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faed1d68-9918-4d47-9a0f-6b7c1394515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeljon00/miniconda3/envs/grandtn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa\n",
    "from GNN import GNN\n",
    "import time\n",
    "from data import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c1e662-6fcf-4c95-97e4-41f5ce755be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customArgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7ee566-4b2c-462c-8437-0ec9d73eb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cora_opt(opt):\n",
    "  opt['dataset'] = 'Cora'\n",
    "  opt['data'] = 'Planetoid'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'rmsprop'\n",
    "  opt['lr'] = 0.0047\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 31\n",
    "  opt['augment'] = True\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['adjoint'] = False\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_computers_opt(opt):\n",
    "  opt['dataset'] = 'Computers'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.01\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['epoch'] = 400\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 50\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442b7c4e-3ad2-455a-907f-38f4f25067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name, parameters, lr, weight_decay=0):\n",
    "  if name == 'sgd':\n",
    "    return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'rmsprop':\n",
    "    return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adagrad':\n",
    "    return torch.optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adam':\n",
    "    return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adamax':\n",
    "    return torch.optim.Adamax(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  else:\n",
    "    raise Exception(\"Unsupported optimizer: {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768697b4-1f65-4c10-a8d8-3744ded132dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x)\n",
    "  lf = torch.nn.CrossEntropyLoss()\n",
    "  loss = lf(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "  # TODO: What is this block about???\n",
    "  if model.odeblock.nreg > 0:  # add regularisation - slower for small data, but faster and better performance for large data\n",
    "    reg_states = tuple(torch.mean(rs) for rs in model.reg_states)\n",
    "    regularization_coeffs = model.regularization_coeffs\n",
    "\n",
    "    reg_loss = sum(\n",
    "      reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
    "    )\n",
    "    loss = loss + reg_loss\n",
    "\n",
    "  # Update count of forward evaluations from ODE solver\n",
    "  # NOTE: fm stands for \"forward meter\"\n",
    "  # TODO: Rename this to be more informative!\n",
    "  model.fm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  # Gradient step\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Update count of backwards evaluations from ODE solver\n",
    "  model.bm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "  model.eval()\n",
    "  logits, accs = model(data.x), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs\n",
    "\n",
    "def print_model_params(model):\n",
    "  print(model)\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "      print(name)\n",
    "      print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433d8568-8f88-4feb-8e97-c466a24e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def run(opt, run_count):\n",
    "\n",
    "    # Load dataset and create model\n",
    "    dataset = get_dataset(opt, '../data', False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, data = GNN(opt, dataset, device).to(device), dataset.data.to(device)\n",
    "    print(opt)\n",
    "\n",
    "    # Todo for some reason the submodule parameters inside the attention module don't show up when running on GPU.\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    print_model_params(model)\n",
    "\n",
    "    # Training/test loop\n",
    "    results = {\n",
    "        'time':[],\n",
    "        'loss':[],\n",
    "        'forward_nfe':[],\n",
    "        'backward_nfe':[],\n",
    "        'train_acc':[],\n",
    "        'test_acc':[],\n",
    "        'val_acc':[],\n",
    "        'best_epoch':0,\n",
    "        'best_val_acc':0.,\n",
    "        'best_test_acc':0.,\n",
    "    }\n",
    "    runtimes = []\n",
    "    losses = []\n",
    "\n",
    "    optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "    best_val_acc = test_acc = train_acc = best_epoch = 0\n",
    "    overall_time = time.time()\n",
    "    for epoch in range(1, opt['epoch']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss = train(model, optimizer, data)\n",
    "        train_acc, val_acc, test_acc = test(model, data)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_train_acc = train_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        results['time'].append(time.time() - start_time)\n",
    "        results['loss'].append(loss)\n",
    "        results['forward_nfe'].append(model.fm.sum)\n",
    "        results['backward_nfe'].append(model.bm.sum)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['val_acc'].append(val_acc)\n",
    "        results['best_epoch'] = best_epoch\n",
    "        results['best_train_acc'] = best_train_acc\n",
    "        results['best_val_acc'] = best_val_acc\n",
    "        results['best_test_acc'] = best_test_acc\n",
    "\n",
    "        log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        print(log.format(epoch, results['time'][-1], results['loss'][-1], results['forward_nfe'][-1], results['backward_nfe'][-1], results['train_acc'][-1], results['val_acc'][-1], results['test_acc'][-1]))\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d}'.format(best_val_acc, best_test_acc, best_epoch))\n",
    "    \n",
    "    results['all_epochs_time'] = time.time() - overall_time\n",
    "\n",
    "    # TODO: Save results\n",
    "    # cora_epoch_101_adjoint_false_... . pickle\n",
    "    pickle.dump( results, open( f\"../results/{opt['dataset']}_{opt['method']}_stepsize_{opt['step_size']}_run_{run_count}.pickle\", \"wb\" ) )\n",
    "\n",
    "    return train_acc, best_val_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769ee298-1c23-42ab-9be1-cec3ccae4b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Doing stepsize 0.5 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.158228, Loss 1.949370, forward nfe 139, backward nfe 93, Train: 0.4286, Val: 0.2740, Test: 0.2840\n",
      "Epoch: 002, Runtime 0.181033, Loss 1.810021, forward nfe 469, backward nfe 198, Train: 0.5000, Val: 0.4440, Test: 0.4550\n",
      "Epoch: 003, Runtime 0.165773, Loss 1.613426, forward nfe 800, backward nfe 301, Train: 0.7000, Val: 0.5160, Test: 0.5150\n",
      "Epoch: 004, Runtime 0.176733, Loss 1.394635, forward nfe 1125, backward nfe 400, Train: 0.8214, Val: 0.6660, Test: 0.6770\n",
      "Epoch: 005, Runtime 0.169978, Loss 1.211607, forward nfe 1451, backward nfe 497, Train: 0.9000, Val: 0.7620, Test: 0.7820\n",
      "Epoch: 006, Runtime 0.171904, Loss 0.980997, forward nfe 1772, backward nfe 592, Train: 0.9000, Val: 0.7560, Test: 0.7910\n",
      "Epoch: 007, Runtime 0.161596, Loss 0.863282, forward nfe 2091, backward nfe 684, Train: 0.9214, Val: 0.7180, Test: 0.7010\n",
      "Epoch: 008, Runtime 0.167305, Loss 0.756054, forward nfe 2407, backward nfe 773, Train: 0.9214, Val: 0.7740, Test: 0.8100\n",
      "Epoch: 009, Runtime 0.157960, Loss 0.689677, forward nfe 2721, backward nfe 860, Train: 0.9286, Val: 0.7560, Test: 0.7470\n",
      "Epoch: 010, Runtime 0.169717, Loss 0.608144, forward nfe 3033, backward nfe 947, Train: 0.9357, Val: 0.7940, Test: 0.8220\n",
      "Epoch: 011, Runtime 0.157984, Loss 0.520284, forward nfe 3343, backward nfe 1033, Train: 0.9571, Val: 0.8060, Test: 0.8260\n",
      "Epoch: 012, Runtime 0.153054, Loss 0.484801, forward nfe 3650, backward nfe 1117, Train: 0.9571, Val: 0.7900, Test: 0.8170\n",
      "Epoch: 013, Runtime 0.151930, Loss 0.430948, forward nfe 3956, backward nfe 1200, Train: 0.9643, Val: 0.8000, Test: 0.8250\n",
      "Epoch: 014, Runtime 0.148605, Loss 0.387441, forward nfe 4261, backward nfe 1282, Train: 0.9786, Val: 0.7980, Test: 0.8170\n",
      "Epoch: 015, Runtime 0.157711, Loss 0.373314, forward nfe 4565, backward nfe 1364, Train: 0.9714, Val: 0.8040, Test: 0.8260\n",
      "Epoch: 016, Runtime 0.159904, Loss 0.339616, forward nfe 4867, backward nfe 1444, Train: 0.9857, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 017, Runtime 0.146804, Loss 0.321860, forward nfe 5169, backward nfe 1521, Train: 0.9857, Val: 0.7940, Test: 0.8200\n",
      "Epoch: 018, Runtime 0.159906, Loss 0.307660, forward nfe 5469, backward nfe 1598, Train: 0.9857, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 019, Runtime 0.161711, Loss 0.295374, forward nfe 5766, backward nfe 1675, Train: 0.9786, Val: 0.7880, Test: 0.8220\n",
      "Epoch: 020, Runtime 0.157607, Loss 0.266288, forward nfe 6062, backward nfe 1750, Train: 0.9857, Val: 0.7980, Test: 0.8200\n",
      "Epoch: 021, Runtime 0.146558, Loss 0.254117, forward nfe 6357, backward nfe 1825, Train: 0.9857, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 022, Runtime 0.153790, Loss 0.228795, forward nfe 6651, backward nfe 1898, Train: 0.9857, Val: 0.7920, Test: 0.8220\n",
      "Epoch: 023, Runtime 0.144216, Loss 0.222218, forward nfe 6944, backward nfe 1971, Train: 0.9857, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 024, Runtime 0.156789, Loss 0.214337, forward nfe 7236, backward nfe 2042, Train: 0.9857, Val: 0.7840, Test: 0.8190\n",
      "Epoch: 025, Runtime 0.158265, Loss 0.198617, forward nfe 7526, backward nfe 2113, Train: 0.9929, Val: 0.7820, Test: 0.8170\n",
      "Epoch: 026, Runtime 0.151119, Loss 0.197135, forward nfe 7816, backward nfe 2184, Train: 0.9929, Val: 0.7860, Test: 0.8170\n",
      "Epoch: 027, Runtime 0.141847, Loss 0.194227, forward nfe 8104, backward nfe 2255, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 028, Runtime 0.149350, Loss 0.186663, forward nfe 8390, backward nfe 2326, Train: 0.9929, Val: 0.7840, Test: 0.8170\n",
      "Epoch: 029, Runtime 0.137202, Loss 0.158731, forward nfe 8677, backward nfe 2397, Train: 0.9929, Val: 0.7840, Test: 0.8080\n",
      "Epoch: 030, Runtime 0.150118, Loss 0.173050, forward nfe 8961, backward nfe 2467, Train: 0.9929, Val: 0.7900, Test: 0.8100\n",
      "best val accuracy 0.806000 with test accuracy 0.826000 at epoch 11\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.157733, Loss 1.950485, forward nfe 143, backward nfe 95, Train: 0.3286, Val: 0.1580, Test: 0.1640\n",
      "Epoch: 002, Runtime 0.188344, Loss 1.854785, forward nfe 485, backward nfe 212, Train: 0.4929, Val: 0.3380, Test: 0.3810\n",
      "Epoch: 003, Runtime 0.174693, Loss 1.665562, forward nfe 830, backward nfe 325, Train: 0.6857, Val: 0.6140, Test: 0.6170\n",
      "Epoch: 004, Runtime 0.181068, Loss 1.447119, forward nfe 1169, backward nfe 434, Train: 0.7500, Val: 0.5440, Test: 0.5640\n",
      "Epoch: 005, Runtime 0.166225, Loss 1.286554, forward nfe 1503, backward nfe 536, Train: 0.7643, Val: 0.6820, Test: 0.7020\n",
      "Epoch: 006, Runtime 0.183246, Loss 1.117455, forward nfe 1837, backward nfe 642, Train: 0.8857, Val: 0.6620, Test: 0.6610\n",
      "Epoch: 007, Runtime 0.158699, Loss 0.974989, forward nfe 2168, backward nfe 741, Train: 0.8857, Val: 0.7300, Test: 0.7520\n",
      "Epoch: 008, Runtime 0.171032, Loss 0.879212, forward nfe 2494, backward nfe 839, Train: 0.9286, Val: 0.7540, Test: 0.7520\n",
      "Epoch: 009, Runtime 0.163089, Loss 0.759278, forward nfe 2820, backward nfe 936, Train: 0.8857, Val: 0.7100, Test: 0.7360\n",
      "Epoch: 010, Runtime 0.176274, Loss 0.717190, forward nfe 3144, backward nfe 1028, Train: 0.9429, Val: 0.8040, Test: 0.8230\n",
      "Epoch: 011, Runtime 0.160987, Loss 0.597854, forward nfe 3467, backward nfe 1121, Train: 0.9500, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 012, Runtime 0.168072, Loss 0.544206, forward nfe 3785, backward nfe 1213, Train: 0.9500, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 013, Runtime 0.150057, Loss 0.479156, forward nfe 4102, backward nfe 1303, Train: 0.9571, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 014, Runtime 0.160650, Loss 0.473577, forward nfe 4418, backward nfe 1391, Train: 0.9571, Val: 0.7920, Test: 0.8230\n",
      "Epoch: 015, Runtime 0.167296, Loss 0.421830, forward nfe 4733, backward nfe 1479, Train: 0.9643, Val: 0.7940, Test: 0.8180\n",
      "Epoch: 016, Runtime 0.160824, Loss 0.393943, forward nfe 5047, backward nfe 1566, Train: 0.9714, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 017, Runtime 0.150082, Loss 0.360842, forward nfe 5359, backward nfe 1652, Train: 0.9643, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 018, Runtime 0.165765, Loss 0.354083, forward nfe 5670, backward nfe 1735, Train: 0.9714, Val: 0.7980, Test: 0.8240\n",
      "Epoch: 019, Runtime 0.156308, Loss 0.325423, forward nfe 5979, backward nfe 1818, Train: 0.9786, Val: 0.7940, Test: 0.8190\n",
      "Epoch: 020, Runtime 0.163841, Loss 0.297086, forward nfe 6285, backward nfe 1900, Train: 0.9786, Val: 0.7820, Test: 0.8110\n",
      "Epoch: 021, Runtime 0.163670, Loss 0.279716, forward nfe 6591, backward nfe 1982, Train: 0.9857, Val: 0.7940, Test: 0.8210\n",
      "Epoch: 022, Runtime 0.160997, Loss 0.279370, forward nfe 6897, backward nfe 2063, Train: 0.9857, Val: 0.7860, Test: 0.8130\n",
      "Epoch: 023, Runtime 0.148268, Loss 0.254116, forward nfe 7203, backward nfe 2144, Train: 0.9929, Val: 0.7980, Test: 0.8230\n",
      "Epoch: 024, Runtime 0.161382, Loss 0.242749, forward nfe 7506, backward nfe 2224, Train: 0.9929, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 025, Runtime 0.145960, Loss 0.235912, forward nfe 7807, backward nfe 2301, Train: 0.9929, Val: 0.7980, Test: 0.8190\n",
      "Epoch: 026, Runtime 0.145145, Loss 0.222161, forward nfe 8108, backward nfe 2379, Train: 0.9929, Val: 0.7840, Test: 0.8100\n",
      "Epoch: 027, Runtime 0.147762, Loss 0.224101, forward nfe 8409, backward nfe 2456, Train: 0.9929, Val: 0.7920, Test: 0.8150\n",
      "Epoch: 028, Runtime 0.157086, Loss 0.202453, forward nfe 8709, backward nfe 2533, Train: 0.9929, Val: 0.8000, Test: 0.8180\n",
      "Epoch: 029, Runtime 0.146538, Loss 0.205554, forward nfe 9009, backward nfe 2608, Train: 0.9857, Val: 0.7980, Test: 0.8240\n",
      "Epoch: 030, Runtime 0.156465, Loss 0.188252, forward nfe 9306, backward nfe 2684, Train: 0.9929, Val: 0.7980, Test: 0.8170\n",
      "best val accuracy 0.804000 with test accuracy 0.823000 at epoch 10\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.157826, Loss 1.952058, forward nfe 144, backward nfe 94, Train: 0.4143, Val: 0.2440, Test: 0.2630\n",
      "Epoch: 002, Runtime 0.191673, Loss 1.772640, forward nfe 489, backward nfe 210, Train: 0.5714, Val: 0.3420, Test: 0.3590\n",
      "Epoch: 003, Runtime 0.178325, Loss 1.559670, forward nfe 834, backward nfe 324, Train: 0.8429, Val: 0.6080, Test: 0.6270\n",
      "Epoch: 004, Runtime 0.187347, Loss 1.332512, forward nfe 1175, backward nfe 432, Train: 0.6214, Val: 0.5060, Test: 0.5010\n",
      "Epoch: 005, Runtime 0.180056, Loss 1.189337, forward nfe 1520, backward nfe 541, Train: 0.8643, Val: 0.6460, Test: 0.6550\n",
      "Epoch: 006, Runtime 0.182110, Loss 1.021526, forward nfe 1855, backward nfe 642, Train: 0.9357, Val: 0.7820, Test: 0.8110\n",
      "Epoch: 007, Runtime 0.177412, Loss 0.817035, forward nfe 2189, backward nfe 742, Train: 0.9214, Val: 0.7460, Test: 0.7670\n",
      "Epoch: 008, Runtime 0.177295, Loss 0.710800, forward nfe 2517, backward nfe 840, Train: 0.9500, Val: 0.8080, Test: 0.8220\n",
      "Epoch: 009, Runtime 0.172644, Loss 0.649959, forward nfe 2843, backward nfe 936, Train: 0.9571, Val: 0.7720, Test: 0.8030\n",
      "Epoch: 010, Runtime 0.170190, Loss 0.578224, forward nfe 3169, backward nfe 1032, Train: 0.9571, Val: 0.8000, Test: 0.8200\n",
      "Epoch: 011, Runtime 0.160290, Loss 0.530596, forward nfe 3494, backward nfe 1128, Train: 0.9500, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 012, Runtime 0.168984, Loss 0.479581, forward nfe 3818, backward nfe 1221, Train: 0.9643, Val: 0.8020, Test: 0.8270\n",
      "Epoch: 013, Runtime 0.168537, Loss 0.453592, forward nfe 4139, backward nfe 1312, Train: 0.9571, Val: 0.7980, Test: 0.8290\n",
      "Epoch: 014, Runtime 0.171559, Loss 0.412359, forward nfe 4458, backward nfe 1402, Train: 0.9714, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 015, Runtime 0.164456, Loss 0.369044, forward nfe 4776, backward nfe 1491, Train: 0.9786, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 016, Runtime 0.166153, Loss 0.359667, forward nfe 5094, backward nfe 1579, Train: 0.9786, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 017, Runtime 0.159938, Loss 0.341110, forward nfe 5408, backward nfe 1667, Train: 0.9857, Val: 0.8020, Test: 0.8200\n",
      "Epoch: 018, Runtime 0.165671, Loss 0.319768, forward nfe 5723, backward nfe 1754, Train: 0.9714, Val: 0.7740, Test: 0.7950\n",
      "Epoch: 019, Runtime 0.151743, Loss 0.302853, forward nfe 6036, backward nfe 1840, Train: 0.9857, Val: 0.8080, Test: 0.8120\n",
      "Epoch: 020, Runtime 0.147460, Loss 0.285252, forward nfe 6347, backward nfe 1923, Train: 0.9786, Val: 0.7840, Test: 0.8040\n",
      "Epoch: 021, Runtime 0.154655, Loss 0.282371, forward nfe 6656, backward nfe 2006, Train: 0.9857, Val: 0.8080, Test: 0.8200\n",
      "Epoch: 022, Runtime 0.150265, Loss 0.259575, forward nfe 6963, backward nfe 2088, Train: 0.9929, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 023, Runtime 0.159710, Loss 0.251836, forward nfe 7269, backward nfe 2170, Train: 0.9929, Val: 0.8080, Test: 0.8120\n",
      "Epoch: 024, Runtime 0.150152, Loss 0.239812, forward nfe 7575, backward nfe 2250, Train: 1.0000, Val: 0.7980, Test: 0.8140\n",
      "Epoch: 025, Runtime 0.162950, Loss 0.212950, forward nfe 7879, backward nfe 2330, Train: 0.9857, Val: 0.8080, Test: 0.8200\n",
      "Epoch: 026, Runtime 0.157528, Loss 0.215130, forward nfe 8182, backward nfe 2407, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 027, Runtime 0.153662, Loss 0.199302, forward nfe 8484, backward nfe 2485, Train: 1.0000, Val: 0.7840, Test: 0.8100\n",
      "Epoch: 028, Runtime 0.151724, Loss 0.200580, forward nfe 8785, backward nfe 2562, Train: 1.0000, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 029, Runtime 0.154670, Loss 0.192706, forward nfe 9086, backward nfe 2639, Train: 0.9857, Val: 0.8080, Test: 0.8170\n",
      "Epoch: 030, Runtime 0.157791, Loss 0.200096, forward nfe 9387, backward nfe 2716, Train: 0.9929, Val: 0.7760, Test: 0.7930\n",
      "best val accuracy 0.808000 with test accuracy 0.822000 at epoch 8\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.165624, Loss 1.956075, forward nfe 144, backward nfe 94, Train: 0.3357, Val: 0.2220, Test: 0.2180\n",
      "Epoch: 002, Runtime 0.174103, Loss 1.780250, forward nfe 466, backward nfe 198, Train: 0.4214, Val: 0.2760, Test: 0.3030\n",
      "Epoch: 003, Runtime 0.180573, Loss 1.561172, forward nfe 799, backward nfe 301, Train: 0.6214, Val: 0.3820, Test: 0.4230\n",
      "Epoch: 004, Runtime 0.176056, Loss 1.354573, forward nfe 1129, backward nfe 402, Train: 0.7929, Val: 0.6600, Test: 0.6800\n",
      "Epoch: 005, Runtime 0.176171, Loss 1.191614, forward nfe 1455, backward nfe 500, Train: 0.8500, Val: 0.5880, Test: 0.6340\n",
      "Epoch: 006, Runtime 0.168974, Loss 1.017562, forward nfe 1781, backward nfe 595, Train: 0.9143, Val: 0.7400, Test: 0.7350\n",
      "Epoch: 007, Runtime 0.164814, Loss 0.893864, forward nfe 2104, backward nfe 688, Train: 0.9214, Val: 0.7880, Test: 0.8240\n",
      "Epoch: 008, Runtime 0.173621, Loss 0.771759, forward nfe 2423, backward nfe 779, Train: 0.9357, Val: 0.7480, Test: 0.7860\n",
      "Epoch: 009, Runtime 0.154166, Loss 0.660997, forward nfe 2741, backward nfe 868, Train: 0.9500, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 010, Runtime 0.174542, Loss 0.606300, forward nfe 3059, backward nfe 957, Train: 0.9500, Val: 0.7760, Test: 0.8100\n",
      "Epoch: 011, Runtime 0.165991, Loss 0.532265, forward nfe 3374, backward nfe 1045, Train: 0.9571, Val: 0.7880, Test: 0.7990\n",
      "Epoch: 012, Runtime 0.168282, Loss 0.485612, forward nfe 3689, backward nfe 1131, Train: 0.9571, Val: 0.7880, Test: 0.8180\n",
      "Epoch: 013, Runtime 0.171851, Loss 0.445328, forward nfe 4003, backward nfe 1216, Train: 0.9786, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 014, Runtime 0.162939, Loss 0.405559, forward nfe 4315, backward nfe 1299, Train: 0.9643, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 015, Runtime 0.150984, Loss 0.400182, forward nfe 4622, backward nfe 1381, Train: 0.9857, Val: 0.7860, Test: 0.8010\n",
      "Epoch: 016, Runtime 0.156381, Loss 0.350481, forward nfe 4928, backward nfe 1462, Train: 0.9714, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 017, Runtime 0.158473, Loss 0.327874, forward nfe 5234, backward nfe 1543, Train: 0.9857, Val: 0.8000, Test: 0.8130\n",
      "Epoch: 018, Runtime 0.157415, Loss 0.313975, forward nfe 5537, backward nfe 1621, Train: 0.9857, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 019, Runtime 0.160643, Loss 0.284361, forward nfe 5840, backward nfe 1699, Train: 0.9929, Val: 0.7800, Test: 0.7970\n",
      "Epoch: 020, Runtime 0.155809, Loss 0.262953, forward nfe 6142, backward nfe 1775, Train: 0.9929, Val: 0.7840, Test: 0.8100\n",
      "Epoch: 021, Runtime 0.162091, Loss 0.257139, forward nfe 6444, backward nfe 1850, Train: 0.9929, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 022, Runtime 0.156463, Loss 0.244425, forward nfe 6742, backward nfe 1925, Train: 0.9929, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 023, Runtime 0.159381, Loss 0.221264, forward nfe 7041, backward nfe 1999, Train: 0.9929, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 024, Runtime 0.158887, Loss 0.211116, forward nfe 7337, backward nfe 2072, Train: 0.9929, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 025, Runtime 0.152085, Loss 0.203523, forward nfe 7632, backward nfe 2144, Train: 0.9929, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 026, Runtime 0.148881, Loss 0.183261, forward nfe 7928, backward nfe 2217, Train: 0.9929, Val: 0.7800, Test: 0.8040\n",
      "Epoch: 027, Runtime 0.155294, Loss 0.182777, forward nfe 8222, backward nfe 2289, Train: 0.9929, Val: 0.8000, Test: 0.8210\n",
      "Epoch: 028, Runtime 0.151550, Loss 0.173546, forward nfe 8513, backward nfe 2360, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 029, Runtime 0.141323, Loss 0.169793, forward nfe 8803, backward nfe 2431, Train: 0.9929, Val: 0.7920, Test: 0.8040\n",
      "Epoch: 030, Runtime 0.152266, Loss 0.161846, forward nfe 9093, backward nfe 2501, Train: 0.9929, Val: 0.7900, Test: 0.8020\n",
      "best val accuracy 0.800000 with test accuracy 0.813000 at epoch 17\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.166900, Loss 1.952981, forward nfe 140, backward nfe 95, Train: 0.3714, Val: 0.2500, Test: 0.2720\n",
      "Epoch: 002, Runtime 0.179158, Loss 1.783399, forward nfe 472, backward nfe 202, Train: 0.6071, Val: 0.5000, Test: 0.5060\n",
      "Epoch: 003, Runtime 0.178554, Loss 1.553506, forward nfe 804, backward nfe 306, Train: 0.6857, Val: 0.4460, Test: 0.4900\n",
      "Epoch: 004, Runtime 0.170890, Loss 1.362038, forward nfe 1129, backward nfe 404, Train: 0.8786, Val: 0.7260, Test: 0.7640\n",
      "Epoch: 005, Runtime 0.175797, Loss 1.098866, forward nfe 1455, backward nfe 500, Train: 0.9071, Val: 0.6880, Test: 0.7000\n",
      "Epoch: 006, Runtime 0.161976, Loss 0.941590, forward nfe 1773, backward nfe 596, Train: 0.9071, Val: 0.7260, Test: 0.7540\n",
      "Epoch: 007, Runtime 0.167868, Loss 0.835297, forward nfe 2093, backward nfe 691, Train: 0.9214, Val: 0.7580, Test: 0.7850\n",
      "Epoch: 008, Runtime 0.167973, Loss 0.733517, forward nfe 2409, backward nfe 783, Train: 0.9571, Val: 0.7680, Test: 0.7900\n",
      "Epoch: 009, Runtime 0.165643, Loss 0.636854, forward nfe 2725, backward nfe 872, Train: 0.9500, Val: 0.7800, Test: 0.8190\n",
      "Epoch: 010, Runtime 0.171037, Loss 0.574732, forward nfe 3039, backward nfe 960, Train: 0.9714, Val: 0.7840, Test: 0.8050\n",
      "Epoch: 011, Runtime 0.160608, Loss 0.532120, forward nfe 3353, backward nfe 1046, Train: 0.9643, Val: 0.7780, Test: 0.8220\n",
      "Epoch: 012, Runtime 0.166733, Loss 0.475457, forward nfe 3666, backward nfe 1133, Train: 0.9714, Val: 0.7760, Test: 0.8050\n",
      "Epoch: 013, Runtime 0.153765, Loss 0.436417, forward nfe 3974, backward nfe 1216, Train: 0.9714, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 014, Runtime 0.162624, Loss 0.407555, forward nfe 4282, backward nfe 1298, Train: 0.9643, Val: 0.7740, Test: 0.8060\n",
      "Epoch: 015, Runtime 0.161769, Loss 0.363800, forward nfe 4588, backward nfe 1380, Train: 0.9714, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 016, Runtime 0.159683, Loss 0.371672, forward nfe 4893, backward nfe 1461, Train: 0.9643, Val: 0.7780, Test: 0.8070\n",
      "Epoch: 017, Runtime 0.157132, Loss 0.336261, forward nfe 5195, backward nfe 1539, Train: 0.9786, Val: 0.7780, Test: 0.8080\n",
      "Epoch: 018, Runtime 0.162719, Loss 0.295965, forward nfe 5497, backward nfe 1616, Train: 0.9857, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 019, Runtime 0.156948, Loss 0.292554, forward nfe 5798, backward nfe 1693, Train: 0.9857, Val: 0.7820, Test: 0.8100\n",
      "Epoch: 020, Runtime 0.158429, Loss 0.287281, forward nfe 6098, backward nfe 1770, Train: 0.9929, Val: 0.7740, Test: 0.8040\n",
      "Epoch: 021, Runtime 0.160235, Loss 0.265051, forward nfe 6397, backward nfe 1845, Train: 0.9857, Val: 0.7820, Test: 0.8190\n",
      "Epoch: 022, Runtime 0.159116, Loss 0.247535, forward nfe 6692, backward nfe 1920, Train: 0.9929, Val: 0.7740, Test: 0.8040\n",
      "Epoch: 023, Runtime 0.151935, Loss 0.232984, forward nfe 6988, backward nfe 1994, Train: 0.9929, Val: 0.7760, Test: 0.8050\n",
      "Epoch: 024, Runtime 0.152271, Loss 0.219347, forward nfe 7282, backward nfe 2065, Train: 0.9929, Val: 0.7800, Test: 0.8000\n",
      "Epoch: 025, Runtime 0.142547, Loss 0.206964, forward nfe 7575, backward nfe 2136, Train: 0.9929, Val: 0.7800, Test: 0.7950\n",
      "Epoch: 026, Runtime 0.159109, Loss 0.211453, forward nfe 7867, backward nfe 2207, Train: 0.9929, Val: 0.7720, Test: 0.7950\n",
      "Epoch: 027, Runtime 0.148644, Loss 0.204549, forward nfe 8157, backward nfe 2278, Train: 1.0000, Val: 0.7700, Test: 0.7910\n",
      "Epoch: 028, Runtime 0.160205, Loss 0.201632, forward nfe 8447, backward nfe 2349, Train: 0.9929, Val: 0.7700, Test: 0.7960\n",
      "Epoch: 029, Runtime 0.151494, Loss 0.182745, forward nfe 8735, backward nfe 2420, Train: 0.9929, Val: 0.7680, Test: 0.7910\n",
      "Epoch: 030, Runtime 0.154681, Loss 0.176980, forward nfe 9023, backward nfe 2490, Train: 0.9857, Val: 0.7640, Test: 0.7830\n",
      "best val accuracy 0.790000 with test accuracy 0.810000 at epoch 15\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.162887, Loss 1.959290, forward nfe 140, backward nfe 93, Train: 0.2571, Val: 0.2120, Test: 0.2270\n",
      "Epoch: 002, Runtime 0.190438, Loss 1.873277, forward nfe 487, backward nfe 214, Train: 0.4929, Val: 0.4840, Test: 0.4980\n",
      "Epoch: 003, Runtime 0.185419, Loss 1.668895, forward nfe 822, backward nfe 326, Train: 0.3071, Val: 0.2180, Test: 0.2280\n",
      "Epoch: 004, Runtime 0.190359, Loss 1.562434, forward nfe 1157, backward nfe 436, Train: 0.6500, Val: 0.5500, Test: 0.5740\n",
      "Epoch: 005, Runtime 0.179654, Loss 1.379094, forward nfe 1499, backward nfe 542, Train: 0.8500, Val: 0.6100, Test: 0.6270\n",
      "Epoch: 006, Runtime 0.178584, Loss 1.130222, forward nfe 1831, backward nfe 643, Train: 0.9143, Val: 0.7820, Test: 0.8210\n",
      "Epoch: 007, Runtime 0.178442, Loss 0.923854, forward nfe 2162, backward nfe 744, Train: 0.9214, Val: 0.7780, Test: 0.8020\n",
      "Epoch: 008, Runtime 0.169875, Loss 0.805684, forward nfe 2488, backward nfe 844, Train: 0.9429, Val: 0.7960, Test: 0.8260\n",
      "Epoch: 009, Runtime 0.167487, Loss 0.720959, forward nfe 2814, backward nfe 942, Train: 0.9571, Val: 0.7820, Test: 0.8110\n",
      "Epoch: 010, Runtime 0.169391, Loss 0.634414, forward nfe 3137, backward nfe 1037, Train: 0.9643, Val: 0.7960, Test: 0.8200\n",
      "Epoch: 011, Runtime 0.157506, Loss 0.557883, forward nfe 3457, backward nfe 1132, Train: 0.9571, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 012, Runtime 0.170542, Loss 0.524321, forward nfe 3777, backward nfe 1225, Train: 0.9643, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 013, Runtime 0.160101, Loss 0.466056, forward nfe 4094, backward nfe 1317, Train: 0.9571, Val: 0.8060, Test: 0.8280\n",
      "Epoch: 014, Runtime 0.164190, Loss 0.447771, forward nfe 4410, backward nfe 1406, Train: 0.9571, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 015, Runtime 0.170274, Loss 0.400397, forward nfe 4726, backward nfe 1495, Train: 0.9714, Val: 0.7980, Test: 0.8200\n",
      "Epoch: 016, Runtime 0.167784, Loss 0.356455, forward nfe 5040, backward nfe 1582, Train: 0.9571, Val: 0.7980, Test: 0.8180\n",
      "Epoch: 017, Runtime 0.170558, Loss 0.344984, forward nfe 5352, backward nfe 1670, Train: 0.9786, Val: 0.8060, Test: 0.8240\n",
      "Epoch: 018, Runtime 0.162777, Loss 0.324012, forward nfe 5662, backward nfe 1755, Train: 0.9714, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 019, Runtime 0.162734, Loss 0.313204, forward nfe 5972, backward nfe 1841, Train: 0.9786, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 020, Runtime 0.163278, Loss 0.293443, forward nfe 6278, backward nfe 1925, Train: 0.9857, Val: 0.7920, Test: 0.8170\n",
      "Epoch: 021, Runtime 0.157288, Loss 0.264342, forward nfe 6584, backward nfe 2007, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "Epoch: 022, Runtime 0.161793, Loss 0.254901, forward nfe 6888, backward nfe 2089, Train: 0.9786, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 023, Runtime 0.145032, Loss 0.241687, forward nfe 7193, backward nfe 2169, Train: 0.9786, Val: 0.7980, Test: 0.8140\n",
      "Epoch: 024, Runtime 0.161187, Loss 0.246089, forward nfe 7495, backward nfe 2248, Train: 0.9857, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 025, Runtime 0.147461, Loss 0.218453, forward nfe 7797, backward nfe 2326, Train: 0.9857, Val: 0.7960, Test: 0.8110\n",
      "Epoch: 026, Runtime 0.159409, Loss 0.208544, forward nfe 8099, backward nfe 2403, Train: 0.9929, Val: 0.7980, Test: 0.8140\n",
      "Epoch: 027, Runtime 0.157082, Loss 0.205594, forward nfe 8398, backward nfe 2479, Train: 0.9929, Val: 0.7780, Test: 0.8040\n",
      "Epoch: 028, Runtime 0.159391, Loss 0.188222, forward nfe 8695, backward nfe 2555, Train: 0.9929, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 029, Runtime 0.149776, Loss 0.184669, forward nfe 8995, backward nfe 2631, Train: 0.9929, Val: 0.7940, Test: 0.8100\n",
      "Epoch: 030, Runtime 0.159922, Loss 0.191612, forward nfe 9291, backward nfe 2706, Train: 0.9929, Val: 0.8000, Test: 0.8100\n",
      "best val accuracy 0.806000 with test accuracy 0.828000 at epoch 13\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.164085, Loss 1.964917, forward nfe 143, backward nfe 94, Train: 0.2714, Val: 0.1740, Test: 0.1890\n",
      "Epoch: 002, Runtime 0.193636, Loss 1.850643, forward nfe 491, backward nfe 224, Train: 0.4929, Val: 0.3400, Test: 0.3410\n",
      "Epoch: 003, Runtime 0.187684, Loss 1.666662, forward nfe 825, backward nfe 334, Train: 0.6643, Val: 0.5700, Test: 0.5600\n",
      "Epoch: 004, Runtime 0.179702, Loss 1.531318, forward nfe 1168, backward nfe 444, Train: 0.8143, Val: 0.6080, Test: 0.6270\n",
      "Epoch: 005, Runtime 0.187392, Loss 1.276864, forward nfe 1507, backward nfe 550, Train: 0.8643, Val: 0.7060, Test: 0.7090\n",
      "Epoch: 006, Runtime 0.179887, Loss 1.078731, forward nfe 1838, backward nfe 654, Train: 0.9071, Val: 0.6960, Test: 0.7170\n",
      "Epoch: 007, Runtime 0.167279, Loss 0.960745, forward nfe 2168, backward nfe 754, Train: 0.9214, Val: 0.7600, Test: 0.7890\n",
      "Epoch: 008, Runtime 0.170661, Loss 0.784378, forward nfe 2494, backward nfe 852, Train: 0.9571, Val: 0.7840, Test: 0.8000\n",
      "Epoch: 009, Runtime 0.171431, Loss 0.676999, forward nfe 2818, backward nfe 947, Train: 0.9500, Val: 0.7900, Test: 0.8090\n",
      "Epoch: 010, Runtime 0.170161, Loss 0.603438, forward nfe 3141, backward nfe 1041, Train: 0.9571, Val: 0.7920, Test: 0.8150\n",
      "Epoch: 011, Runtime 0.173422, Loss 0.554631, forward nfe 3458, backward nfe 1134, Train: 0.9643, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 012, Runtime 0.162225, Loss 0.510719, forward nfe 3775, backward nfe 1224, Train: 0.9571, Val: 0.7940, Test: 0.8180\n",
      "Epoch: 013, Runtime 0.165648, Loss 0.456082, forward nfe 4091, backward nfe 1314, Train: 0.9786, Val: 0.7860, Test: 0.7990\n",
      "Epoch: 014, Runtime 0.165049, Loss 0.419410, forward nfe 4405, backward nfe 1403, Train: 0.9643, Val: 0.7940, Test: 0.8230\n",
      "Epoch: 015, Runtime 0.173555, Loss 0.390205, forward nfe 4719, backward nfe 1491, Train: 0.9857, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 016, Runtime 0.160188, Loss 0.360446, forward nfe 5030, backward nfe 1578, Train: 0.9857, Val: 0.7900, Test: 0.8200\n",
      "Epoch: 017, Runtime 0.151782, Loss 0.338443, forward nfe 5340, backward nfe 1664, Train: 0.9857, Val: 0.7860, Test: 0.8050\n",
      "Epoch: 018, Runtime 0.159048, Loss 0.312167, forward nfe 5647, backward nfe 1748, Train: 0.9786, Val: 0.7960, Test: 0.8220\n",
      "Epoch: 019, Runtime 0.163728, Loss 0.282920, forward nfe 5953, backward nfe 1831, Train: 0.9857, Val: 0.7820, Test: 0.8020\n",
      "Epoch: 020, Runtime 0.159271, Loss 0.281725, forward nfe 6259, backward nfe 1913, Train: 0.9857, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 021, Runtime 0.152758, Loss 0.239120, forward nfe 6565, backward nfe 1995, Train: 0.9857, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 022, Runtime 0.156603, Loss 0.236589, forward nfe 6868, backward nfe 2075, Train: 0.9929, Val: 0.7860, Test: 0.7990\n",
      "Epoch: 023, Runtime 0.157023, Loss 0.226299, forward nfe 7170, backward nfe 2153, Train: 0.9929, Val: 0.7860, Test: 0.8070\n",
      "Epoch: 024, Runtime 0.160521, Loss 0.213031, forward nfe 7472, backward nfe 2231, Train: 0.9929, Val: 0.7880, Test: 0.8110\n",
      "Epoch: 025, Runtime 0.158277, Loss 0.210090, forward nfe 7772, backward nfe 2308, Train: 0.9929, Val: 0.7920, Test: 0.8070\n",
      "Epoch: 026, Runtime 0.161202, Loss 0.185398, forward nfe 8072, backward nfe 2385, Train: 0.9929, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 027, Runtime 0.156222, Loss 0.177818, forward nfe 8370, backward nfe 2462, Train: 1.0000, Val: 0.7780, Test: 0.8070\n",
      "Epoch: 028, Runtime 0.160599, Loss 0.183487, forward nfe 8666, backward nfe 2538, Train: 1.0000, Val: 0.7840, Test: 0.8080\n",
      "Epoch: 029, Runtime 0.158643, Loss 0.165604, forward nfe 8960, backward nfe 2613, Train: 0.9929, Val: 0.7960, Test: 0.8110\n",
      "Epoch: 030, Runtime 0.157554, Loss 0.186331, forward nfe 9254, backward nfe 2686, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "best val accuracy 0.796000 with test accuracy 0.822000 at epoch 18\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.164760, Loss 1.955128, forward nfe 144, backward nfe 95, Train: 0.3714, Val: 0.1760, Test: 0.2100\n",
      "Epoch: 002, Runtime 0.169207, Loss 1.802644, forward nfe 468, backward nfe 197, Train: 0.2929, Val: 0.3340, Test: 0.3450\n",
      "Epoch: 003, Runtime 0.178231, Loss 1.755435, forward nfe 800, backward nfe 305, Train: 0.5429, Val: 0.3040, Test: 0.3300\n",
      "Epoch: 004, Runtime 0.172022, Loss 1.596490, forward nfe 1125, backward nfe 405, Train: 0.7929, Val: 0.5580, Test: 0.5690\n",
      "Epoch: 005, Runtime 0.172953, Loss 1.291545, forward nfe 1451, backward nfe 503, Train: 0.9357, Val: 0.7540, Test: 0.7660\n",
      "Epoch: 006, Runtime 0.174032, Loss 1.063322, forward nfe 1767, backward nfe 597, Train: 0.9357, Val: 0.7740, Test: 0.7760\n",
      "Epoch: 007, Runtime 0.148588, Loss 0.906477, forward nfe 2083, backward nfe 690, Train: 0.9357, Val: 0.7900, Test: 0.7980\n",
      "Epoch: 008, Runtime 0.162142, Loss 0.798505, forward nfe 2399, backward nfe 780, Train: 0.9571, Val: 0.8100, Test: 0.8120\n",
      "Epoch: 009, Runtime 0.163602, Loss 0.698362, forward nfe 2715, backward nfe 869, Train: 0.9500, Val: 0.8020, Test: 0.8020\n",
      "Epoch: 010, Runtime 0.168356, Loss 0.635885, forward nfe 3029, backward nfe 957, Train: 0.9643, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 011, Runtime 0.153698, Loss 0.567426, forward nfe 3341, backward nfe 1044, Train: 0.9571, Val: 0.8020, Test: 0.8020\n",
      "Epoch: 012, Runtime 0.149783, Loss 0.522853, forward nfe 3649, backward nfe 1129, Train: 0.9643, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 013, Runtime 0.152474, Loss 0.448950, forward nfe 3955, backward nfe 1212, Train: 0.9643, Val: 0.8180, Test: 0.8150\n",
      "Epoch: 014, Runtime 0.164991, Loss 0.439723, forward nfe 4261, backward nfe 1294, Train: 0.9857, Val: 0.8080, Test: 0.8110\n",
      "Epoch: 015, Runtime 0.139270, Loss 0.400918, forward nfe 4564, backward nfe 1374, Train: 0.9714, Val: 0.8060, Test: 0.8130\n",
      "Epoch: 016, Runtime 0.157867, Loss 0.372512, forward nfe 4866, backward nfe 1455, Train: 0.9857, Val: 0.8060, Test: 0.8120\n",
      "Epoch: 017, Runtime 0.148149, Loss 0.336961, forward nfe 5168, backward nfe 1532, Train: 0.9786, Val: 0.8060, Test: 0.8130\n",
      "Epoch: 018, Runtime 0.159593, Loss 0.325515, forward nfe 5469, backward nfe 1609, Train: 0.9857, Val: 0.8120, Test: 0.8140\n",
      "Epoch: 019, Runtime 0.147066, Loss 0.314061, forward nfe 5769, backward nfe 1686, Train: 0.9786, Val: 0.7900, Test: 0.8120\n",
      "Epoch: 020, Runtime 0.150539, Loss 0.300360, forward nfe 6064, backward nfe 1762, Train: 0.9857, Val: 0.8000, Test: 0.8100\n",
      "Epoch: 021, Runtime 0.147970, Loss 0.273442, forward nfe 6359, backward nfe 1837, Train: 0.9857, Val: 0.8060, Test: 0.8080\n",
      "Epoch: 022, Runtime 0.161748, Loss 0.255769, forward nfe 6654, backward nfe 1911, Train: 0.9857, Val: 0.8040, Test: 0.8080\n",
      "Epoch: 023, Runtime 0.136923, Loss 0.245337, forward nfe 6948, backward nfe 1983, Train: 0.9857, Val: 0.8020, Test: 0.8070\n",
      "Epoch: 024, Runtime 0.147804, Loss 0.239225, forward nfe 7240, backward nfe 2054, Train: 0.9857, Val: 0.8060, Test: 0.8030\n",
      "Epoch: 025, Runtime 0.150466, Loss 0.216073, forward nfe 7533, backward nfe 2126, Train: 0.9857, Val: 0.8000, Test: 0.8000\n",
      "Epoch: 026, Runtime 0.152779, Loss 0.217063, forward nfe 7823, backward nfe 2197, Train: 0.9929, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 027, Runtime 0.142787, Loss 0.190512, forward nfe 8113, backward nfe 2268, Train: 0.9929, Val: 0.8000, Test: 0.8000\n",
      "Epoch: 028, Runtime 0.148597, Loss 0.192337, forward nfe 8402, backward nfe 2339, Train: 1.0000, Val: 0.7980, Test: 0.8020\n",
      "Epoch: 029, Runtime 0.132240, Loss 0.197224, forward nfe 8689, backward nfe 2409, Train: 1.0000, Val: 0.8020, Test: 0.8080\n",
      "Epoch: 030, Runtime 0.153383, Loss 0.172582, forward nfe 8974, backward nfe 2478, Train: 0.9929, Val: 0.8000, Test: 0.8030\n",
      "best val accuracy 0.818000 with test accuracy 0.815000 at epoch 13\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.165546, Loss 1.957216, forward nfe 139, backward nfe 93, Train: 0.3429, Val: 0.3040, Test: 0.3140\n",
      "Epoch: 002, Runtime 0.195050, Loss 1.837662, forward nfe 481, backward nfe 212, Train: 0.4214, Val: 0.2760, Test: 0.2700\n",
      "Epoch: 003, Runtime 0.172066, Loss 1.644551, forward nfe 819, backward nfe 326, Train: 0.7929, Val: 0.6480, Test: 0.6720\n",
      "Epoch: 004, Runtime 0.186487, Loss 1.438381, forward nfe 1161, backward nfe 434, Train: 0.8143, Val: 0.5120, Test: 0.5320\n",
      "Epoch: 005, Runtime 0.162503, Loss 1.280558, forward nfe 1499, backward nfe 537, Train: 0.8000, Val: 0.6700, Test: 0.6730\n",
      "Epoch: 006, Runtime 0.186877, Loss 1.111688, forward nfe 1834, backward nfe 645, Train: 0.8500, Val: 0.5360, Test: 0.5440\n",
      "Epoch: 007, Runtime 0.167556, Loss 1.020389, forward nfe 2170, backward nfe 745, Train: 0.9143, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 008, Runtime 0.175526, Loss 0.817547, forward nfe 2497, backward nfe 843, Train: 0.9143, Val: 0.7700, Test: 0.7630\n",
      "Epoch: 009, Runtime 0.177134, Loss 0.734392, forward nfe 2823, backward nfe 940, Train: 0.9429, Val: 0.7920, Test: 0.8030\n",
      "Epoch: 010, Runtime 0.180735, Loss 0.624727, forward nfe 3148, backward nfe 1036, Train: 0.9429, Val: 0.7820, Test: 0.8180\n",
      "Epoch: 011, Runtime 0.161787, Loss 0.572045, forward nfe 3471, backward nfe 1130, Train: 0.9643, Val: 0.7980, Test: 0.8180\n",
      "Epoch: 012, Runtime 0.172234, Loss 0.525786, forward nfe 3790, backward nfe 1222, Train: 0.9500, Val: 0.7960, Test: 0.8240\n",
      "Epoch: 013, Runtime 0.157907, Loss 0.479979, forward nfe 4110, backward nfe 1313, Train: 0.9714, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 014, Runtime 0.171612, Loss 0.443233, forward nfe 4430, backward nfe 1402, Train: 0.9571, Val: 0.7840, Test: 0.8190\n",
      "Epoch: 015, Runtime 0.160005, Loss 0.397478, forward nfe 4747, backward nfe 1491, Train: 0.9786, Val: 0.7880, Test: 0.8130\n",
      "Epoch: 016, Runtime 0.165864, Loss 0.390422, forward nfe 5063, backward nfe 1579, Train: 0.9643, Val: 0.7860, Test: 0.8180\n",
      "Epoch: 017, Runtime 0.162345, Loss 0.378590, forward nfe 5376, backward nfe 1667, Train: 0.9786, Val: 0.7940, Test: 0.8190\n",
      "Epoch: 018, Runtime 0.164773, Loss 0.347394, forward nfe 5690, backward nfe 1752, Train: 0.9786, Val: 0.7900, Test: 0.8130\n",
      "Epoch: 019, Runtime 0.160080, Loss 0.304239, forward nfe 6000, backward nfe 1835, Train: 0.9786, Val: 0.7920, Test: 0.8180\n",
      "Epoch: 020, Runtime 0.160214, Loss 0.301167, forward nfe 6306, backward nfe 1917, Train: 0.9786, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 021, Runtime 0.147275, Loss 0.271453, forward nfe 6613, backward nfe 1999, Train: 0.9786, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 022, Runtime 0.161737, Loss 0.276860, forward nfe 6920, backward nfe 2081, Train: 0.9786, Val: 0.7960, Test: 0.8170\n",
      "Epoch: 023, Runtime 0.157124, Loss 0.255981, forward nfe 7225, backward nfe 2163, Train: 0.9786, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 024, Runtime 0.161961, Loss 0.249442, forward nfe 7528, backward nfe 2244, Train: 0.9786, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 025, Runtime 0.139979, Loss 0.238754, forward nfe 7831, backward nfe 2322, Train: 0.9857, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 026, Runtime 0.142816, Loss 0.227655, forward nfe 8134, backward nfe 2399, Train: 0.9786, Val: 0.7840, Test: 0.8110\n",
      "Epoch: 027, Runtime 0.154998, Loss 0.213529, forward nfe 8435, backward nfe 2476, Train: 0.9929, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 028, Runtime 0.155270, Loss 0.210525, forward nfe 8734, backward nfe 2552, Train: 0.9857, Val: 0.7940, Test: 0.8180\n",
      "Epoch: 029, Runtime 0.153846, Loss 0.189088, forward nfe 9034, backward nfe 2627, Train: 0.9929, Val: 0.8020, Test: 0.8220\n",
      "Epoch: 030, Runtime 0.146494, Loss 0.198165, forward nfe 9332, backward nfe 2702, Train: 0.9857, Val: 0.7900, Test: 0.8130\n",
      "best val accuracy 0.802000 with test accuracy 0.822000 at epoch 29\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.175005, Loss 1.946053, forward nfe 144, backward nfe 93, Train: 0.4786, Val: 0.2660, Test: 0.2950\n",
      "Epoch: 002, Runtime 0.180576, Loss 1.784269, forward nfe 468, backward nfe 199, Train: 0.6429, Val: 0.4760, Test: 0.4950\n",
      "Epoch: 003, Runtime 0.178469, Loss 1.537794, forward nfe 799, backward nfe 301, Train: 0.5429, Val: 0.3060, Test: 0.3150\n",
      "Epoch: 004, Runtime 0.176752, Loss 1.414818, forward nfe 1130, backward nfe 399, Train: 0.8357, Val: 0.7200, Test: 0.7250\n",
      "Epoch: 005, Runtime 0.167822, Loss 1.158912, forward nfe 1456, backward nfe 497, Train: 0.9000, Val: 0.7060, Test: 0.7160\n",
      "Epoch: 006, Runtime 0.161193, Loss 0.982820, forward nfe 1774, backward nfe 591, Train: 0.8571, Val: 0.7020, Test: 0.7220\n",
      "Epoch: 007, Runtime 0.168749, Loss 0.838949, forward nfe 2096, backward nfe 684, Train: 0.9000, Val: 0.7120, Test: 0.7390\n",
      "Epoch: 008, Runtime 0.166108, Loss 0.777123, forward nfe 2414, backward nfe 774, Train: 0.9429, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 009, Runtime 0.170048, Loss 0.648274, forward nfe 2732, backward nfe 862, Train: 0.9571, Val: 0.7880, Test: 0.7960\n",
      "Epoch: 010, Runtime 0.160494, Loss 0.572804, forward nfe 3046, backward nfe 950, Train: 0.9500, Val: 0.7920, Test: 0.8210\n",
      "Epoch: 011, Runtime 0.150480, Loss 0.525153, forward nfe 3360, backward nfe 1036, Train: 0.9643, Val: 0.7980, Test: 0.8230\n",
      "Epoch: 012, Runtime 0.156189, Loss 0.474847, forward nfe 3671, backward nfe 1123, Train: 0.9714, Val: 0.7880, Test: 0.7990\n",
      "Epoch: 013, Runtime 0.161092, Loss 0.418844, forward nfe 3983, backward nfe 1207, Train: 0.9643, Val: 0.7940, Test: 0.8190\n",
      "Epoch: 014, Runtime 0.150605, Loss 0.396785, forward nfe 4289, backward nfe 1289, Train: 0.9643, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 015, Runtime 0.158010, Loss 0.372702, forward nfe 4595, backward nfe 1370, Train: 0.9643, Val: 0.8000, Test: 0.8130\n",
      "Epoch: 016, Runtime 0.157550, Loss 0.355701, forward nfe 4901, backward nfe 1452, Train: 0.9714, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 017, Runtime 0.157678, Loss 0.329093, forward nfe 5206, backward nfe 1531, Train: 0.9643, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 018, Runtime 0.150955, Loss 0.305634, forward nfe 5508, backward nfe 1609, Train: 0.9786, Val: 0.7980, Test: 0.8130\n",
      "Epoch: 019, Runtime 0.157513, Loss 0.291476, forward nfe 5809, backward nfe 1686, Train: 0.9857, Val: 0.7840, Test: 0.8070\n",
      "Epoch: 020, Runtime 0.151160, Loss 0.273259, forward nfe 6110, backward nfe 1763, Train: 0.9714, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 021, Runtime 0.152428, Loss 0.266439, forward nfe 6408, backward nfe 1838, Train: 0.9929, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 022, Runtime 0.145905, Loss 0.238157, forward nfe 6705, backward nfe 1913, Train: 0.9929, Val: 0.7880, Test: 0.8100\n",
      "Epoch: 023, Runtime 0.154751, Loss 0.237628, forward nfe 7001, backward nfe 1986, Train: 0.9929, Val: 0.7920, Test: 0.8130\n",
      "Epoch: 024, Runtime 0.141851, Loss 0.215828, forward nfe 7295, backward nfe 2058, Train: 0.9929, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 025, Runtime 0.156386, Loss 0.216350, forward nfe 7587, backward nfe 2129, Train: 0.9929, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 026, Runtime 0.155020, Loss 0.207538, forward nfe 7880, backward nfe 2200, Train: 0.9929, Val: 0.7940, Test: 0.8090\n",
      "Epoch: 027, Runtime 0.146128, Loss 0.189420, forward nfe 8170, backward nfe 2271, Train: 0.9929, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 028, Runtime 0.158895, Loss 0.191191, forward nfe 8460, backward nfe 2342, Train: 0.9929, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 029, Runtime 0.146705, Loss 0.177621, forward nfe 8749, backward nfe 2412, Train: 0.9929, Val: 0.7940, Test: 0.8050\n",
      "Epoch: 030, Runtime 0.142843, Loss 0.180470, forward nfe 9038, backward nfe 2481, Train: 1.0000, Val: 0.7880, Test: 0.8020\n",
      "best val accuracy 0.800000 with test accuracy 0.813000 at epoch 15\n",
      "*** Doing stepsize 0.25 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.209320, Loss 1.951602, forward nfe 207, backward nfe 66, Train: 0.2643, Val: 0.1200, Test: 0.1480\n",
      "Epoch: 002, Runtime 0.230807, Loss 1.848303, forward nfe 714, backward nfe 160, Train: 0.5500, Val: 0.3500, Test: 0.3790\n",
      "Epoch: 003, Runtime 0.226555, Loss 1.656875, forward nfe 1220, backward nfe 248, Train: 0.5714, Val: 0.3480, Test: 0.3820\n",
      "Epoch: 004, Runtime 0.224563, Loss 1.474169, forward nfe 1723, backward nfe 334, Train: 0.8643, Val: 0.6600, Test: 0.7020\n",
      "Epoch: 005, Runtime 0.222345, Loss 1.249211, forward nfe 2219, backward nfe 419, Train: 0.9143, Val: 0.7120, Test: 0.7270\n",
      "Epoch: 006, Runtime 0.211452, Loss 1.071017, forward nfe 2711, backward nfe 500, Train: 0.9071, Val: 0.7880, Test: 0.8050\n",
      "Epoch: 007, Runtime 0.209660, Loss 0.909284, forward nfe 3200, backward nfe 578, Train: 0.9214, Val: 0.7180, Test: 0.7160\n",
      "Epoch: 008, Runtime 0.217512, Loss 0.828224, forward nfe 3683, backward nfe 654, Train: 0.9143, Val: 0.7700, Test: 0.7910\n",
      "Epoch: 009, Runtime 0.215950, Loss 0.696955, forward nfe 4166, backward nfe 729, Train: 0.9571, Val: 0.7780, Test: 0.7880\n",
      "Epoch: 010, Runtime 0.214735, Loss 0.634404, forward nfe 4644, backward nfe 804, Train: 0.9500, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 011, Runtime 0.216466, Loss 0.555343, forward nfe 5118, backward nfe 879, Train: 0.9643, Val: 0.8020, Test: 0.8080\n",
      "Epoch: 012, Runtime 0.208607, Loss 0.494313, forward nfe 5592, backward nfe 952, Train: 0.9643, Val: 0.7980, Test: 0.8010\n",
      "Epoch: 013, Runtime 0.207004, Loss 0.458743, forward nfe 6059, backward nfe 1023, Train: 0.9571, Val: 0.8060, Test: 0.8100\n",
      "Epoch: 014, Runtime 0.210453, Loss 0.429259, forward nfe 6523, backward nfe 1096, Train: 0.9643, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 015, Runtime 0.209207, Loss 0.400868, forward nfe 6984, backward nfe 1167, Train: 0.9643, Val: 0.8040, Test: 0.8110\n",
      "Epoch: 016, Runtime 0.199949, Loss 0.375656, forward nfe 7447, backward nfe 1239, Train: 0.9786, Val: 0.7920, Test: 0.8030\n",
      "Epoch: 017, Runtime 0.204527, Loss 0.334905, forward nfe 7906, backward nfe 1309, Train: 0.9714, Val: 0.8000, Test: 0.8100\n",
      "Epoch: 018, Runtime 0.200657, Loss 0.326609, forward nfe 8363, backward nfe 1378, Train: 0.9857, Val: 0.8000, Test: 0.8080\n",
      "Epoch: 019, Runtime 0.209931, Loss 0.295515, forward nfe 8818, backward nfe 1449, Train: 0.9786, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 020, Runtime 0.196407, Loss 0.290774, forward nfe 9278, backward nfe 1518, Train: 0.9714, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 021, Runtime 0.201596, Loss 0.284579, forward nfe 9731, backward nfe 1586, Train: 0.9857, Val: 0.7900, Test: 0.8040\n",
      "Epoch: 022, Runtime 0.198640, Loss 0.249918, forward nfe 10184, backward nfe 1655, Train: 0.9786, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 023, Runtime 0.199966, Loss 0.244842, forward nfe 10635, backward nfe 1723, Train: 0.9857, Val: 0.7980, Test: 0.8050\n",
      "Epoch: 024, Runtime 0.193086, Loss 0.233928, forward nfe 11083, backward nfe 1791, Train: 0.9857, Val: 0.8060, Test: 0.8070\n",
      "Epoch: 025, Runtime 0.199917, Loss 0.224542, forward nfe 11529, backward nfe 1859, Train: 0.9857, Val: 0.7900, Test: 0.8080\n",
      "Epoch: 026, Runtime 0.203883, Loss 0.218034, forward nfe 11976, backward nfe 1928, Train: 0.9929, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 027, Runtime 0.199996, Loss 0.211237, forward nfe 12420, backward nfe 1996, Train: 0.9929, Val: 0.7980, Test: 0.8040\n",
      "Epoch: 028, Runtime 0.199424, Loss 0.201547, forward nfe 12864, backward nfe 2064, Train: 1.0000, Val: 0.8020, Test: 0.8080\n",
      "Epoch: 029, Runtime 0.194434, Loss 0.189674, forward nfe 13311, backward nfe 2131, Train: 0.9929, Val: 0.7980, Test: 0.8010\n",
      "Epoch: 030, Runtime 0.200885, Loss 0.187504, forward nfe 13752, backward nfe 2198, Train: 0.9929, Val: 0.7840, Test: 0.7970\n",
      "best val accuracy 0.806000 with test accuracy 0.810000 at epoch 13\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.189116, Loss 1.956000, forward nfe 210, backward nfe 67, Train: 0.3571, Val: 0.2800, Test: 0.2600\n",
      "Epoch: 002, Runtime 0.218127, Loss 1.782501, forward nfe 694, backward nfe 152, Train: 0.5786, Val: 0.5420, Test: 0.5570\n",
      "Epoch: 003, Runtime 0.211834, Loss 1.582584, forward nfe 1177, backward nfe 232, Train: 0.6571, Val: 0.4000, Test: 0.4010\n",
      "Epoch: 004, Runtime 0.224202, Loss 1.380350, forward nfe 1664, backward nfe 309, Train: 0.8429, Val: 0.6900, Test: 0.7120\n",
      "Epoch: 005, Runtime 0.213386, Loss 1.201639, forward nfe 2144, backward nfe 385, Train: 0.8714, Val: 0.6380, Test: 0.6380\n",
      "Epoch: 006, Runtime 0.210134, Loss 1.004311, forward nfe 2616, backward nfe 459, Train: 0.9286, Val: 0.7300, Test: 0.7640\n",
      "Epoch: 007, Runtime 0.215335, Loss 0.851004, forward nfe 3087, backward nfe 533, Train: 0.9429, Val: 0.7840, Test: 0.7860\n",
      "Epoch: 008, Runtime 0.209532, Loss 0.728252, forward nfe 3553, backward nfe 606, Train: 0.9286, Val: 0.7780, Test: 0.7960\n",
      "Epoch: 009, Runtime 0.208834, Loss 0.658758, forward nfe 4019, backward nfe 678, Train: 0.9429, Val: 0.7540, Test: 0.7590\n",
      "Epoch: 010, Runtime 0.209528, Loss 0.598840, forward nfe 4479, backward nfe 748, Train: 0.9214, Val: 0.7460, Test: 0.7630\n",
      "Epoch: 011, Runtime 0.208865, Loss 0.584025, forward nfe 4937, backward nfe 818, Train: 0.9571, Val: 0.7860, Test: 0.7880\n",
      "Epoch: 012, Runtime 0.196401, Loss 0.477777, forward nfe 5389, backward nfe 887, Train: 0.9571, Val: 0.7820, Test: 0.8040\n",
      "Epoch: 013, Runtime 0.201709, Loss 0.441409, forward nfe 5842, backward nfe 957, Train: 0.9643, Val: 0.7880, Test: 0.8030\n",
      "Epoch: 014, Runtime 0.204268, Loss 0.400712, forward nfe 6290, backward nfe 1026, Train: 0.9643, Val: 0.7860, Test: 0.8050\n",
      "Epoch: 015, Runtime 0.204675, Loss 0.379744, forward nfe 6748, backward nfe 1095, Train: 0.9714, Val: 0.7880, Test: 0.8030\n",
      "Epoch: 016, Runtime 0.208736, Loss 0.370351, forward nfe 7198, backward nfe 1165, Train: 0.9714, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 017, Runtime 0.204108, Loss 0.343749, forward nfe 7643, backward nfe 1234, Train: 0.9643, Val: 0.7840, Test: 0.8110\n",
      "Epoch: 018, Runtime 0.202325, Loss 0.328538, forward nfe 8083, backward nfe 1303, Train: 0.9857, Val: 0.7940, Test: 0.8050\n",
      "Epoch: 019, Runtime 0.192793, Loss 0.299992, forward nfe 8534, backward nfe 1372, Train: 0.9786, Val: 0.7780, Test: 0.8070\n",
      "Epoch: 020, Runtime 0.205142, Loss 0.282755, forward nfe 8977, backward nfe 1440, Train: 0.9786, Val: 0.7840, Test: 0.8020\n",
      "Epoch: 021, Runtime 0.199586, Loss 0.261997, forward nfe 9423, backward nfe 1507, Train: 0.9857, Val: 0.7780, Test: 0.8020\n",
      "Epoch: 022, Runtime 0.195817, Loss 0.250861, forward nfe 9859, backward nfe 1575, Train: 0.9857, Val: 0.7900, Test: 0.8070\n",
      "Epoch: 023, Runtime 0.194549, Loss 0.243819, forward nfe 10296, backward nfe 1643, Train: 0.9857, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 024, Runtime 0.198664, Loss 0.226307, forward nfe 10726, backward nfe 1710, Train: 0.9857, Val: 0.7760, Test: 0.8070\n",
      "Epoch: 025, Runtime 0.201813, Loss 0.236165, forward nfe 11159, backward nfe 1777, Train: 0.9857, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 026, Runtime 0.187616, Loss 0.200059, forward nfe 11593, backward nfe 1844, Train: 0.9857, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 027, Runtime 0.195552, Loss 0.198547, forward nfe 12016, backward nfe 1912, Train: 0.9929, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 028, Runtime 0.189602, Loss 0.187731, forward nfe 12441, backward nfe 1980, Train: 0.9857, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 029, Runtime 0.197443, Loss 0.197033, forward nfe 12866, backward nfe 2047, Train: 1.0000, Val: 0.7800, Test: 0.8050\n",
      "Epoch: 030, Runtime 0.198858, Loss 0.178946, forward nfe 13291, backward nfe 2114, Train: 1.0000, Val: 0.7860, Test: 0.8010\n",
      "best val accuracy 0.796000 with test accuracy 0.808000 at epoch 25\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.195340, Loss 1.953176, forward nfe 206, backward nfe 64, Train: 0.2286, Val: 0.1420, Test: 0.1510\n",
      "Epoch: 002, Runtime 0.228622, Loss 1.828826, forward nfe 710, backward nfe 159, Train: 0.4000, Val: 0.1780, Test: 0.1940\n",
      "Epoch: 003, Runtime 0.226993, Loss 1.641216, forward nfe 1207, backward nfe 246, Train: 0.7071, Val: 0.6040, Test: 0.5950\n",
      "Epoch: 004, Runtime 0.228802, Loss 1.439986, forward nfe 1711, backward nfe 331, Train: 0.8429, Val: 0.6500, Test: 0.6490\n",
      "Epoch: 005, Runtime 0.228251, Loss 1.185488, forward nfe 2211, backward nfe 416, Train: 0.9071, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 006, Runtime 0.230527, Loss 1.015703, forward nfe 2714, backward nfe 500, Train: 0.9214, Val: 0.7400, Test: 0.7410\n",
      "Epoch: 007, Runtime 0.220991, Loss 0.847562, forward nfe 3207, backward nfe 579, Train: 0.9357, Val: 0.7940, Test: 0.8190\n",
      "Epoch: 008, Runtime 0.218427, Loss 0.748612, forward nfe 3700, backward nfe 657, Train: 0.9357, Val: 0.7740, Test: 0.7920\n",
      "Epoch: 009, Runtime 0.216997, Loss 0.646728, forward nfe 4191, backward nfe 734, Train: 0.9429, Val: 0.8000, Test: 0.8140\n",
      "Epoch: 010, Runtime 0.216444, Loss 0.571307, forward nfe 4675, backward nfe 809, Train: 0.9429, Val: 0.7800, Test: 0.8170\n",
      "Epoch: 011, Runtime 0.209361, Loss 0.514824, forward nfe 5154, backward nfe 884, Train: 0.9500, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 012, Runtime 0.198828, Loss 0.483543, forward nfe 5629, backward nfe 960, Train: 0.9571, Val: 0.7820, Test: 0.8080\n",
      "Epoch: 013, Runtime 0.211502, Loss 0.441694, forward nfe 6104, backward nfe 1036, Train: 0.9571, Val: 0.8040, Test: 0.8110\n",
      "Epoch: 014, Runtime 0.207768, Loss 0.393925, forward nfe 6571, backward nfe 1110, Train: 0.9571, Val: 0.7860, Test: 0.8110\n",
      "Epoch: 015, Runtime 0.209598, Loss 0.382246, forward nfe 7035, backward nfe 1181, Train: 0.9786, Val: 0.7960, Test: 0.8200\n",
      "Epoch: 016, Runtime 0.191806, Loss 0.353189, forward nfe 7500, backward nfe 1252, Train: 0.9643, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 017, Runtime 0.206182, Loss 0.344861, forward nfe 7962, backward nfe 1323, Train: 0.9857, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 018, Runtime 0.209202, Loss 0.320903, forward nfe 8421, backward nfe 1393, Train: 0.9714, Val: 0.7920, Test: 0.8150\n",
      "Epoch: 019, Runtime 0.208228, Loss 0.297544, forward nfe 8876, backward nfe 1464, Train: 0.9857, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 020, Runtime 0.207686, Loss 0.269273, forward nfe 9331, backward nfe 1534, Train: 0.9714, Val: 0.7940, Test: 0.8100\n",
      "Epoch: 021, Runtime 0.208135, Loss 0.264612, forward nfe 9788, backward nfe 1604, Train: 0.9857, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 022, Runtime 0.203748, Loss 0.260297, forward nfe 10239, backward nfe 1674, Train: 0.9714, Val: 0.7620, Test: 0.7860\n",
      "Epoch: 023, Runtime 0.206784, Loss 0.253195, forward nfe 10695, backward nfe 1743, Train: 0.9857, Val: 0.7820, Test: 0.8040\n",
      "Epoch: 024, Runtime 0.208547, Loss 0.240444, forward nfe 11146, backward nfe 1813, Train: 0.9929, Val: 0.7800, Test: 0.7980\n",
      "Epoch: 025, Runtime 0.199094, Loss 0.211518, forward nfe 11598, backward nfe 1882, Train: 0.9857, Val: 0.7820, Test: 0.8100\n",
      "Epoch: 026, Runtime 0.198838, Loss 0.209272, forward nfe 12044, backward nfe 1951, Train: 0.9857, Val: 0.7880, Test: 0.8070\n",
      "Epoch: 027, Runtime 0.194111, Loss 0.203892, forward nfe 12488, backward nfe 2020, Train: 0.9929, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 028, Runtime 0.200434, Loss 0.193869, forward nfe 12931, backward nfe 2088, Train: 0.9857, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 029, Runtime 0.190125, Loss 0.191320, forward nfe 13374, backward nfe 2157, Train: 0.9929, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 030, Runtime 0.223247, Loss 0.173104, forward nfe 13818, backward nfe 2225, Train: 0.9929, Val: 0.7720, Test: 0.8000\n",
      "best val accuracy 0.804000 with test accuracy 0.811000 at epoch 13\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.196430, Loss 1.954002, forward nfe 207, backward nfe 66, Train: 0.3857, Val: 0.2920, Test: 0.2870\n",
      "Epoch: 002, Runtime 0.227596, Loss 1.834692, forward nfe 694, backward nfe 151, Train: 0.5357, Val: 0.3700, Test: 0.3890\n",
      "Epoch: 003, Runtime 0.221217, Loss 1.559969, forward nfe 1185, backward nfe 233, Train: 0.5000, Val: 0.4060, Test: 0.4380\n",
      "Epoch: 004, Runtime 0.215712, Loss 1.453043, forward nfe 1672, backward nfe 310, Train: 0.7643, Val: 0.5600, Test: 0.5590\n",
      "Epoch: 005, Runtime 0.195356, Loss 1.206684, forward nfe 2150, backward nfe 385, Train: 0.9214, Val: 0.7380, Test: 0.7570\n",
      "Epoch: 006, Runtime 0.214539, Loss 1.015967, forward nfe 2617, backward nfe 459, Train: 0.8786, Val: 0.7500, Test: 0.7770\n",
      "Epoch: 007, Runtime 0.206784, Loss 0.901314, forward nfe 3085, backward nfe 532, Train: 0.9429, Val: 0.7620, Test: 0.7820\n",
      "Epoch: 008, Runtime 0.200206, Loss 0.752518, forward nfe 3547, backward nfe 603, Train: 0.9143, Val: 0.7680, Test: 0.8040\n",
      "Epoch: 009, Runtime 0.204857, Loss 0.707764, forward nfe 4010, backward nfe 674, Train: 0.9357, Val: 0.7440, Test: 0.7560\n",
      "Epoch: 010, Runtime 0.202849, Loss 0.616943, forward nfe 4465, backward nfe 743, Train: 0.9286, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 011, Runtime 0.200154, Loss 0.576246, forward nfe 4920, backward nfe 813, Train: 0.9643, Val: 0.7940, Test: 0.7980\n",
      "Epoch: 012, Runtime 0.204799, Loss 0.492928, forward nfe 5372, backward nfe 881, Train: 0.9643, Val: 0.8000, Test: 0.8280\n",
      "Epoch: 013, Runtime 0.193982, Loss 0.461423, forward nfe 5823, backward nfe 949, Train: 0.9714, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 014, Runtime 0.204814, Loss 0.412806, forward nfe 6270, backward nfe 1017, Train: 0.9643, Val: 0.7940, Test: 0.8200\n",
      "Epoch: 015, Runtime 0.203760, Loss 0.401520, forward nfe 6708, backward nfe 1085, Train: 0.9643, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 016, Runtime 0.205706, Loss 0.361374, forward nfe 7157, backward nfe 1153, Train: 0.9857, Val: 0.7960, Test: 0.8170\n",
      "Epoch: 017, Runtime 0.189915, Loss 0.331918, forward nfe 7605, backward nfe 1221, Train: 0.9786, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 018, Runtime 0.206340, Loss 0.321660, forward nfe 8043, backward nfe 1289, Train: 0.9857, Val: 0.7920, Test: 0.8110\n",
      "Epoch: 019, Runtime 0.192368, Loss 0.297460, forward nfe 8482, backward nfe 1357, Train: 0.9857, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 020, Runtime 0.194907, Loss 0.279863, forward nfe 8917, backward nfe 1424, Train: 0.9857, Val: 0.7840, Test: 0.8130\n",
      "Epoch: 021, Runtime 0.203549, Loss 0.263938, forward nfe 9361, backward nfe 1492, Train: 0.9857, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 022, Runtime 0.204599, Loss 0.256419, forward nfe 9791, backward nfe 1559, Train: 0.9929, Val: 0.7900, Test: 0.8040\n",
      "Epoch: 023, Runtime 0.184310, Loss 0.245668, forward nfe 10225, backward nfe 1626, Train: 0.9929, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 024, Runtime 0.203470, Loss 0.229742, forward nfe 10655, backward nfe 1693, Train: 0.9929, Val: 0.7900, Test: 0.8120\n",
      "Epoch: 025, Runtime 0.190308, Loss 0.223180, forward nfe 11085, backward nfe 1760, Train: 0.9929, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 026, Runtime 0.197242, Loss 0.208906, forward nfe 11508, backward nfe 1827, Train: 0.9857, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 027, Runtime 0.198455, Loss 0.217106, forward nfe 11936, backward nfe 1894, Train: 0.9929, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 028, Runtime 0.198901, Loss 0.196603, forward nfe 12364, backward nfe 1961, Train: 0.9929, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 029, Runtime 0.183526, Loss 0.177417, forward nfe 12789, backward nfe 2027, Train: 0.9929, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 030, Runtime 0.194480, Loss 0.173762, forward nfe 13210, backward nfe 2094, Train: 0.9929, Val: 0.7940, Test: 0.8060\n",
      "best val accuracy 0.800000 with test accuracy 0.828000 at epoch 12\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.187384, Loss 1.946419, forward nfe 201, backward nfe 64, Train: 0.2714, Val: 0.3860, Test: 0.4080\n",
      "Epoch: 002, Runtime 0.230109, Loss 1.829044, forward nfe 708, backward nfe 158, Train: 0.5714, Val: 0.3420, Test: 0.3430\n",
      "Epoch: 003, Runtime 0.224230, Loss 1.596352, forward nfe 1213, backward nfe 244, Train: 0.4929, Val: 0.4360, Test: 0.4620\n",
      "Epoch: 004, Runtime 0.225952, Loss 1.413305, forward nfe 1721, backward nfe 331, Train: 0.6857, Val: 0.3640, Test: 0.3680\n",
      "Epoch: 005, Runtime 0.222210, Loss 1.337340, forward nfe 2220, backward nfe 415, Train: 0.8929, Val: 0.7300, Test: 0.7270\n",
      "Epoch: 006, Runtime 0.214240, Loss 1.022967, forward nfe 2712, backward nfe 496, Train: 0.9071, Val: 0.7640, Test: 0.7900\n",
      "Epoch: 007, Runtime 0.211716, Loss 0.900027, forward nfe 3200, backward nfe 572, Train: 0.9357, Val: 0.7800, Test: 0.7980\n",
      "Epoch: 008, Runtime 0.211366, Loss 0.759475, forward nfe 3688, backward nfe 648, Train: 0.9357, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 009, Runtime 0.217053, Loss 0.693053, forward nfe 4170, backward nfe 724, Train: 0.9643, Val: 0.7980, Test: 0.8290\n",
      "Epoch: 010, Runtime 0.214665, Loss 0.618371, forward nfe 4645, backward nfe 799, Train: 0.9643, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 011, Runtime 0.212027, Loss 0.543380, forward nfe 5118, backward nfe 874, Train: 0.9643, Val: 0.7900, Test: 0.8260\n",
      "Epoch: 012, Runtime 0.209946, Loss 0.508185, forward nfe 5583, backward nfe 948, Train: 0.9643, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 013, Runtime 0.204035, Loss 0.459155, forward nfe 6048, backward nfe 1021, Train: 0.9714, Val: 0.7900, Test: 0.8270\n",
      "Epoch: 014, Runtime 0.205199, Loss 0.425846, forward nfe 6511, backward nfe 1092, Train: 0.9714, Val: 0.7880, Test: 0.8210\n",
      "Epoch: 015, Runtime 0.200598, Loss 0.383382, forward nfe 6972, backward nfe 1163, Train: 0.9786, Val: 0.7920, Test: 0.8220\n",
      "Epoch: 016, Runtime 0.205706, Loss 0.380539, forward nfe 7430, backward nfe 1233, Train: 0.9786, Val: 0.7920, Test: 0.8250\n",
      "Epoch: 017, Runtime 0.200185, Loss 0.350066, forward nfe 7888, backward nfe 1303, Train: 0.9929, Val: 0.7880, Test: 0.8200\n",
      "Epoch: 018, Runtime 0.201028, Loss 0.339120, forward nfe 8343, backward nfe 1373, Train: 0.9929, Val: 0.7920, Test: 0.8240\n",
      "Epoch: 019, Runtime 0.198378, Loss 0.310566, forward nfe 8801, backward nfe 1441, Train: 0.9929, Val: 0.7900, Test: 0.8190\n",
      "Epoch: 020, Runtime 0.206527, Loss 0.278098, forward nfe 9254, backward nfe 1509, Train: 0.9857, Val: 0.7900, Test: 0.8180\n",
      "Epoch: 021, Runtime 0.195348, Loss 0.281002, forward nfe 9705, backward nfe 1577, Train: 0.9929, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 022, Runtime 0.203621, Loss 0.268564, forward nfe 10153, backward nfe 1645, Train: 0.9857, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 023, Runtime 0.198676, Loss 0.253584, forward nfe 10600, backward nfe 1713, Train: 0.9929, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 024, Runtime 0.205686, Loss 0.228421, forward nfe 11047, backward nfe 1781, Train: 0.9929, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 025, Runtime 0.197802, Loss 0.224929, forward nfe 11489, backward nfe 1849, Train: 0.9929, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 026, Runtime 0.203399, Loss 0.207747, forward nfe 11931, backward nfe 1916, Train: 0.9929, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 027, Runtime 0.181028, Loss 0.198573, forward nfe 12377, backward nfe 1984, Train: 0.9929, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 028, Runtime 0.187953, Loss 0.211663, forward nfe 12820, backward nfe 2052, Train: 0.9929, Val: 0.7840, Test: 0.8040\n",
      "Epoch: 029, Runtime 0.197095, Loss 0.193273, forward nfe 13257, backward nfe 2120, Train: 0.9929, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 030, Runtime 0.198471, Loss 0.179491, forward nfe 13692, backward nfe 2188, Train: 0.9929, Val: 0.7980, Test: 0.8120\n",
      "best val accuracy 0.800000 with test accuracy 0.819000 at epoch 10\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.186031, Loss 1.958384, forward nfe 198, backward nfe 66, Train: 0.2643, Val: 0.1840, Test: 0.1860\n",
      "Epoch: 002, Runtime 0.224537, Loss 1.829424, forward nfe 712, backward nfe 159, Train: 0.2429, Val: 0.1240, Test: 0.1520\n",
      "Epoch: 003, Runtime 0.228644, Loss 1.675773, forward nfe 1216, backward nfe 245, Train: 0.6714, Val: 0.5520, Test: 0.5950\n",
      "Epoch: 004, Runtime 0.222748, Loss 1.418334, forward nfe 1716, backward nfe 330, Train: 0.6500, Val: 0.4300, Test: 0.4540\n",
      "Epoch: 005, Runtime 0.226102, Loss 1.318013, forward nfe 2220, backward nfe 414, Train: 0.8500, Val: 0.6960, Test: 0.7250\n",
      "Epoch: 006, Runtime 0.221813, Loss 1.074995, forward nfe 2719, backward nfe 496, Train: 0.9071, Val: 0.7180, Test: 0.7320\n",
      "Epoch: 007, Runtime 0.211563, Loss 0.944307, forward nfe 3215, backward nfe 575, Train: 0.9357, Val: 0.7720, Test: 0.8200\n",
      "Epoch: 008, Runtime 0.214509, Loss 0.821543, forward nfe 3704, backward nfe 651, Train: 0.9357, Val: 0.7820, Test: 0.7860\n",
      "Epoch: 009, Runtime 0.218122, Loss 0.735291, forward nfe 4191, backward nfe 727, Train: 0.9500, Val: 0.7880, Test: 0.7930\n",
      "Epoch: 010, Runtime 0.214015, Loss 0.690062, forward nfe 4674, backward nfe 802, Train: 0.9143, Val: 0.7700, Test: 0.8160\n",
      "Epoch: 011, Runtime 0.207491, Loss 0.606197, forward nfe 5150, backward nfe 877, Train: 0.9714, Val: 0.7760, Test: 0.7870\n",
      "Epoch: 012, Runtime 0.209658, Loss 0.555333, forward nfe 5628, backward nfe 952, Train: 0.9357, Val: 0.7920, Test: 0.8200\n",
      "Epoch: 013, Runtime 0.199787, Loss 0.513953, forward nfe 6100, backward nfe 1026, Train: 0.9714, Val: 0.7940, Test: 0.7940\n",
      "Epoch: 014, Runtime 0.214602, Loss 0.460209, forward nfe 6574, backward nfe 1102, Train: 0.9571, Val: 0.7940, Test: 0.8090\n",
      "Epoch: 015, Runtime 0.202798, Loss 0.438206, forward nfe 7041, backward nfe 1175, Train: 0.9643, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 016, Runtime 0.207229, Loss 0.400829, forward nfe 7508, backward nfe 1248, Train: 0.9571, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 017, Runtime 0.202560, Loss 0.378127, forward nfe 7967, backward nfe 1321, Train: 0.9714, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 018, Runtime 0.206136, Loss 0.350549, forward nfe 8436, backward nfe 1392, Train: 0.9714, Val: 0.7840, Test: 0.8100\n",
      "Epoch: 019, Runtime 0.209249, Loss 0.331917, forward nfe 8899, backward nfe 1463, Train: 0.9786, Val: 0.7820, Test: 0.8040\n",
      "Epoch: 020, Runtime 0.199284, Loss 0.314667, forward nfe 9361, backward nfe 1534, Train: 0.9714, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 021, Runtime 0.206591, Loss 0.300384, forward nfe 9816, backward nfe 1605, Train: 0.9857, Val: 0.7840, Test: 0.7990\n",
      "Epoch: 022, Runtime 0.185086, Loss 0.273105, forward nfe 10266, backward nfe 1676, Train: 0.9857, Val: 0.7800, Test: 0.8020\n",
      "Epoch: 023, Runtime 0.206536, Loss 0.266359, forward nfe 10725, backward nfe 1747, Train: 0.9929, Val: 0.7760, Test: 0.7990\n",
      "Epoch: 024, Runtime 0.197845, Loss 0.243406, forward nfe 11175, backward nfe 1817, Train: 0.9929, Val: 0.7820, Test: 0.7940\n",
      "Epoch: 025, Runtime 0.199604, Loss 0.245447, forward nfe 11631, backward nfe 1888, Train: 0.9857, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 026, Runtime 0.191003, Loss 0.235777, forward nfe 12079, backward nfe 1959, Train: 0.9929, Val: 0.7820, Test: 0.7970\n",
      "Epoch: 027, Runtime 0.198135, Loss 0.223482, forward nfe 12528, backward nfe 2028, Train: 0.9929, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 028, Runtime 0.208890, Loss 0.215881, forward nfe 12980, backward nfe 2098, Train: 0.9929, Val: 0.7880, Test: 0.7960\n",
      "Epoch: 029, Runtime 0.197541, Loss 0.206031, forward nfe 13427, backward nfe 2167, Train: 0.9929, Val: 0.7700, Test: 0.7970\n",
      "Epoch: 030, Runtime 0.200270, Loss 0.215778, forward nfe 13872, backward nfe 2236, Train: 0.9929, Val: 0.7800, Test: 0.8010\n",
      "best val accuracy 0.794000 with test accuracy 0.794000 at epoch 13\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.188326, Loss 1.954345, forward nfe 201, backward nfe 64, Train: 0.3500, Val: 0.2920, Test: 0.2790\n",
      "Epoch: 002, Runtime 0.218644, Loss 1.792270, forward nfe 691, backward nfe 149, Train: 0.5143, Val: 0.4080, Test: 0.4300\n",
      "Epoch: 003, Runtime 0.210651, Loss 1.576497, forward nfe 1184, backward nfe 231, Train: 0.7571, Val: 0.5880, Test: 0.6230\n",
      "Epoch: 004, Runtime 0.215402, Loss 1.378940, forward nfe 1662, backward nfe 307, Train: 0.5571, Val: 0.4320, Test: 0.4430\n",
      "Epoch: 005, Runtime 0.211201, Loss 1.304321, forward nfe 2143, backward nfe 382, Train: 0.8643, Val: 0.7020, Test: 0.7170\n",
      "Epoch: 006, Runtime 0.208337, Loss 1.065030, forward nfe 2608, backward nfe 455, Train: 0.9143, Val: 0.7660, Test: 0.7860\n",
      "Epoch: 007, Runtime 0.209758, Loss 0.902777, forward nfe 3070, backward nfe 527, Train: 0.9429, Val: 0.7840, Test: 0.8080\n",
      "Epoch: 008, Runtime 0.206160, Loss 0.763563, forward nfe 3529, backward nfe 597, Train: 0.9571, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 009, Runtime 0.209764, Loss 0.701025, forward nfe 3988, backward nfe 668, Train: 0.9500, Val: 0.8080, Test: 0.8340\n",
      "Epoch: 010, Runtime 0.209501, Loss 0.613738, forward nfe 4446, backward nfe 738, Train: 0.9500, Val: 0.8020, Test: 0.8080\n",
      "Epoch: 011, Runtime 0.206084, Loss 0.557445, forward nfe 4901, backward nfe 806, Train: 0.9571, Val: 0.7980, Test: 0.8260\n",
      "Epoch: 012, Runtime 0.204218, Loss 0.493458, forward nfe 5354, backward nfe 874, Train: 0.9571, Val: 0.7980, Test: 0.8240\n",
      "Epoch: 013, Runtime 0.207855, Loss 0.459574, forward nfe 5809, backward nfe 942, Train: 0.9643, Val: 0.7940, Test: 0.8180\n",
      "Epoch: 014, Runtime 0.208444, Loss 0.439155, forward nfe 6261, backward nfe 1011, Train: 0.9714, Val: 0.7960, Test: 0.8290\n",
      "Epoch: 015, Runtime 0.204178, Loss 0.389667, forward nfe 6710, backward nfe 1079, Train: 0.9786, Val: 0.7980, Test: 0.8190\n",
      "Epoch: 016, Runtime 0.204963, Loss 0.367765, forward nfe 7157, backward nfe 1147, Train: 0.9857, Val: 0.7920, Test: 0.8170\n",
      "Epoch: 017, Runtime 0.197634, Loss 0.344314, forward nfe 7598, backward nfe 1215, Train: 0.9714, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 018, Runtime 0.202093, Loss 0.327824, forward nfe 8041, backward nfe 1283, Train: 0.9786, Val: 0.7960, Test: 0.8180\n",
      "Epoch: 019, Runtime 0.207077, Loss 0.297130, forward nfe 8485, backward nfe 1352, Train: 0.9857, Val: 0.7940, Test: 0.8260\n",
      "Epoch: 020, Runtime 0.202729, Loss 0.292034, forward nfe 8929, backward nfe 1420, Train: 0.9929, Val: 0.7880, Test: 0.8140\n",
      "Epoch: 021, Runtime 0.184046, Loss 0.278399, forward nfe 9369, backward nfe 1487, Train: 0.9929, Val: 0.7840, Test: 0.8090\n",
      "Epoch: 022, Runtime 0.194950, Loss 0.265902, forward nfe 9805, backward nfe 1555, Train: 0.9857, Val: 0.7840, Test: 0.8170\n",
      "Epoch: 023, Runtime 0.197127, Loss 0.251556, forward nfe 10250, backward nfe 1622, Train: 0.9857, Val: 0.7840, Test: 0.8020\n",
      "Epoch: 024, Runtime 0.205273, Loss 0.235561, forward nfe 10683, backward nfe 1690, Train: 0.9786, Val: 0.8000, Test: 0.8070\n",
      "Epoch: 025, Runtime 0.197232, Loss 0.227816, forward nfe 11116, backward nfe 1757, Train: 0.9500, Val: 0.7600, Test: 0.7600\n",
      "Epoch: 026, Runtime 0.201198, Loss 0.253783, forward nfe 11542, backward nfe 1824, Train: 0.9857, Val: 0.7860, Test: 0.8170\n",
      "Epoch: 027, Runtime 0.187520, Loss 0.202803, forward nfe 11973, backward nfe 1891, Train: 0.9929, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 028, Runtime 0.199576, Loss 0.212254, forward nfe 12393, backward nfe 1958, Train: 0.9929, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 029, Runtime 0.201859, Loss 0.205935, forward nfe 12816, backward nfe 2026, Train: 0.9929, Val: 0.7840, Test: 0.8020\n",
      "Epoch: 030, Runtime 0.193853, Loss 0.182569, forward nfe 13240, backward nfe 2093, Train: 1.0000, Val: 0.7820, Test: 0.7960\n",
      "best val accuracy 0.808000 with test accuracy 0.834000 at epoch 9\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.197640, Loss 1.955899, forward nfe 200, backward nfe 64, Train: 0.3429, Val: 0.2460, Test: 0.2820\n",
      "Epoch: 002, Runtime 0.228453, Loss 1.876863, forward nfe 711, backward nfe 158, Train: 0.4000, Val: 0.2520, Test: 0.2470\n",
      "Epoch: 003, Runtime 0.227606, Loss 1.700734, forward nfe 1214, backward nfe 245, Train: 0.5214, Val: 0.2980, Test: 0.3400\n",
      "Epoch: 004, Runtime 0.228446, Loss 1.547166, forward nfe 1714, backward nfe 329, Train: 0.8071, Val: 0.6220, Test: 0.6590\n",
      "Epoch: 005, Runtime 0.228763, Loss 1.344370, forward nfe 2217, backward nfe 414, Train: 0.7143, Val: 0.4400, Test: 0.4670\n",
      "Epoch: 006, Runtime 0.219529, Loss 1.248728, forward nfe 2707, backward nfe 492, Train: 0.8286, Val: 0.6920, Test: 0.7370\n",
      "Epoch: 007, Runtime 0.203102, Loss 1.104344, forward nfe 3195, backward nfe 569, Train: 0.8929, Val: 0.6900, Test: 0.6920\n",
      "Epoch: 008, Runtime 0.216144, Loss 0.952455, forward nfe 3681, backward nfe 645, Train: 0.9143, Val: 0.7520, Test: 0.7800\n",
      "Epoch: 009, Runtime 0.207118, Loss 0.796431, forward nfe 4157, backward nfe 720, Train: 0.9357, Val: 0.7700, Test: 0.7870\n",
      "Epoch: 010, Runtime 0.211076, Loss 0.712207, forward nfe 4629, backward nfe 795, Train: 0.9643, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 011, Runtime 0.196973, Loss 0.634624, forward nfe 5096, backward nfe 869, Train: 0.9714, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 012, Runtime 0.209156, Loss 0.592796, forward nfe 5562, backward nfe 942, Train: 0.9714, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 013, Runtime 0.201761, Loss 0.539830, forward nfe 6023, backward nfe 1013, Train: 0.9643, Val: 0.7940, Test: 0.8100\n",
      "Epoch: 014, Runtime 0.205509, Loss 0.492554, forward nfe 6488, backward nfe 1085, Train: 0.9643, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 015, Runtime 0.210605, Loss 0.452745, forward nfe 6944, backward nfe 1156, Train: 0.9643, Val: 0.7960, Test: 0.8110\n",
      "Epoch: 016, Runtime 0.208583, Loss 0.418537, forward nfe 7403, backward nfe 1225, Train: 0.9714, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 017, Runtime 0.192962, Loss 0.398156, forward nfe 7859, backward nfe 1295, Train: 0.9714, Val: 0.7940, Test: 0.8170\n",
      "Epoch: 018, Runtime 0.202917, Loss 0.377893, forward nfe 8308, backward nfe 1364, Train: 0.9714, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 019, Runtime 0.196584, Loss 0.355216, forward nfe 8761, backward nfe 1432, Train: 0.9714, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 020, Runtime 0.202657, Loss 0.319650, forward nfe 9216, backward nfe 1500, Train: 0.9786, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 021, Runtime 0.204984, Loss 0.321795, forward nfe 9665, backward nfe 1569, Train: 0.9714, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 022, Runtime 0.199841, Loss 0.292601, forward nfe 10109, backward nfe 1637, Train: 0.9786, Val: 0.8000, Test: 0.8110\n",
      "Epoch: 023, Runtime 0.201805, Loss 0.294443, forward nfe 10558, backward nfe 1705, Train: 0.9786, Val: 0.8000, Test: 0.8130\n",
      "Epoch: 024, Runtime 0.184487, Loss 0.267924, forward nfe 10994, backward nfe 1773, Train: 0.9857, Val: 0.8000, Test: 0.8090\n",
      "Epoch: 025, Runtime 0.199327, Loss 0.255328, forward nfe 11435, backward nfe 1842, Train: 0.9786, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 026, Runtime 0.195914, Loss 0.241443, forward nfe 11872, backward nfe 1910, Train: 0.9929, Val: 0.7980, Test: 0.8110\n",
      "Epoch: 027, Runtime 0.198614, Loss 0.229542, forward nfe 12312, backward nfe 1977, Train: 0.9929, Val: 0.7980, Test: 0.8120\n",
      "Epoch: 028, Runtime 0.195833, Loss 0.233199, forward nfe 12751, backward nfe 2045, Train: 0.9857, Val: 0.7760, Test: 0.8060\n",
      "Epoch: 029, Runtime 0.199321, Loss 0.209786, forward nfe 13185, backward nfe 2112, Train: 0.9929, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 030, Runtime 0.191747, Loss 0.208874, forward nfe 13617, backward nfe 2179, Train: 0.9929, Val: 0.7900, Test: 0.8100\n",
      "best val accuracy 0.800000 with test accuracy 0.819000 at epoch 18\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.206661, Loss 1.957926, forward nfe 204, backward nfe 66, Train: 0.3857, Val: 0.3180, Test: 0.2990\n",
      "Epoch: 002, Runtime 0.224306, Loss 1.811975, forward nfe 703, backward nfe 151, Train: 0.4643, Val: 0.4120, Test: 0.4060\n",
      "Epoch: 003, Runtime 0.213741, Loss 1.666284, forward nfe 1192, backward nfe 231, Train: 0.6286, Val: 0.4380, Test: 0.4150\n",
      "Epoch: 004, Runtime 0.210238, Loss 1.580163, forward nfe 1671, backward nfe 308, Train: 0.8071, Val: 0.6420, Test: 0.6540\n",
      "Epoch: 005, Runtime 0.215987, Loss 1.324664, forward nfe 2146, backward nfe 382, Train: 0.9286, Val: 0.7760, Test: 0.7720\n",
      "Epoch: 006, Runtime 0.211281, Loss 1.073685, forward nfe 2611, backward nfe 456, Train: 0.9143, Val: 0.7380, Test: 0.7570\n",
      "Epoch: 007, Runtime 0.208201, Loss 0.917413, forward nfe 3080, backward nfe 530, Train: 0.9357, Val: 0.8080, Test: 0.8170\n",
      "Epoch: 008, Runtime 0.207943, Loss 0.802360, forward nfe 3542, backward nfe 601, Train: 0.9286, Val: 0.7640, Test: 0.7750\n",
      "Epoch: 009, Runtime 0.207520, Loss 0.711201, forward nfe 4000, backward nfe 672, Train: 0.9571, Val: 0.8060, Test: 0.8210\n",
      "Epoch: 010, Runtime 0.208503, Loss 0.611814, forward nfe 4457, backward nfe 742, Train: 0.9571, Val: 0.7960, Test: 0.8140\n",
      "Epoch: 011, Runtime 0.207030, Loss 0.539353, forward nfe 4910, backward nfe 811, Train: 0.9500, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 012, Runtime 0.209134, Loss 0.477441, forward nfe 5363, backward nfe 879, Train: 0.9714, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 013, Runtime 0.202053, Loss 0.447297, forward nfe 5814, backward nfe 947, Train: 0.9500, Val: 0.7840, Test: 0.7910\n",
      "Epoch: 014, Runtime 0.206746, Loss 0.436793, forward nfe 6263, backward nfe 1015, Train: 0.9714, Val: 0.7980, Test: 0.8130\n",
      "Epoch: 015, Runtime 0.206145, Loss 0.382674, forward nfe 6712, backward nfe 1084, Train: 0.9500, Val: 0.7780, Test: 0.7870\n",
      "Epoch: 016, Runtime 0.199582, Loss 0.407104, forward nfe 7148, backward nfe 1152, Train: 0.9786, Val: 0.8040, Test: 0.8110\n",
      "Epoch: 017, Runtime 0.200906, Loss 0.319643, forward nfe 7584, backward nfe 1220, Train: 0.9714, Val: 0.7940, Test: 0.8070\n",
      "Epoch: 018, Runtime 0.204094, Loss 0.313276, forward nfe 8022, backward nfe 1288, Train: 0.9786, Val: 0.7940, Test: 0.7970\n",
      "Epoch: 019, Runtime 0.202423, Loss 0.281192, forward nfe 8466, backward nfe 1357, Train: 0.9714, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 020, Runtime 0.199373, Loss 0.275478, forward nfe 8905, backward nfe 1424, Train: 0.9714, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 021, Runtime 0.199266, Loss 0.262153, forward nfe 9340, backward nfe 1492, Train: 0.9857, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 022, Runtime 0.195767, Loss 0.240475, forward nfe 9769, backward nfe 1559, Train: 0.9857, Val: 0.7940, Test: 0.8020\n",
      "Epoch: 023, Runtime 0.193256, Loss 0.223682, forward nfe 10201, backward nfe 1626, Train: 0.9929, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 024, Runtime 0.188420, Loss 0.212337, forward nfe 10631, backward nfe 1693, Train: 0.9929, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 025, Runtime 0.197261, Loss 0.205542, forward nfe 11061, backward nfe 1760, Train: 0.9929, Val: 0.7900, Test: 0.8090\n",
      "Epoch: 026, Runtime 0.185922, Loss 0.206669, forward nfe 11495, backward nfe 1827, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 027, Runtime 0.195605, Loss 0.193147, forward nfe 11927, backward nfe 1894, Train: 0.9929, Val: 0.7980, Test: 0.8170\n",
      "Epoch: 028, Runtime 0.183965, Loss 0.183564, forward nfe 12357, backward nfe 1961, Train: 0.9929, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 029, Runtime 0.190146, Loss 0.194099, forward nfe 12780, backward nfe 2028, Train: 0.9929, Val: 0.7820, Test: 0.8130\n",
      "Epoch: 030, Runtime 0.189966, Loss 0.175141, forward nfe 13199, backward nfe 2094, Train: 0.9929, Val: 0.7780, Test: 0.8050\n",
      "best val accuracy 0.808000 with test accuracy 0.817000 at epoch 7\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.25, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.25, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.203348, Loss 1.955731, forward nfe 201, backward nfe 66, Train: 0.4071, Val: 0.2060, Test: 0.2310\n",
      "Epoch: 002, Runtime 0.228968, Loss 1.833794, forward nfe 708, backward nfe 159, Train: 0.4929, Val: 0.3840, Test: 0.4200\n",
      "Epoch: 003, Runtime 0.229922, Loss 1.622719, forward nfe 1219, backward nfe 248, Train: 0.5571, Val: 0.4000, Test: 0.4520\n",
      "Epoch: 004, Runtime 0.226949, Loss 1.455433, forward nfe 1719, backward nfe 332, Train: 0.6929, Val: 0.6100, Test: 0.6320\n",
      "Epoch: 005, Runtime 0.222011, Loss 1.313577, forward nfe 2224, backward nfe 418, Train: 0.7571, Val: 0.5340, Test: 0.5540\n",
      "Epoch: 006, Runtime 0.217783, Loss 1.125481, forward nfe 2720, backward nfe 497, Train: 0.8500, Val: 0.7040, Test: 0.7140\n",
      "Epoch: 007, Runtime 0.209538, Loss 1.031665, forward nfe 3211, backward nfe 577, Train: 0.7929, Val: 0.5420, Test: 0.5990\n",
      "Epoch: 008, Runtime 0.211283, Loss 0.970479, forward nfe 3693, backward nfe 652, Train: 0.9286, Val: 0.7560, Test: 0.7940\n",
      "Epoch: 009, Runtime 0.206711, Loss 0.802568, forward nfe 4177, backward nfe 727, Train: 0.9357, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 010, Runtime 0.208402, Loss 0.709491, forward nfe 4650, backward nfe 802, Train: 0.9429, Val: 0.8020, Test: 0.8260\n",
      "Epoch: 011, Runtime 0.200166, Loss 0.623150, forward nfe 5120, backward nfe 876, Train: 0.9571, Val: 0.8060, Test: 0.8290\n",
      "Epoch: 012, Runtime 0.206019, Loss 0.561194, forward nfe 5594, backward nfe 951, Train: 0.9571, Val: 0.8020, Test: 0.8330\n",
      "Epoch: 013, Runtime 0.210481, Loss 0.499769, forward nfe 6060, backward nfe 1025, Train: 0.9571, Val: 0.7920, Test: 0.8220\n",
      "Epoch: 014, Runtime 0.204045, Loss 0.456307, forward nfe 6522, backward nfe 1096, Train: 0.9571, Val: 0.8080, Test: 0.8410\n",
      "Epoch: 015, Runtime 0.204084, Loss 0.424554, forward nfe 6988, backward nfe 1168, Train: 0.9571, Val: 0.7880, Test: 0.8180\n",
      "Epoch: 016, Runtime 0.210054, Loss 0.383725, forward nfe 7448, backward nfe 1238, Train: 0.9714, Val: 0.8060, Test: 0.8380\n",
      "Epoch: 017, Runtime 0.199824, Loss 0.349907, forward nfe 7907, backward nfe 1310, Train: 0.9714, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 018, Runtime 0.204767, Loss 0.329763, forward nfe 8362, backward nfe 1381, Train: 0.9857, Val: 0.8040, Test: 0.8300\n",
      "Epoch: 019, Runtime 0.200483, Loss 0.317431, forward nfe 8814, backward nfe 1450, Train: 0.9786, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 020, Runtime 0.202066, Loss 0.287330, forward nfe 9265, backward nfe 1518, Train: 0.9857, Val: 0.7960, Test: 0.8220\n",
      "Epoch: 021, Runtime 0.201664, Loss 0.275540, forward nfe 9716, backward nfe 1587, Train: 0.9857, Val: 0.7940, Test: 0.8100\n",
      "Epoch: 022, Runtime 0.198750, Loss 0.259646, forward nfe 10166, backward nfe 1655, Train: 0.9786, Val: 0.7900, Test: 0.8190\n",
      "Epoch: 023, Runtime 0.199496, Loss 0.245862, forward nfe 10606, backward nfe 1723, Train: 0.9929, Val: 0.7820, Test: 0.8100\n",
      "Epoch: 024, Runtime 0.193717, Loss 0.237216, forward nfe 11052, backward nfe 1791, Train: 0.9929, Val: 0.7860, Test: 0.8070\n",
      "Epoch: 025, Runtime 0.204275, Loss 0.216150, forward nfe 11495, backward nfe 1859, Train: 0.9929, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 026, Runtime 0.196078, Loss 0.209161, forward nfe 11933, backward nfe 1926, Train: 0.9929, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 027, Runtime 0.195906, Loss 0.210483, forward nfe 12371, backward nfe 1993, Train: 0.9929, Val: 0.7820, Test: 0.8090\n",
      "Epoch: 028, Runtime 0.190959, Loss 0.182809, forward nfe 12810, backward nfe 2060, Train: 1.0000, Val: 0.7800, Test: 0.8050\n",
      "Epoch: 029, Runtime 0.195455, Loss 0.197145, forward nfe 13246, backward nfe 2128, Train: 0.9929, Val: 0.7860, Test: 0.8100\n",
      "Epoch: 030, Runtime 0.190784, Loss 0.174933, forward nfe 13685, backward nfe 2195, Train: 1.0000, Val: 0.7820, Test: 0.8070\n",
      "best val accuracy 0.808000 with test accuracy 0.841000 at epoch 14\n",
      "*** Doing stepsize 0.1 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.272085, Loss 1.954435, forward nfe 249, backward nfe 51, Train: 0.3143, Val: 0.1720, Test: 0.1910\n",
      "Epoch: 002, Runtime 0.312903, Loss 1.748321, forward nfe 1057, backward nfe 118, Train: 0.6786, Val: 0.4300, Test: 0.4520\n",
      "Epoch: 003, Runtime 0.315120, Loss 1.546579, forward nfe 1872, backward nfe 183, Train: 0.4643, Val: 0.2940, Test: 0.2960\n",
      "Epoch: 004, Runtime 0.320521, Loss 1.411295, forward nfe 2654, backward nfe 254, Train: 0.8786, Val: 0.6040, Test: 0.6330\n",
      "Epoch: 005, Runtime 0.303185, Loss 1.221429, forward nfe 3408, backward nfe 327, Train: 0.8500, Val: 0.6440, Test: 0.6550\n",
      "Epoch: 006, Runtime 0.296184, Loss 1.009476, forward nfe 4160, backward nfe 397, Train: 0.9357, Val: 0.7760, Test: 0.7790\n",
      "Epoch: 007, Runtime 0.295271, Loss 0.873568, forward nfe 4887, backward nfe 468, Train: 0.9143, Val: 0.7280, Test: 0.7410\n",
      "Epoch: 008, Runtime 0.298403, Loss 0.762122, forward nfe 5615, backward nfe 539, Train: 0.9286, Val: 0.8040, Test: 0.8140\n",
      "Epoch: 009, Runtime 0.301735, Loss 0.663166, forward nfe 6340, backward nfe 610, Train: 0.9714, Val: 0.7640, Test: 0.7830\n",
      "Epoch: 010, Runtime 0.297740, Loss 0.580483, forward nfe 7067, backward nfe 681, Train: 0.9571, Val: 0.8000, Test: 0.8110\n",
      "Epoch: 011, Runtime 0.298776, Loss 0.522805, forward nfe 7791, backward nfe 752, Train: 0.9714, Val: 0.7880, Test: 0.8130\n",
      "Epoch: 012, Runtime 0.300253, Loss 0.493987, forward nfe 8517, backward nfe 822, Train: 0.9714, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 013, Runtime 0.300510, Loss 0.426728, forward nfe 9246, backward nfe 893, Train: 0.9857, Val: 0.7820, Test: 0.8090\n",
      "Epoch: 014, Runtime 0.309034, Loss 0.407964, forward nfe 9971, backward nfe 964, Train: 0.9714, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 015, Runtime 0.300942, Loss 0.378788, forward nfe 10696, backward nfe 1036, Train: 0.9857, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 016, Runtime 0.300021, Loss 0.342777, forward nfe 11422, backward nfe 1107, Train: 0.9786, Val: 0.7880, Test: 0.8190\n",
      "Epoch: 017, Runtime 0.307696, Loss 0.322855, forward nfe 12147, backward nfe 1178, Train: 0.9857, Val: 0.7800, Test: 0.8140\n",
      "Epoch: 018, Runtime 0.288568, Loss 0.294608, forward nfe 12873, backward nfe 1247, Train: 0.9786, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 019, Runtime 0.300668, Loss 0.283589, forward nfe 13600, backward nfe 1316, Train: 0.9857, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 020, Runtime 0.298146, Loss 0.259528, forward nfe 14332, backward nfe 1384, Train: 0.9857, Val: 0.7840, Test: 0.8140\n",
      "Epoch: 021, Runtime 0.301501, Loss 0.250684, forward nfe 15065, backward nfe 1452, Train: 0.9857, Val: 0.7880, Test: 0.8170\n",
      "Epoch: 022, Runtime 0.290635, Loss 0.231156, forward nfe 15792, backward nfe 1522, Train: 0.9857, Val: 0.7780, Test: 0.8150\n",
      "Epoch: 023, Runtime 0.300223, Loss 0.240413, forward nfe 16519, backward nfe 1590, Train: 0.9929, Val: 0.7860, Test: 0.8090\n",
      "Epoch: 024, Runtime 0.299165, Loss 0.224024, forward nfe 17245, backward nfe 1660, Train: 0.9857, Val: 0.7840, Test: 0.8110\n",
      "Epoch: 025, Runtime 0.303364, Loss 0.210372, forward nfe 17972, backward nfe 1729, Train: 0.9857, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 026, Runtime 0.300400, Loss 0.197831, forward nfe 18699, backward nfe 1797, Train: 0.9929, Val: 0.7820, Test: 0.8080\n",
      "Epoch: 027, Runtime 0.305078, Loss 0.195852, forward nfe 19426, backward nfe 1866, Train: 0.9857, Val: 0.7840, Test: 0.8070\n",
      "Epoch: 028, Runtime 0.298395, Loss 0.182913, forward nfe 20153, backward nfe 1936, Train: 0.9929, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 029, Runtime 0.307260, Loss 0.179428, forward nfe 20881, backward nfe 2006, Train: 0.9929, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 030, Runtime 0.291230, Loss 0.163146, forward nfe 21607, backward nfe 2075, Train: 0.9929, Val: 0.7840, Test: 0.8080\n",
      "best val accuracy 0.804000 with test accuracy 0.814000 at epoch 8\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.270287, Loss 1.963867, forward nfe 247, backward nfe 52, Train: 0.2500, Val: 0.2260, Test: 0.2210\n",
      "Epoch: 002, Runtime 0.303968, Loss 1.922827, forward nfe 987, backward nfe 120, Train: 0.4214, Val: 0.3180, Test: 0.3230\n",
      "Epoch: 003, Runtime 0.280988, Loss 1.671779, forward nfe 1709, backward nfe 187, Train: 0.6214, Val: 0.3900, Test: 0.4150\n",
      "Epoch: 004, Runtime 0.294806, Loss 1.499006, forward nfe 2431, backward nfe 252, Train: 0.8214, Val: 0.5460, Test: 0.5880\n",
      "Epoch: 005, Runtime 0.304327, Loss 1.290464, forward nfe 3158, backward nfe 321, Train: 0.7929, Val: 0.5780, Test: 0.6140\n",
      "Epoch: 006, Runtime 0.294334, Loss 1.164311, forward nfe 3882, backward nfe 389, Train: 0.9286, Val: 0.7480, Test: 0.7650\n",
      "Epoch: 007, Runtime 0.300928, Loss 0.934531, forward nfe 4606, backward nfe 457, Train: 0.9071, Val: 0.7660, Test: 0.8050\n",
      "Epoch: 008, Runtime 0.300647, Loss 0.834636, forward nfe 5332, backward nfe 526, Train: 0.9500, Val: 0.7500, Test: 0.7900\n",
      "Epoch: 009, Runtime 0.300618, Loss 0.738144, forward nfe 6055, backward nfe 594, Train: 0.9214, Val: 0.7680, Test: 0.8000\n",
      "Epoch: 010, Runtime 0.302141, Loss 0.675857, forward nfe 6779, backward nfe 662, Train: 0.9571, Val: 0.7680, Test: 0.7970\n",
      "Epoch: 011, Runtime 0.294873, Loss 0.582843, forward nfe 7505, backward nfe 730, Train: 0.9429, Val: 0.7680, Test: 0.7990\n",
      "Epoch: 012, Runtime 0.299252, Loss 0.550450, forward nfe 8229, backward nfe 798, Train: 0.9571, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 013, Runtime 0.289938, Loss 0.480018, forward nfe 8954, backward nfe 866, Train: 0.9357, Val: 0.7400, Test: 0.7600\n",
      "Epoch: 014, Runtime 0.303190, Loss 0.486928, forward nfe 9678, backward nfe 934, Train: 0.9643, Val: 0.8000, Test: 0.8290\n",
      "Epoch: 015, Runtime 0.299341, Loss 0.415423, forward nfe 10403, backward nfe 1002, Train: 0.9643, Val: 0.7820, Test: 0.8150\n",
      "Epoch: 016, Runtime 0.304837, Loss 0.380405, forward nfe 11126, backward nfe 1070, Train: 0.9714, Val: 0.7980, Test: 0.8320\n",
      "Epoch: 017, Runtime 0.303970, Loss 0.346623, forward nfe 11849, backward nfe 1138, Train: 0.9714, Val: 0.7900, Test: 0.8210\n",
      "Epoch: 018, Runtime 0.298178, Loss 0.333554, forward nfe 12573, backward nfe 1206, Train: 0.9786, Val: 0.7900, Test: 0.8190\n",
      "Epoch: 019, Runtime 0.302561, Loss 0.305725, forward nfe 13297, backward nfe 1274, Train: 0.9857, Val: 0.8020, Test: 0.8180\n",
      "Epoch: 020, Runtime 0.297049, Loss 0.286679, forward nfe 14021, backward nfe 1340, Train: 0.9643, Val: 0.7820, Test: 0.8150\n",
      "Epoch: 021, Runtime 0.292441, Loss 0.282451, forward nfe 14745, backward nfe 1408, Train: 0.9786, Val: 0.7960, Test: 0.8210\n",
      "Epoch: 022, Runtime 0.303652, Loss 0.246283, forward nfe 15470, backward nfe 1476, Train: 0.9929, Val: 0.7980, Test: 0.8120\n",
      "Epoch: 023, Runtime 0.287724, Loss 0.261755, forward nfe 16193, backward nfe 1543, Train: 0.9929, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 024, Runtime 0.299745, Loss 0.236838, forward nfe 16917, backward nfe 1609, Train: 0.9929, Val: 0.8000, Test: 0.8140\n",
      "Epoch: 025, Runtime 0.306846, Loss 0.227983, forward nfe 17641, backward nfe 1676, Train: 0.9929, Val: 0.7940, Test: 0.8020\n",
      "Epoch: 026, Runtime 0.287192, Loss 0.221214, forward nfe 18366, backward nfe 1742, Train: 0.9857, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 027, Runtime 0.295807, Loss 0.223316, forward nfe 19090, backward nfe 1807, Train: 0.9929, Val: 0.8000, Test: 0.8140\n",
      "Epoch: 028, Runtime 0.305429, Loss 0.195939, forward nfe 19814, backward nfe 1874, Train: 0.9929, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 029, Runtime 0.300123, Loss 0.189323, forward nfe 20540, backward nfe 1939, Train: 0.9929, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 030, Runtime 0.291696, Loss 0.190259, forward nfe 21264, backward nfe 2004, Train: 0.9857, Val: 0.7980, Test: 0.8150\n",
      "best val accuracy 0.802000 with test accuracy 0.818000 at epoch 19\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.293537, Loss 1.962759, forward nfe 333, backward nfe 52, Train: 0.4286, Val: 0.2060, Test: 0.2240\n",
      "Epoch: 002, Runtime 0.314217, Loss 1.839985, forward nfe 1112, backward nfe 117, Train: 0.5857, Val: 0.3000, Test: 0.3350\n",
      "Epoch: 003, Runtime 0.294634, Loss 1.620781, forward nfe 1880, backward nfe 184, Train: 0.5643, Val: 0.5080, Test: 0.5160\n",
      "Epoch: 004, Runtime 0.305026, Loss 1.529392, forward nfe 2606, backward nfe 252, Train: 0.6786, Val: 0.4420, Test: 0.4490\n",
      "Epoch: 005, Runtime 0.290386, Loss 1.424845, forward nfe 3339, backward nfe 321, Train: 0.9000, Val: 0.6880, Test: 0.6990\n",
      "Epoch: 006, Runtime 0.301685, Loss 1.033258, forward nfe 4061, backward nfe 389, Train: 0.9357, Val: 0.7720, Test: 0.7660\n",
      "Epoch: 007, Runtime 0.295554, Loss 0.888743, forward nfe 4787, backward nfe 458, Train: 0.9071, Val: 0.7660, Test: 0.7890\n",
      "Epoch: 008, Runtime 0.302386, Loss 0.759906, forward nfe 5513, backward nfe 527, Train: 0.9500, Val: 0.7980, Test: 0.7840\n",
      "Epoch: 009, Runtime 0.299606, Loss 0.685779, forward nfe 6236, backward nfe 596, Train: 0.9429, Val: 0.7880, Test: 0.8050\n",
      "Epoch: 010, Runtime 0.301679, Loss 0.607571, forward nfe 6959, backward nfe 665, Train: 0.9571, Val: 0.8000, Test: 0.8020\n",
      "Epoch: 011, Runtime 0.291072, Loss 0.541426, forward nfe 7685, backward nfe 733, Train: 0.9571, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 012, Runtime 0.297620, Loss 0.495583, forward nfe 8410, backward nfe 801, Train: 0.9571, Val: 0.7940, Test: 0.8010\n",
      "Epoch: 013, Runtime 0.300795, Loss 0.468277, forward nfe 9136, backward nfe 869, Train: 0.9786, Val: 0.7980, Test: 0.8090\n",
      "Epoch: 014, Runtime 0.297735, Loss 0.411441, forward nfe 9863, backward nfe 937, Train: 0.9786, Val: 0.8020, Test: 0.8100\n",
      "Epoch: 015, Runtime 0.299338, Loss 0.392699, forward nfe 10589, backward nfe 1005, Train: 0.9857, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 016, Runtime 0.300490, Loss 0.351576, forward nfe 11314, backward nfe 1073, Train: 0.9714, Val: 0.8020, Test: 0.8110\n",
      "Epoch: 017, Runtime 0.281182, Loss 0.339569, forward nfe 12040, backward nfe 1142, Train: 0.9857, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 018, Runtime 0.298464, Loss 0.318664, forward nfe 12767, backward nfe 1211, Train: 0.9857, Val: 0.7940, Test: 0.8070\n",
      "Epoch: 019, Runtime 0.299743, Loss 0.301262, forward nfe 13492, backward nfe 1278, Train: 0.9857, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 020, Runtime 0.290816, Loss 0.293819, forward nfe 14217, backward nfe 1346, Train: 0.9929, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 021, Runtime 0.298540, Loss 0.277390, forward nfe 14943, backward nfe 1413, Train: 0.9786, Val: 0.7840, Test: 0.7960\n",
      "Epoch: 022, Runtime 0.298950, Loss 0.253093, forward nfe 15667, backward nfe 1481, Train: 0.9929, Val: 0.7880, Test: 0.8000\n",
      "Epoch: 023, Runtime 0.298558, Loss 0.239589, forward nfe 16394, backward nfe 1548, Train: 0.9857, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 024, Runtime 0.302242, Loss 0.227643, forward nfe 17121, backward nfe 1616, Train: 0.9929, Val: 0.7900, Test: 0.8010\n",
      "Epoch: 025, Runtime 0.298797, Loss 0.228411, forward nfe 17846, backward nfe 1683, Train: 0.9929, Val: 0.7920, Test: 0.8030\n",
      "Epoch: 026, Runtime 0.298270, Loss 0.207118, forward nfe 18571, backward nfe 1751, Train: 0.9929, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 027, Runtime 0.299913, Loss 0.203669, forward nfe 19298, backward nfe 1819, Train: 1.0000, Val: 0.7900, Test: 0.8040\n",
      "Epoch: 028, Runtime 0.297121, Loss 0.187225, forward nfe 20022, backward nfe 1886, Train: 1.0000, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 029, Runtime 0.304039, Loss 0.186183, forward nfe 20748, backward nfe 1953, Train: 1.0000, Val: 0.7940, Test: 0.7970\n",
      "Epoch: 030, Runtime 0.284526, Loss 0.182229, forward nfe 21473, backward nfe 2020, Train: 1.0000, Val: 0.7820, Test: 0.7980\n",
      "best val accuracy 0.802000 with test accuracy 0.810000 at epoch 14\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.268435, Loss 1.952912, forward nfe 258, backward nfe 52, Train: 0.1929, Val: 0.1760, Test: 0.1610\n",
      "Epoch: 002, Runtime 0.299006, Loss 1.809672, forward nfe 996, backward nfe 117, Train: 0.5571, Val: 0.3520, Test: 0.3690\n",
      "Epoch: 003, Runtime 0.292222, Loss 1.651389, forward nfe 1721, backward nfe 184, Train: 0.7143, Val: 0.5780, Test: 0.6180\n",
      "Epoch: 004, Runtime 0.296954, Loss 1.395950, forward nfe 2446, backward nfe 251, Train: 0.8143, Val: 0.5180, Test: 0.5230\n",
      "Epoch: 005, Runtime 0.290788, Loss 1.210435, forward nfe 3170, backward nfe 320, Train: 0.8786, Val: 0.7580, Test: 0.7890\n",
      "Epoch: 006, Runtime 0.299469, Loss 1.028597, forward nfe 3896, backward nfe 389, Train: 0.9143, Val: 0.7320, Test: 0.7170\n",
      "Epoch: 007, Runtime 0.299063, Loss 0.866904, forward nfe 4622, backward nfe 458, Train: 0.9357, Val: 0.7860, Test: 0.8110\n",
      "Epoch: 008, Runtime 0.309829, Loss 0.734197, forward nfe 5349, backward nfe 528, Train: 0.9571, Val: 0.8060, Test: 0.8070\n",
      "Epoch: 009, Runtime 0.295609, Loss 0.661269, forward nfe 6075, backward nfe 597, Train: 0.9429, Val: 0.8040, Test: 0.8200\n",
      "Epoch: 010, Runtime 0.302598, Loss 0.578780, forward nfe 6800, backward nfe 665, Train: 0.9643, Val: 0.8160, Test: 0.8110\n",
      "Epoch: 011, Runtime 0.297482, Loss 0.513841, forward nfe 7524, backward nfe 733, Train: 0.9643, Val: 0.8020, Test: 0.8140\n",
      "Epoch: 012, Runtime 0.303131, Loss 0.470441, forward nfe 8249, backward nfe 801, Train: 0.9714, Val: 0.8140, Test: 0.8170\n",
      "Epoch: 013, Runtime 0.291156, Loss 0.441809, forward nfe 8973, backward nfe 869, Train: 0.9786, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 014, Runtime 0.298467, Loss 0.389694, forward nfe 9699, backward nfe 937, Train: 0.9714, Val: 0.8060, Test: 0.8110\n",
      "Epoch: 015, Runtime 0.295954, Loss 0.371636, forward nfe 10424, backward nfe 1005, Train: 0.9714, Val: 0.8040, Test: 0.8050\n",
      "Epoch: 016, Runtime 0.300339, Loss 0.331078, forward nfe 11150, backward nfe 1073, Train: 0.9714, Val: 0.8080, Test: 0.8130\n",
      "Epoch: 017, Runtime 0.300617, Loss 0.312606, forward nfe 11876, backward nfe 1141, Train: 0.9786, Val: 0.8140, Test: 0.8220\n",
      "Epoch: 018, Runtime 0.300572, Loss 0.305042, forward nfe 12600, backward nfe 1209, Train: 0.9857, Val: 0.7980, Test: 0.8050\n",
      "Epoch: 019, Runtime 0.299981, Loss 0.274242, forward nfe 13323, backward nfe 1277, Train: 0.9714, Val: 0.8040, Test: 0.8120\n",
      "Epoch: 020, Runtime 0.300105, Loss 0.256551, forward nfe 14048, backward nfe 1345, Train: 0.9929, Val: 0.8080, Test: 0.8090\n",
      "Epoch: 021, Runtime 0.289687, Loss 0.251792, forward nfe 14772, backward nfe 1412, Train: 0.9643, Val: 0.7680, Test: 0.7780\n",
      "Epoch: 022, Runtime 0.291929, Loss 0.270445, forward nfe 15498, backward nfe 1480, Train: 0.9929, Val: 0.7900, Test: 0.8130\n",
      "Epoch: 023, Runtime 0.298915, Loss 0.216005, forward nfe 16222, backward nfe 1547, Train: 0.9857, Val: 0.8060, Test: 0.8090\n",
      "Epoch: 024, Runtime 0.300050, Loss 0.208113, forward nfe 16947, backward nfe 1614, Train: 0.9929, Val: 0.8000, Test: 0.8170\n",
      "Epoch: 025, Runtime 0.298879, Loss 0.206357, forward nfe 17671, backward nfe 1681, Train: 0.9929, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 026, Runtime 0.290804, Loss 0.187303, forward nfe 18395, backward nfe 1748, Train: 0.9929, Val: 0.7920, Test: 0.8010\n",
      "Epoch: 027, Runtime 0.294535, Loss 0.186635, forward nfe 19119, backward nfe 1814, Train: 0.9929, Val: 0.8020, Test: 0.8070\n",
      "Epoch: 028, Runtime 0.296803, Loss 0.157209, forward nfe 19843, backward nfe 1881, Train: 0.9929, Val: 0.7940, Test: 0.8000\n",
      "Epoch: 029, Runtime 0.300426, Loss 0.157250, forward nfe 20567, backward nfe 1948, Train: 0.9929, Val: 0.8060, Test: 0.8070\n",
      "Epoch: 030, Runtime 0.305229, Loss 0.156506, forward nfe 21292, backward nfe 2014, Train: 1.0000, Val: 0.7980, Test: 0.8050\n",
      "best val accuracy 0.816000 with test accuracy 0.811000 at epoch 10\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.261308, Loss 1.949944, forward nfe 247, backward nfe 54, Train: 0.4714, Val: 0.3480, Test: 0.3680\n",
      "Epoch: 002, Runtime 0.307542, Loss 1.779227, forward nfe 1002, backward nfe 119, Train: 0.7857, Val: 0.5540, Test: 0.5750\n",
      "Epoch: 003, Runtime 0.298541, Loss 1.509843, forward nfe 1730, backward nfe 186, Train: 0.7214, Val: 0.6400, Test: 0.6760\n",
      "Epoch: 004, Runtime 0.286127, Loss 1.355043, forward nfe 2458, backward nfe 257, Train: 0.6857, Val: 0.4120, Test: 0.4330\n",
      "Epoch: 005, Runtime 0.302300, Loss 1.233241, forward nfe 3183, backward nfe 325, Train: 0.8714, Val: 0.7460, Test: 0.7510\n",
      "Epoch: 006, Runtime 0.299660, Loss 0.972984, forward nfe 3907, backward nfe 393, Train: 0.9214, Val: 0.7180, Test: 0.7360\n",
      "Epoch: 007, Runtime 0.302066, Loss 0.817783, forward nfe 4630, backward nfe 462, Train: 0.9357, Val: 0.7980, Test: 0.8010\n",
      "Epoch: 008, Runtime 0.295204, Loss 0.702135, forward nfe 5355, backward nfe 530, Train: 0.9571, Val: 0.8000, Test: 0.8110\n",
      "Epoch: 009, Runtime 0.294383, Loss 0.617183, forward nfe 6082, backward nfe 599, Train: 0.9571, Val: 0.8080, Test: 0.8220\n",
      "Epoch: 010, Runtime 0.303473, Loss 0.564586, forward nfe 6807, backward nfe 668, Train: 0.9714, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 011, Runtime 0.294311, Loss 0.529834, forward nfe 7533, backward nfe 737, Train: 0.9643, Val: 0.8020, Test: 0.8130\n",
      "Epoch: 012, Runtime 0.298022, Loss 0.485335, forward nfe 8259, backward nfe 805, Train: 0.9643, Val: 0.8040, Test: 0.8080\n",
      "Epoch: 013, Runtime 0.302673, Loss 0.436310, forward nfe 8985, backward nfe 873, Train: 0.9643, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 014, Runtime 0.298356, Loss 0.409363, forward nfe 9709, backward nfe 941, Train: 0.9714, Val: 0.8100, Test: 0.8210\n",
      "Epoch: 015, Runtime 0.308306, Loss 0.380265, forward nfe 10433, backward nfe 1009, Train: 0.9714, Val: 0.8060, Test: 0.8120\n",
      "Epoch: 016, Runtime 0.298959, Loss 0.348907, forward nfe 11160, backward nfe 1077, Train: 0.9786, Val: 0.8040, Test: 0.8120\n",
      "Epoch: 017, Runtime 0.291970, Loss 0.328276, forward nfe 11885, backward nfe 1145, Train: 0.9714, Val: 0.8000, Test: 0.8070\n",
      "Epoch: 018, Runtime 0.298709, Loss 0.329072, forward nfe 12610, backward nfe 1213, Train: 0.9786, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 019, Runtime 0.287956, Loss 0.293623, forward nfe 13335, backward nfe 1281, Train: 0.9857, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 020, Runtime 0.299552, Loss 0.286772, forward nfe 14059, backward nfe 1348, Train: 0.9929, Val: 0.7840, Test: 0.8010\n",
      "Epoch: 021, Runtime 0.293994, Loss 0.270937, forward nfe 14783, backward nfe 1416, Train: 0.9929, Val: 0.7880, Test: 0.8020\n",
      "Epoch: 022, Runtime 0.307387, Loss 0.261266, forward nfe 15509, backward nfe 1483, Train: 0.9857, Val: 0.8000, Test: 0.8140\n",
      "Epoch: 023, Runtime 0.299556, Loss 0.255433, forward nfe 16235, backward nfe 1550, Train: 0.9929, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 024, Runtime 0.287300, Loss 0.245662, forward nfe 16958, backward nfe 1617, Train: 0.9929, Val: 0.7880, Test: 0.8050\n",
      "Epoch: 025, Runtime 0.298327, Loss 0.226716, forward nfe 17682, backward nfe 1684, Train: 1.0000, Val: 0.7980, Test: 0.8050\n",
      "Epoch: 026, Runtime 0.298620, Loss 0.210620, forward nfe 18406, backward nfe 1751, Train: 1.0000, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 027, Runtime 0.287743, Loss 0.196037, forward nfe 19130, backward nfe 1818, Train: 1.0000, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 028, Runtime 0.296981, Loss 0.196793, forward nfe 19854, backward nfe 1885, Train: 1.0000, Val: 0.7980, Test: 0.8100\n",
      "Epoch: 029, Runtime 0.299075, Loss 0.198419, forward nfe 20579, backward nfe 1952, Train: 1.0000, Val: 0.7940, Test: 0.8040\n",
      "Epoch: 030, Runtime 0.298337, Loss 0.185151, forward nfe 21304, backward nfe 2019, Train: 1.0000, Val: 0.7860, Test: 0.7950\n",
      "best val accuracy 0.810000 with test accuracy 0.817000 at epoch 13\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.259698, Loss 1.949832, forward nfe 249, backward nfe 51, Train: 0.3929, Val: 0.3100, Test: 0.2960\n",
      "Epoch: 002, Runtime 0.311681, Loss 1.824773, forward nfe 990, backward nfe 117, Train: 0.5571, Val: 0.4980, Test: 0.5350\n",
      "Epoch: 003, Runtime 0.308056, Loss 1.599327, forward nfe 1753, backward nfe 184, Train: 0.6786, Val: 0.4420, Test: 0.4390\n",
      "Epoch: 004, Runtime 0.296262, Loss 1.395678, forward nfe 2476, backward nfe 253, Train: 0.8857, Val: 0.7320, Test: 0.7540\n",
      "Epoch: 005, Runtime 0.287881, Loss 1.175627, forward nfe 3201, backward nfe 321, Train: 0.8143, Val: 0.7120, Test: 0.7230\n",
      "Epoch: 006, Runtime 0.305750, Loss 1.010076, forward nfe 3927, backward nfe 390, Train: 0.8357, Val: 0.5820, Test: 0.6050\n",
      "Epoch: 007, Runtime 0.294276, Loss 0.995819, forward nfe 4653, backward nfe 460, Train: 0.8929, Val: 0.7460, Test: 0.8070\n",
      "Epoch: 008, Runtime 0.297585, Loss 0.788021, forward nfe 5378, backward nfe 528, Train: 0.9286, Val: 0.7420, Test: 0.7320\n",
      "Epoch: 009, Runtime 0.299495, Loss 0.682774, forward nfe 6103, backward nfe 598, Train: 0.9643, Val: 0.8000, Test: 0.8230\n",
      "Epoch: 010, Runtime 0.292003, Loss 0.598825, forward nfe 6827, backward nfe 667, Train: 0.9429, Val: 0.8100, Test: 0.8050\n",
      "Epoch: 011, Runtime 0.306493, Loss 0.532438, forward nfe 7552, backward nfe 736, Train: 0.9571, Val: 0.8040, Test: 0.8230\n",
      "Epoch: 012, Runtime 0.304578, Loss 0.499456, forward nfe 8278, backward nfe 804, Train: 0.9571, Val: 0.7960, Test: 0.8020\n",
      "Epoch: 013, Runtime 0.295527, Loss 0.451499, forward nfe 9002, backward nfe 873, Train: 0.9571, Val: 0.8020, Test: 0.8110\n",
      "Epoch: 014, Runtime 0.298438, Loss 0.415510, forward nfe 9728, backward nfe 941, Train: 0.9571, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 015, Runtime 0.303840, Loss 0.415448, forward nfe 10453, backward nfe 1009, Train: 0.9786, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 016, Runtime 0.296731, Loss 0.370928, forward nfe 11181, backward nfe 1077, Train: 0.9786, Val: 0.7940, Test: 0.8070\n",
      "Epoch: 017, Runtime 0.300894, Loss 0.341150, forward nfe 11906, backward nfe 1145, Train: 0.9786, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 018, Runtime 0.300199, Loss 0.332178, forward nfe 12630, backward nfe 1214, Train: 0.9786, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 019, Runtime 0.299033, Loss 0.297681, forward nfe 13357, backward nfe 1281, Train: 0.9786, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 020, Runtime 0.298011, Loss 0.282210, forward nfe 14080, backward nfe 1349, Train: 0.9786, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 021, Runtime 0.292894, Loss 0.292998, forward nfe 14805, backward nfe 1417, Train: 0.9857, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 022, Runtime 0.307025, Loss 0.266437, forward nfe 15530, backward nfe 1484, Train: 0.9929, Val: 0.8000, Test: 0.8180\n",
      "Epoch: 023, Runtime 0.299014, Loss 0.245034, forward nfe 16254, backward nfe 1551, Train: 0.9857, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 024, Runtime 0.291516, Loss 0.226978, forward nfe 16979, backward nfe 1619, Train: 0.9929, Val: 0.8040, Test: 0.8180\n",
      "Epoch: 025, Runtime 0.304990, Loss 0.223233, forward nfe 17704, backward nfe 1686, Train: 0.9929, Val: 0.8000, Test: 0.8040\n",
      "Epoch: 026, Runtime 0.296220, Loss 0.212215, forward nfe 18428, backward nfe 1753, Train: 0.9929, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 027, Runtime 0.286851, Loss 0.209056, forward nfe 19155, backward nfe 1820, Train: 0.9929, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 028, Runtime 0.298786, Loss 0.195576, forward nfe 19880, backward nfe 1887, Train: 1.0000, Val: 0.7920, Test: 0.8110\n",
      "Epoch: 029, Runtime 0.308142, Loss 0.192929, forward nfe 20605, backward nfe 1954, Train: 0.9929, Val: 0.7980, Test: 0.8110\n",
      "Epoch: 030, Runtime 0.302294, Loss 0.188369, forward nfe 21331, backward nfe 2021, Train: 0.9929, Val: 0.7960, Test: 0.8040\n",
      "best val accuracy 0.810000 with test accuracy 0.805000 at epoch 10\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.257553, Loss 1.947823, forward nfe 247, backward nfe 52, Train: 0.6071, Val: 0.5200, Test: 0.5320\n",
      "Epoch: 002, Runtime 0.303790, Loss 1.783077, forward nfe 978, backward nfe 117, Train: 0.7214, Val: 0.6600, Test: 0.6650\n",
      "Epoch: 003, Runtime 0.297359, Loss 1.515677, forward nfe 1700, backward nfe 184, Train: 0.8000, Val: 0.5640, Test: 0.5620\n",
      "Epoch: 004, Runtime 0.300233, Loss 1.260813, forward nfe 2424, backward nfe 253, Train: 0.8357, Val: 0.7280, Test: 0.7580\n",
      "Epoch: 005, Runtime 0.301201, Loss 1.044335, forward nfe 3150, backward nfe 323, Train: 0.7643, Val: 0.5040, Test: 0.5090\n",
      "Epoch: 006, Runtime 0.295899, Loss 0.973413, forward nfe 3884, backward nfe 394, Train: 0.8214, Val: 0.6420, Test: 0.6630\n",
      "Epoch: 007, Runtime 0.305271, Loss 0.941403, forward nfe 4609, backward nfe 462, Train: 0.9071, Val: 0.6560, Test: 0.6790\n",
      "Epoch: 008, Runtime 0.303563, Loss 0.751417, forward nfe 5336, backward nfe 532, Train: 0.9143, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 009, Runtime 0.305510, Loss 0.641507, forward nfe 6062, backward nfe 601, Train: 0.9357, Val: 0.7960, Test: 0.8110\n",
      "Epoch: 010, Runtime 0.296888, Loss 0.591195, forward nfe 6788, backward nfe 669, Train: 0.9643, Val: 0.7600, Test: 0.7700\n",
      "Epoch: 011, Runtime 0.295896, Loss 0.544773, forward nfe 7514, backward nfe 738, Train: 0.9643, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 012, Runtime 0.305541, Loss 0.471640, forward nfe 8240, backward nfe 806, Train: 0.9643, Val: 0.7640, Test: 0.7790\n",
      "Epoch: 013, Runtime 0.291499, Loss 0.446382, forward nfe 8964, backward nfe 874, Train: 0.9786, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 014, Runtime 0.300824, Loss 0.413020, forward nfe 9689, backward nfe 943, Train: 0.9786, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 015, Runtime 0.299088, Loss 0.398499, forward nfe 10412, backward nfe 1011, Train: 0.9786, Val: 0.7980, Test: 0.8180\n",
      "Epoch: 016, Runtime 0.297077, Loss 0.363961, forward nfe 11138, backward nfe 1079, Train: 0.9857, Val: 0.8000, Test: 0.8050\n",
      "Epoch: 017, Runtime 0.301187, Loss 0.331112, forward nfe 11862, backward nfe 1147, Train: 0.9786, Val: 0.8000, Test: 0.8140\n",
      "Epoch: 018, Runtime 0.292007, Loss 0.306951, forward nfe 12586, backward nfe 1215, Train: 0.9786, Val: 0.8060, Test: 0.8120\n",
      "Epoch: 019, Runtime 0.299386, Loss 0.304438, forward nfe 13311, backward nfe 1283, Train: 0.9857, Val: 0.8000, Test: 0.8080\n",
      "Epoch: 020, Runtime 0.298200, Loss 0.272582, forward nfe 14036, backward nfe 1351, Train: 0.9786, Val: 0.8000, Test: 0.8130\n",
      "Epoch: 021, Runtime 0.290465, Loss 0.264985, forward nfe 14762, backward nfe 1419, Train: 0.9929, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 022, Runtime 0.298869, Loss 0.244649, forward nfe 15486, backward nfe 1486, Train: 0.9929, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 023, Runtime 0.296310, Loss 0.244991, forward nfe 16210, backward nfe 1553, Train: 0.9929, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 024, Runtime 0.299975, Loss 0.229366, forward nfe 16934, backward nfe 1620, Train: 0.9929, Val: 0.7940, Test: 0.8110\n",
      "Epoch: 025, Runtime 0.299656, Loss 0.220385, forward nfe 17660, backward nfe 1687, Train: 0.9929, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 026, Runtime 0.303190, Loss 0.202440, forward nfe 18382, backward nfe 1754, Train: 0.9929, Val: 0.7940, Test: 0.8100\n",
      "Epoch: 027, Runtime 0.298194, Loss 0.201217, forward nfe 19106, backward nfe 1821, Train: 0.9929, Val: 0.7920, Test: 0.8070\n",
      "Epoch: 028, Runtime 0.307127, Loss 0.196344, forward nfe 19830, backward nfe 1888, Train: 1.0000, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 029, Runtime 0.300250, Loss 0.211437, forward nfe 20556, backward nfe 1955, Train: 0.9929, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 030, Runtime 0.292999, Loss 0.182365, forward nfe 21281, backward nfe 2022, Train: 0.9929, Val: 0.7880, Test: 0.8020\n",
      "best val accuracy 0.806000 with test accuracy 0.812000 at epoch 18\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.281287, Loss 1.954805, forward nfe 294, backward nfe 52, Train: 0.3643, Val: 0.2500, Test: 0.2740\n",
      "Epoch: 002, Runtime 0.316210, Loss 1.811296, forward nfe 1092, backward nfe 118, Train: 0.5429, Val: 0.3640, Test: 0.3660\n",
      "Epoch: 003, Runtime 0.314255, Loss 1.664636, forward nfe 1910, backward nfe 189, Train: 0.5286, Val: 0.3380, Test: 0.3490\n",
      "Epoch: 004, Runtime 0.315038, Loss 1.465246, forward nfe 2678, backward nfe 258, Train: 0.8429, Val: 0.6340, Test: 0.6580\n",
      "Epoch: 005, Runtime 0.307246, Loss 1.274941, forward nfe 3453, backward nfe 328, Train: 0.8643, Val: 0.6540, Test: 0.6680\n",
      "Epoch: 006, Runtime 0.307889, Loss 1.044739, forward nfe 4230, backward nfe 398, Train: 0.9286, Val: 0.7600, Test: 0.7810\n",
      "Epoch: 007, Runtime 0.311750, Loss 0.899537, forward nfe 4960, backward nfe 469, Train: 0.9214, Val: 0.7340, Test: 0.7400\n",
      "Epoch: 008, Runtime 0.304955, Loss 0.788875, forward nfe 5711, backward nfe 541, Train: 0.9357, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 009, Runtime 0.310678, Loss 0.697801, forward nfe 6439, backward nfe 612, Train: 0.9500, Val: 0.7720, Test: 0.7850\n",
      "Epoch: 010, Runtime 0.299814, Loss 0.631835, forward nfe 7167, backward nfe 682, Train: 0.9571, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 011, Runtime 0.299704, Loss 0.582814, forward nfe 7893, backward nfe 753, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
      "Epoch: 012, Runtime 0.299751, Loss 0.507074, forward nfe 8618, backward nfe 824, Train: 0.9714, Val: 0.8100, Test: 0.8130\n",
      "Epoch: 013, Runtime 0.308995, Loss 0.456801, forward nfe 9344, backward nfe 896, Train: 0.9786, Val: 0.8040, Test: 0.8090\n",
      "Epoch: 014, Runtime 0.289493, Loss 0.432719, forward nfe 10069, backward nfe 967, Train: 0.9714, Val: 0.8120, Test: 0.8140\n",
      "Epoch: 015, Runtime 0.300083, Loss 0.400202, forward nfe 10798, backward nfe 1038, Train: 0.9786, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 016, Runtime 0.304565, Loss 0.363354, forward nfe 11523, backward nfe 1107, Train: 0.9714, Val: 0.8080, Test: 0.8140\n",
      "Epoch: 017, Runtime 0.300268, Loss 0.354960, forward nfe 12250, backward nfe 1176, Train: 0.9857, Val: 0.8100, Test: 0.8070\n",
      "Epoch: 018, Runtime 0.304547, Loss 0.323896, forward nfe 12981, backward nfe 1245, Train: 0.9643, Val: 0.8060, Test: 0.8130\n",
      "Epoch: 019, Runtime 0.293872, Loss 0.301962, forward nfe 13708, backward nfe 1315, Train: 0.9857, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 020, Runtime 0.306713, Loss 0.292460, forward nfe 14435, backward nfe 1384, Train: 0.9714, Val: 0.7960, Test: 0.8090\n",
      "Epoch: 021, Runtime 0.297756, Loss 0.283823, forward nfe 15161, backward nfe 1452, Train: 0.9857, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 022, Runtime 0.304069, Loss 0.244743, forward nfe 15887, backward nfe 1521, Train: 0.9929, Val: 0.8040, Test: 0.8090\n",
      "Epoch: 023, Runtime 0.297221, Loss 0.241297, forward nfe 16614, backward nfe 1590, Train: 0.9929, Val: 0.8020, Test: 0.8070\n",
      "Epoch: 024, Runtime 0.299365, Loss 0.219497, forward nfe 17342, backward nfe 1658, Train: 0.9929, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 025, Runtime 0.307689, Loss 0.212803, forward nfe 18068, backward nfe 1726, Train: 0.9929, Val: 0.7980, Test: 0.8030\n",
      "Epoch: 026, Runtime 0.284293, Loss 0.194446, forward nfe 18793, backward nfe 1795, Train: 0.9929, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 027, Runtime 0.297807, Loss 0.207600, forward nfe 19519, backward nfe 1862, Train: 0.9929, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 028, Runtime 0.308040, Loss 0.176799, forward nfe 20246, backward nfe 1931, Train: 0.9929, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 029, Runtime 0.285506, Loss 0.185696, forward nfe 20974, backward nfe 1998, Train: 0.9929, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 030, Runtime 0.303812, Loss 0.168608, forward nfe 21699, backward nfe 2065, Train: 0.9929, Val: 0.7940, Test: 0.8010\n",
      "best val accuracy 0.812000 with test accuracy 0.814000 at epoch 14\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.273215, Loss 1.956179, forward nfe 249, backward nfe 51, Train: 0.2500, Val: 0.1560, Test: 0.1570\n",
      "Epoch: 002, Runtime 0.317344, Loss 1.864709, forward nfe 1065, backward nfe 120, Train: 0.3357, Val: 0.2320, Test: 0.2100\n",
      "Epoch: 003, Runtime 0.313744, Loss 1.662664, forward nfe 1879, backward nfe 188, Train: 0.6000, Val: 0.4500, Test: 0.4820\n",
      "Epoch: 004, Runtime 0.302239, Loss 1.500684, forward nfe 2656, backward nfe 259, Train: 0.6500, Val: 0.5860, Test: 0.6060\n",
      "Epoch: 005, Runtime 0.295757, Loss 1.384530, forward nfe 3428, backward nfe 332, Train: 0.8143, Val: 0.5680, Test: 0.5830\n",
      "Epoch: 006, Runtime 0.301905, Loss 1.157161, forward nfe 4168, backward nfe 404, Train: 0.8929, Val: 0.7340, Test: 0.7650\n",
      "Epoch: 007, Runtime 0.295607, Loss 0.966187, forward nfe 4905, backward nfe 475, Train: 0.9214, Val: 0.7760, Test: 0.7880\n",
      "Epoch: 008, Runtime 0.306021, Loss 0.835207, forward nfe 5666, backward nfe 547, Train: 0.9500, Val: 0.7620, Test: 0.7770\n",
      "Epoch: 009, Runtime 0.303086, Loss 0.775925, forward nfe 6394, backward nfe 619, Train: 0.9429, Val: 0.7800, Test: 0.8020\n",
      "Epoch: 010, Runtime 0.303249, Loss 0.672817, forward nfe 7133, backward nfe 693, Train: 0.9500, Val: 0.7740, Test: 0.7970\n",
      "Epoch: 011, Runtime 0.298702, Loss 0.629041, forward nfe 7859, backward nfe 764, Train: 0.9643, Val: 0.7860, Test: 0.8020\n",
      "Epoch: 012, Runtime 0.288449, Loss 0.543229, forward nfe 8585, backward nfe 834, Train: 0.9500, Val: 0.8040, Test: 0.8120\n",
      "Epoch: 013, Runtime 0.301655, Loss 0.512120, forward nfe 9316, backward nfe 906, Train: 0.9714, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 014, Runtime 0.302215, Loss 0.450558, forward nfe 10041, backward nfe 978, Train: 0.9571, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 015, Runtime 0.298134, Loss 0.437140, forward nfe 10786, backward nfe 1048, Train: 0.9571, Val: 0.7880, Test: 0.7980\n",
      "Epoch: 016, Runtime 0.296305, Loss 0.382104, forward nfe 11539, backward nfe 1121, Train: 0.9714, Val: 0.8060, Test: 0.8190\n",
      "Epoch: 017, Runtime 0.302462, Loss 0.357820, forward nfe 12266, backward nfe 1189, Train: 0.9786, Val: 0.8020, Test: 0.8140\n",
      "Epoch: 018, Runtime 0.299107, Loss 0.326277, forward nfe 12993, backward nfe 1257, Train: 0.9857, Val: 0.8020, Test: 0.8140\n",
      "Epoch: 019, Runtime 0.293836, Loss 0.300373, forward nfe 13719, backward nfe 1327, Train: 0.9857, Val: 0.8040, Test: 0.8080\n",
      "Epoch: 020, Runtime 0.288305, Loss 0.281560, forward nfe 14446, backward nfe 1398, Train: 0.9786, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 021, Runtime 0.305315, Loss 0.268331, forward nfe 15173, backward nfe 1468, Train: 0.9929, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 022, Runtime 0.301900, Loss 0.255370, forward nfe 15900, backward nfe 1537, Train: 0.9786, Val: 0.7840, Test: 0.8040\n",
      "Epoch: 023, Runtime 0.309414, Loss 0.244544, forward nfe 16626, backward nfe 1606, Train: 0.9929, Val: 0.8040, Test: 0.8100\n",
      "Epoch: 024, Runtime 0.286642, Loss 0.236527, forward nfe 17357, backward nfe 1676, Train: 0.9857, Val: 0.7960, Test: 0.8140\n",
      "Epoch: 025, Runtime 0.284760, Loss 0.227260, forward nfe 18082, backward nfe 1745, Train: 0.9929, Val: 0.8060, Test: 0.8120\n",
      "Epoch: 026, Runtime 0.299283, Loss 0.210605, forward nfe 18808, backward nfe 1813, Train: 0.9929, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 027, Runtime 0.307791, Loss 0.215909, forward nfe 19534, backward nfe 1882, Train: 0.9929, Val: 0.8040, Test: 0.8140\n",
      "Epoch: 028, Runtime 0.280579, Loss 0.195628, forward nfe 20260, backward nfe 1950, Train: 0.9929, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 029, Runtime 0.306323, Loss 0.184180, forward nfe 20985, backward nfe 2017, Train: 0.9929, Val: 0.7880, Test: 0.8040\n",
      "Epoch: 030, Runtime 0.284688, Loss 0.166003, forward nfe 21711, backward nfe 2085, Train: 0.9929, Val: 0.7920, Test: 0.8040\n",
      "best val accuracy 0.806000 with test accuracy 0.819000 at epoch 16\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.271191, Loss 1.960083, forward nfe 247, backward nfe 53, Train: 0.3429, Val: 0.2180, Test: 0.2240\n",
      "Epoch: 002, Runtime 0.327868, Loss 1.805255, forward nfe 1082, backward nfe 122, Train: 0.8214, Val: 0.6120, Test: 0.6150\n",
      "Epoch: 003, Runtime 0.303121, Loss 1.534876, forward nfe 1908, backward nfe 191, Train: 0.7500, Val: 0.5120, Test: 0.5100\n",
      "Epoch: 004, Runtime 0.305401, Loss 1.312876, forward nfe 2709, backward nfe 265, Train: 0.7357, Val: 0.4220, Test: 0.4600\n",
      "Epoch: 005, Runtime 0.312038, Loss 1.251762, forward nfe 3458, backward nfe 336, Train: 0.7786, Val: 0.6620, Test: 0.6900\n",
      "Epoch: 006, Runtime 0.300389, Loss 1.014293, forward nfe 4223, backward nfe 410, Train: 0.8571, Val: 0.5740, Test: 0.5810\n",
      "Epoch: 007, Runtime 0.303718, Loss 0.941059, forward nfe 4973, backward nfe 480, Train: 0.9143, Val: 0.7460, Test: 0.7500\n",
      "Epoch: 008, Runtime 0.297973, Loss 0.786788, forward nfe 5709, backward nfe 552, Train: 0.9286, Val: 0.7700, Test: 0.7640\n",
      "Epoch: 009, Runtime 0.299488, Loss 0.700099, forward nfe 6455, backward nfe 623, Train: 0.9429, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 010, Runtime 0.308364, Loss 0.629011, forward nfe 7183, backward nfe 695, Train: 0.9571, Val: 0.7880, Test: 0.7850\n",
      "Epoch: 011, Runtime 0.292320, Loss 0.566487, forward nfe 7910, backward nfe 766, Train: 0.9571, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 012, Runtime 0.294065, Loss 0.535405, forward nfe 8638, backward nfe 837, Train: 0.9643, Val: 0.7920, Test: 0.7890\n",
      "Epoch: 013, Runtime 0.302569, Loss 0.494537, forward nfe 9365, backward nfe 908, Train: 0.9714, Val: 0.7940, Test: 0.8030\n",
      "Epoch: 014, Runtime 0.307511, Loss 0.456739, forward nfe 10092, backward nfe 979, Train: 0.9643, Val: 0.7820, Test: 0.8000\n",
      "Epoch: 015, Runtime 0.300736, Loss 0.429886, forward nfe 10820, backward nfe 1050, Train: 0.9714, Val: 0.7880, Test: 0.8000\n",
      "Epoch: 016, Runtime 0.308442, Loss 0.404090, forward nfe 11546, backward nfe 1120, Train: 0.9714, Val: 0.7780, Test: 0.7850\n",
      "Epoch: 017, Runtime 0.301089, Loss 0.380313, forward nfe 12273, backward nfe 1190, Train: 0.9786, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 018, Runtime 0.302953, Loss 0.361360, forward nfe 13000, backward nfe 1260, Train: 0.9714, Val: 0.7920, Test: 0.8030\n",
      "Epoch: 019, Runtime 0.294411, Loss 0.342343, forward nfe 13727, backward nfe 1330, Train: 0.9786, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 020, Runtime 0.337138, Loss 0.314712, forward nfe 14455, backward nfe 1399, Train: 0.9786, Val: 0.7840, Test: 0.7920\n",
      "Epoch: 021, Runtime 0.306481, Loss 0.291684, forward nfe 15181, backward nfe 1467, Train: 0.9786, Val: 0.7880, Test: 0.7950\n",
      "Epoch: 022, Runtime 0.292776, Loss 0.286871, forward nfe 15908, backward nfe 1536, Train: 0.9786, Val: 0.7980, Test: 0.8100\n",
      "Epoch: 023, Runtime 0.309444, Loss 0.283156, forward nfe 16635, backward nfe 1606, Train: 0.9786, Val: 0.7800, Test: 0.7980\n",
      "Epoch: 024, Runtime 0.298131, Loss 0.254123, forward nfe 17362, backward nfe 1674, Train: 0.9786, Val: 0.7880, Test: 0.7990\n",
      "Epoch: 025, Runtime 0.301446, Loss 0.247836, forward nfe 18087, backward nfe 1743, Train: 0.9786, Val: 0.7840, Test: 0.7950\n",
      "Epoch: 026, Runtime 0.308623, Loss 0.229168, forward nfe 18814, backward nfe 1811, Train: 0.9786, Val: 0.7900, Test: 0.8010\n",
      "Epoch: 027, Runtime 0.296230, Loss 0.219010, forward nfe 19542, backward nfe 1881, Train: 0.9786, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 028, Runtime 0.302906, Loss 0.206705, forward nfe 20269, backward nfe 1948, Train: 0.9929, Val: 0.7940, Test: 0.7990\n",
      "Epoch: 029, Runtime 0.301862, Loss 0.202483, forward nfe 20996, backward nfe 2017, Train: 0.9857, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 030, Runtime 0.308203, Loss 0.203800, forward nfe 21724, backward nfe 2084, Train: 0.9929, Val: 0.7920, Test: 0.7990\n",
      "best val accuracy 0.798000 with test accuracy 0.810000 at epoch 22\n",
      "*** Doing stepsize 0.01 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.811815, Loss 1.957836, forward nfe 2424, backward nfe 50, Train: 0.3286, Val: 0.2140, Test: 0.2490\n",
      "Epoch: 002, Runtime 1.923830, Loss 1.774928, forward nfe 7704, backward nfe 119, Train: 0.4000, Val: 0.4400, Test: 0.4300\n",
      "Epoch: 003, Runtime 1.966795, Loss 1.700205, forward nfe 13307, backward nfe 192, Train: 0.5714, Val: 0.3460, Test: 0.3840\n",
      "Epoch: 004, Runtime 2.004093, Loss 1.482824, forward nfe 19131, backward nfe 261, Train: 0.8714, Val: 0.7360, Test: 0.7380\n",
      "Epoch: 005, Runtime 2.039683, Loss 1.280801, forward nfe 25156, backward nfe 334, Train: 0.8643, Val: 0.6800, Test: 0.6880\n",
      "Epoch: 006, Runtime 2.082646, Loss 1.114554, forward nfe 31105, backward nfe 404, Train: 0.8929, Val: 0.7460, Test: 0.7500\n",
      "Epoch: 007, Runtime 2.140786, Loss 0.994486, forward nfe 37662, backward nfe 475, Train: 0.8929, Val: 0.7120, Test: 0.7280\n",
      "Epoch: 008, Runtime 2.127788, Loss 0.861859, forward nfe 44083, backward nfe 544, Train: 0.9357, Val: 0.7840, Test: 0.8100\n",
      "Epoch: 009, Runtime 2.147101, Loss 0.746956, forward nfe 50653, backward nfe 613, Train: 0.9571, Val: 0.7860, Test: 0.8020\n",
      "Epoch: 010, Runtime 2.178432, Loss 0.654513, forward nfe 57260, backward nfe 684, Train: 0.9429, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 011, Runtime 2.165737, Loss 0.585109, forward nfe 63939, backward nfe 754, Train: 0.9643, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 012, Runtime 2.180076, Loss 0.565812, forward nfe 70552, backward nfe 825, Train: 0.9643, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 013, Runtime 2.198064, Loss 0.499401, forward nfe 77307, backward nfe 896, Train: 0.9643, Val: 0.8020, Test: 0.8100\n",
      "Epoch: 014, Runtime 2.210825, Loss 0.469433, forward nfe 84115, backward nfe 967, Train: 0.9643, Val: 0.7940, Test: 0.8050\n",
      "Epoch: 015, Runtime 2.207474, Loss 0.436625, forward nfe 90949, backward nfe 1037, Train: 0.9643, Val: 0.7980, Test: 0.8090\n",
      "Epoch: 016, Runtime 2.214110, Loss 0.398856, forward nfe 97777, backward nfe 1107, Train: 0.9643, Val: 0.8000, Test: 0.8090\n",
      "Epoch: 017, Runtime 2.228755, Loss 0.371641, forward nfe 104689, backward nfe 1176, Train: 0.9786, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 018, Runtime 2.222767, Loss 0.347676, forward nfe 111617, backward nfe 1245, Train: 0.9857, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 019, Runtime 2.219200, Loss 0.320216, forward nfe 118448, backward nfe 1313, Train: 0.9786, Val: 0.7980, Test: 0.8050\n",
      "Epoch: 020, Runtime 2.175010, Loss 0.319360, forward nfe 125329, backward nfe 1383, Train: 0.9929, Val: 0.7980, Test: 0.8040\n",
      "Epoch: 021, Runtime 2.172304, Loss 0.298729, forward nfe 132255, backward nfe 1451, Train: 0.9929, Val: 0.7960, Test: 0.8020\n",
      "Epoch: 022, Runtime 2.179097, Loss 0.289530, forward nfe 139227, backward nfe 1520, Train: 0.9857, Val: 0.8000, Test: 0.8000\n",
      "Epoch: 023, Runtime 2.181479, Loss 0.266985, forward nfe 146257, backward nfe 1591, Train: 0.9929, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 024, Runtime 2.183663, Loss 0.258770, forward nfe 153220, backward nfe 1659, Train: 0.9786, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 025, Runtime 2.171252, Loss 0.245844, forward nfe 160190, backward nfe 1729, Train: 0.9929, Val: 0.7920, Test: 0.7970\n",
      "Epoch: 026, Runtime 2.176167, Loss 0.235515, forward nfe 167160, backward nfe 1796, Train: 0.9857, Val: 0.7960, Test: 0.7950\n",
      "Epoch: 027, Runtime 2.182368, Loss 0.220453, forward nfe 174171, backward nfe 1864, Train: 0.9929, Val: 0.7980, Test: 0.8050\n",
      "Epoch: 028, Runtime 2.173591, Loss 0.214230, forward nfe 181151, backward nfe 1932, Train: 0.9929, Val: 0.7980, Test: 0.8000\n",
      "Epoch: 029, Runtime 2.187087, Loss 0.211956, forward nfe 188131, backward nfe 2001, Train: 0.9929, Val: 0.7940, Test: 0.8020\n",
      "Epoch: 030, Runtime 2.187237, Loss 0.205416, forward nfe 195188, backward nfe 2068, Train: 0.9929, Val: 0.7940, Test: 0.7940\n",
      "best val accuracy 0.802000 with test accuracy 0.810000 at epoch 13\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.779021, Loss 1.950594, forward nfe 2424, backward nfe 52, Train: 0.2143, Val: 0.1220, Test: 0.1530\n",
      "Epoch: 002, Runtime 1.821913, Loss 1.820406, forward nfe 7829, backward nfe 122, Train: 0.4714, Val: 0.2080, Test: 0.2240\n",
      "Epoch: 003, Runtime 1.896321, Loss 1.685358, forward nfe 13173, backward nfe 190, Train: 0.4643, Val: 0.3120, Test: 0.3040\n",
      "Epoch: 004, Runtime 1.898678, Loss 1.521825, forward nfe 19060, backward nfe 260, Train: 0.8857, Val: 0.6580, Test: 0.7030\n",
      "Epoch: 005, Runtime 1.961021, Loss 1.307612, forward nfe 24647, backward nfe 329, Train: 0.8357, Val: 0.5880, Test: 0.6020\n",
      "Epoch: 006, Runtime 2.013068, Loss 1.176401, forward nfe 30893, backward nfe 400, Train: 0.9143, Val: 0.6680, Test: 0.6930\n",
      "Epoch: 007, Runtime 2.066439, Loss 1.016940, forward nfe 37149, backward nfe 470, Train: 0.9143, Val: 0.6820, Test: 0.6980\n",
      "Epoch: 008, Runtime 2.073740, Loss 0.939708, forward nfe 43741, backward nfe 539, Train: 0.9357, Val: 0.7660, Test: 0.8150\n",
      "Epoch: 009, Runtime 2.107955, Loss 0.805057, forward nfe 50271, backward nfe 609, Train: 0.9500, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 010, Runtime 2.122905, Loss 0.722559, forward nfe 56999, backward nfe 680, Train: 0.9571, Val: 0.7900, Test: 0.8250\n",
      "Epoch: 011, Runtime 2.114873, Loss 0.637927, forward nfe 63678, backward nfe 750, Train: 0.9571, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 012, Runtime 2.126977, Loss 0.595956, forward nfe 70368, backward nfe 820, Train: 0.9714, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 013, Runtime 2.134753, Loss 0.548988, forward nfe 77163, backward nfe 889, Train: 0.9643, Val: 0.7980, Test: 0.8190\n",
      "Epoch: 014, Runtime 2.151665, Loss 0.504380, forward nfe 84008, backward nfe 958, Train: 0.9643, Val: 0.8020, Test: 0.8230\n",
      "Epoch: 015, Runtime 2.147949, Loss 0.475134, forward nfe 90822, backward nfe 1028, Train: 0.9643, Val: 0.7960, Test: 0.8170\n",
      "Epoch: 016, Runtime 2.160509, Loss 0.437785, forward nfe 97712, backward nfe 1096, Train: 0.9643, Val: 0.7980, Test: 0.8320\n",
      "Epoch: 017, Runtime 2.174828, Loss 0.399316, forward nfe 104671, backward nfe 1167, Train: 0.9786, Val: 0.7940, Test: 0.8220\n",
      "Epoch: 018, Runtime 2.171206, Loss 0.380884, forward nfe 111620, backward nfe 1235, Train: 0.9786, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 019, Runtime 2.176989, Loss 0.333271, forward nfe 118603, backward nfe 1304, Train: 0.9786, Val: 0.7980, Test: 0.8250\n",
      "Epoch: 020, Runtime 2.186312, Loss 0.320239, forward nfe 125561, backward nfe 1373, Train: 0.9857, Val: 0.8000, Test: 0.8310\n",
      "Epoch: 021, Runtime 2.192875, Loss 0.306322, forward nfe 132605, backward nfe 1442, Train: 0.9786, Val: 0.7940, Test: 0.8170\n",
      "Epoch: 022, Runtime 2.187050, Loss 0.289884, forward nfe 139616, backward nfe 1511, Train: 0.9857, Val: 0.8020, Test: 0.8290\n",
      "Epoch: 023, Runtime 2.199016, Loss 0.274706, forward nfe 146663, backward nfe 1581, Train: 0.9857, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 024, Runtime 2.187592, Loss 0.265166, forward nfe 153692, backward nfe 1650, Train: 0.9929, Val: 0.7980, Test: 0.8200\n",
      "Epoch: 025, Runtime 2.190176, Loss 0.248062, forward nfe 160744, backward nfe 1719, Train: 0.9929, Val: 0.7980, Test: 0.8230\n",
      "Epoch: 026, Runtime 2.187212, Loss 0.235290, forward nfe 167777, backward nfe 1786, Train: 0.9929, Val: 0.7920, Test: 0.8190\n",
      "Epoch: 027, Runtime 2.195555, Loss 0.231528, forward nfe 174806, backward nfe 1855, Train: 0.9857, Val: 0.8060, Test: 0.8220\n",
      "Epoch: 028, Runtime 2.196275, Loss 0.212282, forward nfe 181878, backward nfe 1922, Train: 0.9929, Val: 0.7920, Test: 0.8040\n",
      "Epoch: 029, Runtime 2.185500, Loss 0.212151, forward nfe 188918, backward nfe 1989, Train: 0.9929, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 030, Runtime 2.193698, Loss 0.208812, forward nfe 195992, backward nfe 2057, Train: 0.9929, Val: 0.7860, Test: 0.8000\n",
      "best val accuracy 0.806000 with test accuracy 0.822000 at epoch 27\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.729323, Loss 1.951531, forward nfe 2424, backward nfe 53, Train: 0.2929, Val: 0.1160, Test: 0.1490\n",
      "Epoch: 002, Runtime 1.761239, Loss 1.828820, forward nfe 7397, backward nfe 118, Train: 0.6286, Val: 0.4160, Test: 0.4260\n",
      "Epoch: 003, Runtime 1.869719, Loss 1.604352, forward nfe 12573, backward nfe 185, Train: 0.4571, Val: 0.2440, Test: 0.2790\n",
      "Epoch: 004, Runtime 1.934011, Loss 1.444156, forward nfe 18481, backward nfe 255, Train: 0.7571, Val: 0.5980, Test: 0.6360\n",
      "Epoch: 005, Runtime 1.933563, Loss 1.220770, forward nfe 24282, backward nfe 324, Train: 0.8643, Val: 0.6260, Test: 0.6210\n",
      "Epoch: 006, Runtime 1.959936, Loss 1.054044, forward nfe 30178, backward nfe 392, Train: 0.9143, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 007, Runtime 1.999133, Loss 0.857468, forward nfe 36216, backward nfe 461, Train: 0.9571, Val: 0.7760, Test: 0.7830\n",
      "Epoch: 008, Runtime 2.031739, Loss 0.744622, forward nfe 42510, backward nfe 529, Train: 0.9429, Val: 0.7980, Test: 0.8220\n",
      "Epoch: 009, Runtime 2.049432, Loss 0.664632, forward nfe 48899, backward nfe 598, Train: 0.9571, Val: 0.8000, Test: 0.7900\n",
      "Epoch: 010, Runtime 2.080441, Loss 0.595686, forward nfe 55321, backward nfe 667, Train: 0.9643, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 011, Runtime 2.127814, Loss 0.512848, forward nfe 61979, backward nfe 735, Train: 0.9571, Val: 0.7940, Test: 0.8040\n",
      "Epoch: 012, Runtime 2.132667, Loss 0.465817, forward nfe 68778, backward nfe 804, Train: 0.9571, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 013, Runtime 2.153498, Loss 0.445930, forward nfe 75626, backward nfe 873, Train: 0.9643, Val: 0.7880, Test: 0.8000\n",
      "Epoch: 014, Runtime 2.170339, Loss 0.394692, forward nfe 82553, backward nfe 941, Train: 0.9786, Val: 0.8000, Test: 0.8120\n",
      "Epoch: 015, Runtime 2.178051, Loss 0.384061, forward nfe 89515, backward nfe 1011, Train: 0.9643, Val: 0.7940, Test: 0.8030\n",
      "Epoch: 016, Runtime 2.182475, Loss 0.330965, forward nfe 96535, backward nfe 1079, Train: 0.9857, Val: 0.7920, Test: 0.8010\n",
      "Epoch: 017, Runtime 2.182016, Loss 0.328130, forward nfe 103573, backward nfe 1146, Train: 0.9714, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 018, Runtime 2.197776, Loss 0.307193, forward nfe 110600, backward nfe 1215, Train: 0.9857, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 019, Runtime 2.184536, Loss 0.286254, forward nfe 117625, backward nfe 1282, Train: 0.9786, Val: 0.7860, Test: 0.7900\n",
      "Epoch: 020, Runtime 2.191840, Loss 0.293027, forward nfe 124683, backward nfe 1350, Train: 0.9857, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 021, Runtime 2.195978, Loss 0.246320, forward nfe 131744, backward nfe 1418, Train: 0.9786, Val: 0.7840, Test: 0.7900\n",
      "Epoch: 022, Runtime 2.192741, Loss 0.249675, forward nfe 138814, backward nfe 1486, Train: 0.9929, Val: 0.7940, Test: 0.7960\n",
      "Epoch: 023, Runtime 2.190676, Loss 0.231065, forward nfe 145900, backward nfe 1553, Train: 0.9857, Val: 0.7760, Test: 0.7880\n",
      "Epoch: 024, Runtime 2.197192, Loss 0.223343, forward nfe 152984, backward nfe 1621, Train: 0.9929, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 025, Runtime 2.191617, Loss 0.216838, forward nfe 160059, backward nfe 1688, Train: 1.0000, Val: 0.7820, Test: 0.7950\n",
      "Epoch: 026, Runtime 2.202039, Loss 0.193112, forward nfe 167148, backward nfe 1756, Train: 0.9929, Val: 0.7900, Test: 0.7990\n",
      "Epoch: 027, Runtime 2.192225, Loss 0.185278, forward nfe 174231, backward nfe 1822, Train: 0.9929, Val: 0.7880, Test: 0.8000\n",
      "Epoch: 028, Runtime 2.191473, Loss 0.178570, forward nfe 181312, backward nfe 1889, Train: 0.9929, Val: 0.7760, Test: 0.7910\n",
      "Epoch: 029, Runtime 2.199543, Loss 0.174060, forward nfe 188418, backward nfe 1956, Train: 0.9929, Val: 0.7880, Test: 0.8000\n",
      "Epoch: 030, Runtime 2.195031, Loss 0.157316, forward nfe 195509, backward nfe 2024, Train: 0.9929, Val: 0.7860, Test: 0.7930\n",
      "best val accuracy 0.806000 with test accuracy 0.816000 at epoch 12\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.757438, Loss 1.953788, forward nfe 2424, backward nfe 54, Train: 0.4429, Val: 0.2740, Test: 0.3100\n",
      "Epoch: 002, Runtime 1.827382, Loss 1.760828, forward nfe 7684, backward nfe 120, Train: 0.5714, Val: 0.5280, Test: 0.5730\n",
      "Epoch: 003, Runtime 1.915693, Loss 1.527285, forward nfe 13133, backward nfe 186, Train: 0.6286, Val: 0.3980, Test: 0.4180\n",
      "Epoch: 004, Runtime 1.981561, Loss 1.332182, forward nfe 19350, backward nfe 255, Train: 0.7429, Val: 0.5960, Test: 0.6260\n",
      "Epoch: 005, Runtime 2.002684, Loss 1.161831, forward nfe 25154, backward nfe 321, Train: 0.8786, Val: 0.7360, Test: 0.7560\n",
      "Epoch: 006, Runtime 2.079523, Loss 1.002509, forward nfe 31711, backward nfe 390, Train: 0.9071, Val: 0.7340, Test: 0.7400\n",
      "Epoch: 007, Runtime 2.078954, Loss 0.843046, forward nfe 38045, backward nfe 458, Train: 0.9286, Val: 0.7560, Test: 0.7810\n",
      "Epoch: 008, Runtime 2.123723, Loss 0.754258, forward nfe 44767, backward nfe 527, Train: 0.9500, Val: 0.7400, Test: 0.7550\n",
      "Epoch: 009, Runtime 2.144726, Loss 0.672284, forward nfe 51561, backward nfe 595, Train: 0.9357, Val: 0.7840, Test: 0.8040\n",
      "Epoch: 010, Runtime 2.153580, Loss 0.595416, forward nfe 58387, backward nfe 663, Train: 0.9571, Val: 0.7600, Test: 0.7690\n",
      "Epoch: 011, Runtime 2.168758, Loss 0.563214, forward nfe 65289, backward nfe 731, Train: 0.9429, Val: 0.7860, Test: 0.8170\n",
      "Epoch: 012, Runtime 2.172302, Loss 0.516121, forward nfe 72211, backward nfe 800, Train: 0.9714, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 013, Runtime 2.168983, Loss 0.455926, forward nfe 79184, backward nfe 868, Train: 0.9714, Val: 0.7860, Test: 0.8170\n",
      "Epoch: 014, Runtime 2.166060, Loss 0.442328, forward nfe 86085, backward nfe 936, Train: 0.9786, Val: 0.7940, Test: 0.8070\n",
      "Epoch: 015, Runtime 2.163400, Loss 0.406739, forward nfe 93054, backward nfe 1004, Train: 0.9786, Val: 0.7980, Test: 0.8130\n",
      "Epoch: 016, Runtime 2.179444, Loss 0.382675, forward nfe 99952, backward nfe 1072, Train: 0.9857, Val: 0.7900, Test: 0.8040\n",
      "Epoch: 017, Runtime 2.169965, Loss 0.359254, forward nfe 106927, backward nfe 1140, Train: 0.9857, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 018, Runtime 2.175524, Loss 0.333798, forward nfe 113868, backward nfe 1208, Train: 0.9857, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 019, Runtime 2.190429, Loss 0.307203, forward nfe 120913, backward nfe 1276, Train: 0.9929, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 020, Runtime 2.184729, Loss 0.300845, forward nfe 127899, backward nfe 1344, Train: 0.9929, Val: 0.7940, Test: 0.8090\n",
      "Epoch: 021, Runtime 2.181751, Loss 0.273647, forward nfe 134904, backward nfe 1411, Train: 0.9857, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 022, Runtime 2.182555, Loss 0.257755, forward nfe 141912, backward nfe 1478, Train: 0.9929, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 023, Runtime 2.181780, Loss 0.254718, forward nfe 148899, backward nfe 1545, Train: 0.9929, Val: 0.7980, Test: 0.8040\n",
      "Epoch: 024, Runtime 2.169464, Loss 0.250607, forward nfe 155890, backward nfe 1612, Train: 0.9929, Val: 0.7920, Test: 0.8030\n",
      "Epoch: 025, Runtime 2.179584, Loss 0.221350, forward nfe 162826, backward nfe 1679, Train: 0.9929, Val: 0.7860, Test: 0.7990\n",
      "Epoch: 026, Runtime 2.186600, Loss 0.219192, forward nfe 169852, backward nfe 1745, Train: 0.9929, Val: 0.7900, Test: 0.8010\n",
      "Epoch: 027, Runtime 2.175462, Loss 0.209572, forward nfe 176785, backward nfe 1812, Train: 0.9929, Val: 0.7880, Test: 0.8030\n",
      "Epoch: 028, Runtime 2.178019, Loss 0.192351, forward nfe 183787, backward nfe 1879, Train: 0.9929, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 029, Runtime 2.181267, Loss 0.201327, forward nfe 190765, backward nfe 1944, Train: 1.0000, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 030, Runtime 2.184912, Loss 0.187725, forward nfe 197770, backward nfe 2010, Train: 1.0000, Val: 0.8000, Test: 0.8080\n",
      "best val accuracy 0.800000 with test accuracy 0.808000 at epoch 30\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.769420, Loss 1.955619, forward nfe 2424, backward nfe 51, Train: 0.1500, Val: 0.1220, Test: 0.1320\n",
      "Epoch: 002, Runtime 1.840961, Loss 1.829974, forward nfe 7793, backward nfe 117, Train: 0.5143, Val: 0.4300, Test: 0.4680\n",
      "Epoch: 003, Runtime 1.896995, Loss 1.628847, forward nfe 13286, backward nfe 186, Train: 0.4214, Val: 0.2980, Test: 0.2830\n",
      "Epoch: 004, Runtime 1.980841, Loss 1.525174, forward nfe 19252, backward nfe 256, Train: 0.5714, Val: 0.3800, Test: 0.3950\n",
      "Epoch: 005, Runtime 2.037210, Loss 1.414600, forward nfe 25487, backward nfe 329, Train: 0.8786, Val: 0.7720, Test: 0.7990\n",
      "Epoch: 006, Runtime 2.035934, Loss 1.110100, forward nfe 31815, backward nfe 399, Train: 0.8857, Val: 0.7100, Test: 0.7100\n",
      "Epoch: 007, Runtime 2.056965, Loss 0.965063, forward nfe 38144, backward nfe 471, Train: 0.9214, Val: 0.7840, Test: 0.8090\n",
      "Epoch: 008, Runtime 2.068010, Loss 0.840210, forward nfe 44661, backward nfe 542, Train: 0.9357, Val: 0.7700, Test: 0.7780\n",
      "Epoch: 009, Runtime 2.084292, Loss 0.746210, forward nfe 51140, backward nfe 613, Train: 0.9500, Val: 0.7780, Test: 0.8100\n",
      "Epoch: 010, Runtime 2.103470, Loss 0.693973, forward nfe 57738, backward nfe 685, Train: 0.9643, Val: 0.7840, Test: 0.8040\n",
      "Epoch: 011, Runtime 2.123605, Loss 0.607864, forward nfe 64440, backward nfe 757, Train: 0.9571, Val: 0.7900, Test: 0.8170\n",
      "Epoch: 012, Runtime 2.133039, Loss 0.575435, forward nfe 71205, backward nfe 827, Train: 0.9643, Val: 0.7920, Test: 0.8220\n",
      "Epoch: 013, Runtime 2.139642, Loss 0.527113, forward nfe 78001, backward nfe 899, Train: 0.9643, Val: 0.7880, Test: 0.8170\n",
      "Epoch: 014, Runtime 2.135283, Loss 0.472592, forward nfe 84860, backward nfe 970, Train: 0.9714, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 015, Runtime 2.153000, Loss 0.434859, forward nfe 91691, backward nfe 1042, Train: 0.9714, Val: 0.7920, Test: 0.8170\n",
      "Epoch: 016, Runtime 2.156846, Loss 0.393629, forward nfe 98589, backward nfe 1114, Train: 0.9714, Val: 0.7900, Test: 0.8150\n",
      "Epoch: 017, Runtime 2.156179, Loss 0.393412, forward nfe 105474, backward nfe 1184, Train: 0.9786, Val: 0.7940, Test: 0.8170\n",
      "Epoch: 018, Runtime 2.160574, Loss 0.359836, forward nfe 112412, backward nfe 1254, Train: 0.9786, Val: 0.7960, Test: 0.8180\n",
      "Epoch: 019, Runtime 2.159264, Loss 0.344251, forward nfe 119328, backward nfe 1323, Train: 0.9786, Val: 0.7840, Test: 0.8070\n",
      "Epoch: 020, Runtime 2.172758, Loss 0.307786, forward nfe 126279, backward nfe 1395, Train: 0.9786, Val: 0.7920, Test: 0.8130\n",
      "Epoch: 021, Runtime 2.161036, Loss 0.305689, forward nfe 133207, backward nfe 1464, Train: 0.9786, Val: 0.8000, Test: 0.8180\n",
      "Epoch: 022, Runtime 2.165881, Loss 0.285207, forward nfe 140139, backward nfe 1534, Train: 0.9786, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 023, Runtime 2.162830, Loss 0.277991, forward nfe 147071, backward nfe 1603, Train: 0.9857, Val: 0.7840, Test: 0.8050\n",
      "Epoch: 024, Runtime 2.165059, Loss 0.272644, forward nfe 153992, backward nfe 1673, Train: 0.9786, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 025, Runtime 2.163157, Loss 0.251922, forward nfe 160931, backward nfe 1743, Train: 0.9786, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 026, Runtime 2.163947, Loss 0.235314, forward nfe 167818, backward nfe 1814, Train: 0.9929, Val: 0.7840, Test: 0.8090\n",
      "Epoch: 027, Runtime 2.158043, Loss 0.222663, forward nfe 174757, backward nfe 1882, Train: 0.9857, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 028, Runtime 2.170291, Loss 0.218466, forward nfe 181719, backward nfe 1951, Train: 0.9929, Val: 0.7800, Test: 0.8080\n",
      "Epoch: 029, Runtime 2.149192, Loss 0.211614, forward nfe 188614, backward nfe 2020, Train: 0.9929, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 030, Runtime 2.158021, Loss 0.200088, forward nfe 195493, backward nfe 2087, Train: 0.9929, Val: 0.7860, Test: 0.8110\n",
      "best val accuracy 0.800000 with test accuracy 0.818000 at epoch 21\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.733834, Loss 1.951708, forward nfe 2424, backward nfe 53, Train: 0.2429, Val: 0.1760, Test: 0.1910\n",
      "Epoch: 002, Runtime 1.844908, Loss 1.814487, forward nfe 7435, backward nfe 120, Train: 0.5643, Val: 0.3560, Test: 0.3650\n",
      "Epoch: 003, Runtime 1.915433, Loss 1.577645, forward nfe 13258, backward nfe 191, Train: 0.6714, Val: 0.6300, Test: 0.6690\n",
      "Epoch: 004, Runtime 1.987985, Loss 1.357524, forward nfe 18922, backward nfe 260, Train: 0.7214, Val: 0.4860, Test: 0.4730\n",
      "Epoch: 005, Runtime 2.067826, Loss 1.271743, forward nfe 25443, backward nfe 333, Train: 0.9000, Val: 0.7360, Test: 0.7590\n",
      "Epoch: 006, Runtime 2.049264, Loss 0.971385, forward nfe 31790, backward nfe 404, Train: 0.9000, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 007, Runtime 2.102878, Loss 0.870324, forward nfe 38200, backward nfe 475, Train: 0.9357, Val: 0.7760, Test: 0.7800\n",
      "Epoch: 008, Runtime 2.153505, Loss 0.745060, forward nfe 44991, backward nfe 546, Train: 0.9429, Val: 0.7900, Test: 0.8070\n",
      "Epoch: 009, Runtime 2.159313, Loss 0.666519, forward nfe 51868, backward nfe 617, Train: 0.9643, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 010, Runtime 2.171043, Loss 0.581958, forward nfe 58799, backward nfe 688, Train: 0.9643, Val: 0.7880, Test: 0.8110\n",
      "Epoch: 011, Runtime 2.178183, Loss 0.519769, forward nfe 65737, backward nfe 760, Train: 0.9714, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 012, Runtime 2.193290, Loss 0.494768, forward nfe 72744, backward nfe 830, Train: 0.9500, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 013, Runtime 2.202059, Loss 0.433120, forward nfe 79778, backward nfe 901, Train: 0.9714, Val: 0.7780, Test: 0.8050\n",
      "Epoch: 014, Runtime 2.190396, Loss 0.427717, forward nfe 86839, backward nfe 971, Train: 0.9643, Val: 0.8000, Test: 0.8110\n",
      "Epoch: 015, Runtime 2.198415, Loss 0.384251, forward nfe 93874, backward nfe 1042, Train: 0.9786, Val: 0.7840, Test: 0.8050\n",
      "Epoch: 016, Runtime 2.203511, Loss 0.366661, forward nfe 100944, backward nfe 1112, Train: 0.9714, Val: 0.8040, Test: 0.8100\n",
      "Epoch: 017, Runtime 2.196737, Loss 0.322894, forward nfe 108019, backward nfe 1182, Train: 0.9786, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 018, Runtime 2.197559, Loss 0.316059, forward nfe 115069, backward nfe 1251, Train: 0.9786, Val: 0.7980, Test: 0.8100\n",
      "Epoch: 019, Runtime 2.198207, Loss 0.306583, forward nfe 122147, backward nfe 1320, Train: 0.9857, Val: 0.7940, Test: 0.8050\n",
      "Epoch: 020, Runtime 2.183438, Loss 0.290452, forward nfe 129176, backward nfe 1388, Train: 0.9929, Val: 0.8020, Test: 0.8110\n",
      "Epoch: 021, Runtime 2.195109, Loss 0.266395, forward nfe 136215, backward nfe 1457, Train: 0.9857, Val: 0.7940, Test: 0.8100\n",
      "Epoch: 022, Runtime 2.199253, Loss 0.249813, forward nfe 143272, backward nfe 1526, Train: 0.9929, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 023, Runtime 2.200321, Loss 0.255572, forward nfe 150331, backward nfe 1594, Train: 1.0000, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 024, Runtime 2.197992, Loss 0.231437, forward nfe 157403, backward nfe 1663, Train: 1.0000, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 025, Runtime 2.190521, Loss 0.218280, forward nfe 164441, backward nfe 1730, Train: 1.0000, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 026, Runtime 2.200392, Loss 0.204552, forward nfe 171502, backward nfe 1798, Train: 1.0000, Val: 0.7940, Test: 0.8070\n",
      "Epoch: 027, Runtime 2.184694, Loss 0.200524, forward nfe 178532, backward nfe 1865, Train: 1.0000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 028, Runtime 2.199011, Loss 0.199986, forward nfe 185567, backward nfe 1933, Train: 1.0000, Val: 0.7900, Test: 0.8070\n",
      "Epoch: 029, Runtime 2.197649, Loss 0.178639, forward nfe 192626, backward nfe 2000, Train: 1.0000, Val: 0.7920, Test: 0.8090\n",
      "Epoch: 030, Runtime 2.191900, Loss 0.180958, forward nfe 199676, backward nfe 2067, Train: 1.0000, Val: 0.7900, Test: 0.8060\n",
      "best val accuracy 0.804000 with test accuracy 0.810000 at epoch 16\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.752704, Loss 1.955901, forward nfe 2424, backward nfe 52, Train: 0.3143, Val: 0.1560, Test: 0.1700\n",
      "Epoch: 002, Runtime 1.851471, Loss 1.814051, forward nfe 7625, backward nfe 117, Train: 0.5143, Val: 0.3240, Test: 0.3480\n",
      "Epoch: 003, Runtime 1.968097, Loss 1.590526, forward nfe 13408, backward nfe 185, Train: 0.6071, Val: 0.3580, Test: 0.4070\n",
      "Epoch: 004, Runtime 1.968576, Loss 1.409878, forward nfe 19573, backward nfe 255, Train: 0.9286, Val: 0.7200, Test: 0.7350\n",
      "Epoch: 005, Runtime 2.025111, Loss 1.138439, forward nfe 25447, backward nfe 323, Train: 0.8214, Val: 0.5700, Test: 0.5990\n",
      "Epoch: 006, Runtime 2.101003, Loss 1.012159, forward nfe 32077, backward nfe 392, Train: 0.9000, Val: 0.7760, Test: 0.7900\n",
      "Epoch: 007, Runtime 2.132225, Loss 0.896197, forward nfe 38737, backward nfe 461, Train: 0.8714, Val: 0.6000, Test: 0.5960\n",
      "Epoch: 008, Runtime 2.134954, Loss 0.818484, forward nfe 45488, backward nfe 531, Train: 0.9286, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 009, Runtime 2.150875, Loss 0.692887, forward nfe 52368, backward nfe 600, Train: 0.9286, Val: 0.7500, Test: 0.7640\n",
      "Epoch: 010, Runtime 2.150126, Loss 0.607757, forward nfe 59223, backward nfe 669, Train: 0.9643, Val: 0.7980, Test: 0.8260\n",
      "Epoch: 011, Runtime 2.161538, Loss 0.534718, forward nfe 66113, backward nfe 737, Train: 0.9643, Val: 0.7900, Test: 0.8090\n",
      "Epoch: 012, Runtime 2.160808, Loss 0.495486, forward nfe 73026, backward nfe 805, Train: 0.9714, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 013, Runtime 2.169187, Loss 0.454116, forward nfe 79949, backward nfe 873, Train: 0.9714, Val: 0.7940, Test: 0.8170\n",
      "Epoch: 014, Runtime 2.180418, Loss 0.425144, forward nfe 86925, backward nfe 941, Train: 0.9643, Val: 0.8000, Test: 0.8170\n",
      "Epoch: 015, Runtime 2.184449, Loss 0.389030, forward nfe 93924, backward nfe 1009, Train: 0.9714, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 016, Runtime 2.172863, Loss 0.368298, forward nfe 100914, backward nfe 1077, Train: 0.9714, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 017, Runtime 2.181966, Loss 0.341326, forward nfe 107904, backward nfe 1145, Train: 0.9714, Val: 0.7860, Test: 0.8140\n",
      "Epoch: 018, Runtime 2.175235, Loss 0.324022, forward nfe 114880, backward nfe 1213, Train: 0.9857, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 019, Runtime 2.184136, Loss 0.292529, forward nfe 121908, backward nfe 1281, Train: 0.9786, Val: 0.7820, Test: 0.8050\n",
      "Epoch: 020, Runtime 2.181224, Loss 0.300102, forward nfe 128895, backward nfe 1348, Train: 0.9786, Val: 0.7920, Test: 0.8210\n",
      "Epoch: 021, Runtime 2.181762, Loss 0.273525, forward nfe 135915, backward nfe 1415, Train: 0.9786, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 022, Runtime 2.181859, Loss 0.250662, forward nfe 142912, backward nfe 1482, Train: 0.9857, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 023, Runtime 2.183525, Loss 0.243817, forward nfe 149928, backward nfe 1549, Train: 0.9857, Val: 0.7880, Test: 0.8070\n",
      "Epoch: 024, Runtime 2.187454, Loss 0.235495, forward nfe 156972, backward nfe 1616, Train: 0.9929, Val: 0.8000, Test: 0.8110\n",
      "Epoch: 025, Runtime 2.187382, Loss 0.218962, forward nfe 164005, backward nfe 1683, Train: 0.9929, Val: 0.7860, Test: 0.8140\n",
      "Epoch: 026, Runtime 2.190522, Loss 0.210845, forward nfe 171047, backward nfe 1750, Train: 0.9857, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 027, Runtime 2.191409, Loss 0.196538, forward nfe 178096, backward nfe 1816, Train: 1.0000, Val: 0.7860, Test: 0.8070\n",
      "Epoch: 028, Runtime 2.194039, Loss 0.195831, forward nfe 185177, backward nfe 1883, Train: 0.9857, Val: 0.7820, Test: 0.8040\n",
      "Epoch: 029, Runtime 2.191882, Loss 0.193710, forward nfe 192234, backward nfe 1950, Train: 1.0000, Val: 0.8000, Test: 0.8130\n",
      "Epoch: 030, Runtime 2.189573, Loss 0.183009, forward nfe 199268, backward nfe 2017, Train: 0.9929, Val: 0.7800, Test: 0.8060\n",
      "best val accuracy 0.800000 with test accuracy 0.817000 at epoch 14\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.746475, Loss 1.951897, forward nfe 2424, backward nfe 53, Train: 0.2643, Val: 0.1820, Test: 0.1950\n",
      "Epoch: 002, Runtime 1.868149, Loss 1.796915, forward nfe 7609, backward nfe 120, Train: 0.5071, Val: 0.3200, Test: 0.3350\n",
      "Epoch: 003, Runtime 1.890033, Loss 1.584233, forward nfe 13451, backward nfe 190, Train: 0.6500, Val: 0.4260, Test: 0.4530\n",
      "Epoch: 004, Runtime 1.959696, Loss 1.369946, forward nfe 18948, backward nfe 258, Train: 0.6857, Val: 0.5260, Test: 0.5460\n",
      "Epoch: 005, Runtime 2.049214, Loss 1.237604, forward nfe 25360, backward nfe 330, Train: 0.7786, Val: 0.5560, Test: 0.5530\n",
      "Epoch: 006, Runtime 2.072348, Loss 1.150135, forward nfe 31700, backward nfe 401, Train: 0.8571, Val: 0.7400, Test: 0.7730\n",
      "Epoch: 007, Runtime 2.096090, Loss 0.965659, forward nfe 38297, backward nfe 472, Train: 0.9214, Val: 0.7320, Test: 0.7400\n",
      "Epoch: 008, Runtime 2.131064, Loss 0.844342, forward nfe 44985, backward nfe 541, Train: 0.9357, Val: 0.7800, Test: 0.8070\n",
      "Epoch: 009, Runtime 2.166946, Loss 0.733924, forward nfe 51816, backward nfe 611, Train: 0.9500, Val: 0.7760, Test: 0.7850\n",
      "Epoch: 010, Runtime 2.172311, Loss 0.636825, forward nfe 58782, backward nfe 680, Train: 0.9571, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 011, Runtime 2.182748, Loss 0.579057, forward nfe 65800, backward nfe 749, Train: 0.9571, Val: 0.7820, Test: 0.7800\n",
      "Epoch: 012, Runtime 2.214385, Loss 0.516940, forward nfe 72819, backward nfe 819, Train: 0.9786, Val: 0.8020, Test: 0.8240\n",
      "Epoch: 013, Runtime 2.196460, Loss 0.490410, forward nfe 79900, backward nfe 888, Train: 0.9714, Val: 0.7780, Test: 0.8110\n",
      "Epoch: 014, Runtime 2.200795, Loss 0.432470, forward nfe 86957, backward nfe 957, Train: 0.9786, Val: 0.8020, Test: 0.8240\n",
      "Epoch: 015, Runtime 2.205394, Loss 0.408879, forward nfe 94074, backward nfe 1026, Train: 0.9786, Val: 0.8040, Test: 0.8230\n",
      "Epoch: 016, Runtime 2.207902, Loss 0.362861, forward nfe 101194, backward nfe 1095, Train: 0.9857, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 017, Runtime 2.198978, Loss 0.343042, forward nfe 108290, backward nfe 1164, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
      "Epoch: 018, Runtime 2.200739, Loss 0.312635, forward nfe 115405, backward nfe 1233, Train: 0.9786, Val: 0.8080, Test: 0.8230\n",
      "Epoch: 019, Runtime 2.204479, Loss 0.320014, forward nfe 122492, backward nfe 1302, Train: 0.9857, Val: 0.7840, Test: 0.8070\n",
      "Epoch: 020, Runtime 2.202482, Loss 0.283935, forward nfe 129578, backward nfe 1370, Train: 0.9929, Val: 0.7980, Test: 0.8200\n",
      "Epoch: 021, Runtime 2.203770, Loss 0.276634, forward nfe 136688, backward nfe 1438, Train: 0.9929, Val: 0.7820, Test: 0.8100\n",
      "Epoch: 022, Runtime 2.199706, Loss 0.256542, forward nfe 143790, backward nfe 1506, Train: 0.9929, Val: 0.7900, Test: 0.8130\n",
      "Epoch: 023, Runtime 2.203474, Loss 0.243966, forward nfe 150902, backward nfe 1574, Train: 0.9786, Val: 0.8020, Test: 0.8120\n",
      "Epoch: 024, Runtime 2.207903, Loss 0.235587, forward nfe 158024, backward nfe 1642, Train: 0.9929, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 025, Runtime 2.201844, Loss 0.243295, forward nfe 165126, backward nfe 1709, Train: 0.9929, Val: 0.7760, Test: 0.7990\n",
      "Epoch: 026, Runtime 2.206966, Loss 0.213757, forward nfe 172244, backward nfe 1777, Train: 0.9929, Val: 0.7860, Test: 0.8000\n",
      "Epoch: 027, Runtime 2.199930, Loss 0.199635, forward nfe 179337, backward nfe 1845, Train: 0.9929, Val: 0.7940, Test: 0.8110\n",
      "Epoch: 028, Runtime 2.202927, Loss 0.191497, forward nfe 186428, backward nfe 1913, Train: 0.9929, Val: 0.7940, Test: 0.8020\n",
      "Epoch: 029, Runtime 2.195433, Loss 0.188140, forward nfe 193508, backward nfe 1980, Train: 0.9929, Val: 0.7860, Test: 0.8000\n",
      "Epoch: 030, Runtime 2.199799, Loss 0.183824, forward nfe 200579, backward nfe 2047, Train: 0.9929, Val: 0.7880, Test: 0.8010\n",
      "best val accuracy 0.808000 with test accuracy 0.823000 at epoch 18\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.744658, Loss 1.957068, forward nfe 2424, backward nfe 52, Train: 0.4429, Val: 0.2280, Test: 0.2400\n",
      "Epoch: 002, Runtime 1.847459, Loss 1.813894, forward nfe 7608, backward nfe 119, Train: 0.6929, Val: 0.4000, Test: 0.4340\n",
      "Epoch: 003, Runtime 1.995722, Loss 1.563053, forward nfe 13289, backward nfe 185, Train: 0.7071, Val: 0.4520, Test: 0.4860\n",
      "Epoch: 004, Runtime 2.056439, Loss 1.356679, forward nfe 19914, backward nfe 257, Train: 0.6571, Val: 0.4320, Test: 0.4510\n",
      "Epoch: 005, Runtime 2.077197, Loss 1.273454, forward nfe 26128, backward nfe 329, Train: 0.8143, Val: 0.6000, Test: 0.6140\n",
      "Epoch: 006, Runtime 2.141462, Loss 1.072358, forward nfe 32952, backward nfe 399, Train: 0.9571, Val: 0.7660, Test: 0.7800\n",
      "Epoch: 007, Runtime 2.164310, Loss 0.876211, forward nfe 39754, backward nfe 468, Train: 0.9500, Val: 0.7620, Test: 0.7970\n",
      "Epoch: 008, Runtime 2.188030, Loss 0.755508, forward nfe 46765, backward nfe 538, Train: 0.9429, Val: 0.7480, Test: 0.7560\n",
      "Epoch: 009, Runtime 2.194348, Loss 0.671859, forward nfe 53807, backward nfe 608, Train: 0.9286, Val: 0.7660, Test: 0.7980\n",
      "Epoch: 010, Runtime 2.197007, Loss 0.612388, forward nfe 60905, backward nfe 677, Train: 0.9500, Val: 0.7560, Test: 0.7510\n",
      "Epoch: 011, Runtime 2.201952, Loss 0.570823, forward nfe 67973, backward nfe 747, Train: 0.9429, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 012, Runtime 2.211859, Loss 0.473878, forward nfe 75104, backward nfe 816, Train: 0.9714, Val: 0.7720, Test: 0.7980\n",
      "Epoch: 013, Runtime 2.209006, Loss 0.445064, forward nfe 82219, backward nfe 885, Train: 0.9714, Val: 0.7900, Test: 0.8130\n",
      "Epoch: 014, Runtime 2.214905, Loss 0.413058, forward nfe 89371, backward nfe 954, Train: 0.9643, Val: 0.7880, Test: 0.8130\n",
      "Epoch: 015, Runtime 2.214275, Loss 0.375942, forward nfe 96516, backward nfe 1024, Train: 0.9786, Val: 0.7860, Test: 0.8100\n",
      "Epoch: 016, Runtime 2.205822, Loss 0.363162, forward nfe 103643, backward nfe 1092, Train: 0.9786, Val: 0.7800, Test: 0.8040\n",
      "Epoch: 017, Runtime 2.209816, Loss 0.319227, forward nfe 110786, backward nfe 1161, Train: 0.9857, Val: 0.7980, Test: 0.8170\n",
      "Epoch: 018, Runtime 2.219929, Loss 0.297717, forward nfe 117942, backward nfe 1229, Train: 0.9786, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 019, Runtime 2.209832, Loss 0.281661, forward nfe 125100, backward nfe 1298, Train: 0.9857, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 020, Runtime 2.212311, Loss 0.268971, forward nfe 132256, backward nfe 1366, Train: 0.9786, Val: 0.7760, Test: 0.8060\n",
      "Epoch: 021, Runtime 2.216405, Loss 0.264969, forward nfe 139397, backward nfe 1434, Train: 0.9857, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 022, Runtime 2.214067, Loss 0.252231, forward nfe 146568, backward nfe 1502, Train: 0.9714, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 023, Runtime 2.219389, Loss 0.243060, forward nfe 153734, backward nfe 1570, Train: 0.9929, Val: 0.7880, Test: 0.8040\n",
      "Epoch: 024, Runtime 2.215850, Loss 0.216184, forward nfe 160903, backward nfe 1638, Train: 0.9929, Val: 0.7920, Test: 0.8110\n",
      "Epoch: 025, Runtime 2.212576, Loss 0.210037, forward nfe 168066, backward nfe 1706, Train: 0.9929, Val: 0.7800, Test: 0.8000\n",
      "Epoch: 026, Runtime 2.211316, Loss 0.198704, forward nfe 175208, backward nfe 1774, Train: 0.9929, Val: 0.7700, Test: 0.8000\n",
      "Epoch: 027, Runtime 2.222298, Loss 0.199753, forward nfe 182385, backward nfe 1842, Train: 0.9929, Val: 0.7780, Test: 0.7950\n",
      "Epoch: 028, Runtime 2.211895, Loss 0.191380, forward nfe 189548, backward nfe 1909, Train: 0.9929, Val: 0.7780, Test: 0.8050\n",
      "Epoch: 029, Runtime 2.213109, Loss 0.183135, forward nfe 196705, backward nfe 1976, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 030, Runtime 2.218532, Loss 0.168796, forward nfe 203888, backward nfe 2043, Train: 1.0000, Val: 0.7880, Test: 0.8020\n",
      "best val accuracy 0.798000 with test accuracy 0.817000 at epoch 17\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Cora', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'rmsprop', 'lr': 0.0047, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 31, 'alpha': 0.918, 'time': 12.1, 'augment': True, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'count_runs': 10, 'beltrami': False, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'data': 'Planetoid', 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 10000}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 1433])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([7, 16])\n",
      "m2.bias\n",
      "torch.Size([7])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.775589, Loss 1.957474, forward nfe 2424, backward nfe 53, Train: 0.3143, Val: 0.4180, Test: 0.4360\n",
      "Epoch: 002, Runtime 1.870750, Loss 1.799563, forward nfe 7883, backward nfe 120, Train: 0.4214, Val: 0.1780, Test: 0.1940\n",
      "Epoch: 003, Runtime 1.899847, Loss 1.688620, forward nfe 13374, backward nfe 190, Train: 0.6214, Val: 0.5880, Test: 0.5910\n",
      "Epoch: 004, Runtime 1.946915, Loss 1.475791, forward nfe 19251, backward nfe 260, Train: 0.8857, Val: 0.7540, Test: 0.7430\n",
      "Epoch: 005, Runtime 2.013555, Loss 1.269139, forward nfe 25234, backward nfe 329, Train: 0.8857, Val: 0.7340, Test: 0.7440\n",
      "Epoch: 006, Runtime 2.056426, Loss 1.065803, forward nfe 31594, backward nfe 401, Train: 0.8571, Val: 0.7420, Test: 0.7550\n",
      "Epoch: 007, Runtime 2.060708, Loss 0.952209, forward nfe 37963, backward nfe 470, Train: 0.8714, Val: 0.7120, Test: 0.7250\n",
      "Epoch: 008, Runtime 2.079176, Loss 0.942039, forward nfe 44603, backward nfe 541, Train: 0.8929, Val: 0.7240, Test: 0.7320\n",
      "Epoch: 009, Runtime 2.077531, Loss 0.754553, forward nfe 51032, backward nfe 609, Train: 0.9214, Val: 0.7840, Test: 0.8020\n",
      "Epoch: 010, Runtime 2.100009, Loss 0.689294, forward nfe 57592, backward nfe 678, Train: 0.9429, Val: 0.7780, Test: 0.7980\n",
      "Epoch: 011, Runtime 2.101011, Loss 0.618678, forward nfe 64182, backward nfe 748, Train: 0.9571, Val: 0.7940, Test: 0.8190\n",
      "Epoch: 012, Runtime 2.101768, Loss 0.558878, forward nfe 70859, backward nfe 816, Train: 0.9571, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 013, Runtime 2.116416, Loss 0.514099, forward nfe 77526, backward nfe 885, Train: 0.9571, Val: 0.7900, Test: 0.8190\n",
      "Epoch: 014, Runtime 2.134024, Loss 0.468663, forward nfe 84259, backward nfe 955, Train: 0.9643, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 015, Runtime 2.152007, Loss 0.437280, forward nfe 91069, backward nfe 1025, Train: 0.9571, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 016, Runtime 2.147691, Loss 0.389219, forward nfe 97964, backward nfe 1094, Train: 0.9786, Val: 0.7960, Test: 0.8090\n",
      "Epoch: 017, Runtime 2.166112, Loss 0.381792, forward nfe 104822, backward nfe 1164, Train: 0.9714, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 018, Runtime 2.157810, Loss 0.343651, forward nfe 111730, backward nfe 1232, Train: 0.9857, Val: 0.7980, Test: 0.8090\n",
      "Epoch: 019, Runtime 2.163127, Loss 0.347984, forward nfe 118649, backward nfe 1300, Train: 0.9786, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 020, Runtime 2.162612, Loss 0.314620, forward nfe 125559, backward nfe 1368, Train: 0.9786, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 021, Runtime 2.175930, Loss 0.294936, forward nfe 132522, backward nfe 1437, Train: 0.9857, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 022, Runtime 2.172782, Loss 0.275402, forward nfe 139458, backward nfe 1506, Train: 0.9857, Val: 0.7920, Test: 0.8170\n",
      "Epoch: 023, Runtime 2.174801, Loss 0.253873, forward nfe 146444, backward nfe 1575, Train: 0.9857, Val: 0.7840, Test: 0.8150\n",
      "Epoch: 024, Runtime 2.173511, Loss 0.241808, forward nfe 153387, backward nfe 1643, Train: 0.9857, Val: 0.7920, Test: 0.8190\n",
      "Epoch: 025, Runtime 2.175845, Loss 0.234788, forward nfe 160385, backward nfe 1711, Train: 0.9857, Val: 0.7840, Test: 0.8150\n",
      "Epoch: 026, Runtime 2.175815, Loss 0.216987, forward nfe 167360, backward nfe 1778, Train: 0.9929, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 027, Runtime 2.175190, Loss 0.209319, forward nfe 174360, backward nfe 1845, Train: 1.0000, Val: 0.7900, Test: 0.8070\n",
      "Epoch: 028, Runtime 2.172574, Loss 0.194653, forward nfe 181316, backward nfe 1912, Train: 0.9929, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 029, Runtime 2.184104, Loss 0.185972, forward nfe 188294, backward nfe 1980, Train: 0.9857, Val: 0.7900, Test: 0.8120\n",
      "Epoch: 030, Runtime 2.173881, Loss 0.192588, forward nfe 195288, backward nfe 2047, Train: 1.0000, Val: 0.7860, Test: 0.8060\n",
      "best val accuracy 0.798000 with test accuracy 0.809000 at epoch 18\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_cora_defaults', action='store_true',\n",
    "                  help='Whether to run with best params for cora. Overrides the choice of dataset')\n",
    "parser.add_argument('--dataset', type=str, default='Cora',\n",
    "                  help='Cora, Citeseer, Pubmed, Computers, Photo, CoauthorCS')\n",
    "parser.add_argument('--data_norm', type=str, default='rw',\n",
    "                  help='rw for random walk, gcn for symmetric gcn norm')\n",
    "parser.add_argument('--hidden_dim', type=int, default=16, help='Hidden dimension.')\n",
    "parser.add_argument('--input_dropout', type=float, default=0.5, help='Input dropout rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='One from sgd, rmsprop, adam, adagrad, adamax.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Learning rate.')\n",
    "parser.add_argument('--decay', type=float, default=5e-4, help='Weight decay for optimization')\n",
    "parser.add_argument('--self_loop_weight', type=float, default=1.0, help='Weight of self-loops.')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='Number of training epochs per iteration.')\n",
    "parser.add_argument('--alpha', type=float, default=1.0, help='Factor in front matrix A.')\n",
    "parser.add_argument('--time', type=float, default=1.0, help='End time of ODE integrator.')\n",
    "parser.add_argument('--augment', action='store_true',\n",
    "                  help='double the length of the feature vector by appending zeros to stabilist ODE learning')\n",
    "parser.add_argument('--alpha_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) alpha')\n",
    "parser.add_argument('--no_alpha_sigmoid', dest='no_alpha_sigmoid', action='store_true', help='apply sigmoid before multiplying by alpha')\n",
    "parser.add_argument('--beta_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) beta')\n",
    "parser.add_argument('--block', type=str, default='constant', help='constant, mixed, attention, SDE')\n",
    "parser.add_argument('--function', type=str, default='laplacian', help='laplacian, transformer, dorsey, GAT, SDE')\n",
    "# ODE args\n",
    "parser.add_argument('--method', type=str, default='dopri5',\n",
    "                  help=\"set the numerical solver: dopri5, euler, rk4, midpoint\")\n",
    "parser.add_argument('--step_size', type=float, default=1, help='fixed step size when using fixed step solvers e.g. rk4')\n",
    "parser.add_argument(\n",
    "    \"--adjoint_method\", type=str, default=\"adaptive_heun\",\n",
    "    help=\"set the numerical solver for the backward pass: dopri5, euler, rk4, midpoint\"\n",
    ")\n",
    "parser.add_argument('--adjoint_step_size', type=float, default=1, help='fixed step size when using fixed step adjoint solvers e.g. rk4')\n",
    "parser.add_argument('--adjoint', default=False, help='use the adjoint ODE method to reduce memory footprint')\n",
    "parser.add_argument('--tol_scale', type=float, default=1., help='multiplier for atol and rtol')\n",
    "parser.add_argument(\"--tol_scale_adjoint\", type=float, default=1.0,\n",
    "                  help=\"multiplier for adjoint_atol and adjoint_rtol\")\n",
    "parser.add_argument('--ode_blocks', type=int, default=1, help='number of ode blocks to run')\n",
    "parser.add_argument('--add_source', dest='add_source', action='store_true',\n",
    "                  help='If try get rid of alpha param and the beta*x0 source term')\n",
    "# SDE args\n",
    "parser.add_argument('--dt_min', type=float, default=1e-5, help='minimum timestep for the SDE solver')\n",
    "parser.add_argument('--dt', type=float, default=1e-3, help='fixed step size')\n",
    "parser.add_argument('--adaptive', dest='adaptive', action='store_true', help='use adaptive step sizes')\n",
    "# Attention args\n",
    "parser.add_argument('--leaky_relu_slope', type=float, default=0.2,\n",
    "                  help='slope of the negative part of the leaky relu used in attention')\n",
    "parser.add_argument('--attention_dropout', type=float, default=0., help='dropout of attention weights')\n",
    "parser.add_argument('--heads', type=int, default=4, help='number of attention heads')\n",
    "parser.add_argument('--attention_norm_idx', type=int, default=0, help='0 = normalise rows, 1 = normalise cols')\n",
    "parser.add_argument('--attention_dim', type=int, default=64,\n",
    "                  help='the size to project x to before calculating att scores')\n",
    "parser.add_argument('--mix_features', dest='mix_features', action='store_true',\n",
    "                  help='apply a feature transformation xW to the ODE')\n",
    "parser.add_argument(\"--max_nfe\", type=int, default=1000, help=\"Maximum number of function evaluations allowed.\")\n",
    "parser.add_argument('--reweight_attention', dest='reweight_attention', action='store_true', help=\"multiply attention scores by edge weights before softmax\")\n",
    "# regularisation args\n",
    "parser.add_argument('--jacobian_norm2', type=float, default=None, help=\"int_t ||df/dx||_F^2\")\n",
    "parser.add_argument('--total_deriv', type=float, default=None, help=\"int_t ||df/dt||^2\")\n",
    "\n",
    "parser.add_argument('--kinetic_energy', type=float, default=None, help=\"int_t ||f||_2^2\")\n",
    "parser.add_argument('--directional_penalty', type=float, default=None, help=\"int_t ||(df/dx)^T f||^2\")\n",
    "\n",
    "# rewiring args\n",
    "parser.add_argument('--rewiring', type=str, default=None, help=\"two_hop, gdc\")\n",
    "parser.add_argument('--gdc_method', type=str, default='ppr', help=\"ppr, heat, coeff\")\n",
    "parser.add_argument('--gdc_sparsification', type=str, default='topk', help=\"threshold, topk\")\n",
    "parser.add_argument('--gdc_k', type=int, default=64, help=\"number of neighbours to sparsify to when using topk\")\n",
    "parser.add_argument('--gdc_threshold', type=float, default=0.0001, help=\"obove this edge weight, keep edges when using threshold\")\n",
    "parser.add_argument('--gdc_avg_degree', type=int, default=64,\n",
    "                  help=\"if gdc_threshold is not given can be calculated by specifying avg degree\")\n",
    "parser.add_argument('--ppr_alpha', type=float, default=0.05, help=\"teleport probability\")\n",
    "parser.add_argument('--heat_time', type=float, default=3., help=\"time to run gdc heat kernal diffusion for\")\n",
    "\n",
    "# Stefan's experiment args\n",
    "parser.add_argument('--count_runs', type=int, default=10,\n",
    "                  help=\"number of runs to average results over per parameter settings for each experiment\")\n",
    "\n",
    "#added\n",
    "parser.add_argument('--beltrami', action='store_true', help='perform diffusion beltrami style')\n",
    "parser.add_argument('--use_mlp', dest='use_mlp', action='store_true',\n",
    "                  help='Add a fully connected layer to the encoder.')\n",
    "parser.add_argument('--use_labels', dest='use_labels', action='store_true', help='Also diffuse labels')\n",
    "parser.add_argument('--fc_out', dest='fc_out', action='store_true',\n",
    "                  help='Add a fully connected layer to the decoder.')\n",
    "parser.add_argument(\"--batch_norm\", dest='batch_norm', action='store_true', help='search over reg params')\n",
    "\n",
    "args = parser.parse_args(customArgs)\n",
    "opt = vars(args)\n",
    "opt = get_cora_opt(opt)\n",
    "\n",
    "opt['epoch'] = 31\n",
    "opt['adjoint'] = True\n",
    "#opt['method'] = 'explicit_adams'\n",
    "opt['method'] = 'implicit_adams'\n",
    "#opt['method'] = 'dopri5'\n",
    "opt['adjoint_method'] = opt['method']\n",
    "opt['max_iters'] = 10000\n",
    "opt['step_size'] = opt['dt_min'] = 0.01\n",
    "opt['tol_scale'] = 100.0\n",
    "opt['tol_scale_adjoint'] = 100.0\n",
    "#added\n",
    "opt['max_nfe'] = 100000\n",
    "\n",
    "# DEBUG\n",
    "#for k in ['dataset', 'epoch', 'adjoint', 'rewiring', 'adaptive', 'dt', 'dt_min', 'method', 'adjoint_method']:\n",
    "#  print(k, opt[k])\n",
    "#main(opt, 0)\n",
    "\n",
    "# Run combination of experiments\n",
    "for stepsize in [0.5, 0.25, 0.1, 0.01]: # 2.0, 1.0\n",
    "    print(f'*** Doing stepsize {stepsize} ***')\n",
    "    for idx in range(opt['count_runs']):\n",
    "        print(f'*** Doing run {idx} ***')\n",
    "        # NOTE: I think setting dt_min may not be necessary, doing it just to be safe!\n",
    "        opt['step_size'] = opt['dt_min'] = stepsize\n",
    "        run(opt, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ff871-6d10-462c-b9f4-c0d2d6dbc161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedf3b1-af5d-49d0-8ddb-834f9e6dea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
