{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faed1d68-9918-4d47-9a0f-6b7c1394515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeljon00/miniconda3/envs/grandtn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor, GNNBenchmarkDataset, Reddit2, Flickr\n",
    "from GNN import GNN\n",
    "from GNN_KNN import GNN_KNN\n",
    "import time\n",
    "from data import get_dataset, Data\n",
    "\n",
    "from graph_rewiring import get_two_hop, apply_gdc\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import to_undirected\n",
    "from graph_rewiring import make_symmetric, apply_pos_dist_rewire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c1e662-6fcf-4c95-97e4-41f5ce755be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customArgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7ee566-4b2c-462c-8437-0ec9d73eb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cora_opt(opt):\n",
    "  opt['dataset'] = 'Cora'\n",
    "  opt['data'] = 'Planetoid'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'rmsprop'\n",
    "  opt['lr'] = 0.0047\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 31\n",
    "  opt['augment'] = True\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['adjoint'] = False\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_computers_opt(opt):\n",
    "  opt['dataset'] = 'Computers'\n",
    "  opt['hidden_dim'] = 16\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.01\n",
    "  opt['decay'] = 5e-4\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 0.918\n",
    "  opt['epoch'] = 400\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 1433\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 2708\n",
    "  opt['epoch'] = 100\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  return opt\n",
    "\n",
    "def get_flickr_opt(opt):\n",
    "  opt['dataset'] = 'Flickr'\n",
    "  opt['hidden_dim'] = 128\n",
    "  opt['feature_hidden_dim'] = 64\n",
    "  opt['input_dropout'] = 0.5\n",
    "  opt['dropout'] = 0\n",
    "  opt['optimizer'] = 'adam'\n",
    "  opt['lr'] = 0.005451476553977102\n",
    "  opt['decay'] = 0\n",
    "  opt['self_loop_weight'] = 0.555\n",
    "  opt['alpha'] = 1.0\n",
    "  opt['time'] = 12.1\n",
    "  opt['num_feature'] = 500\n",
    "  opt['num_class'] = 7\n",
    "  opt['num_nodes'] = 89250\n",
    "  opt['epoch'] = 100\n",
    "  opt['attention_dropout'] = 0\n",
    "  opt['ode'] = 'ode'\n",
    "  opt['gdc_avg_degree']= 48\n",
    "  opt['gdc_k'] = 48\n",
    "  opt['gdc_method'] = 'ppr'\n",
    "  opt['gdc_sparsification'] = 'topk'\n",
    "  opt['gdc_threshold'] =  0.01\n",
    "  return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442b7c4e-3ad2-455a-907f-38f4f25067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name, parameters, lr, weight_decay=0):\n",
    "  if name == 'sgd':\n",
    "    return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'rmsprop':\n",
    "    return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adagrad':\n",
    "    return torch.optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adam':\n",
    "    return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  elif name == 'adamax':\n",
    "    return torch.optim.Adamax(parameters, lr=lr, weight_decay=weight_decay)\n",
    "  else:\n",
    "    raise Exception(\"Unsupported optimizer: {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768697b4-1f65-4c10-a8d8-3744ded132dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x)\n",
    "  lf = torch.nn.CrossEntropyLoss()\n",
    "  loss = lf(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "  # TODO: What is this block about???\n",
    "  if model.odeblock.nreg > 0:  # add regularisation - slower for small data, but faster and better performance for large data\n",
    "    reg_states = tuple(torch.mean(rs) for rs in model.reg_states)\n",
    "    regularization_coeffs = model.regularization_coeffs\n",
    "\n",
    "    reg_loss = sum(\n",
    "      reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
    "    )\n",
    "    loss = loss + reg_loss\n",
    "\n",
    "  # Update count of forward evaluations from ODE solver\n",
    "  # NOTE: fm stands for \"forward meter\"\n",
    "  # TODO: Rename this to be more informative!\n",
    "  model.fm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  # Gradient step\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Update count of backwards evaluations from ODE solver\n",
    "  model.bm.update(model.getNFE())\n",
    "  model.resetNFE()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "  model.eval()\n",
    "  logits, accs = model(data.x), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return accs\n",
    "\n",
    "def print_model_params(model):\n",
    "  print(model)\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "      print(name)\n",
    "      print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05dd71b0-cad8-45a6-ba14-e62cc96a8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_benchmark(opt: dict, data_dir, use_lcc: bool = False) -> InMemoryDataset:\n",
    "    ds = opt['dataset']\n",
    "    dataset = Flickr(root=os.path.join(data_dir,'Flickr'), transform=T.ToSparseTensor())#T.NormalizeFeatures())  \n",
    "\n",
    "    \"\"\"\n",
    "    d_train = GNNBenchmarkDataset(name=ds, root=path, split='train', transform=T.NormalizeFeatures())\n",
    "    d_val = GNNBenchmarkDataset(name=ds, root=path, split='val', transform=T.NormalizeFeatures())\n",
    "    d_test = GNNBenchmarkDataset(name=ds, root=path, split='test', transform=T.NormalizeFeatures())\n",
    "    \n",
    "    print(d_train.data)\n",
    "    print(d_val.data)\n",
    "    print(d_test.data)\n",
    "    print(torch.max(d_train.data.edge_index))\n",
    "    print(torch.max(d_val.data.edge_index))\n",
    "    print(torch.max(d_test.data.edge_index))\n",
    "    \n",
    "    d_temp = Planetoid(root=os.path.join(data_dir,'Cora'), name='Cora')\n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    d_temp = PygNodePropPredDataset(name='ogbn-arxiv', root=os.path.join(data_dir,'ogbn-arxiv'), transform=T.ToSparseTensor())\n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    d_temp = Flickr(root=os.path.join(data_dir,'Flickr'), transform=T.NormalizeFeatures())  \n",
    "    print(d_temp.data)\n",
    "    print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    #d_temp = Reddit2(root=os.path.join(data_dir,'Reddit2'), transform=T.NormalizeFeatures())\n",
    "    #print(d_temp.data)\n",
    "    #print(torch.max(d_temp.data.edge_index))\n",
    "    \n",
    "    return\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "\n",
    "        data = Data(\n",
    "          x=x_new,\n",
    "          edge_index=torch.LongTensor(edges),\n",
    "          y=y_new,\n",
    "          train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "          test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "          val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "    if opt['rewiring'] is not None:\n",
    "        dataset.data = rewire(dataset.data, opt, data_dir)\n",
    "    train_mask_exists = True\n",
    "    try:\n",
    "        dataset.data.train_mask\n",
    "    except AttributeError:\n",
    "        train_mask_exists = False\n",
    "\n",
    "    if ds == 'ogbn-arxiv':\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        ei = to_undirected(dataset.data.edge_index)\n",
    "        data = Data(\n",
    "        x=dataset.data.x,\n",
    "        edge_index=ei,\n",
    "        y=dataset.data.y,\n",
    "        train_mask=split_idx['train'],\n",
    "        test_mask=split_idx['test'],\n",
    "        val_mask=split_idx['valid'])\n",
    "        dataset.data = data\n",
    "        train_mask_exists = True\n",
    "\n",
    "    #todo this currently breaks with heterophilic datasets if you don't pass --geom_gcn_splits\n",
    "    if (use_lcc or not train_mask_exists) and not opt['geom_gcn_splits']:\n",
    "        dataset.data = set_train_val_test_split(\n",
    "          12345,\n",
    "          dataset.data,\n",
    "          num_development=5000 if ds == \"CoauthorCS\" else 1500)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433d8568-8f88-4feb-8e97-c466a24e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def run(opt, run_count):\n",
    "\n",
    "    # Load dataset and create model\n",
    "    if opt['dataset'] == 'Flickr':\n",
    "        dataset = get_dataset_benchmark(opt, '../data', False)\n",
    "    else:\n",
    "        dataset = get_dataset(opt, '../data', False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if opt['rewire_KNN'] or opt['fa_layer']:\n",
    "        model = GNN_KNN(opt, dataset, device).to(device)\n",
    "    else:\n",
    "        model = GNN(opt, dataset, device).to(device)\n",
    "    data = dataset.data.to(device)\n",
    "    #model, data = GNN(opt, dataset, device).to(device), dataset.data.to(device)\n",
    "    print(opt)\n",
    "\n",
    "    # Todo for some reason the submodule parameters inside the attention module don't show up when running on GPU.\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    print_model_params(model)\n",
    "\n",
    "    # Training/test loop\n",
    "    results = {\n",
    "        'time':[],\n",
    "        'loss':[],\n",
    "        'forward_nfe':[],\n",
    "        'backward_nfe':[],\n",
    "        'train_acc':[],\n",
    "        'test_acc':[],\n",
    "        'val_acc':[],\n",
    "        'best_epoch':0,\n",
    "        'best_val_acc':0.,\n",
    "        'best_test_acc':0.,\n",
    "    }\n",
    "    runtimes = []\n",
    "    losses = []\n",
    "\n",
    "    optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "    best_val_acc = test_acc = train_acc = best_epoch = 0\n",
    "    overall_time = time.time()\n",
    "    for epoch in range(1, opt['epoch']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss = train(model, optimizer, data)\n",
    "        train_acc, val_acc, test_acc = test(model, data)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_train_acc = train_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        results['time'].append(time.time() - start_time)\n",
    "        results['loss'].append(loss)\n",
    "        results['forward_nfe'].append(model.fm.sum)\n",
    "        results['backward_nfe'].append(model.bm.sum)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['val_acc'].append(val_acc)\n",
    "        results['best_epoch'] = best_epoch\n",
    "        results['best_train_acc'] = best_train_acc\n",
    "        results['best_val_acc'] = best_val_acc\n",
    "        results['best_test_acc'] = best_test_acc\n",
    "\n",
    "        log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        print(log.format(epoch, results['time'][-1], results['loss'][-1], results['forward_nfe'][-1], results['backward_nfe'][-1], results['train_acc'][-1], results['val_acc'][-1], results['test_acc'][-1]))\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d}'.format(best_val_acc, best_test_acc, best_epoch))\n",
    "    \n",
    "    results['all_epochs_time'] = time.time() - overall_time\n",
    "\n",
    "    # TODO: Save results\n",
    "    # cora_epoch_101_adjoint_false_... . pickle\n",
    "    pickle.dump( results, open( f\"../results/{opt['dataset']}_{opt['method']}_stepsize_{opt['step_size']}_run_{run_count}.pickle\", \"wb\" ) )\n",
    "\n",
    "    return train_acc, best_val_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3b2294-ed94-4c12-b2eb-1115d36300a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Doing stepsize 1.0 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeljon00/miniconda3/envs/grandtn/lib/python3.9/site-packages/torchdiffeq/_impl/fixed_adams.py:220: UserWarning: Functional iteration did not converge. Solution may be incorrect.\n",
      "  warnings.warn('Functional iteration did not converge. Solution may be incorrect.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Runtime 0.828876, Loss 2.311668, forward nfe 119, backward nfe 294, Train: 0.0950, Val: 0.0262, Test: 0.0386\n",
      "Epoch: 002, Runtime 1.912070, Loss 2.355083, forward nfe 2163, backward nfe 1318, Train: 0.1050, Val: 0.1346, Test: 0.1347\n",
      "Epoch: 003, Runtime 1.914775, Loss 2.346608, forward nfe 4207, backward nfe 2342, Train: 0.1850, Val: 0.1069, Test: 0.1167\n",
      "Epoch: 004, Runtime 1.914927, Loss 2.244116, forward nfe 6255, backward nfe 3366, Train: 0.1100, Val: 0.0808, Test: 0.0850\n",
      "Epoch: 005, Runtime 1.918711, Loss 2.242403, forward nfe 8302, backward nfe 4392, Train: 0.1850, Val: 0.1185, Test: 0.1318\n",
      "Epoch: 006, Runtime 1.942705, Loss 2.213710, forward nfe 10357, backward nfe 5426, Train: 0.2200, Val: 0.1754, Test: 0.1916\n",
      "Epoch: 007, Runtime 1.958310, Loss 2.170889, forward nfe 12428, backward nfe 6450, Train: 0.1700, Val: 0.1677, Test: 0.1644\n",
      "Epoch: 008, Runtime 1.994545, Loss 2.135077, forward nfe 14616, backward nfe 7483, Train: 0.2350, Val: 0.2254, Test: 0.2271\n",
      "Epoch: 009, Runtime 2.034891, Loss 2.124215, forward nfe 16854, backward nfe 8548, Train: 0.3050, Val: 0.2669, Test: 0.2791\n",
      "Epoch: 010, Runtime 2.048622, Loss 2.087226, forward nfe 19073, backward nfe 9617, Train: 0.3000, Val: 0.2915, Test: 0.3040\n",
      "Epoch: 011, Runtime 2.062070, Loss 2.067804, forward nfe 21245, backward nfe 10736, Train: 0.3000, Val: 0.2831, Test: 0.2890\n",
      "Epoch: 012, Runtime 2.078869, Loss 2.032021, forward nfe 23483, backward nfe 11839, Train: 0.3550, Val: 0.2592, Test: 0.2816\n",
      "Epoch: 013, Runtime 2.093084, Loss 2.003740, forward nfe 25721, backward nfe 12958, Train: 0.3800, Val: 0.2731, Test: 0.2948\n",
      "Epoch: 014, Runtime 2.091666, Loss 1.974745, forward nfe 27959, backward nfe 14075, Train: 0.4150, Val: 0.3038, Test: 0.3140\n",
      "Epoch: 015, Runtime 2.094147, Loss 1.942731, forward nfe 30197, backward nfe 15194, Train: 0.4250, Val: 0.2992, Test: 0.3125\n",
      "Epoch: 016, Runtime 2.092094, Loss 1.907694, forward nfe 32435, backward nfe 16313, Train: 0.3650, Val: 0.2146, Test: 0.2438\n",
      "Epoch: 017, Runtime 2.017809, Loss 1.899257, forward nfe 34673, backward nfe 17348, Train: 0.4550, Val: 0.2838, Test: 0.3048\n",
      "Epoch: 018, Runtime 2.095381, Loss 1.842998, forward nfe 36911, backward nfe 18467, Train: 0.4150, Val: 0.2862, Test: 0.3018\n",
      "Epoch: 019, Runtime 2.019738, Loss 1.817930, forward nfe 39149, backward nfe 19505, Train: 0.4550, Val: 0.2892, Test: 0.3147\n",
      "Epoch: 020, Runtime 2.093543, Loss 1.796388, forward nfe 41387, backward nfe 20624, Train: 0.4650, Val: 0.3062, Test: 0.3250\n",
      "Epoch: 021, Runtime 2.096400, Loss 1.757180, forward nfe 43625, backward nfe 21743, Train: 0.4400, Val: 0.3015, Test: 0.3185\n",
      "Epoch: 022, Runtime 2.094231, Loss 1.729087, forward nfe 45863, backward nfe 22862, Train: 0.4650, Val: 0.3169, Test: 0.3275\n",
      "Epoch: 023, Runtime 2.044355, Loss 1.700226, forward nfe 48101, backward nfe 23928, Train: 0.4850, Val: 0.3246, Test: 0.3480\n",
      "Epoch: 024, Runtime 2.098327, Loss 1.667843, forward nfe 50339, backward nfe 25047, Train: 0.5200, Val: 0.3154, Test: 0.3405\n",
      "Epoch: 025, Runtime 2.092393, Loss 1.631468, forward nfe 52577, backward nfe 26166, Train: 0.5050, Val: 0.3054, Test: 0.3360\n",
      "Epoch: 026, Runtime 2.051166, Loss 1.607278, forward nfe 54815, backward nfe 27238, Train: 0.5000, Val: 0.3192, Test: 0.3417\n",
      "Epoch: 027, Runtime 2.098643, Loss 1.570981, forward nfe 57053, backward nfe 28357, Train: 0.4900, Val: 0.3308, Test: 0.3579\n",
      "Epoch: 028, Runtime 2.086942, Loss 1.538306, forward nfe 59279, backward nfe 29476, Train: 0.5250, Val: 0.3469, Test: 0.3731\n",
      "Epoch: 029, Runtime 2.104697, Loss 1.509403, forward nfe 61517, backward nfe 30595, Train: 0.5500, Val: 0.3492, Test: 0.3797\n",
      "Epoch: 030, Runtime 2.018243, Loss 1.475771, forward nfe 63755, backward nfe 31630, Train: 0.5900, Val: 0.3431, Test: 0.3777\n",
      "Epoch: 031, Runtime 2.039327, Loss 1.443029, forward nfe 65993, backward nfe 32688, Train: 0.6000, Val: 0.3592, Test: 0.3953\n",
      "Epoch: 032, Runtime 2.013802, Loss 1.410608, forward nfe 68231, backward nfe 33720, Train: 0.5950, Val: 0.3785, Test: 0.4268\n",
      "Epoch: 033, Runtime 2.081675, Loss 1.381448, forward nfe 70469, backward nfe 34839, Train: 0.6050, Val: 0.4192, Test: 0.4592\n",
      "Epoch: 034, Runtime 2.049804, Loss 1.353392, forward nfe 72667, backward nfe 35908, Train: 0.6150, Val: 0.4277, Test: 0.4706\n",
      "Epoch: 035, Runtime 2.028453, Loss 1.320982, forward nfe 74842, backward nfe 36985, Train: 0.6250, Val: 0.4346, Test: 0.4755\n",
      "Epoch: 036, Runtime 2.096104, Loss 1.292804, forward nfe 77080, backward nfe 38104, Train: 0.6350, Val: 0.4777, Test: 0.5109\n",
      "Epoch: 037, Runtime 2.051644, Loss 1.268349, forward nfe 79318, backward nfe 39176, Train: 0.6400, Val: 0.5585, Test: 0.5866\n",
      "Epoch: 038, Runtime 1.996411, Loss 1.243864, forward nfe 81488, backward nfe 40221, Train: 0.6350, Val: 0.5854, Test: 0.6077\n",
      "Epoch: 039, Runtime 2.099049, Loss 1.219698, forward nfe 83726, backward nfe 41340, Train: 0.6550, Val: 0.5869, Test: 0.6152\n",
      "Epoch: 040, Runtime 2.023737, Loss 1.190901, forward nfe 85958, backward nfe 42386, Train: 0.6900, Val: 0.5977, Test: 0.6203\n",
      "Epoch: 041, Runtime 2.003600, Loss 1.175062, forward nfe 88174, backward nfe 43415, Train: 0.7050, Val: 0.5977, Test: 0.6204\n",
      "Epoch: 042, Runtime 2.096726, Loss 1.161962, forward nfe 90412, backward nfe 44534, Train: 0.7000, Val: 0.5985, Test: 0.6225\n",
      "Epoch: 043, Runtime 2.020933, Loss 1.129336, forward nfe 92650, backward nfe 45572, Train: 0.7050, Val: 0.5969, Test: 0.6232\n",
      "Epoch: 044, Runtime 1.962949, Loss 1.106216, forward nfe 94793, backward nfe 46636, Train: 0.7250, Val: 0.5969, Test: 0.6287\n",
      "Epoch: 045, Runtime 1.986350, Loss 1.083725, forward nfe 96886, backward nfe 47686, Train: 0.7250, Val: 0.5885, Test: 0.6259\n",
      "Epoch: 046, Runtime 2.005381, Loss 1.063745, forward nfe 99089, backward nfe 48710, Train: 0.7250, Val: 0.5992, Test: 0.6272\n",
      "Epoch: 047, Runtime 1.950644, Loss 1.047377, forward nfe 101241, backward nfe 49752, Train: 0.7350, Val: 0.6077, Test: 0.6339\n",
      "Epoch: 048, Runtime 2.023484, Loss 1.030494, forward nfe 103403, backward nfe 50814, Train: 0.7350, Val: 0.6123, Test: 0.6405\n",
      "Epoch: 049, Runtime 1.947060, Loss 0.997877, forward nfe 105524, backward nfe 51853, Train: 0.7350, Val: 0.6092, Test: 0.6432\n",
      "Epoch: 050, Runtime 2.023228, Loss 0.981382, forward nfe 107674, backward nfe 52910, Train: 0.7500, Val: 0.6062, Test: 0.6414\n",
      "Epoch: 051, Runtime 1.968368, Loss 0.972724, forward nfe 109834, backward nfe 53941, Train: 0.7650, Val: 0.6231, Test: 0.6460\n",
      "Epoch: 052, Runtime 1.931875, Loss 0.957244, forward nfe 111914, backward nfe 54968, Train: 0.7550, Val: 0.6238, Test: 0.6448\n",
      "Epoch: 053, Runtime 1.976352, Loss 0.947875, forward nfe 114030, backward nfe 55998, Train: 0.7750, Val: 0.6246, Test: 0.6502\n",
      "Epoch: 054, Runtime 1.962844, Loss 0.916692, forward nfe 116156, backward nfe 57030, Train: 0.7850, Val: 0.6269, Test: 0.6561\n",
      "Epoch: 055, Runtime 2.016250, Loss 0.900704, forward nfe 118301, backward nfe 58123, Train: 0.7900, Val: 0.6331, Test: 0.6602\n",
      "Epoch: 056, Runtime 1.931386, Loss 0.887498, forward nfe 120406, backward nfe 59149, Train: 0.7850, Val: 0.6346, Test: 0.6622\n",
      "Epoch: 057, Runtime 1.937332, Loss 0.860965, forward nfe 122464, backward nfe 60180, Train: 0.7800, Val: 0.6385, Test: 0.6603\n",
      "Epoch: 058, Runtime 1.965684, Loss 0.851021, forward nfe 124538, backward nfe 61224, Train: 0.7850, Val: 0.6369, Test: 0.6610\n",
      "Epoch: 059, Runtime 1.949072, Loss 0.839949, forward nfe 126649, backward nfe 62253, Train: 0.7900, Val: 0.6423, Test: 0.6666\n",
      "Epoch: 060, Runtime 1.944789, Loss 0.821351, forward nfe 128771, backward nfe 63278, Train: 0.7900, Val: 0.6492, Test: 0.6735\n",
      "Epoch: 061, Runtime 1.930573, Loss 0.807764, forward nfe 130832, backward nfe 64305, Train: 0.8000, Val: 0.6546, Test: 0.6810\n",
      "Epoch: 062, Runtime 1.930999, Loss 0.784464, forward nfe 132881, backward nfe 65329, Train: 0.8150, Val: 0.6608, Test: 0.6799\n",
      "Epoch: 063, Runtime 1.931114, Loss 0.784209, forward nfe 134958, backward nfe 66357, Train: 0.8200, Val: 0.6515, Test: 0.6765\n",
      "Epoch: 064, Runtime 1.923490, Loss 0.753247, forward nfe 137025, backward nfe 67381, Train: 0.8200, Val: 0.6508, Test: 0.6732\n",
      "Epoch: 065, Runtime 1.930498, Loss 0.763047, forward nfe 139078, backward nfe 68406, Train: 0.8300, Val: 0.6546, Test: 0.6759\n",
      "Epoch: 066, Runtime 1.930621, Loss 0.730695, forward nfe 141154, backward nfe 69431, Train: 0.8350, Val: 0.6577, Test: 0.6810\n",
      "Epoch: 067, Runtime 1.924454, Loss 0.739097, forward nfe 143218, backward nfe 70458, Train: 0.8400, Val: 0.6654, Test: 0.6912\n",
      "Epoch: 068, Runtime 1.931708, Loss 0.719377, forward nfe 145272, backward nfe 71488, Train: 0.8400, Val: 0.6715, Test: 0.6960\n",
      "Epoch: 069, Runtime 1.921857, Loss 0.699319, forward nfe 147332, backward nfe 72515, Train: 0.8400, Val: 0.6754, Test: 0.6944\n",
      "Epoch: 070, Runtime 1.923967, Loss 0.692192, forward nfe 149391, backward nfe 73540, Train: 0.8450, Val: 0.6700, Test: 0.6905\n",
      "Epoch: 071, Runtime 1.923788, Loss 0.667620, forward nfe 151444, backward nfe 74567, Train: 0.8400, Val: 0.6646, Test: 0.6889\n",
      "Epoch: 072, Runtime 1.933743, Loss 0.660024, forward nfe 153509, backward nfe 75593, Train: 0.8350, Val: 0.6669, Test: 0.6904\n",
      "Epoch: 073, Runtime 1.929549, Loss 0.652119, forward nfe 155577, backward nfe 76622, Train: 0.8500, Val: 0.6731, Test: 0.6983\n",
      "Epoch: 074, Runtime 1.927668, Loss 0.656931, forward nfe 157632, backward nfe 77651, Train: 0.8350, Val: 0.6769, Test: 0.6992\n",
      "Epoch: 075, Runtime 1.929779, Loss 0.628224, forward nfe 159690, backward nfe 78680, Train: 0.8400, Val: 0.6715, Test: 0.6966\n",
      "Epoch: 076, Runtime 1.922907, Loss 0.619822, forward nfe 161744, backward nfe 79705, Train: 0.8400, Val: 0.6762, Test: 0.6964\n",
      "Epoch: 077, Runtime 1.927088, Loss 0.605834, forward nfe 163797, backward nfe 80732, Train: 0.8400, Val: 0.6808, Test: 0.7011\n",
      "Epoch: 078, Runtime 1.932854, Loss 0.606661, forward nfe 165848, backward nfe 81768, Train: 0.8450, Val: 0.6854, Test: 0.7085\n",
      "Epoch: 079, Runtime 1.921735, Loss 0.590981, forward nfe 167894, backward nfe 82795, Train: 0.8400, Val: 0.6869, Test: 0.7116\n",
      "Epoch: 080, Runtime 1.921897, Loss 0.600149, forward nfe 169946, backward nfe 83821, Train: 0.8500, Val: 0.6862, Test: 0.7107\n",
      "Epoch: 081, Runtime 1.921405, Loss 0.568802, forward nfe 171993, backward nfe 84846, Train: 0.8400, Val: 0.6815, Test: 0.7007\n",
      "Epoch: 082, Runtime 1.925163, Loss 0.556142, forward nfe 174042, backward nfe 85876, Train: 0.8450, Val: 0.6777, Test: 0.7018\n",
      "Epoch: 083, Runtime 1.920353, Loss 0.535283, forward nfe 176088, backward nfe 86900, Train: 0.8500, Val: 0.6808, Test: 0.7057\n",
      "Epoch: 084, Runtime 1.924771, Loss 0.537354, forward nfe 178140, backward nfe 87925, Train: 0.8450, Val: 0.6900, Test: 0.7094\n",
      "Epoch: 085, Runtime 1.921569, Loss 0.540108, forward nfe 180189, backward nfe 88953, Train: 0.8450, Val: 0.6877, Test: 0.7094\n",
      "Epoch: 086, Runtime 1.921251, Loss 0.522224, forward nfe 182235, backward nfe 89977, Train: 0.8550, Val: 0.6954, Test: 0.7109\n",
      "Epoch: 087, Runtime 1.920396, Loss 0.525585, forward nfe 184284, backward nfe 91001, Train: 0.8450, Val: 0.6946, Test: 0.7162\n",
      "Epoch: 088, Runtime 1.922205, Loss 0.511708, forward nfe 186331, backward nfe 92029, Train: 0.8500, Val: 0.6931, Test: 0.7173\n",
      "Epoch: 089, Runtime 1.927009, Loss 0.511530, forward nfe 188377, backward nfe 93053, Train: 0.8500, Val: 0.6954, Test: 0.7169\n",
      "Epoch: 090, Runtime 1.924698, Loss 0.518406, forward nfe 190430, backward nfe 94079, Train: 0.8500, Val: 0.6923, Test: 0.7167\n",
      "Epoch: 091, Runtime 1.927911, Loss 0.510005, forward nfe 192479, backward nfe 95110, Train: 0.8500, Val: 0.6954, Test: 0.7181\n",
      "Epoch: 092, Runtime 1.924791, Loss 0.494392, forward nfe 194530, backward nfe 96139, Train: 0.8500, Val: 0.6962, Test: 0.7187\n",
      "Epoch: 093, Runtime 1.919225, Loss 0.491210, forward nfe 196577, backward nfe 97163, Train: 0.8550, Val: 0.6977, Test: 0.7191\n",
      "Epoch: 094, Runtime 1.918923, Loss 0.482264, forward nfe 198623, backward nfe 98189, Train: 0.8600, Val: 0.6969, Test: 0.7151\n",
      "Epoch: 095, Runtime 1.923182, Loss 0.483394, forward nfe 200673, backward nfe 99215, Train: 0.8600, Val: 0.6969, Test: 0.7151\n",
      "Epoch: 096, Runtime 1.921579, Loss 0.462997, forward nfe 202719, backward nfe 100240, Train: 0.8500, Val: 0.7008, Test: 0.7224\n",
      "Epoch: 097, Runtime 1.918233, Loss 0.464346, forward nfe 204767, backward nfe 101265, Train: 0.8600, Val: 0.7154, Test: 0.7337\n",
      "Epoch: 098, Runtime 1.919865, Loss 0.455757, forward nfe 206813, backward nfe 102288, Train: 0.8550, Val: 0.7162, Test: 0.7402\n",
      "Epoch: 099, Runtime 1.930208, Loss 0.453506, forward nfe 208862, backward nfe 103321, Train: 0.8550, Val: 0.7131, Test: 0.7393\n",
      "best val accuracy 0.716154 with test accuracy 0.740206 at epoch 98\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.764802, Loss 2.313539, forward nfe 113, backward nfe 228, Train: 0.1000, Val: 0.0569, Test: 0.0596\n",
      "Epoch: 002, Runtime 1.920503, Loss 2.578533, forward nfe 2157, backward nfe 1252, Train: 0.1100, Val: 0.1038, Test: 0.1039\n",
      "Epoch: 003, Runtime 1.920713, Loss 2.298977, forward nfe 4202, backward nfe 2276, Train: 0.1000, Val: 0.1038, Test: 0.1031\n",
      "Epoch: 004, Runtime 1.919701, Loss 2.310815, forward nfe 6247, backward nfe 3300, Train: 0.1000, Val: 0.1638, Test: 0.1567\n",
      "Epoch: 005, Runtime 1.924549, Loss 2.308787, forward nfe 8292, backward nfe 4324, Train: 0.1250, Val: 0.1815, Test: 0.1763\n",
      "Epoch: 006, Runtime 1.923890, Loss 2.302875, forward nfe 10343, backward nfe 5349, Train: 0.1400, Val: 0.1692, Test: 0.1597\n",
      "Epoch: 007, Runtime 1.949640, Loss 2.242468, forward nfe 12400, backward nfe 6384, Train: 0.1200, Val: 0.1085, Test: 0.1082\n",
      "Epoch: 008, Runtime 2.013845, Loss 2.254236, forward nfe 14577, backward nfe 7412, Train: 0.1450, Val: 0.1346, Test: 0.1343\n",
      "Epoch: 009, Runtime 2.070407, Loss 2.228539, forward nfe 16753, backward nfe 8531, Train: 0.2200, Val: 0.2585, Test: 0.2559\n",
      "Epoch: 010, Runtime 2.085198, Loss 2.243448, forward nfe 18962, backward nfe 9650, Train: 0.2300, Val: 0.2654, Test: 0.2601\n",
      "Epoch: 011, Runtime 2.099462, Loss 2.218975, forward nfe 21200, backward nfe 10769, Train: 0.1200, Val: 0.1085, Test: 0.1072\n",
      "Epoch: 012, Runtime 2.096912, Loss 2.207702, forward nfe 23438, backward nfe 11888, Train: 0.1050, Val: 0.0915, Test: 0.0896\n",
      "Epoch: 013, Runtime 2.099501, Loss 2.211203, forward nfe 25676, backward nfe 13007, Train: 0.1250, Val: 0.1546, Test: 0.1456\n",
      "Epoch: 014, Runtime 2.098836, Loss 2.182470, forward nfe 27914, backward nfe 14126, Train: 0.2000, Val: 0.2408, Test: 0.2334\n",
      "Epoch: 015, Runtime 2.097106, Loss 2.181114, forward nfe 30152, backward nfe 15245, Train: 0.2050, Val: 0.1585, Test: 0.1631\n",
      "Epoch: 016, Runtime 2.099393, Loss 2.171268, forward nfe 32390, backward nfe 16364, Train: 0.1850, Val: 0.1131, Test: 0.1217\n",
      "Epoch: 017, Runtime 2.099420, Loss 2.138132, forward nfe 34628, backward nfe 17483, Train: 0.1850, Val: 0.0892, Test: 0.1024\n",
      "Epoch: 018, Runtime 2.096865, Loss 2.132300, forward nfe 36866, backward nfe 18602, Train: 0.1950, Val: 0.0900, Test: 0.0968\n",
      "Epoch: 019, Runtime 2.100429, Loss 2.117388, forward nfe 39104, backward nfe 19721, Train: 0.1950, Val: 0.0862, Test: 0.0964\n",
      "Epoch: 020, Runtime 2.098579, Loss 2.089757, forward nfe 41342, backward nfe 20840, Train: 0.2050, Val: 0.0854, Test: 0.0979\n",
      "Epoch: 021, Runtime 2.097167, Loss 2.073561, forward nfe 43580, backward nfe 21959, Train: 0.2000, Val: 0.1069, Test: 0.1124\n",
      "Epoch: 022, Runtime 2.099876, Loss 2.052379, forward nfe 45818, backward nfe 23078, Train: 0.2250, Val: 0.1369, Test: 0.1397\n",
      "Epoch: 023, Runtime 2.102082, Loss 2.035959, forward nfe 48056, backward nfe 24197, Train: 0.2250, Val: 0.1423, Test: 0.1461\n",
      "Epoch: 024, Runtime 2.100711, Loss 2.024077, forward nfe 50294, backward nfe 25316, Train: 0.2250, Val: 0.1254, Test: 0.1321\n",
      "Epoch: 025, Runtime 2.098199, Loss 1.995364, forward nfe 52532, backward nfe 26435, Train: 0.2150, Val: 0.1085, Test: 0.1182\n",
      "Epoch: 026, Runtime 2.097851, Loss 1.977601, forward nfe 54770, backward nfe 27554, Train: 0.2150, Val: 0.1038, Test: 0.1127\n",
      "Epoch: 027, Runtime 2.094064, Loss 1.959281, forward nfe 57008, backward nfe 28673, Train: 0.2200, Val: 0.1062, Test: 0.1161\n",
      "Epoch: 028, Runtime 2.098355, Loss 1.940023, forward nfe 59246, backward nfe 29792, Train: 0.2200, Val: 0.1169, Test: 0.1217\n",
      "Epoch: 029, Runtime 2.097258, Loss 1.930290, forward nfe 61484, backward nfe 30911, Train: 0.2300, Val: 0.1223, Test: 0.1254\n",
      "Epoch: 030, Runtime 2.097741, Loss 1.908414, forward nfe 63722, backward nfe 32030, Train: 0.2400, Val: 0.1262, Test: 0.1313\n",
      "Epoch: 031, Runtime 2.100253, Loss 1.889533, forward nfe 65960, backward nfe 33149, Train: 0.2350, Val: 0.1362, Test: 0.1434\n",
      "Epoch: 032, Runtime 2.099482, Loss 1.873716, forward nfe 68198, backward nfe 34268, Train: 0.2350, Val: 0.1562, Test: 0.1608\n",
      "Epoch: 033, Runtime 2.097494, Loss 1.856167, forward nfe 70436, backward nfe 35387, Train: 0.2400, Val: 0.1708, Test: 0.1756\n",
      "Epoch: 034, Runtime 2.099129, Loss 1.844894, forward nfe 72674, backward nfe 36506, Train: 0.2400, Val: 0.1638, Test: 0.1685\n",
      "Epoch: 035, Runtime 2.105928, Loss 1.823972, forward nfe 74912, backward nfe 37625, Train: 0.2500, Val: 0.1454, Test: 0.1574\n",
      "Epoch: 036, Runtime 2.098854, Loss 1.811906, forward nfe 77150, backward nfe 38744, Train: 0.2550, Val: 0.1492, Test: 0.1579\n",
      "Epoch: 037, Runtime 2.097516, Loss 1.797832, forward nfe 79388, backward nfe 39863, Train: 0.2700, Val: 0.1608, Test: 0.1730\n",
      "Epoch: 038, Runtime 2.097890, Loss 1.782961, forward nfe 81626, backward nfe 40982, Train: 0.2750, Val: 0.1777, Test: 0.1910\n",
      "Epoch: 039, Runtime 2.099337, Loss 1.774392, forward nfe 83864, backward nfe 42101, Train: 0.2850, Val: 0.1892, Test: 0.2051\n",
      "Epoch: 040, Runtime 2.096844, Loss 1.761653, forward nfe 86102, backward nfe 43220, Train: 0.2850, Val: 0.1977, Test: 0.2151\n",
      "Epoch: 041, Runtime 2.098500, Loss 1.740965, forward nfe 88340, backward nfe 44339, Train: 0.2850, Val: 0.2154, Test: 0.2306\n",
      "Epoch: 042, Runtime 2.096400, Loss 1.728781, forward nfe 90578, backward nfe 45458, Train: 0.3050, Val: 0.2208, Test: 0.2315\n",
      "Epoch: 043, Runtime 2.058752, Loss 1.717325, forward nfe 92816, backward nfe 46533, Train: 0.3250, Val: 0.2000, Test: 0.2138\n",
      "Epoch: 044, Runtime 2.097261, Loss 1.698931, forward nfe 95054, backward nfe 47652, Train: 0.3350, Val: 0.1892, Test: 0.2027\n",
      "Epoch: 045, Runtime 2.073050, Loss 1.695988, forward nfe 97292, backward nfe 48743, Train: 0.3500, Val: 0.2031, Test: 0.2156\n",
      "Epoch: 046, Runtime 2.099955, Loss 1.682576, forward nfe 99530, backward nfe 49862, Train: 0.3600, Val: 0.2254, Test: 0.2427\n",
      "Epoch: 047, Runtime 2.036078, Loss 1.667542, forward nfe 101768, backward nfe 50913, Train: 0.3650, Val: 0.2415, Test: 0.2599\n",
      "Epoch: 048, Runtime 2.010008, Loss 1.663721, forward nfe 104006, backward nfe 51938, Train: 0.3750, Val: 0.2415, Test: 0.2606\n",
      "Epoch: 049, Runtime 2.014750, Loss 1.644829, forward nfe 106244, backward nfe 52966, Train: 0.3700, Val: 0.2323, Test: 0.2510\n",
      "Epoch: 050, Runtime 2.098158, Loss 1.633986, forward nfe 108482, backward nfe 54085, Train: 0.3650, Val: 0.2123, Test: 0.2329\n",
      "Epoch: 051, Runtime 2.089563, Loss 1.617609, forward nfe 110720, backward nfe 55192, Train: 0.3750, Val: 0.2200, Test: 0.2408\n",
      "Epoch: 052, Runtime 2.036306, Loss 1.608792, forward nfe 112958, backward nfe 56245, Train: 0.3850, Val: 0.2338, Test: 0.2568\n",
      "Epoch: 053, Runtime 2.082834, Loss 1.603073, forward nfe 115196, backward nfe 57348, Train: 0.4200, Val: 0.2554, Test: 0.2764\n",
      "Epoch: 054, Runtime 2.098843, Loss 1.593582, forward nfe 117434, backward nfe 58467, Train: 0.4100, Val: 0.2500, Test: 0.2705\n",
      "Epoch: 055, Runtime 2.098237, Loss 1.573988, forward nfe 119672, backward nfe 59586, Train: 0.3800, Val: 0.2469, Test: 0.2574\n",
      "Epoch: 056, Runtime 2.024833, Loss 1.562123, forward nfe 121910, backward nfe 60626, Train: 0.4200, Val: 0.2454, Test: 0.2689\n",
      "Epoch: 057, Runtime 2.062479, Loss 1.549582, forward nfe 124148, backward nfe 61730, Train: 0.4450, Val: 0.2808, Test: 0.2991\n",
      "Epoch: 058, Runtime 2.021526, Loss 1.533712, forward nfe 126341, backward nfe 62768, Train: 0.4500, Val: 0.3069, Test: 0.3187\n",
      "Epoch: 059, Runtime 2.003739, Loss 1.525134, forward nfe 128579, backward nfe 63796, Train: 0.4900, Val: 0.3392, Test: 0.3533\n",
      "Epoch: 060, Runtime 2.009106, Loss 1.519134, forward nfe 130796, backward nfe 64843, Train: 0.5350, Val: 0.3454, Test: 0.3599\n",
      "Epoch: 061, Runtime 1.988039, Loss 1.510694, forward nfe 132920, backward nfe 65886, Train: 0.5200, Val: 0.3615, Test: 0.3740\n",
      "Epoch: 062, Runtime 1.968024, Loss 1.494425, forward nfe 135093, backward nfe 66930, Train: 0.5350, Val: 0.3646, Test: 0.3808\n",
      "Epoch: 063, Runtime 1.950244, Loss 1.477109, forward nfe 137150, backward nfe 67957, Train: 0.5450, Val: 0.3754, Test: 0.3852\n",
      "Epoch: 064, Runtime 1.969564, Loss 1.459632, forward nfe 139346, backward nfe 68983, Train: 0.5350, Val: 0.3669, Test: 0.3790\n",
      "Epoch: 065, Runtime 1.937083, Loss 1.453050, forward nfe 141406, backward nfe 70014, Train: 0.5500, Val: 0.3738, Test: 0.3855\n",
      "Epoch: 066, Runtime 1.940106, Loss 1.435530, forward nfe 143483, backward nfe 71048, Train: 0.5450, Val: 0.3823, Test: 0.3978\n",
      "Epoch: 067, Runtime 1.982233, Loss 1.424157, forward nfe 145584, backward nfe 72082, Train: 0.5650, Val: 0.3915, Test: 0.4048\n",
      "Epoch: 068, Runtime 1.949717, Loss 1.410881, forward nfe 147708, backward nfe 73125, Train: 0.5800, Val: 0.3908, Test: 0.4064\n",
      "Epoch: 069, Runtime 1.981612, Loss 1.402711, forward nfe 149790, backward nfe 74165, Train: 0.5650, Val: 0.3823, Test: 0.3935\n",
      "Epoch: 070, Runtime 1.932906, Loss 1.391994, forward nfe 151911, backward nfe 75200, Train: 0.5500, Val: 0.3815, Test: 0.3967\n",
      "Epoch: 071, Runtime 1.925402, Loss 1.385361, forward nfe 153959, backward nfe 76228, Train: 0.5850, Val: 0.3985, Test: 0.4154\n",
      "Epoch: 072, Runtime 1.939969, Loss 1.364978, forward nfe 156013, backward nfe 77253, Train: 0.5800, Val: 0.4000, Test: 0.4132\n",
      "Epoch: 073, Runtime 1.941036, Loss 1.344202, forward nfe 158093, backward nfe 78288, Train: 0.5600, Val: 0.3862, Test: 0.4021\n",
      "Epoch: 074, Runtime 1.933716, Loss 1.340331, forward nfe 160165, backward nfe 79312, Train: 0.5750, Val: 0.3969, Test: 0.4133\n",
      "Epoch: 075, Runtime 1.925772, Loss 1.337489, forward nfe 162228, backward nfe 80336, Train: 0.5800, Val: 0.3962, Test: 0.4102\n",
      "Epoch: 076, Runtime 1.956378, Loss 1.311143, forward nfe 164294, backward nfe 81360, Train: 0.5500, Val: 0.3892, Test: 0.3999\n",
      "Epoch: 077, Runtime 1.930996, Loss 1.312565, forward nfe 166408, backward nfe 82387, Train: 0.5900, Val: 0.3985, Test: 0.4112\n",
      "Epoch: 078, Runtime 1.928216, Loss 1.285348, forward nfe 168457, backward nfe 83413, Train: 0.6000, Val: 0.3985, Test: 0.4124\n",
      "Epoch: 079, Runtime 1.938911, Loss 1.285246, forward nfe 170512, backward nfe 84440, Train: 0.5800, Val: 0.3923, Test: 0.4074\n",
      "Epoch: 080, Runtime 1.929410, Loss 1.274722, forward nfe 172572, backward nfe 85464, Train: 0.5750, Val: 0.3877, Test: 0.4012\n",
      "Epoch: 081, Runtime 1.938327, Loss 1.269578, forward nfe 174649, backward nfe 86491, Train: 0.6250, Val: 0.4000, Test: 0.4114\n",
      "Epoch: 082, Runtime 1.932256, Loss 1.250223, forward nfe 176704, backward nfe 87524, Train: 0.6300, Val: 0.4008, Test: 0.4117\n",
      "Epoch: 083, Runtime 1.926847, Loss 1.242134, forward nfe 178753, backward nfe 88551, Train: 0.5650, Val: 0.3792, Test: 0.3960\n",
      "Epoch: 084, Runtime 1.929779, Loss 1.247205, forward nfe 180806, backward nfe 89585, Train: 0.5600, Val: 0.3723, Test: 0.3907\n",
      "Epoch: 085, Runtime 1.927150, Loss 1.239997, forward nfe 182869, backward nfe 90609, Train: 0.6150, Val: 0.3969, Test: 0.4083\n",
      "Epoch: 086, Runtime 1.923927, Loss 1.218460, forward nfe 184918, backward nfe 91635, Train: 0.6300, Val: 0.4000, Test: 0.4101\n",
      "Epoch: 087, Runtime 1.925209, Loss 1.215648, forward nfe 186968, backward nfe 92662, Train: 0.5950, Val: 0.3846, Test: 0.4036\n",
      "Epoch: 088, Runtime 1.926298, Loss 1.216430, forward nfe 189016, backward nfe 93688, Train: 0.5900, Val: 0.3823, Test: 0.4030\n",
      "Epoch: 089, Runtime 1.923124, Loss 1.198362, forward nfe 191066, backward nfe 94712, Train: 0.6300, Val: 0.3985, Test: 0.4119\n",
      "Epoch: 090, Runtime 1.921421, Loss 1.188165, forward nfe 193116, backward nfe 95736, Train: 0.6400, Val: 0.3992, Test: 0.4105\n",
      "Epoch: 091, Runtime 1.921207, Loss 1.197535, forward nfe 195165, backward nfe 96760, Train: 0.5900, Val: 0.3862, Test: 0.4037\n",
      "Epoch: 092, Runtime 1.921150, Loss 1.169465, forward nfe 197212, backward nfe 97784, Train: 0.6000, Val: 0.3877, Test: 0.4039\n",
      "Epoch: 093, Runtime 1.921686, Loss 1.181100, forward nfe 199260, backward nfe 98808, Train: 0.6300, Val: 0.4000, Test: 0.4107\n",
      "Epoch: 094, Runtime 1.920150, Loss 1.168115, forward nfe 201305, backward nfe 99832, Train: 0.6500, Val: 0.3992, Test: 0.4118\n",
      "Epoch: 095, Runtime 1.921321, Loss 1.147225, forward nfe 203351, backward nfe 100856, Train: 0.6450, Val: 0.3862, Test: 0.4074\n",
      "Epoch: 096, Runtime 1.920825, Loss 1.153884, forward nfe 205398, backward nfe 101880, Train: 0.6300, Val: 0.3862, Test: 0.4063\n",
      "Epoch: 097, Runtime 1.922163, Loss 1.144389, forward nfe 207442, backward nfe 102905, Train: 0.6500, Val: 0.3931, Test: 0.4096\n",
      "Epoch: 098, Runtime 1.920132, Loss 1.133428, forward nfe 209489, backward nfe 103928, Train: 0.6350, Val: 0.4000, Test: 0.4123\n",
      "Epoch: 099, Runtime 1.921709, Loss 1.144378, forward nfe 211535, backward nfe 104953, Train: 0.6400, Val: 0.3946, Test: 0.4094\n",
      "best val accuracy 0.400769 with test accuracy 0.411688 at epoch 82\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.837946, Loss 2.321291, forward nfe 122, backward nfe 303, Train: 0.0900, Val: 0.1100, Test: 0.1105\n",
      "Epoch: 002, Runtime 1.923542, Loss 2.534184, forward nfe 2166, backward nfe 1327, Train: 0.1000, Val: 0.0562, Test: 0.0592\n",
      "Epoch: 003, Runtime 1.924586, Loss 2.352874, forward nfe 4211, backward nfe 2351, Train: 0.2000, Val: 0.1685, Test: 0.1627\n",
      "Epoch: 004, Runtime 1.924796, Loss 2.323121, forward nfe 6258, backward nfe 3375, Train: 0.1900, Val: 0.1323, Test: 0.1342\n",
      "Epoch: 005, Runtime 1.924725, Loss 2.286227, forward nfe 8307, backward nfe 4400, Train: 0.1450, Val: 0.0362, Test: 0.0406\n",
      "Epoch: 006, Runtime 1.924648, Loss 2.267201, forward nfe 10353, backward nfe 5427, Train: 0.1750, Val: 0.0515, Test: 0.0554\n",
      "Epoch: 007, Runtime 1.941165, Loss 2.259680, forward nfe 12400, backward nfe 6460, Train: 0.1750, Val: 0.0546, Test: 0.0619\n",
      "Epoch: 008, Runtime 2.035944, Loss 2.247069, forward nfe 14509, backward nfe 7536, Train: 0.1100, Val: 0.0523, Test: 0.0615\n",
      "Epoch: 009, Runtime 1.985414, Loss 2.240681, forward nfe 16747, backward nfe 8575, Train: 0.2100, Val: 0.0854, Test: 0.0966\n",
      "Epoch: 010, Runtime 2.020481, Loss 2.213496, forward nfe 18893, backward nfe 9609, Train: 0.1900, Val: 0.0723, Test: 0.0804\n",
      "Epoch: 011, Runtime 2.037442, Loss 2.203345, forward nfe 21131, backward nfe 10659, Train: 0.1900, Val: 0.0569, Test: 0.0693\n",
      "Epoch: 012, Runtime 2.083998, Loss 2.185901, forward nfe 23369, backward nfe 11761, Train: 0.2200, Val: 0.0677, Test: 0.0819\n",
      "Epoch: 013, Runtime 2.101143, Loss 2.153477, forward nfe 25607, backward nfe 12880, Train: 0.1950, Val: 0.0708, Test: 0.0795\n",
      "Epoch: 014, Runtime 2.102329, Loss 2.137898, forward nfe 27845, backward nfe 13999, Train: 0.2000, Val: 0.0692, Test: 0.0789\n",
      "Epoch: 015, Runtime 2.100839, Loss 2.116278, forward nfe 30083, backward nfe 15118, Train: 0.2250, Val: 0.0669, Test: 0.0831\n",
      "Epoch: 016, Runtime 2.101595, Loss 2.094683, forward nfe 32321, backward nfe 16237, Train: 0.2300, Val: 0.0969, Test: 0.1162\n",
      "Epoch: 017, Runtime 2.101801, Loss 2.066868, forward nfe 34559, backward nfe 17356, Train: 0.2400, Val: 0.1038, Test: 0.1260\n",
      "Epoch: 018, Runtime 2.083792, Loss 2.045724, forward nfe 36797, backward nfe 18457, Train: 0.2550, Val: 0.0977, Test: 0.1176\n",
      "Epoch: 019, Runtime 2.100493, Loss 2.025943, forward nfe 39035, backward nfe 19576, Train: 0.2650, Val: 0.0946, Test: 0.1172\n",
      "Epoch: 020, Runtime 2.101265, Loss 2.002309, forward nfe 41273, backward nfe 20695, Train: 0.2600, Val: 0.1131, Test: 0.1197\n",
      "Epoch: 021, Runtime 2.103413, Loss 1.976505, forward nfe 43511, backward nfe 21814, Train: 0.2450, Val: 0.1077, Test: 0.1182\n",
      "Epoch: 022, Runtime 2.100173, Loss 1.952462, forward nfe 45749, backward nfe 22933, Train: 0.2400, Val: 0.1162, Test: 0.1267\n",
      "Epoch: 023, Runtime 2.101501, Loss 1.922123, forward nfe 47987, backward nfe 24052, Train: 0.2700, Val: 0.1269, Test: 0.1369\n",
      "Epoch: 024, Runtime 2.100919, Loss 1.895977, forward nfe 50225, backward nfe 25171, Train: 0.2800, Val: 0.1369, Test: 0.1490\n",
      "Epoch: 025, Runtime 2.101695, Loss 1.873245, forward nfe 52463, backward nfe 26290, Train: 0.3300, Val: 0.1669, Test: 0.1836\n",
      "Epoch: 026, Runtime 2.102979, Loss 1.846397, forward nfe 54701, backward nfe 27409, Train: 0.3450, Val: 0.1954, Test: 0.2143\n",
      "Epoch: 027, Runtime 2.101970, Loss 1.824127, forward nfe 56939, backward nfe 28528, Train: 0.3900, Val: 0.2177, Test: 0.2413\n",
      "Epoch: 028, Runtime 2.063370, Loss 1.797589, forward nfe 59100, backward nfe 29647, Train: 0.4150, Val: 0.2369, Test: 0.2612\n",
      "Epoch: 029, Runtime 2.100006, Loss 1.763380, forward nfe 61338, backward nfe 30766, Train: 0.4350, Val: 0.2538, Test: 0.2733\n",
      "Epoch: 030, Runtime 2.108559, Loss 1.737154, forward nfe 63576, backward nfe 31885, Train: 0.4500, Val: 0.2569, Test: 0.2829\n",
      "Epoch: 031, Runtime 2.100814, Loss 1.712095, forward nfe 65814, backward nfe 33004, Train: 0.4500, Val: 0.2754, Test: 0.2948\n",
      "Epoch: 032, Runtime 2.098657, Loss 1.686123, forward nfe 68052, backward nfe 34123, Train: 0.4850, Val: 0.2915, Test: 0.3168\n",
      "Epoch: 033, Runtime 2.100289, Loss 1.657393, forward nfe 70290, backward nfe 35242, Train: 0.5200, Val: 0.3115, Test: 0.3362\n",
      "Epoch: 034, Runtime 2.101357, Loss 1.626284, forward nfe 72528, backward nfe 36361, Train: 0.5400, Val: 0.3362, Test: 0.3645\n",
      "Epoch: 035, Runtime 2.097255, Loss 1.596046, forward nfe 74756, backward nfe 37480, Train: 0.5550, Val: 0.3992, Test: 0.4150\n",
      "Epoch: 036, Runtime 2.026153, Loss 1.574788, forward nfe 76994, backward nfe 38519, Train: 0.5550, Val: 0.4454, Test: 0.4609\n",
      "Epoch: 037, Runtime 2.031765, Loss 1.543632, forward nfe 79232, backward nfe 39575, Train: 0.5900, Val: 0.4969, Test: 0.5136\n",
      "Epoch: 038, Runtime 2.051561, Loss 1.514057, forward nfe 81448, backward nfe 40642, Train: 0.6300, Val: 0.5438, Test: 0.5587\n",
      "Epoch: 039, Runtime 2.100717, Loss 1.483586, forward nfe 83686, backward nfe 41761, Train: 0.6450, Val: 0.5654, Test: 0.5886\n",
      "Epoch: 040, Runtime 2.059180, Loss 1.457188, forward nfe 85924, backward nfe 42833, Train: 0.6300, Val: 0.5746, Test: 0.5938\n",
      "Epoch: 041, Runtime 2.101754, Loss 1.426052, forward nfe 88162, backward nfe 43952, Train: 0.6000, Val: 0.5677, Test: 0.5908\n",
      "Epoch: 042, Runtime 2.053705, Loss 1.411707, forward nfe 90400, backward nfe 45020, Train: 0.6050, Val: 0.5715, Test: 0.5944\n",
      "Epoch: 043, Runtime 2.030605, Loss 1.377516, forward nfe 92638, backward nfe 46063, Train: 0.6200, Val: 0.5708, Test: 0.5965\n",
      "Epoch: 044, Runtime 2.100911, Loss 1.359728, forward nfe 94876, backward nfe 47182, Train: 0.6200, Val: 0.5708, Test: 0.5924\n",
      "Epoch: 045, Runtime 2.010822, Loss 1.336050, forward nfe 97114, backward nfe 48209, Train: 0.6200, Val: 0.5646, Test: 0.5944\n",
      "Epoch: 046, Runtime 2.031902, Loss 1.308192, forward nfe 99344, backward nfe 49253, Train: 0.6250, Val: 0.5746, Test: 0.5996\n",
      "Epoch: 047, Runtime 2.025797, Loss 1.285878, forward nfe 101582, backward nfe 50291, Train: 0.6500, Val: 0.5862, Test: 0.6083\n",
      "Epoch: 048, Runtime 2.098984, Loss 1.252500, forward nfe 103820, backward nfe 51410, Train: 0.6850, Val: 0.5885, Test: 0.6090\n",
      "Epoch: 049, Runtime 1.979840, Loss 1.237626, forward nfe 105981, backward nfe 52437, Train: 0.6900, Val: 0.5831, Test: 0.6028\n",
      "Epoch: 050, Runtime 2.034861, Loss 1.217229, forward nfe 108200, backward nfe 53521, Train: 0.6850, Val: 0.5831, Test: 0.6035\n",
      "Epoch: 051, Runtime 2.060200, Loss 1.198480, forward nfe 110384, backward nfe 54596, Train: 0.7050, Val: 0.5923, Test: 0.6103\n",
      "Epoch: 052, Runtime 2.101474, Loss 1.183111, forward nfe 112622, backward nfe 55715, Train: 0.7200, Val: 0.6008, Test: 0.6150\n",
      "Epoch: 053, Runtime 2.090953, Loss 1.151271, forward nfe 114860, backward nfe 56822, Train: 0.7150, Val: 0.5977, Test: 0.6158\n",
      "Epoch: 054, Runtime 1.970417, Loss 1.130621, forward nfe 117037, backward nfe 57864, Train: 0.7000, Val: 0.5938, Test: 0.6112\n",
      "Epoch: 055, Runtime 1.972163, Loss 1.105041, forward nfe 119116, backward nfe 58933, Train: 0.7200, Val: 0.5915, Test: 0.6148\n",
      "Epoch: 056, Runtime 2.043010, Loss 1.087412, forward nfe 121246, backward nfe 60052, Train: 0.7250, Val: 0.5969, Test: 0.6219\n",
      "Epoch: 057, Runtime 2.012091, Loss 1.074762, forward nfe 123357, backward nfe 61094, Train: 0.7350, Val: 0.6023, Test: 0.6236\n",
      "Epoch: 058, Runtime 1.971496, Loss 1.062004, forward nfe 125501, backward nfe 62122, Train: 0.7300, Val: 0.5962, Test: 0.6154\n",
      "Epoch: 059, Runtime 2.013625, Loss 1.031582, forward nfe 127739, backward nfe 63148, Train: 0.7250, Val: 0.5969, Test: 0.6142\n",
      "Epoch: 060, Runtime 1.988755, Loss 1.021188, forward nfe 129914, backward nfe 64178, Train: 0.7300, Val: 0.6000, Test: 0.6199\n",
      "Epoch: 061, Runtime 1.999448, Loss 1.004038, forward nfe 132152, backward nfe 65204, Train: 0.7400, Val: 0.6046, Test: 0.6204\n",
      "Epoch: 062, Runtime 2.041452, Loss 0.984004, forward nfe 134339, backward nfe 66270, Train: 0.7550, Val: 0.6077, Test: 0.6210\n",
      "Epoch: 063, Runtime 1.988753, Loss 0.978829, forward nfe 136556, backward nfe 67300, Train: 0.7600, Val: 0.6023, Test: 0.6243\n",
      "Epoch: 064, Runtime 1.975442, Loss 0.954316, forward nfe 138643, backward nfe 68327, Train: 0.7600, Val: 0.6023, Test: 0.6231\n",
      "Epoch: 065, Runtime 1.982623, Loss 0.947781, forward nfe 140878, backward nfe 69360, Train: 0.7500, Val: 0.6085, Test: 0.6259\n",
      "Epoch: 066, Runtime 1.974069, Loss 0.931358, forward nfe 142952, backward nfe 70386, Train: 0.7500, Val: 0.6077, Test: 0.6296\n",
      "Epoch: 067, Runtime 1.948455, Loss 0.911516, forward nfe 145094, backward nfe 71433, Train: 0.7550, Val: 0.6054, Test: 0.6293\n",
      "Epoch: 068, Runtime 1.976761, Loss 0.906298, forward nfe 147189, backward nfe 72459, Train: 0.7550, Val: 0.6031, Test: 0.6255\n",
      "Epoch: 069, Runtime 1.958247, Loss 0.903396, forward nfe 149322, backward nfe 73483, Train: 0.7600, Val: 0.6038, Test: 0.6237\n",
      "Epoch: 070, Runtime 1.942629, Loss 0.885069, forward nfe 151461, backward nfe 74510, Train: 0.7800, Val: 0.6038, Test: 0.6276\n",
      "Epoch: 071, Runtime 1.929440, Loss 0.877345, forward nfe 153514, backward nfe 75538, Train: 0.7700, Val: 0.6085, Test: 0.6344\n",
      "Epoch: 072, Runtime 1.943888, Loss 0.851370, forward nfe 155590, backward nfe 76571, Train: 0.7650, Val: 0.6131, Test: 0.6386\n",
      "Epoch: 073, Runtime 1.963294, Loss 0.853456, forward nfe 157645, backward nfe 77611, Train: 0.7800, Val: 0.6185, Test: 0.6343\n",
      "Epoch: 074, Runtime 1.937857, Loss 0.835354, forward nfe 159753, backward nfe 78647, Train: 0.7800, Val: 0.6146, Test: 0.6284\n",
      "Epoch: 075, Runtime 1.935068, Loss 0.828050, forward nfe 161809, backward nfe 79676, Train: 0.7750, Val: 0.6154, Test: 0.6334\n",
      "Epoch: 076, Runtime 1.935146, Loss 0.826661, forward nfe 163863, backward nfe 80703, Train: 0.7950, Val: 0.6238, Test: 0.6423\n",
      "Epoch: 077, Runtime 1.927473, Loss 0.804192, forward nfe 165933, backward nfe 81727, Train: 0.7800, Val: 0.6408, Test: 0.6533\n",
      "Epoch: 078, Runtime 1.935571, Loss 0.794285, forward nfe 167991, backward nfe 82754, Train: 0.7800, Val: 0.6454, Test: 0.6628\n",
      "Epoch: 079, Runtime 1.926498, Loss 0.783779, forward nfe 170060, backward nfe 83779, Train: 0.8100, Val: 0.6315, Test: 0.6503\n",
      "Epoch: 080, Runtime 1.943806, Loss 0.772892, forward nfe 172125, backward nfe 84814, Train: 0.8000, Val: 0.6400, Test: 0.6592\n",
      "Epoch: 081, Runtime 1.923006, Loss 0.763501, forward nfe 174188, backward nfe 85839, Train: 0.8150, Val: 0.6538, Test: 0.6717\n",
      "Epoch: 082, Runtime 1.927940, Loss 0.742164, forward nfe 176239, backward nfe 86867, Train: 0.8300, Val: 0.6592, Test: 0.6802\n",
      "Epoch: 083, Runtime 1.932261, Loss 0.728142, forward nfe 178287, backward nfe 87895, Train: 0.8100, Val: 0.6785, Test: 0.6999\n",
      "Epoch: 084, Runtime 1.924158, Loss 0.700809, forward nfe 180345, backward nfe 88919, Train: 0.8250, Val: 0.6854, Test: 0.7030\n",
      "Epoch: 085, Runtime 1.928889, Loss 0.692629, forward nfe 182398, backward nfe 89945, Train: 0.8350, Val: 0.6715, Test: 0.6982\n",
      "Epoch: 086, Runtime 1.925680, Loss 0.651137, forward nfe 184450, backward nfe 90972, Train: 0.8300, Val: 0.6954, Test: 0.7096\n",
      "Epoch: 087, Runtime 1.924257, Loss 0.652781, forward nfe 186496, backward nfe 91997, Train: 0.8450, Val: 0.7062, Test: 0.7214\n",
      "Epoch: 088, Runtime 1.924584, Loss 0.648883, forward nfe 188544, backward nfe 93022, Train: 0.8450, Val: 0.7115, Test: 0.7296\n",
      "Epoch: 089, Runtime 1.923659, Loss 0.604063, forward nfe 190591, backward nfe 94046, Train: 0.8500, Val: 0.7308, Test: 0.7402\n",
      "Epoch: 090, Runtime 1.933311, Loss 0.604865, forward nfe 192648, backward nfe 95077, Train: 0.8400, Val: 0.7185, Test: 0.7373\n",
      "Epoch: 091, Runtime 1.923065, Loss 0.571854, forward nfe 194694, backward nfe 96100, Train: 0.8450, Val: 0.7238, Test: 0.7396\n",
      "Epoch: 092, Runtime 1.925715, Loss 0.573992, forward nfe 196740, backward nfe 97127, Train: 0.8600, Val: 0.7415, Test: 0.7493\n",
      "Epoch: 093, Runtime 1.930398, Loss 0.545077, forward nfe 198789, backward nfe 98156, Train: 0.8550, Val: 0.7315, Test: 0.7483\n",
      "Epoch: 094, Runtime 1.935198, Loss 0.534496, forward nfe 200838, backward nfe 99183, Train: 0.8550, Val: 0.7492, Test: 0.7591\n",
      "Epoch: 095, Runtime 1.922882, Loss 0.504268, forward nfe 202887, backward nfe 100207, Train: 0.8650, Val: 0.7585, Test: 0.7653\n",
      "Epoch: 096, Runtime 1.920997, Loss 0.505767, forward nfe 204934, backward nfe 101231, Train: 0.8700, Val: 0.7546, Test: 0.7614\n",
      "Epoch: 097, Runtime 1.926429, Loss 0.505689, forward nfe 206984, backward nfe 102257, Train: 0.8700, Val: 0.7608, Test: 0.7652\n",
      "Epoch: 098, Runtime 1.926001, Loss 0.483846, forward nfe 209030, backward nfe 103283, Train: 0.8650, Val: 0.7600, Test: 0.7702\n",
      "Epoch: 099, Runtime 1.924934, Loss 0.451491, forward nfe 211079, backward nfe 104308, Train: 0.8750, Val: 0.7608, Test: 0.7668\n",
      "best val accuracy 0.760769 with test accuracy 0.765181 at epoch 97\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.831989, Loss 2.310276, forward nfe 118, backward nfe 295, Train: 0.1000, Val: 0.0562, Test: 0.0592\n",
      "Epoch: 002, Runtime 1.924326, Loss 2.450653, forward nfe 2162, backward nfe 1319, Train: 0.0850, Val: 0.0308, Test: 0.0372\n",
      "Epoch: 003, Runtime 1.922130, Loss 2.345031, forward nfe 4207, backward nfe 2344, Train: 0.1000, Val: 0.0292, Test: 0.0398\n",
      "Epoch: 004, Runtime 1.922215, Loss 2.307754, forward nfe 6252, backward nfe 3369, Train: 0.1050, Val: 0.1169, Test: 0.1134\n",
      "Epoch: 005, Runtime 1.924543, Loss 2.287002, forward nfe 8300, backward nfe 4394, Train: 0.1000, Val: 0.1231, Test: 0.1237\n",
      "Epoch: 006, Runtime 1.922719, Loss 2.288579, forward nfe 10344, backward nfe 5419, Train: 0.1000, Val: 0.1262, Test: 0.1260\n",
      "Epoch: 007, Runtime 1.925072, Loss 2.281243, forward nfe 12398, backward nfe 6443, Train: 0.1150, Val: 0.1208, Test: 0.1225\n",
      "Epoch: 008, Runtime 1.965090, Loss 2.270143, forward nfe 14463, backward nfe 7499, Train: 0.1600, Val: 0.0985, Test: 0.1021\n",
      "Epoch: 009, Runtime 1.939986, Loss 2.269315, forward nfe 16553, backward nfe 8524, Train: 0.1750, Val: 0.1092, Test: 0.1128\n",
      "Epoch: 010, Runtime 1.982210, Loss 2.263642, forward nfe 18606, backward nfe 9563, Train: 0.1250, Val: 0.1254, Test: 0.1286\n",
      "Epoch: 011, Runtime 1.958224, Loss 2.249386, forward nfe 20758, backward nfe 10588, Train: 0.1150, Val: 0.1392, Test: 0.1362\n",
      "Epoch: 012, Runtime 2.033997, Loss 2.249023, forward nfe 22912, backward nfe 11660, Train: 0.1150, Val: 0.1408, Test: 0.1373\n",
      "Epoch: 013, Runtime 2.016238, Loss 2.239169, forward nfe 25150, backward nfe 12689, Train: 0.1300, Val: 0.1323, Test: 0.1358\n",
      "Epoch: 014, Runtime 2.065483, Loss 2.227194, forward nfe 27313, backward nfe 13808, Train: 0.1600, Val: 0.1354, Test: 0.1366\n",
      "Epoch: 015, Runtime 2.013120, Loss 2.221537, forward nfe 29551, backward nfe 14850, Train: 0.1700, Val: 0.1377, Test: 0.1402\n",
      "Epoch: 016, Runtime 2.099523, Loss 2.212435, forward nfe 31760, backward nfe 15969, Train: 0.1350, Val: 0.1362, Test: 0.1427\n",
      "Epoch: 017, Runtime 2.016180, Loss 2.200214, forward nfe 33998, backward nfe 16999, Train: 0.1300, Val: 0.1446, Test: 0.1434\n",
      "Epoch: 018, Runtime 2.093895, Loss 2.194810, forward nfe 36236, backward nfe 18118, Train: 0.1150, Val: 0.1354, Test: 0.1299\n",
      "Epoch: 019, Runtime 2.043367, Loss 2.186496, forward nfe 38459, backward nfe 19177, Train: 0.1250, Val: 0.0408, Test: 0.0553\n",
      "Epoch: 020, Runtime 2.033587, Loss 2.178888, forward nfe 40697, backward nfe 20225, Train: 0.1500, Val: 0.1362, Test: 0.1421\n",
      "Epoch: 021, Runtime 2.102144, Loss 2.167197, forward nfe 42935, backward nfe 21344, Train: 0.1400, Val: 0.1292, Test: 0.1295\n",
      "Epoch: 022, Runtime 2.023535, Loss 2.159384, forward nfe 45141, backward nfe 22398, Train: 0.1600, Val: 0.1408, Test: 0.1490\n",
      "Epoch: 023, Runtime 2.100393, Loss 2.146513, forward nfe 47379, backward nfe 23517, Train: 0.1600, Val: 0.0808, Test: 0.0819\n",
      "Epoch: 024, Runtime 2.029074, Loss 2.154952, forward nfe 49617, backward nfe 24558, Train: 0.1350, Val: 0.1523, Test: 0.1526\n",
      "Epoch: 025, Runtime 2.079764, Loss 2.137243, forward nfe 51855, backward nfe 25655, Train: 0.1700, Val: 0.1477, Test: 0.1561\n",
      "Epoch: 026, Runtime 1.997626, Loss 2.123470, forward nfe 54059, backward nfe 26681, Train: 0.1500, Val: 0.1377, Test: 0.1406\n",
      "Epoch: 027, Runtime 2.086658, Loss 2.113117, forward nfe 56297, backward nfe 27785, Train: 0.1650, Val: 0.1338, Test: 0.1406\n",
      "Epoch: 028, Runtime 2.101744, Loss 2.113631, forward nfe 58535, backward nfe 28904, Train: 0.1600, Val: 0.1400, Test: 0.1438\n",
      "Epoch: 029, Runtime 2.017981, Loss 2.098946, forward nfe 60773, backward nfe 29933, Train: 0.1550, Val: 0.1485, Test: 0.1506\n",
      "Epoch: 030, Runtime 2.073749, Loss 2.088832, forward nfe 63011, backward nfe 31024, Train: 0.1500, Val: 0.1546, Test: 0.1577\n",
      "Epoch: 031, Runtime 2.068238, Loss 2.084317, forward nfe 65249, backward nfe 32099, Train: 0.2400, Val: 0.2062, Test: 0.2205\n",
      "Epoch: 032, Runtime 1.985923, Loss 2.077846, forward nfe 67417, backward nfe 33131, Train: 0.2500, Val: 0.1823, Test: 0.1913\n",
      "Epoch: 033, Runtime 2.013449, Loss 2.064900, forward nfe 69655, backward nfe 34158, Train: 0.2300, Val: 0.1669, Test: 0.1690\n",
      "Epoch: 034, Runtime 1.979855, Loss 2.058332, forward nfe 71822, backward nfe 35183, Train: 0.2350, Val: 0.1515, Test: 0.1646\n",
      "Epoch: 035, Runtime 2.099879, Loss 2.052347, forward nfe 74060, backward nfe 36302, Train: 0.2200, Val: 0.1500, Test: 0.1636\n",
      "Epoch: 036, Runtime 2.023045, Loss 2.044627, forward nfe 76213, backward nfe 37382, Train: 0.2250, Val: 0.1492, Test: 0.1632\n",
      "Epoch: 037, Runtime 2.016865, Loss 2.038698, forward nfe 78451, backward nfe 38411, Train: 0.2100, Val: 0.1477, Test: 0.1579\n",
      "Epoch: 038, Runtime 2.074089, Loss 2.027608, forward nfe 80689, backward nfe 39502, Train: 0.2150, Val: 0.1454, Test: 0.1552\n",
      "Epoch: 039, Runtime 2.067056, Loss 2.028015, forward nfe 82927, backward nfe 40621, Train: 0.2100, Val: 0.1454, Test: 0.1562\n",
      "Epoch: 040, Runtime 1.995009, Loss 2.015826, forward nfe 85095, backward nfe 41645, Train: 0.2150, Val: 0.1508, Test: 0.1635\n",
      "Epoch: 041, Runtime 1.971424, Loss 2.015501, forward nfe 87294, backward nfe 42669, Train: 0.2150, Val: 0.1508, Test: 0.1632\n",
      "Epoch: 042, Runtime 2.063270, Loss 2.007578, forward nfe 89445, backward nfe 43750, Train: 0.2150, Val: 0.1462, Test: 0.1572\n",
      "Epoch: 043, Runtime 2.076672, Loss 2.000284, forward nfe 91639, backward nfe 44869, Train: 0.2150, Val: 0.1454, Test: 0.1565\n",
      "Epoch: 044, Runtime 2.079068, Loss 1.998779, forward nfe 93877, backward nfe 45965, Train: 0.2150, Val: 0.1469, Test: 0.1582\n",
      "Epoch: 045, Runtime 2.060872, Loss 1.991119, forward nfe 96115, backward nfe 47041, Train: 0.2100, Val: 0.1546, Test: 0.1601\n",
      "Epoch: 046, Runtime 2.013788, Loss 1.991740, forward nfe 98353, backward nfe 48066, Train: 0.2150, Val: 0.1485, Test: 0.1588\n",
      "Epoch: 047, Runtime 2.030285, Loss 1.981473, forward nfe 100591, backward nfe 49110, Train: 0.2150, Val: 0.1492, Test: 0.1583\n",
      "Epoch: 048, Runtime 1.970037, Loss 1.976970, forward nfe 102753, backward nfe 50164, Train: 0.2150, Val: 0.1438, Test: 0.1511\n",
      "Epoch: 049, Runtime 1.996935, Loss 1.973039, forward nfe 104841, backward nfe 51213, Train: 0.2150, Val: 0.1454, Test: 0.1456\n",
      "Epoch: 050, Runtime 1.988201, Loss 1.967523, forward nfe 107060, backward nfe 52253, Train: 0.2250, Val: 0.1500, Test: 0.1472\n",
      "Epoch: 051, Runtime 1.960666, Loss 1.964382, forward nfe 109150, backward nfe 53305, Train: 0.2250, Val: 0.1592, Test: 0.1508\n",
      "Epoch: 052, Runtime 1.967732, Loss 1.960054, forward nfe 111304, backward nfe 54330, Train: 0.2250, Val: 0.1515, Test: 0.1508\n",
      "Epoch: 053, Runtime 2.011851, Loss 1.953298, forward nfe 113446, backward nfe 55389, Train: 0.2200, Val: 0.1523, Test: 0.1535\n",
      "Epoch: 054, Runtime 1.963077, Loss 1.952540, forward nfe 115606, backward nfe 56413, Train: 0.2200, Val: 0.1523, Test: 0.1552\n",
      "Epoch: 055, Runtime 1.993885, Loss 1.948436, forward nfe 117673, backward nfe 57475, Train: 0.2250, Val: 0.1546, Test: 0.1568\n",
      "Epoch: 056, Runtime 1.979277, Loss 1.946495, forward nfe 119877, backward nfe 58500, Train: 0.2250, Val: 0.1554, Test: 0.1584\n",
      "Epoch: 057, Runtime 2.002830, Loss 1.943167, forward nfe 122019, backward nfe 59566, Train: 0.2250, Val: 0.1600, Test: 0.1595\n",
      "Epoch: 058, Runtime 1.954013, Loss 1.940999, forward nfe 124125, backward nfe 60594, Train: 0.2250, Val: 0.1592, Test: 0.1612\n",
      "Epoch: 059, Runtime 1.963623, Loss 1.936936, forward nfe 126192, backward nfe 61622, Train: 0.2300, Val: 0.1623, Test: 0.1615\n",
      "Epoch: 060, Runtime 1.945931, Loss 1.932796, forward nfe 128353, backward nfe 62647, Train: 0.2300, Val: 0.1654, Test: 0.1636\n",
      "Epoch: 061, Runtime 1.979281, Loss 1.929185, forward nfe 130502, backward nfe 63674, Train: 0.2300, Val: 0.1685, Test: 0.1647\n",
      "Epoch: 062, Runtime 1.990066, Loss 1.927258, forward nfe 132593, backward nfe 64719, Train: 0.2300, Val: 0.1646, Test: 0.1645\n",
      "Epoch: 063, Runtime 1.966330, Loss 1.926480, forward nfe 134773, backward nfe 65759, Train: 0.2300, Val: 0.1646, Test: 0.1644\n",
      "Epoch: 064, Runtime 1.978013, Loss 1.918543, forward nfe 136854, backward nfe 66789, Train: 0.2300, Val: 0.1677, Test: 0.1659\n",
      "Epoch: 065, Runtime 1.972519, Loss 1.919224, forward nfe 139062, backward nfe 67814, Train: 0.2350, Val: 0.1731, Test: 0.1679\n",
      "Epoch: 066, Runtime 1.953083, Loss 1.921195, forward nfe 141154, backward nfe 68847, Train: 0.2300, Val: 0.1708, Test: 0.1662\n",
      "Epoch: 067, Runtime 1.961414, Loss 1.911272, forward nfe 143223, backward nfe 69878, Train: 0.2300, Val: 0.1662, Test: 0.1666\n",
      "Epoch: 068, Runtime 1.960255, Loss 1.912463, forward nfe 145384, backward nfe 70902, Train: 0.2300, Val: 0.1715, Test: 0.1672\n",
      "Epoch: 069, Runtime 1.931464, Loss 1.909809, forward nfe 147474, backward nfe 71934, Train: 0.2350, Val: 0.1785, Test: 0.1716\n",
      "Epoch: 070, Runtime 1.932112, Loss 1.907880, forward nfe 149539, backward nfe 72958, Train: 0.2350, Val: 0.1785, Test: 0.1725\n",
      "Epoch: 071, Runtime 1.925293, Loss 1.907328, forward nfe 151600, backward nfe 73982, Train: 0.2250, Val: 0.1669, Test: 0.1685\n",
      "Epoch: 072, Runtime 1.946409, Loss 1.905731, forward nfe 153648, backward nfe 75008, Train: 0.2150, Val: 0.1592, Test: 0.1576\n",
      "Epoch: 073, Runtime 1.927737, Loss 1.908807, forward nfe 155743, backward nfe 76034, Train: 0.2300, Val: 0.1723, Test: 0.1765\n",
      "Epoch: 074, Runtime 1.935319, Loss 1.895885, forward nfe 157795, backward nfe 77062, Train: 0.2350, Val: 0.1877, Test: 0.1821\n",
      "Epoch: 075, Runtime 1.926114, Loss 1.886574, forward nfe 159862, backward nfe 78087, Train: 0.3100, Val: 0.2208, Test: 0.2202\n",
      "Epoch: 076, Runtime 1.944551, Loss 1.869237, forward nfe 161938, backward nfe 79111, Train: 0.2200, Val: 0.1954, Test: 0.2003\n",
      "Epoch: 077, Runtime 1.933481, Loss 1.847939, forward nfe 164016, backward nfe 80137, Train: 0.2200, Val: 0.1854, Test: 0.2003\n",
      "Epoch: 078, Runtime 1.932851, Loss 1.825964, forward nfe 166065, backward nfe 81161, Train: 0.3300, Val: 0.1985, Test: 0.2086\n",
      "Epoch: 079, Runtime 1.925259, Loss 1.797405, forward nfe 168132, backward nfe 82186, Train: 0.3700, Val: 0.2200, Test: 0.2341\n",
      "Epoch: 080, Runtime 1.935771, Loss 1.773529, forward nfe 170188, backward nfe 83219, Train: 0.3950, Val: 0.2146, Test: 0.2324\n",
      "Epoch: 081, Runtime 1.941931, Loss 1.744189, forward nfe 172250, backward nfe 84243, Train: 0.4100, Val: 0.2354, Test: 0.2526\n",
      "Epoch: 082, Runtime 1.947972, Loss 1.709163, forward nfe 174320, backward nfe 85275, Train: 0.3800, Val: 0.2300, Test: 0.2436\n",
      "Epoch: 083, Runtime 1.955655, Loss 1.681723, forward nfe 176410, backward nfe 86299, Train: 0.3700, Val: 0.2200, Test: 0.2434\n",
      "Epoch: 084, Runtime 1.947685, Loss 1.658034, forward nfe 178549, backward nfe 87323, Train: 0.4350, Val: 0.2323, Test: 0.2576\n",
      "Epoch: 085, Runtime 1.941444, Loss 1.621509, forward nfe 180621, backward nfe 88364, Train: 0.4550, Val: 0.2500, Test: 0.2785\n",
      "Epoch: 086, Runtime 1.942593, Loss 1.589861, forward nfe 182680, backward nfe 89405, Train: 0.4300, Val: 0.2262, Test: 0.2527\n",
      "Epoch: 087, Runtime 1.933074, Loss 1.575236, forward nfe 184740, backward nfe 90432, Train: 0.4450, Val: 0.2162, Test: 0.2498\n",
      "Epoch: 088, Runtime 1.957530, Loss 1.548892, forward nfe 186805, backward nfe 91483, Train: 0.4550, Val: 0.2131, Test: 0.2501\n",
      "Epoch: 089, Runtime 1.936380, Loss 1.516018, forward nfe 188873, backward nfe 92508, Train: 0.5250, Val: 0.2900, Test: 0.3066\n",
      "Epoch: 090, Runtime 1.947006, Loss 1.490265, forward nfe 190970, backward nfe 93535, Train: 0.5150, Val: 0.3162, Test: 0.3317\n",
      "Epoch: 091, Runtime 1.938507, Loss 1.470160, forward nfe 193035, backward nfe 94567, Train: 0.5050, Val: 0.3138, Test: 0.3397\n",
      "Epoch: 092, Runtime 1.955965, Loss 1.435128, forward nfe 195091, backward nfe 95619, Train: 0.4900, Val: 0.2546, Test: 0.2852\n",
      "Epoch: 093, Runtime 1.939685, Loss 1.412712, forward nfe 197150, backward nfe 96656, Train: 0.5050, Val: 0.2546, Test: 0.2851\n",
      "Epoch: 094, Runtime 1.931439, Loss 1.369233, forward nfe 199215, backward nfe 97683, Train: 0.5650, Val: 0.2862, Test: 0.3146\n",
      "Epoch: 095, Runtime 1.927720, Loss 1.346926, forward nfe 201269, backward nfe 98707, Train: 0.6050, Val: 0.3185, Test: 0.3481\n",
      "Epoch: 096, Runtime 1.939836, Loss 1.315176, forward nfe 203324, backward nfe 99738, Train: 0.6050, Val: 0.3777, Test: 0.3937\n",
      "Epoch: 097, Runtime 1.922351, Loss 1.283739, forward nfe 205370, backward nfe 100762, Train: 0.5900, Val: 0.4146, Test: 0.4182\n",
      "Epoch: 098, Runtime 1.929280, Loss 1.267309, forward nfe 207418, backward nfe 101789, Train: 0.5450, Val: 0.4315, Test: 0.4434\n",
      "Epoch: 099, Runtime 1.933457, Loss 1.227069, forward nfe 209470, backward nfe 102816, Train: 0.6000, Val: 0.4446, Test: 0.4566\n",
      "best val accuracy 0.444615 with test accuracy 0.456579 at epoch 99\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.853442, Loss 2.319408, forward nfe 115, backward nfe 318, Train: 0.1700, Val: 0.1215, Test: 0.1387\n",
      "Epoch: 002, Runtime 1.919798, Loss 2.364126, forward nfe 2159, backward nfe 1342, Train: 0.1150, Val: 0.1369, Test: 0.1450\n",
      "Epoch: 003, Runtime 1.921094, Loss 2.322513, forward nfe 4203, backward nfe 2367, Train: 0.1150, Val: 0.3185, Test: 0.3029\n",
      "Epoch: 004, Runtime 1.923697, Loss 2.307386, forward nfe 6247, backward nfe 3392, Train: 0.1050, Val: 0.3877, Test: 0.3622\n",
      "Epoch: 005, Runtime 1.928098, Loss 2.289531, forward nfe 8298, backward nfe 4417, Train: 0.1600, Val: 0.2238, Test: 0.2204\n",
      "Epoch: 006, Runtime 1.926111, Loss 2.284091, forward nfe 10358, backward nfe 5442, Train: 0.1050, Val: 0.0277, Test: 0.0322\n",
      "Epoch: 007, Runtime 1.927021, Loss 2.280429, forward nfe 12413, backward nfe 6468, Train: 0.1000, Val: 0.0200, Test: 0.0230\n",
      "Epoch: 008, Runtime 1.978404, Loss 2.276329, forward nfe 14488, backward nfe 7496, Train: 0.1000, Val: 0.0162, Test: 0.0218\n",
      "Epoch: 009, Runtime 1.975923, Loss 2.267231, forward nfe 16641, backward nfe 8528, Train: 0.1050, Val: 0.0162, Test: 0.0220\n",
      "Epoch: 010, Runtime 2.099901, Loss 2.257137, forward nfe 18879, backward nfe 9647, Train: 0.1050, Val: 0.0162, Test: 0.0220\n",
      "Epoch: 011, Runtime 2.083987, Loss 2.251969, forward nfe 21117, backward nfe 10751, Train: 0.1050, Val: 0.0169, Test: 0.0227\n",
      "Epoch: 012, Runtime 2.080117, Loss 2.239753, forward nfe 23355, backward nfe 11849, Train: 0.1050, Val: 0.0169, Test: 0.0225\n",
      "Epoch: 013, Runtime 2.099972, Loss 2.229710, forward nfe 25593, backward nfe 12968, Train: 0.1050, Val: 0.0169, Test: 0.0233\n",
      "Epoch: 014, Runtime 2.096301, Loss 2.219128, forward nfe 27831, backward nfe 14087, Train: 0.1350, Val: 0.0200, Test: 0.0322\n",
      "Epoch: 015, Runtime 2.098514, Loss 2.206289, forward nfe 30069, backward nfe 15206, Train: 0.1600, Val: 0.0277, Test: 0.0423\n",
      "Epoch: 016, Runtime 2.098117, Loss 2.197520, forward nfe 32307, backward nfe 16325, Train: 0.1500, Val: 0.0223, Test: 0.0368\n",
      "Epoch: 017, Runtime 2.097139, Loss 2.182990, forward nfe 34545, backward nfe 17444, Train: 0.1500, Val: 0.0215, Test: 0.0356\n",
      "Epoch: 018, Runtime 2.099156, Loss 2.172283, forward nfe 36783, backward nfe 18563, Train: 0.1500, Val: 0.0254, Test: 0.0400\n",
      "Epoch: 019, Runtime 2.073953, Loss 2.158768, forward nfe 39021, backward nfe 19682, Train: 0.1650, Val: 0.0277, Test: 0.0429\n",
      "Epoch: 020, Runtime 2.098289, Loss 2.148582, forward nfe 41209, backward nfe 20800, Train: 0.1550, Val: 0.0262, Test: 0.0419\n",
      "Epoch: 021, Runtime 2.106054, Loss 2.135422, forward nfe 43447, backward nfe 21919, Train: 0.1550, Val: 0.0262, Test: 0.0413\n",
      "Epoch: 022, Runtime 2.094557, Loss 2.123774, forward nfe 45685, backward nfe 23038, Train: 0.1750, Val: 0.0277, Test: 0.0439\n",
      "Epoch: 023, Runtime 2.082994, Loss 2.111129, forward nfe 47923, backward nfe 24142, Train: 0.1750, Val: 0.0277, Test: 0.0451\n",
      "Epoch: 024, Runtime 2.098898, Loss 2.104204, forward nfe 50161, backward nfe 25261, Train: 0.1750, Val: 0.0285, Test: 0.0475\n",
      "Epoch: 025, Runtime 2.094483, Loss 2.091587, forward nfe 52399, backward nfe 26380, Train: 0.1850, Val: 0.0292, Test: 0.0492\n",
      "Epoch: 026, Runtime 2.094270, Loss 2.080513, forward nfe 54637, backward nfe 27499, Train: 0.2000, Val: 0.0308, Test: 0.0522\n",
      "Epoch: 027, Runtime 2.095967, Loss 2.070251, forward nfe 56875, backward nfe 28618, Train: 0.2050, Val: 0.0331, Test: 0.0537\n",
      "Epoch: 028, Runtime 2.096392, Loss 2.064579, forward nfe 59113, backward nfe 29737, Train: 0.2000, Val: 0.0346, Test: 0.0534\n",
      "Epoch: 029, Runtime 2.075409, Loss 2.050833, forward nfe 61351, backward nfe 30835, Train: 0.2000, Val: 0.0354, Test: 0.0534\n",
      "Epoch: 030, Runtime 2.104640, Loss 2.040738, forward nfe 63589, backward nfe 31954, Train: 0.2050, Val: 0.0362, Test: 0.0550\n",
      "Epoch: 031, Runtime 2.098632, Loss 2.031133, forward nfe 65827, backward nfe 33073, Train: 0.2050, Val: 0.0362, Test: 0.0558\n",
      "Epoch: 032, Runtime 2.094593, Loss 2.024483, forward nfe 68065, backward nfe 34192, Train: 0.2050, Val: 0.0369, Test: 0.0567\n",
      "Epoch: 033, Runtime 2.094953, Loss 2.015771, forward nfe 70303, backward nfe 35311, Train: 0.2050, Val: 0.0377, Test: 0.0581\n",
      "Epoch: 034, Runtime 2.096855, Loss 2.006778, forward nfe 72541, backward nfe 36430, Train: 0.2150, Val: 0.0500, Test: 0.0680\n",
      "Epoch: 035, Runtime 2.097663, Loss 2.001027, forward nfe 74779, backward nfe 37549, Train: 0.2650, Val: 0.3254, Test: 0.3224\n",
      "Epoch: 036, Runtime 2.011434, Loss 1.974229, forward nfe 77017, backward nfe 38576, Train: 0.2300, Val: 0.4208, Test: 0.4052\n",
      "Epoch: 037, Runtime 2.095085, Loss 2.057274, forward nfe 79255, backward nfe 39695, Train: 0.2100, Val: 0.3831, Test: 0.3754\n",
      "Epoch: 038, Runtime 2.086901, Loss 1.933757, forward nfe 81493, backward nfe 40803, Train: 0.2250, Val: 0.1600, Test: 0.1757\n",
      "Epoch: 039, Runtime 2.057199, Loss 1.931944, forward nfe 83731, backward nfe 41879, Train: 0.3000, Val: 0.1562, Test: 0.1717\n",
      "Epoch: 040, Runtime 2.096177, Loss 1.914385, forward nfe 85969, backward nfe 42998, Train: 0.2900, Val: 0.1469, Test: 0.1621\n",
      "Epoch: 041, Runtime 2.098104, Loss 1.902219, forward nfe 88207, backward nfe 44117, Train: 0.2850, Val: 0.1731, Test: 0.1813\n",
      "Epoch: 042, Runtime 2.095551, Loss 1.867640, forward nfe 90445, backward nfe 45236, Train: 0.3800, Val: 0.2108, Test: 0.2245\n",
      "Epoch: 043, Runtime 2.097199, Loss 1.863503, forward nfe 92683, backward nfe 46355, Train: 0.3350, Val: 0.2131, Test: 0.2278\n",
      "Epoch: 044, Runtime 2.096876, Loss 1.841399, forward nfe 94921, backward nfe 47474, Train: 0.2900, Val: 0.1892, Test: 0.2008\n",
      "Epoch: 045, Runtime 2.096266, Loss 1.833259, forward nfe 97159, backward nfe 48593, Train: 0.3100, Val: 0.2385, Test: 0.2427\n",
      "Epoch: 046, Runtime 2.098462, Loss 1.806341, forward nfe 99397, backward nfe 49712, Train: 0.3950, Val: 0.2615, Test: 0.2719\n",
      "Epoch: 047, Runtime 2.094704, Loss 1.795369, forward nfe 101635, backward nfe 50831, Train: 0.4150, Val: 0.2569, Test: 0.2691\n",
      "Epoch: 048, Runtime 2.094536, Loss 1.768882, forward nfe 103873, backward nfe 51950, Train: 0.3500, Val: 0.2385, Test: 0.2440\n",
      "Epoch: 049, Runtime 2.011458, Loss 1.754223, forward nfe 106111, backward nfe 52976, Train: 0.3650, Val: 0.2215, Test: 0.2372\n",
      "Epoch: 050, Runtime 2.012219, Loss 1.741977, forward nfe 108349, backward nfe 54004, Train: 0.3750, Val: 0.2031, Test: 0.2249\n",
      "Epoch: 051, Runtime 2.012155, Loss 1.719032, forward nfe 110587, backward nfe 55030, Train: 0.3850, Val: 0.1977, Test: 0.2169\n",
      "Epoch: 052, Runtime 2.012358, Loss 1.705678, forward nfe 112825, backward nfe 56057, Train: 0.3800, Val: 0.2000, Test: 0.2176\n",
      "Epoch: 053, Runtime 1.992230, Loss 1.692461, forward nfe 114977, backward nfe 57124, Train: 0.4000, Val: 0.2062, Test: 0.2275\n",
      "Epoch: 054, Runtime 2.013189, Loss 1.679332, forward nfe 117178, backward nfe 58152, Train: 0.4050, Val: 0.2115, Test: 0.2355\n",
      "Epoch: 055, Runtime 2.019498, Loss 1.662046, forward nfe 119416, backward nfe 59186, Train: 0.4200, Val: 0.2415, Test: 0.2581\n",
      "Epoch: 056, Runtime 2.068176, Loss 1.653598, forward nfe 121654, backward nfe 60274, Train: 0.4450, Val: 0.2723, Test: 0.2840\n",
      "Epoch: 057, Runtime 2.010987, Loss 1.626766, forward nfe 123890, backward nfe 61302, Train: 0.4350, Val: 0.2785, Test: 0.2906\n",
      "Epoch: 058, Runtime 2.020698, Loss 1.615069, forward nfe 126128, backward nfe 62349, Train: 0.4350, Val: 0.2854, Test: 0.2920\n",
      "Epoch: 059, Runtime 1.984671, Loss 1.600584, forward nfe 128347, backward nfe 63385, Train: 0.4300, Val: 0.2777, Test: 0.2905\n",
      "Epoch: 060, Runtime 2.010597, Loss 1.595698, forward nfe 130509, backward nfe 64411, Train: 0.4250, Val: 0.2785, Test: 0.2938\n",
      "Epoch: 061, Runtime 1.966317, Loss 1.572963, forward nfe 132708, backward nfe 65455, Train: 0.4250, Val: 0.2846, Test: 0.2954\n",
      "Epoch: 062, Runtime 1.976434, Loss 1.558775, forward nfe 134819, backward nfe 66487, Train: 0.4400, Val: 0.2915, Test: 0.2980\n",
      "Epoch: 063, Runtime 1.933775, Loss 1.545241, forward nfe 136899, backward nfe 67517, Train: 0.4400, Val: 0.2915, Test: 0.2977\n",
      "Epoch: 064, Runtime 1.960682, Loss 1.526753, forward nfe 139001, backward nfe 68550, Train: 0.4450, Val: 0.2862, Test: 0.2960\n",
      "Epoch: 065, Runtime 1.966616, Loss 1.521876, forward nfe 141174, backward nfe 69575, Train: 0.4600, Val: 0.2785, Test: 0.2925\n",
      "Epoch: 066, Runtime 1.943247, Loss 1.505486, forward nfe 143244, backward nfe 70603, Train: 0.4550, Val: 0.2746, Test: 0.2902\n",
      "Epoch: 067, Runtime 1.939348, Loss 1.486208, forward nfe 145325, backward nfe 71628, Train: 0.4500, Val: 0.2823, Test: 0.2919\n",
      "Epoch: 068, Runtime 1.934763, Loss 1.484457, forward nfe 147397, backward nfe 72659, Train: 0.4550, Val: 0.2915, Test: 0.2960\n",
      "Epoch: 069, Runtime 1.932941, Loss 1.478386, forward nfe 149454, backward nfe 73688, Train: 0.4650, Val: 0.2908, Test: 0.3023\n",
      "Epoch: 070, Runtime 1.923679, Loss 1.463975, forward nfe 151524, backward nfe 74713, Train: 0.4700, Val: 0.2938, Test: 0.3034\n",
      "Epoch: 071, Runtime 1.927034, Loss 1.445420, forward nfe 153574, backward nfe 75741, Train: 0.4750, Val: 0.2977, Test: 0.3048\n",
      "Epoch: 072, Runtime 1.927254, Loss 1.430585, forward nfe 155629, backward nfe 76766, Train: 0.4800, Val: 0.2977, Test: 0.3010\n",
      "Epoch: 073, Runtime 1.935069, Loss 1.431325, forward nfe 157688, backward nfe 77790, Train: 0.4800, Val: 0.2877, Test: 0.2951\n",
      "Epoch: 074, Runtime 1.935686, Loss 1.421998, forward nfe 159788, backward nfe 78818, Train: 0.4700, Val: 0.2785, Test: 0.2941\n",
      "Epoch: 075, Runtime 1.938289, Loss 1.410688, forward nfe 161838, backward nfe 79851, Train: 0.4800, Val: 0.2838, Test: 0.2997\n",
      "Epoch: 076, Runtime 1.949569, Loss 1.399195, forward nfe 163917, backward nfe 80885, Train: 0.5050, Val: 0.2908, Test: 0.3044\n",
      "Epoch: 077, Runtime 1.965670, Loss 1.392361, forward nfe 166027, backward nfe 81918, Train: 0.5250, Val: 0.2962, Test: 0.3084\n",
      "Epoch: 078, Runtime 1.934918, Loss 1.399323, forward nfe 168126, backward nfe 82949, Train: 0.5100, Val: 0.2931, Test: 0.3065\n",
      "Epoch: 079, Runtime 1.928536, Loss 1.374984, forward nfe 170184, backward nfe 83978, Train: 0.5200, Val: 0.2854, Test: 0.3059\n",
      "Epoch: 080, Runtime 1.926291, Loss 1.354315, forward nfe 172236, backward nfe 85002, Train: 0.5400, Val: 0.2885, Test: 0.3071\n",
      "Epoch: 081, Runtime 1.923409, Loss 1.354330, forward nfe 174296, backward nfe 86027, Train: 0.5250, Val: 0.2938, Test: 0.3118\n",
      "Epoch: 082, Runtime 1.924123, Loss 1.354555, forward nfe 176349, backward nfe 87052, Train: 0.5250, Val: 0.2992, Test: 0.3146\n",
      "Epoch: 083, Runtime 1.922420, Loss 1.324425, forward nfe 178396, backward nfe 88077, Train: 0.5450, Val: 0.2762, Test: 0.3017\n",
      "Epoch: 084, Runtime 1.928222, Loss 1.322283, forward nfe 180445, backward nfe 89104, Train: 0.5500, Val: 0.2800, Test: 0.3072\n",
      "Epoch: 085, Runtime 1.939100, Loss 1.302354, forward nfe 182506, backward nfe 90139, Train: 0.5900, Val: 0.3062, Test: 0.3266\n",
      "Epoch: 086, Runtime 1.936028, Loss 1.285836, forward nfe 184593, backward nfe 91163, Train: 0.5900, Val: 0.3154, Test: 0.3298\n",
      "Epoch: 087, Runtime 1.927413, Loss 1.253943, forward nfe 186651, backward nfe 92188, Train: 0.6100, Val: 0.3077, Test: 0.3253\n",
      "Epoch: 088, Runtime 1.924073, Loss 1.235500, forward nfe 188700, backward nfe 93214, Train: 0.6050, Val: 0.3254, Test: 0.3364\n",
      "Epoch: 089, Runtime 1.922584, Loss 1.177405, forward nfe 190747, backward nfe 94238, Train: 0.6100, Val: 0.3338, Test: 0.3566\n",
      "Epoch: 090, Runtime 1.926344, Loss 1.160657, forward nfe 192798, backward nfe 95264, Train: 0.6200, Val: 0.3415, Test: 0.3730\n",
      "Epoch: 091, Runtime 1.927258, Loss 1.131242, forward nfe 194855, backward nfe 96290, Train: 0.6450, Val: 0.3331, Test: 0.3608\n",
      "Epoch: 092, Runtime 1.924189, Loss 1.078606, forward nfe 196912, backward nfe 97313, Train: 0.6650, Val: 0.3385, Test: 0.3640\n",
      "Epoch: 093, Runtime 1.925524, Loss 1.060229, forward nfe 198962, backward nfe 98342, Train: 0.6750, Val: 0.3431, Test: 0.3730\n",
      "Epoch: 094, Runtime 1.930005, Loss 1.052167, forward nfe 201019, backward nfe 99369, Train: 0.6800, Val: 0.3346, Test: 0.3711\n",
      "Epoch: 095, Runtime 1.948104, Loss 1.025175, forward nfe 203079, backward nfe 100408, Train: 0.6700, Val: 0.3308, Test: 0.3692\n",
      "Epoch: 096, Runtime 1.922969, Loss 0.999269, forward nfe 205125, backward nfe 101432, Train: 0.6750, Val: 0.3554, Test: 0.3902\n",
      "Epoch: 097, Runtime 1.924126, Loss 0.982091, forward nfe 207171, backward nfe 102459, Train: 0.6800, Val: 0.3754, Test: 0.4082\n",
      "Epoch: 098, Runtime 1.921266, Loss 0.944944, forward nfe 209224, backward nfe 103482, Train: 0.6900, Val: 0.3646, Test: 0.4007\n",
      "Epoch: 099, Runtime 1.924182, Loss 0.937365, forward nfe 211273, backward nfe 104508, Train: 0.6850, Val: 0.3469, Test: 0.3861\n",
      "best val accuracy 0.420769 with test accuracy 0.405240 at epoch 36\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.892947, Loss 2.324668, forward nfe 119, backward nfe 360, Train: 0.0950, Val: 0.0400, Test: 0.0394\n",
      "Epoch: 002, Runtime 1.921350, Loss 2.470163, forward nfe 2163, backward nfe 1384, Train: 0.0750, Val: 0.0708, Test: 0.0694\n",
      "Epoch: 003, Runtime 1.921131, Loss 2.342802, forward nfe 4207, backward nfe 2409, Train: 0.0950, Val: 0.0592, Test: 0.0609\n",
      "Epoch: 004, Runtime 1.920742, Loss 2.304811, forward nfe 6253, backward nfe 3434, Train: 0.0850, Val: 0.0562, Test: 0.0582\n",
      "Epoch: 005, Runtime 1.920744, Loss 2.296474, forward nfe 8299, backward nfe 4458, Train: 0.0900, Val: 0.0554, Test: 0.0582\n",
      "Epoch: 006, Runtime 1.931813, Loss 2.298213, forward nfe 10346, backward nfe 5485, Train: 0.1100, Val: 0.1123, Test: 0.1121\n",
      "Epoch: 007, Runtime 1.932596, Loss 2.292498, forward nfe 12419, backward nfe 6511, Train: 0.0700, Val: 0.1085, Test: 0.1059\n",
      "Epoch: 008, Runtime 2.011590, Loss 2.292627, forward nfe 14574, backward nfe 7544, Train: 0.0750, Val: 0.1277, Test: 0.1189\n",
      "Epoch: 009, Runtime 1.992781, Loss 2.288411, forward nfe 16713, backward nfe 8589, Train: 0.0750, Val: 0.1300, Test: 0.1243\n",
      "Epoch: 010, Runtime 2.022763, Loss 2.283092, forward nfe 18951, backward nfe 9627, Train: 0.1250, Val: 0.1685, Test: 0.1615\n",
      "Epoch: 011, Runtime 2.062881, Loss 2.278768, forward nfe 21189, backward nfe 10707, Train: 0.1200, Val: 0.1608, Test: 0.1570\n",
      "Epoch: 012, Runtime 2.076502, Loss 2.274193, forward nfe 23427, backward nfe 11801, Train: 0.0800, Val: 0.1231, Test: 0.1209\n",
      "Epoch: 013, Runtime 2.025344, Loss 2.268485, forward nfe 25665, backward nfe 12843, Train: 0.0800, Val: 0.1092, Test: 0.1090\n",
      "Epoch: 014, Runtime 2.095911, Loss 2.263486, forward nfe 27903, backward nfe 13962, Train: 0.1150, Val: 0.1192, Test: 0.1183\n",
      "Epoch: 015, Runtime 2.099234, Loss 2.255229, forward nfe 30141, backward nfe 15081, Train: 0.1350, Val: 0.1300, Test: 0.1314\n",
      "Epoch: 016, Runtime 2.094882, Loss 2.248157, forward nfe 32379, backward nfe 16200, Train: 0.1350, Val: 0.1292, Test: 0.1317\n",
      "Epoch: 017, Runtime 2.099635, Loss 2.240848, forward nfe 34617, backward nfe 17319, Train: 0.1300, Val: 0.1146, Test: 0.1184\n",
      "Epoch: 018, Runtime 2.096710, Loss 2.233389, forward nfe 36855, backward nfe 18438, Train: 0.1150, Val: 0.1023, Test: 0.1043\n",
      "Epoch: 019, Runtime 2.098702, Loss 2.226070, forward nfe 39093, backward nfe 19557, Train: 0.1200, Val: 0.1054, Test: 0.1085\n",
      "Epoch: 020, Runtime 2.096546, Loss 2.217251, forward nfe 41331, backward nfe 20676, Train: 0.1350, Val: 0.1215, Test: 0.1245\n",
      "Epoch: 021, Runtime 2.051514, Loss 2.205537, forward nfe 43569, backward nfe 21744, Train: 0.1350, Val: 0.1208, Test: 0.1228\n",
      "Epoch: 022, Runtime 2.098274, Loss 2.197267, forward nfe 45807, backward nfe 22863, Train: 0.1200, Val: 0.1108, Test: 0.1127\n",
      "Epoch: 023, Runtime 2.054803, Loss 2.189022, forward nfe 48045, backward nfe 23936, Train: 0.1250, Val: 0.0938, Test: 0.0944\n",
      "Epoch: 024, Runtime 2.096798, Loss 2.178796, forward nfe 50283, backward nfe 25055, Train: 0.1200, Val: 0.0808, Test: 0.0845\n",
      "Epoch: 025, Runtime 2.097763, Loss 2.166820, forward nfe 52521, backward nfe 26174, Train: 0.1200, Val: 0.0654, Test: 0.0784\n",
      "Epoch: 026, Runtime 2.029765, Loss 2.156529, forward nfe 54759, backward nfe 27221, Train: 0.1200, Val: 0.0646, Test: 0.0772\n",
      "Epoch: 027, Runtime 2.098539, Loss 2.148019, forward nfe 56997, backward nfe 28340, Train: 0.1200, Val: 0.0631, Test: 0.0738\n",
      "Epoch: 028, Runtime 2.096973, Loss 2.136575, forward nfe 59235, backward nfe 29459, Train: 0.1150, Val: 0.0631, Test: 0.0748\n",
      "Epoch: 029, Runtime 2.098508, Loss 2.127164, forward nfe 61473, backward nfe 30578, Train: 0.1350, Val: 0.0646, Test: 0.0784\n",
      "Epoch: 030, Runtime 2.105164, Loss 2.118379, forward nfe 63711, backward nfe 31697, Train: 0.1400, Val: 0.0631, Test: 0.0806\n",
      "Epoch: 031, Runtime 2.097795, Loss 2.108105, forward nfe 65949, backward nfe 32816, Train: 0.1450, Val: 0.0646, Test: 0.0810\n",
      "Epoch: 032, Runtime 2.096651, Loss 2.097018, forward nfe 68187, backward nfe 33935, Train: 0.1600, Val: 0.0646, Test: 0.0837\n",
      "Epoch: 033, Runtime 2.099244, Loss 2.090288, forward nfe 70425, backward nfe 35054, Train: 0.1750, Val: 0.0708, Test: 0.0898\n",
      "Epoch: 034, Runtime 2.095848, Loss 2.081894, forward nfe 72663, backward nfe 36173, Train: 0.1750, Val: 0.0677, Test: 0.0886\n",
      "Epoch: 035, Runtime 2.023190, Loss 2.078346, forward nfe 74901, backward nfe 37211, Train: 0.1800, Val: 0.0662, Test: 0.0846\n",
      "Epoch: 036, Runtime 2.100023, Loss 2.066187, forward nfe 77139, backward nfe 38330, Train: 0.1850, Val: 0.0638, Test: 0.0824\n",
      "Epoch: 037, Runtime 2.095270, Loss 2.058300, forward nfe 79377, backward nfe 39449, Train: 0.2000, Val: 0.0631, Test: 0.0809\n",
      "Epoch: 038, Runtime 2.100088, Loss 2.051425, forward nfe 81615, backward nfe 40568, Train: 0.2000, Val: 0.0638, Test: 0.0805\n",
      "Epoch: 039, Runtime 2.096656, Loss 2.042634, forward nfe 83853, backward nfe 41687, Train: 0.1950, Val: 0.0623, Test: 0.0784\n",
      "Epoch: 040, Runtime 2.099498, Loss 2.036606, forward nfe 86091, backward nfe 42806, Train: 0.1950, Val: 0.0623, Test: 0.0769\n",
      "Epoch: 041, Runtime 2.096895, Loss 2.031717, forward nfe 88329, backward nfe 43925, Train: 0.2000, Val: 0.0623, Test: 0.0774\n",
      "Epoch: 042, Runtime 2.051391, Loss 2.025126, forward nfe 90567, backward nfe 45022, Train: 0.2050, Val: 0.0592, Test: 0.0775\n",
      "Epoch: 043, Runtime 2.041359, Loss 2.018791, forward nfe 92746, backward nfe 46079, Train: 0.2050, Val: 0.0615, Test: 0.0774\n",
      "Epoch: 044, Runtime 2.012518, Loss 2.010272, forward nfe 94984, backward nfe 47104, Train: 0.2800, Val: 0.0723, Test: 0.1015\n",
      "Epoch: 045, Runtime 2.021765, Loss 2.003630, forward nfe 97222, backward nfe 48139, Train: 0.3050, Val: 0.0731, Test: 0.1085\n",
      "Epoch: 046, Runtime 2.097889, Loss 1.991717, forward nfe 99460, backward nfe 49258, Train: 0.3100, Val: 0.0838, Test: 0.1207\n",
      "Epoch: 047, Runtime 2.004469, Loss 1.966588, forward nfe 101680, backward nfe 50284, Train: 0.2450, Val: 0.0785, Test: 0.0982\n",
      "Epoch: 048, Runtime 2.044956, Loss 1.944023, forward nfe 103918, backward nfe 51345, Train: 0.2500, Val: 0.0769, Test: 0.0974\n",
      "Epoch: 049, Runtime 2.098247, Loss 1.923605, forward nfe 106156, backward nfe 52464, Train: 0.2550, Val: 0.1400, Test: 0.1634\n",
      "Epoch: 050, Runtime 2.097436, Loss 1.891190, forward nfe 108394, backward nfe 53583, Train: 0.2200, Val: 0.1892, Test: 0.1826\n",
      "Epoch: 051, Runtime 2.087664, Loss 1.877126, forward nfe 110632, backward nfe 54688, Train: 0.2300, Val: 0.1469, Test: 0.1495\n",
      "Epoch: 052, Runtime 2.030118, Loss 1.858854, forward nfe 112870, backward nfe 55735, Train: 0.3800, Val: 0.1762, Test: 0.1884\n",
      "Epoch: 053, Runtime 2.066930, Loss 1.830371, forward nfe 115108, backward nfe 56819, Train: 0.3900, Val: 0.1992, Test: 0.2169\n",
      "Epoch: 054, Runtime 2.027389, Loss 1.810583, forward nfe 117346, backward nfe 57863, Train: 0.4250, Val: 0.2477, Test: 0.2659\n",
      "Epoch: 055, Runtime 2.098602, Loss 1.784529, forward nfe 119584, backward nfe 58982, Train: 0.4250, Val: 0.2308, Test: 0.2518\n",
      "Epoch: 056, Runtime 2.046364, Loss 1.751628, forward nfe 121822, backward nfe 60046, Train: 0.4700, Val: 0.2008, Test: 0.2386\n",
      "Epoch: 057, Runtime 2.099141, Loss 1.716037, forward nfe 124060, backward nfe 61165, Train: 0.4450, Val: 0.1662, Test: 0.2033\n",
      "Epoch: 058, Runtime 2.047629, Loss 1.672805, forward nfe 126298, backward nfe 62232, Train: 0.4450, Val: 0.1508, Test: 0.1874\n",
      "Epoch: 059, Runtime 2.035779, Loss 1.632912, forward nfe 128536, backward nfe 63324, Train: 0.4700, Val: 0.1708, Test: 0.2069\n",
      "Epoch: 060, Runtime 1.964076, Loss 1.604326, forward nfe 130653, backward nfe 64352, Train: 0.4950, Val: 0.1962, Test: 0.2361\n",
      "Epoch: 061, Runtime 2.014229, Loss 1.558651, forward nfe 132788, backward nfe 65399, Train: 0.5600, Val: 0.2385, Test: 0.2804\n",
      "Epoch: 062, Runtime 2.019329, Loss 1.521318, forward nfe 135026, backward nfe 66429, Train: 0.5450, Val: 0.2746, Test: 0.3116\n",
      "Epoch: 063, Runtime 1.997045, Loss 1.478729, forward nfe 137264, backward nfe 67458, Train: 0.5500, Val: 0.3069, Test: 0.3333\n",
      "Epoch: 064, Runtime 1.998970, Loss 1.435155, forward nfe 139402, backward nfe 68490, Train: 0.5700, Val: 0.3262, Test: 0.3497\n",
      "Epoch: 065, Runtime 2.025703, Loss 1.404300, forward nfe 141640, backward nfe 69530, Train: 0.5850, Val: 0.3362, Test: 0.3652\n",
      "Epoch: 066, Runtime 1.995308, Loss 1.368050, forward nfe 143839, backward nfe 70556, Train: 0.5900, Val: 0.3408, Test: 0.3692\n",
      "Epoch: 067, Runtime 1.996244, Loss 1.337188, forward nfe 146052, backward nfe 71596, Train: 0.5900, Val: 0.3354, Test: 0.3692\n",
      "Epoch: 068, Runtime 1.948868, Loss 1.300606, forward nfe 148176, backward nfe 72627, Train: 0.6400, Val: 0.3231, Test: 0.3685\n",
      "Epoch: 069, Runtime 2.001462, Loss 1.274064, forward nfe 150319, backward nfe 73690, Train: 0.6550, Val: 0.3162, Test: 0.3615\n",
      "Epoch: 070, Runtime 1.997193, Loss 1.246649, forward nfe 152471, backward nfe 74720, Train: 0.6450, Val: 0.3177, Test: 0.3639\n",
      "Epoch: 071, Runtime 2.026392, Loss 1.205933, forward nfe 154662, backward nfe 75776, Train: 0.6500, Val: 0.3308, Test: 0.3772\n",
      "Epoch: 072, Runtime 1.995059, Loss 1.194741, forward nfe 156890, backward nfe 76803, Train: 0.6450, Val: 0.3369, Test: 0.3923\n",
      "Epoch: 073, Runtime 1.979825, Loss 1.155901, forward nfe 159019, backward nfe 77830, Train: 0.6400, Val: 0.3477, Test: 0.3947\n",
      "Epoch: 074, Runtime 1.976370, Loss 1.124873, forward nfe 161169, backward nfe 78881, Train: 0.6750, Val: 0.3408, Test: 0.3896\n",
      "Epoch: 075, Runtime 1.973733, Loss 1.105021, forward nfe 163316, backward nfe 79925, Train: 0.6800, Val: 0.3277, Test: 0.3843\n",
      "Epoch: 076, Runtime 1.963037, Loss 1.086442, forward nfe 165398, backward nfe 80970, Train: 0.6850, Val: 0.3338, Test: 0.3870\n",
      "Epoch: 077, Runtime 1.961367, Loss 1.053411, forward nfe 167557, backward nfe 82003, Train: 0.6900, Val: 0.3469, Test: 0.4035\n",
      "Epoch: 078, Runtime 1.980761, Loss 1.032187, forward nfe 169633, backward nfe 83052, Train: 0.6900, Val: 0.3600, Test: 0.4157\n",
      "Epoch: 079, Runtime 1.962008, Loss 1.026252, forward nfe 171765, backward nfe 84078, Train: 0.7050, Val: 0.3662, Test: 0.4130\n",
      "Epoch: 080, Runtime 1.937167, Loss 0.994954, forward nfe 173867, backward nfe 85111, Train: 0.7150, Val: 0.3646, Test: 0.4091\n",
      "Epoch: 081, Runtime 1.948396, Loss 0.977613, forward nfe 175936, backward nfe 86146, Train: 0.7250, Val: 0.3631, Test: 0.4167\n",
      "Epoch: 082, Runtime 1.939396, Loss 0.952645, forward nfe 178025, backward nfe 87181, Train: 0.7250, Val: 0.3900, Test: 0.4356\n",
      "Epoch: 083, Runtime 1.949882, Loss 0.928010, forward nfe 180105, backward nfe 88208, Train: 0.7300, Val: 0.3954, Test: 0.4426\n",
      "Epoch: 084, Runtime 1.952890, Loss 0.932526, forward nfe 182196, backward nfe 89245, Train: 0.7200, Val: 0.3954, Test: 0.4354\n",
      "Epoch: 085, Runtime 1.921978, Loss 0.898009, forward nfe 184266, backward nfe 90271, Train: 0.7150, Val: 0.4054, Test: 0.4407\n",
      "Epoch: 086, Runtime 1.936847, Loss 0.897571, forward nfe 186324, backward nfe 91298, Train: 0.7250, Val: 0.4008, Test: 0.4392\n",
      "Epoch: 087, Runtime 1.935506, Loss 0.892848, forward nfe 188407, backward nfe 92323, Train: 0.7400, Val: 0.4208, Test: 0.4674\n",
      "Epoch: 088, Runtime 1.937302, Loss 0.855491, forward nfe 190474, backward nfe 93357, Train: 0.7550, Val: 0.4631, Test: 0.4962\n",
      "Epoch: 089, Runtime 1.940403, Loss 0.860484, forward nfe 192524, backward nfe 94398, Train: 0.7500, Val: 0.4085, Test: 0.4568\n",
      "Epoch: 090, Runtime 1.927146, Loss 0.845361, forward nfe 194575, backward nfe 95427, Train: 0.7350, Val: 0.4146, Test: 0.4558\n",
      "Epoch: 091, Runtime 1.928499, Loss 0.821831, forward nfe 196628, backward nfe 96452, Train: 0.7550, Val: 0.4485, Test: 0.4827\n",
      "Epoch: 092, Runtime 1.927997, Loss 0.800473, forward nfe 198691, backward nfe 97479, Train: 0.7400, Val: 0.4623, Test: 0.4966\n",
      "Epoch: 093, Runtime 1.929171, Loss 0.802400, forward nfe 200751, backward nfe 98503, Train: 0.7500, Val: 0.4754, Test: 0.5098\n",
      "Epoch: 094, Runtime 1.928190, Loss 0.782431, forward nfe 202805, backward nfe 99531, Train: 0.7700, Val: 0.5077, Test: 0.5358\n",
      "Epoch: 095, Runtime 1.934490, Loss 0.745917, forward nfe 204871, backward nfe 100556, Train: 0.7800, Val: 0.5146, Test: 0.5399\n",
      "Epoch: 096, Runtime 1.951855, Loss 0.752682, forward nfe 206928, backward nfe 101609, Train: 0.7500, Val: 0.4646, Test: 0.4919\n",
      "Epoch: 097, Runtime 1.940098, Loss 0.736909, forward nfe 208986, backward nfe 102643, Train: 0.7500, Val: 0.4738, Test: 0.5027\n",
      "Epoch: 098, Runtime 1.927036, Loss 0.754838, forward nfe 211048, backward nfe 103666, Train: 0.7900, Val: 0.5485, Test: 0.5717\n",
      "Epoch: 099, Runtime 1.944508, Loss 0.741261, forward nfe 213111, backward nfe 104691, Train: 0.7850, Val: 0.5600, Test: 0.5788\n",
      "best val accuracy 0.560000 with test accuracy 0.578844 at epoch 99\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.832767, Loss 2.309138, forward nfe 119, backward nfe 297, Train: 0.1050, Val: 0.0469, Test: 0.0533\n",
      "Epoch: 002, Runtime 1.918664, Loss 2.305227, forward nfe 2163, backward nfe 1321, Train: 0.2700, Val: 0.2492, Test: 0.2615\n",
      "Epoch: 003, Runtime 1.920727, Loss 2.279170, forward nfe 4207, backward nfe 2345, Train: 0.1050, Val: 0.1538, Test: 0.1449\n",
      "Epoch: 004, Runtime 1.921069, Loss 2.361810, forward nfe 6251, backward nfe 3369, Train: 0.1900, Val: 0.0723, Test: 0.0771\n",
      "Epoch: 005, Runtime 1.920850, Loss 2.209267, forward nfe 8297, backward nfe 4393, Train: 0.2250, Val: 0.0338, Test: 0.0542\n",
      "Epoch: 006, Runtime 1.919975, Loss 2.232524, forward nfe 10342, backward nfe 5417, Train: 0.2000, Val: 0.0315, Test: 0.0467\n",
      "Epoch: 007, Runtime 1.922633, Loss 2.229953, forward nfe 12390, backward nfe 6442, Train: 0.1750, Val: 0.0669, Test: 0.0748\n",
      "Epoch: 008, Runtime 1.921819, Loss 2.171260, forward nfe 14441, backward nfe 7466, Train: 0.2250, Val: 0.1015, Test: 0.1085\n",
      "Epoch: 009, Runtime 1.930126, Loss 2.122930, forward nfe 16488, backward nfe 8497, Train: 0.2550, Val: 0.0892, Test: 0.1040\n",
      "Epoch: 010, Runtime 1.926650, Loss 2.126539, forward nfe 18546, backward nfe 9521, Train: 0.3050, Val: 0.1315, Test: 0.1553\n",
      "Epoch: 011, Runtime 1.928441, Loss 2.084302, forward nfe 20607, backward nfe 10546, Train: 0.4650, Val: 0.2746, Test: 0.3080\n",
      "Epoch: 012, Runtime 1.939158, Loss 2.031765, forward nfe 22671, backward nfe 11577, Train: 0.5150, Val: 0.3262, Test: 0.3541\n",
      "Epoch: 013, Runtime 1.923977, Loss 2.016617, forward nfe 24719, backward nfe 12601, Train: 0.5350, Val: 0.3215, Test: 0.3533\n",
      "Epoch: 014, Runtime 1.937040, Loss 1.987696, forward nfe 26769, backward nfe 13630, Train: 0.5400, Val: 0.3800, Test: 0.4101\n",
      "Epoch: 015, Runtime 1.931685, Loss 1.946204, forward nfe 28843, backward nfe 14654, Train: 0.5350, Val: 0.4800, Test: 0.4928\n",
      "Epoch: 016, Runtime 1.949280, Loss 1.912973, forward nfe 30919, backward nfe 15679, Train: 0.5500, Val: 0.5000, Test: 0.5166\n",
      "Epoch: 017, Runtime 1.961777, Loss 1.879616, forward nfe 33035, backward nfe 16711, Train: 0.5400, Val: 0.5185, Test: 0.5286\n",
      "Epoch: 018, Runtime 1.957434, Loss 1.835126, forward nfe 35129, backward nfe 17750, Train: 0.5900, Val: 0.5546, Test: 0.5583\n",
      "Epoch: 019, Runtime 1.942586, Loss 1.786481, forward nfe 37228, backward nfe 18778, Train: 0.6300, Val: 0.5854, Test: 0.5952\n",
      "Epoch: 020, Runtime 1.953685, Loss 1.750489, forward nfe 39330, backward nfe 19810, Train: 0.6600, Val: 0.6492, Test: 0.6545\n",
      "Epoch: 021, Runtime 1.967576, Loss 1.701293, forward nfe 41410, backward nfe 20842, Train: 0.6550, Val: 0.6854, Test: 0.6850\n",
      "Epoch: 022, Runtime 1.943795, Loss 1.653002, forward nfe 43543, backward nfe 21879, Train: 0.6750, Val: 0.6792, Test: 0.6882\n",
      "Epoch: 023, Runtime 1.963734, Loss 1.615161, forward nfe 45621, backward nfe 22929, Train: 0.6750, Val: 0.6723, Test: 0.6889\n",
      "Epoch: 024, Runtime 1.953185, Loss 1.567741, forward nfe 47675, backward nfe 23958, Train: 0.6700, Val: 0.6700, Test: 0.6866\n",
      "Epoch: 025, Runtime 1.934796, Loss 1.510473, forward nfe 49788, backward nfe 24990, Train: 0.6950, Val: 0.6731, Test: 0.6808\n",
      "Epoch: 026, Runtime 1.969242, Loss 1.463225, forward nfe 51885, backward nfe 26026, Train: 0.6950, Val: 0.6738, Test: 0.6841\n",
      "Epoch: 027, Runtime 1.962280, Loss 1.416420, forward nfe 53959, backward nfe 27070, Train: 0.6750, Val: 0.6731, Test: 0.6829\n",
      "Epoch: 028, Runtime 2.024856, Loss 1.374224, forward nfe 56139, backward nfe 28121, Train: 0.6750, Val: 0.6738, Test: 0.6747\n",
      "Epoch: 029, Runtime 1.972978, Loss 1.328996, forward nfe 58276, backward nfe 29158, Train: 0.6750, Val: 0.6638, Test: 0.6729\n",
      "Epoch: 030, Runtime 1.985863, Loss 1.282149, forward nfe 60497, backward nfe 30195, Train: 0.7150, Val: 0.6708, Test: 0.6810\n",
      "Epoch: 031, Runtime 1.983930, Loss 1.234936, forward nfe 62657, backward nfe 31222, Train: 0.7650, Val: 0.6869, Test: 0.7031\n",
      "Epoch: 032, Runtime 1.994134, Loss 1.204536, forward nfe 64835, backward nfe 32250, Train: 0.7850, Val: 0.7015, Test: 0.7183\n",
      "Epoch: 033, Runtime 1.961203, Loss 1.161863, forward nfe 66976, backward nfe 33283, Train: 0.7750, Val: 0.7108, Test: 0.7318\n",
      "Epoch: 034, Runtime 1.923204, Loss 1.127856, forward nfe 69050, backward nfe 34307, Train: 0.7950, Val: 0.7200, Test: 0.7407\n",
      "Epoch: 035, Runtime 1.939466, Loss 1.088301, forward nfe 71119, backward nfe 35335, Train: 0.8000, Val: 0.7146, Test: 0.7332\n",
      "Epoch: 036, Runtime 1.963496, Loss 1.060710, forward nfe 73222, backward nfe 36380, Train: 0.8000, Val: 0.7046, Test: 0.7262\n",
      "Epoch: 037, Runtime 1.953821, Loss 1.028061, forward nfe 75308, backward nfe 37409, Train: 0.8000, Val: 0.7031, Test: 0.7274\n",
      "Epoch: 038, Runtime 1.975145, Loss 0.994865, forward nfe 77381, backward nfe 38437, Train: 0.7950, Val: 0.7077, Test: 0.7301\n",
      "Epoch: 039, Runtime 1.968320, Loss 0.961836, forward nfe 79541, backward nfe 39484, Train: 0.8000, Val: 0.7008, Test: 0.7271\n",
      "Epoch: 040, Runtime 1.953153, Loss 0.937586, forward nfe 81625, backward nfe 40512, Train: 0.8000, Val: 0.7000, Test: 0.7270\n",
      "Epoch: 041, Runtime 1.943335, Loss 0.903437, forward nfe 83769, backward nfe 41536, Train: 0.8100, Val: 0.7069, Test: 0.7338\n",
      "Epoch: 042, Runtime 1.929960, Loss 0.877969, forward nfe 85821, backward nfe 42567, Train: 0.8000, Val: 0.7138, Test: 0.7391\n",
      "Epoch: 043, Runtime 1.962143, Loss 0.850753, forward nfe 87900, backward nfe 43607, Train: 0.8050, Val: 0.7215, Test: 0.7452\n",
      "Epoch: 044, Runtime 1.930305, Loss 0.829341, forward nfe 89973, backward nfe 44631, Train: 0.8150, Val: 0.7085, Test: 0.7369\n",
      "Epoch: 045, Runtime 1.960674, Loss 0.801719, forward nfe 92041, backward nfe 45689, Train: 0.8200, Val: 0.7046, Test: 0.7312\n",
      "Epoch: 046, Runtime 1.934312, Loss 0.784610, forward nfe 94100, backward nfe 46713, Train: 0.8200, Val: 0.7100, Test: 0.7363\n",
      "Epoch: 047, Runtime 1.924974, Loss 0.758399, forward nfe 96173, backward nfe 47740, Train: 0.8200, Val: 0.7223, Test: 0.7508\n",
      "Epoch: 048, Runtime 1.947011, Loss 0.741386, forward nfe 98231, backward nfe 48771, Train: 0.8150, Val: 0.7215, Test: 0.7497\n",
      "Epoch: 049, Runtime 1.938815, Loss 0.715046, forward nfe 100309, backward nfe 49811, Train: 0.8250, Val: 0.7177, Test: 0.7417\n",
      "Epoch: 050, Runtime 1.923059, Loss 0.697718, forward nfe 102362, backward nfe 50837, Train: 0.8250, Val: 0.7208, Test: 0.7449\n",
      "Epoch: 051, Runtime 1.935953, Loss 0.673507, forward nfe 104415, backward nfe 51862, Train: 0.8250, Val: 0.7308, Test: 0.7582\n",
      "Epoch: 052, Runtime 1.928978, Loss 0.653553, forward nfe 106485, backward nfe 52894, Train: 0.8300, Val: 0.7315, Test: 0.7605\n",
      "Epoch: 053, Runtime 1.928136, Loss 0.638424, forward nfe 108538, backward nfe 53924, Train: 0.8250, Val: 0.7277, Test: 0.7515\n",
      "Epoch: 054, Runtime 1.933330, Loss 0.614270, forward nfe 110595, backward nfe 54948, Train: 0.8300, Val: 0.7231, Test: 0.7501\n",
      "Epoch: 055, Runtime 1.919934, Loss 0.593114, forward nfe 112656, backward nfe 55972, Train: 0.8350, Val: 0.7238, Test: 0.7550\n",
      "Epoch: 056, Runtime 1.942562, Loss 0.583882, forward nfe 114706, backward nfe 57007, Train: 0.8350, Val: 0.7346, Test: 0.7641\n",
      "Epoch: 057, Runtime 1.924608, Loss 0.563092, forward nfe 116782, backward nfe 58031, Train: 0.8400, Val: 0.7331, Test: 0.7635\n",
      "Epoch: 058, Runtime 1.928666, Loss 0.547350, forward nfe 118833, backward nfe 59055, Train: 0.8550, Val: 0.7300, Test: 0.7594\n",
      "Epoch: 059, Runtime 1.937046, Loss 0.533178, forward nfe 120900, backward nfe 60083, Train: 0.8600, Val: 0.7323, Test: 0.7596\n",
      "Epoch: 060, Runtime 1.927146, Loss 0.529834, forward nfe 122968, backward nfe 61110, Train: 0.8600, Val: 0.7392, Test: 0.7658\n",
      "Epoch: 061, Runtime 1.926489, Loss 0.516485, forward nfe 125018, backward nfe 62134, Train: 0.8600, Val: 0.7423, Test: 0.7687\n",
      "Epoch: 062, Runtime 1.921958, Loss 0.495257, forward nfe 127074, backward nfe 63159, Train: 0.8600, Val: 0.7431, Test: 0.7693\n",
      "Epoch: 063, Runtime 1.923015, Loss 0.488804, forward nfe 129122, backward nfe 64185, Train: 0.8700, Val: 0.7446, Test: 0.7692\n",
      "Epoch: 064, Runtime 1.946079, Loss 0.476116, forward nfe 131179, backward nfe 65220, Train: 0.8700, Val: 0.7492, Test: 0.7692\n",
      "Epoch: 065, Runtime 1.925846, Loss 0.462035, forward nfe 133251, backward nfe 66244, Train: 0.8650, Val: 0.7554, Test: 0.7770\n",
      "Epoch: 066, Runtime 1.933762, Loss 0.453037, forward nfe 135299, backward nfe 67277, Train: 0.8700, Val: 0.7462, Test: 0.7724\n",
      "Epoch: 067, Runtime 1.925694, Loss 0.456547, forward nfe 137352, backward nfe 68305, Train: 0.8700, Val: 0.7508, Test: 0.7713\n",
      "Epoch: 068, Runtime 1.930897, Loss 0.437538, forward nfe 139400, backward nfe 69339, Train: 0.8700, Val: 0.7515, Test: 0.7737\n",
      "Epoch: 069, Runtime 1.933355, Loss 0.431246, forward nfe 141452, backward nfe 70375, Train: 0.8750, Val: 0.7546, Test: 0.7748\n",
      "Epoch: 070, Runtime 1.929921, Loss 0.418087, forward nfe 143497, backward nfe 71400, Train: 0.8700, Val: 0.7638, Test: 0.7821\n",
      "Epoch: 071, Runtime 1.931891, Loss 0.413523, forward nfe 145549, backward nfe 72428, Train: 0.8750, Val: 0.7623, Test: 0.7813\n",
      "Epoch: 072, Runtime 1.922698, Loss 0.393849, forward nfe 147605, backward nfe 73452, Train: 0.8700, Val: 0.7562, Test: 0.7773\n",
      "Epoch: 073, Runtime 1.933065, Loss 0.397582, forward nfe 149654, backward nfe 74483, Train: 0.8700, Val: 0.7592, Test: 0.7809\n",
      "Epoch: 074, Runtime 1.924021, Loss 0.384503, forward nfe 151708, backward nfe 75510, Train: 0.8700, Val: 0.7692, Test: 0.7897\n",
      "Epoch: 075, Runtime 1.942696, Loss 0.379081, forward nfe 153753, backward nfe 76558, Train: 0.8750, Val: 0.7592, Test: 0.7856\n",
      "Epoch: 076, Runtime 1.924218, Loss 0.376299, forward nfe 155797, backward nfe 77587, Train: 0.8850, Val: 0.7538, Test: 0.7761\n",
      "Epoch: 077, Runtime 1.921445, Loss 0.378030, forward nfe 157842, backward nfe 78611, Train: 0.8750, Val: 0.7677, Test: 0.7884\n",
      "Epoch: 078, Runtime 1.925191, Loss 0.363431, forward nfe 159887, backward nfe 79639, Train: 0.8700, Val: 0.7754, Test: 0.7933\n",
      "Epoch: 079, Runtime 1.927922, Loss 0.366591, forward nfe 161934, backward nfe 80669, Train: 0.8750, Val: 0.7692, Test: 0.7908\n",
      "Epoch: 080, Runtime 1.920758, Loss 0.343518, forward nfe 163979, backward nfe 81693, Train: 0.8800, Val: 0.7692, Test: 0.7872\n",
      "Epoch: 081, Runtime 1.923794, Loss 0.354193, forward nfe 166029, backward nfe 82720, Train: 0.8800, Val: 0.7800, Test: 0.7966\n",
      "Epoch: 082, Runtime 1.923492, Loss 0.337543, forward nfe 168074, backward nfe 83746, Train: 0.8750, Val: 0.7808, Test: 0.7981\n",
      "Epoch: 083, Runtime 1.926180, Loss 0.339470, forward nfe 170121, backward nfe 84774, Train: 0.8850, Val: 0.7677, Test: 0.7913\n",
      "Epoch: 084, Runtime 1.926789, Loss 0.327324, forward nfe 172171, backward nfe 85801, Train: 0.8850, Val: 0.7615, Test: 0.7889\n",
      "Epoch: 085, Runtime 1.921783, Loss 0.334353, forward nfe 174222, backward nfe 86827, Train: 0.8800, Val: 0.7685, Test: 0.7942\n",
      "Epoch: 086, Runtime 1.924261, Loss 0.334955, forward nfe 176270, backward nfe 87851, Train: 0.8900, Val: 0.7762, Test: 0.7990\n",
      "Epoch: 087, Runtime 1.924274, Loss 0.316482, forward nfe 178320, backward nfe 88876, Train: 0.8900, Val: 0.7777, Test: 0.8025\n",
      "Epoch: 088, Runtime 1.922517, Loss 0.308886, forward nfe 180366, backward nfe 89901, Train: 0.8950, Val: 0.7677, Test: 0.7925\n",
      "Epoch: 089, Runtime 1.928796, Loss 0.316544, forward nfe 182410, backward nfe 90931, Train: 0.8900, Val: 0.7862, Test: 0.8093\n",
      "Epoch: 090, Runtime 1.923723, Loss 0.337537, forward nfe 184457, backward nfe 91955, Train: 0.8900, Val: 0.7800, Test: 0.8056\n",
      "Epoch: 091, Runtime 1.925116, Loss 0.300253, forward nfe 186506, backward nfe 92983, Train: 0.9000, Val: 0.7700, Test: 0.7970\n",
      "Epoch: 092, Runtime 1.927749, Loss 0.293862, forward nfe 188552, backward nfe 94013, Train: 0.8950, Val: 0.7915, Test: 0.8075\n",
      "Epoch: 093, Runtime 1.921826, Loss 0.304609, forward nfe 190598, backward nfe 95037, Train: 0.8900, Val: 0.7946, Test: 0.8140\n",
      "Epoch: 094, Runtime 1.918090, Loss 0.302966, forward nfe 192646, backward nfe 96060, Train: 0.9000, Val: 0.7846, Test: 0.8062\n",
      "Epoch: 095, Runtime 1.928107, Loss 0.288444, forward nfe 194693, backward nfe 97090, Train: 0.9150, Val: 0.7785, Test: 0.8017\n",
      "Epoch: 096, Runtime 1.921572, Loss 0.290171, forward nfe 196741, backward nfe 98113, Train: 0.9050, Val: 0.7815, Test: 0.8093\n",
      "Epoch: 097, Runtime 1.922654, Loss 0.279188, forward nfe 198788, backward nfe 99138, Train: 0.9050, Val: 0.7823, Test: 0.8124\n",
      "Epoch: 098, Runtime 1.926081, Loss 0.270052, forward nfe 200839, backward nfe 100162, Train: 0.9250, Val: 0.7831, Test: 0.8054\n",
      "Epoch: 099, Runtime 1.921950, Loss 0.268295, forward nfe 202887, backward nfe 101186, Train: 0.9100, Val: 0.7900, Test: 0.8111\n",
      "best val accuracy 0.794615 with test accuracy 0.813990 at epoch 93\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.914060, Loss 2.317536, forward nfe 121, backward nfe 386, Train: 0.1050, Val: 0.1062, Test: 0.1046\n",
      "Epoch: 002, Runtime 1.921678, Loss 2.557335, forward nfe 2165, backward nfe 1410, Train: 0.1250, Val: 0.1031, Test: 0.1056\n",
      "Epoch: 003, Runtime 1.921775, Loss 2.313234, forward nfe 4209, backward nfe 2434, Train: 0.1000, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 004, Runtime 1.924124, Loss 2.302119, forward nfe 6257, backward nfe 3458, Train: 0.1000, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 005, Runtime 1.920749, Loss 2.309066, forward nfe 8305, backward nfe 4482, Train: 0.1000, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 006, Runtime 1.930005, Loss 2.300958, forward nfe 10351, backward nfe 5512, Train: 0.1050, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 007, Runtime 1.928587, Loss 2.300861, forward nfe 12403, backward nfe 6536, Train: 0.1050, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 008, Runtime 1.963433, Loss 2.300139, forward nfe 14476, backward nfe 7597, Train: 0.1050, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 009, Runtime 1.970701, Loss 2.299768, forward nfe 16540, backward nfe 8621, Train: 0.1050, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 010, Runtime 2.040797, Loss 2.294420, forward nfe 18778, backward nfe 9678, Train: 0.1400, Val: 0.0408, Test: 0.0525\n",
      "Epoch: 011, Runtime 2.092669, Loss 2.267504, forward nfe 21016, backward nfe 10792, Train: 0.1550, Val: 0.0377, Test: 0.0524\n",
      "Epoch: 012, Runtime 2.090925, Loss 2.267460, forward nfe 23254, backward nfe 11911, Train: 0.1350, Val: 0.0346, Test: 0.0488\n",
      "Epoch: 013, Runtime 2.056828, Loss 2.258593, forward nfe 25481, backward nfe 12987, Train: 0.1100, Val: 0.0315, Test: 0.0451\n",
      "Epoch: 014, Runtime 2.100447, Loss 2.256304, forward nfe 27719, backward nfe 14106, Train: 0.1600, Val: 0.0415, Test: 0.0570\n",
      "Epoch: 015, Runtime 2.096249, Loss 2.239781, forward nfe 29957, backward nfe 15225, Train: 0.1250, Val: 0.0315, Test: 0.0493\n",
      "Epoch: 016, Runtime 2.098082, Loss 2.247118, forward nfe 32195, backward nfe 16344, Train: 0.1400, Val: 0.0385, Test: 0.0548\n",
      "Epoch: 017, Runtime 2.095132, Loss 2.225997, forward nfe 34433, backward nfe 17463, Train: 0.2000, Val: 0.0569, Test: 0.0713\n",
      "Epoch: 018, Runtime 2.099779, Loss 2.218345, forward nfe 36671, backward nfe 18582, Train: 0.1800, Val: 0.0546, Test: 0.0717\n",
      "Epoch: 019, Runtime 2.093107, Loss 2.216170, forward nfe 38909, backward nfe 19701, Train: 0.1600, Val: 0.0662, Test: 0.0753\n",
      "Epoch: 020, Runtime 2.060345, Loss 2.199661, forward nfe 41147, backward nfe 20779, Train: 0.1350, Val: 0.0600, Test: 0.0701\n",
      "Epoch: 021, Runtime 2.094879, Loss 2.192374, forward nfe 43385, backward nfe 21898, Train: 0.1400, Val: 0.0631, Test: 0.0758\n",
      "Epoch: 022, Runtime 2.095695, Loss 2.185349, forward nfe 45623, backward nfe 23017, Train: 0.1600, Val: 0.1108, Test: 0.1139\n",
      "Epoch: 023, Runtime 2.099577, Loss 2.157297, forward nfe 47861, backward nfe 24136, Train: 0.2100, Val: 0.4023, Test: 0.3761\n",
      "Epoch: 024, Runtime 2.032309, Loss 2.142831, forward nfe 50099, backward nfe 25184, Train: 0.2350, Val: 0.4138, Test: 0.3890\n",
      "Epoch: 025, Runtime 2.012984, Loss 2.138726, forward nfe 52337, backward nfe 26213, Train: 0.1800, Val: 0.3346, Test: 0.3146\n",
      "Epoch: 026, Runtime 2.099003, Loss 2.106368, forward nfe 54575, backward nfe 27332, Train: 0.1650, Val: 0.2308, Test: 0.2178\n",
      "Epoch: 027, Runtime 2.095926, Loss 2.112738, forward nfe 56813, backward nfe 28451, Train: 0.1900, Val: 0.3585, Test: 0.3368\n",
      "Epoch: 028, Runtime 2.099901, Loss 2.075854, forward nfe 59051, backward nfe 29570, Train: 0.2800, Val: 0.1631, Test: 0.1610\n",
      "Epoch: 029, Runtime 2.054039, Loss 2.070068, forward nfe 61289, backward nfe 30643, Train: 0.2450, Val: 0.1038, Test: 0.1090\n",
      "Epoch: 030, Runtime 2.099786, Loss 2.052759, forward nfe 63527, backward nfe 31762, Train: 0.2550, Val: 0.1569, Test: 0.1582\n",
      "Epoch: 031, Runtime 2.104495, Loss 2.026961, forward nfe 65765, backward nfe 32881, Train: 0.2550, Val: 0.1608, Test: 0.1591\n",
      "Epoch: 032, Runtime 2.096744, Loss 2.018218, forward nfe 68003, backward nfe 34000, Train: 0.2200, Val: 0.1177, Test: 0.1227\n",
      "Epoch: 033, Runtime 2.097760, Loss 1.989952, forward nfe 70241, backward nfe 35119, Train: 0.2600, Val: 0.1031, Test: 0.1070\n",
      "Epoch: 034, Runtime 2.098878, Loss 1.986298, forward nfe 72479, backward nfe 36238, Train: 0.2700, Val: 0.1146, Test: 0.1175\n",
      "Epoch: 035, Runtime 2.022662, Loss 1.957005, forward nfe 74717, backward nfe 37276, Train: 0.2400, Val: 0.1423, Test: 0.1437\n",
      "Epoch: 036, Runtime 2.096514, Loss 1.951238, forward nfe 76955, backward nfe 38395, Train: 0.2500, Val: 0.1531, Test: 0.1561\n",
      "Epoch: 037, Runtime 2.070980, Loss 1.933761, forward nfe 79193, backward nfe 39484, Train: 0.2450, Val: 0.1408, Test: 0.1390\n",
      "Epoch: 038, Runtime 2.095184, Loss 1.914439, forward nfe 81431, backward nfe 40603, Train: 0.2550, Val: 0.1477, Test: 0.1461\n",
      "Epoch: 039, Runtime 2.099526, Loss 1.905466, forward nfe 83669, backward nfe 41722, Train: 0.2750, Val: 0.2015, Test: 0.1969\n",
      "Epoch: 040, Runtime 2.063182, Loss 1.888483, forward nfe 85907, backward nfe 42804, Train: 0.3100, Val: 0.2162, Test: 0.2144\n",
      "Epoch: 041, Runtime 2.021646, Loss 1.878252, forward nfe 88145, backward nfe 43840, Train: 0.2850, Val: 0.1977, Test: 0.1998\n",
      "Epoch: 042, Runtime 2.095455, Loss 1.858869, forward nfe 90383, backward nfe 44959, Train: 0.3000, Val: 0.1669, Test: 0.1702\n",
      "Epoch: 043, Runtime 2.022286, Loss 1.851941, forward nfe 92621, backward nfe 45997, Train: 0.3500, Val: 0.1908, Test: 0.1998\n",
      "Epoch: 044, Runtime 2.069256, Loss 1.835008, forward nfe 94800, backward nfe 47116, Train: 0.3250, Val: 0.2085, Test: 0.2138\n",
      "Epoch: 045, Runtime 2.039764, Loss 1.823273, forward nfe 96962, backward nfe 48209, Train: 0.3400, Val: 0.2154, Test: 0.2186\n",
      "Epoch: 046, Runtime 2.012935, Loss 1.811136, forward nfe 99200, backward nfe 49236, Train: 0.3600, Val: 0.2008, Test: 0.2078\n",
      "Epoch: 047, Runtime 2.096267, Loss 1.800709, forward nfe 101438, backward nfe 50355, Train: 0.3700, Val: 0.2108, Test: 0.2164\n",
      "Epoch: 048, Runtime 2.098013, Loss 1.787386, forward nfe 103676, backward nfe 51473, Train: 0.3550, Val: 0.2292, Test: 0.2329\n",
      "Epoch: 049, Runtime 2.029931, Loss 1.770871, forward nfe 105914, backward nfe 52537, Train: 0.3700, Val: 0.2315, Test: 0.2364\n",
      "Epoch: 050, Runtime 2.038856, Loss 1.761207, forward nfe 108098, backward nfe 53604, Train: 0.3900, Val: 0.2169, Test: 0.2311\n",
      "Epoch: 051, Runtime 2.017296, Loss 1.743950, forward nfe 110336, backward nfe 54638, Train: 0.4050, Val: 0.2046, Test: 0.2167\n",
      "Epoch: 052, Runtime 2.015780, Loss 1.738208, forward nfe 112566, backward nfe 55670, Train: 0.4050, Val: 0.2177, Test: 0.2373\n",
      "Epoch: 053, Runtime 1.975819, Loss 1.722644, forward nfe 114803, backward nfe 56704, Train: 0.4200, Val: 0.2354, Test: 0.2507\n",
      "Epoch: 054, Runtime 1.978003, Loss 1.715262, forward nfe 116854, backward nfe 57739, Train: 0.4350, Val: 0.2438, Test: 0.2532\n",
      "Epoch: 055, Runtime 2.020895, Loss 1.701268, forward nfe 119092, backward nfe 58773, Train: 0.4350, Val: 0.2369, Test: 0.2542\n",
      "Epoch: 056, Runtime 2.017547, Loss 1.692102, forward nfe 121330, backward nfe 59811, Train: 0.4450, Val: 0.2462, Test: 0.2596\n",
      "Epoch: 057, Runtime 1.978184, Loss 1.675091, forward nfe 123477, backward nfe 60843, Train: 0.4350, Val: 0.2477, Test: 0.2644\n",
      "Epoch: 058, Runtime 2.018013, Loss 1.664428, forward nfe 125715, backward nfe 61875, Train: 0.4550, Val: 0.2600, Test: 0.2742\n",
      "Epoch: 059, Runtime 1.953310, Loss 1.658645, forward nfe 127911, backward nfe 62900, Train: 0.4500, Val: 0.2762, Test: 0.2842\n",
      "Epoch: 060, Runtime 1.953776, Loss 1.639806, forward nfe 130019, backward nfe 63924, Train: 0.4500, Val: 0.3000, Test: 0.3012\n",
      "Epoch: 061, Runtime 1.926950, Loss 1.630937, forward nfe 132079, backward nfe 64953, Train: 0.4650, Val: 0.3192, Test: 0.3129\n",
      "Epoch: 062, Runtime 1.948748, Loss 1.622290, forward nfe 134170, backward nfe 65977, Train: 0.4600, Val: 0.3369, Test: 0.3286\n",
      "Epoch: 063, Runtime 1.921237, Loss 1.601964, forward nfe 136229, backward nfe 67001, Train: 0.4600, Val: 0.3292, Test: 0.3224\n",
      "Epoch: 064, Runtime 1.936312, Loss 1.590396, forward nfe 138300, backward nfe 68031, Train: 0.4550, Val: 0.3177, Test: 0.3151\n",
      "Epoch: 065, Runtime 1.940296, Loss 1.581485, forward nfe 140385, backward nfe 69055, Train: 0.4550, Val: 0.3492, Test: 0.3422\n",
      "Epoch: 066, Runtime 1.935801, Loss 1.575909, forward nfe 142458, backward nfe 70080, Train: 0.4700, Val: 0.3508, Test: 0.3426\n",
      "Epoch: 067, Runtime 1.932574, Loss 1.552642, forward nfe 144522, backward nfe 71104, Train: 0.4700, Val: 0.3315, Test: 0.3304\n",
      "Epoch: 068, Runtime 1.957540, Loss 1.544258, forward nfe 146632, backward nfe 72136, Train: 0.4650, Val: 0.3400, Test: 0.3383\n",
      "Epoch: 069, Runtime 1.929795, Loss 1.535974, forward nfe 148704, backward nfe 73161, Train: 0.4650, Val: 0.3408, Test: 0.3368\n",
      "Epoch: 070, Runtime 1.932760, Loss 1.525014, forward nfe 150775, backward nfe 74186, Train: 0.4850, Val: 0.3346, Test: 0.3337\n",
      "Epoch: 071, Runtime 1.926133, Loss 1.503639, forward nfe 152834, backward nfe 75210, Train: 0.4600, Val: 0.3315, Test: 0.3209\n",
      "Epoch: 072, Runtime 1.924912, Loss 1.505778, forward nfe 154887, backward nfe 76234, Train: 0.4850, Val: 0.3292, Test: 0.3333\n",
      "Epoch: 073, Runtime 1.941948, Loss 1.494519, forward nfe 156964, backward nfe 77260, Train: 0.5100, Val: 0.3300, Test: 0.3317\n",
      "Epoch: 074, Runtime 1.930889, Loss 1.483442, forward nfe 159028, backward nfe 78290, Train: 0.5150, Val: 0.3200, Test: 0.3275\n",
      "Epoch: 075, Runtime 1.928063, Loss 1.479788, forward nfe 161091, backward nfe 79316, Train: 0.4950, Val: 0.3200, Test: 0.3196\n",
      "Epoch: 076, Runtime 1.924579, Loss 1.472269, forward nfe 163138, backward nfe 80344, Train: 0.5350, Val: 0.3162, Test: 0.3252\n",
      "Epoch: 077, Runtime 1.926620, Loss 1.458439, forward nfe 165186, backward nfe 81372, Train: 0.5400, Val: 0.3185, Test: 0.3278\n",
      "Epoch: 078, Runtime 1.934609, Loss 1.442319, forward nfe 167235, backward nfe 82405, Train: 0.4650, Val: 0.3269, Test: 0.3161\n",
      "Epoch: 079, Runtime 1.927602, Loss 1.428931, forward nfe 169296, backward nfe 83429, Train: 0.4950, Val: 0.3215, Test: 0.3293\n",
      "Epoch: 080, Runtime 1.924551, Loss 1.421497, forward nfe 171350, backward nfe 84454, Train: 0.5250, Val: 0.3254, Test: 0.3342\n",
      "Epoch: 081, Runtime 1.926108, Loss 1.409213, forward nfe 173412, backward nfe 85478, Train: 0.5250, Val: 0.3177, Test: 0.3336\n",
      "Epoch: 082, Runtime 1.921325, Loss 1.413736, forward nfe 175458, backward nfe 86502, Train: 0.4900, Val: 0.3162, Test: 0.3195\n",
      "Epoch: 083, Runtime 1.921279, Loss 1.392550, forward nfe 177505, backward nfe 87527, Train: 0.5100, Val: 0.3146, Test: 0.3234\n",
      "Epoch: 084, Runtime 1.926253, Loss 1.394286, forward nfe 179559, backward nfe 88551, Train: 0.5350, Val: 0.3085, Test: 0.3255\n",
      "Epoch: 085, Runtime 1.923002, Loss 1.379825, forward nfe 181605, backward nfe 89575, Train: 0.5350, Val: 0.2977, Test: 0.3196\n",
      "Epoch: 086, Runtime 1.921782, Loss 1.358647, forward nfe 183654, backward nfe 90601, Train: 0.5150, Val: 0.2962, Test: 0.3115\n",
      "Epoch: 087, Runtime 1.928627, Loss 1.366228, forward nfe 185713, backward nfe 91625, Train: 0.5500, Val: 0.2985, Test: 0.3159\n",
      "Epoch: 088, Runtime 1.919630, Loss 1.351531, forward nfe 187759, backward nfe 92649, Train: 0.5350, Val: 0.2938, Test: 0.3142\n",
      "Epoch: 089, Runtime 1.923825, Loss 1.344878, forward nfe 189811, backward nfe 93673, Train: 0.5500, Val: 0.3000, Test: 0.3175\n",
      "Epoch: 090, Runtime 1.920985, Loss 1.335370, forward nfe 191859, backward nfe 94697, Train: 0.5200, Val: 0.3008, Test: 0.3089\n",
      "Epoch: 091, Runtime 1.929434, Loss 1.331274, forward nfe 193906, backward nfe 95721, Train: 0.5500, Val: 0.2915, Test: 0.3126\n",
      "Epoch: 092, Runtime 1.919050, Loss 1.305540, forward nfe 195953, backward nfe 96744, Train: 0.5400, Val: 0.2838, Test: 0.3098\n",
      "Epoch: 093, Runtime 1.920887, Loss 1.315625, forward nfe 198002, backward nfe 97769, Train: 0.5500, Val: 0.2931, Test: 0.3143\n",
      "Epoch: 094, Runtime 1.924230, Loss 1.302047, forward nfe 200048, backward nfe 98793, Train: 0.5200, Val: 0.2977, Test: 0.3098\n",
      "Epoch: 095, Runtime 1.920778, Loss 1.297408, forward nfe 202099, backward nfe 99816, Train: 0.5550, Val: 0.2762, Test: 0.3003\n",
      "Epoch: 096, Runtime 1.920728, Loss 1.285865, forward nfe 204147, backward nfe 100840, Train: 0.5350, Val: 0.2654, Test: 0.2945\n",
      "Epoch: 097, Runtime 1.922473, Loss 1.283350, forward nfe 206193, backward nfe 101864, Train: 0.5500, Val: 0.2692, Test: 0.2974\n",
      "Epoch: 098, Runtime 1.921201, Loss 1.262549, forward nfe 208238, backward nfe 102887, Train: 0.5500, Val: 0.2777, Test: 0.3006\n",
      "Epoch: 099, Runtime 1.922574, Loss 1.271313, forward nfe 210287, backward nfe 103910, Train: 0.5450, Val: 0.2908, Test: 0.3015\n",
      "best val accuracy 0.413846 with test accuracy 0.388998 at epoch 24\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.850278, Loss 2.321738, forward nfe 118, backward nfe 316, Train: 0.1100, Val: 0.0831, Test: 0.0822\n",
      "Epoch: 002, Runtime 1.920030, Loss 2.524098, forward nfe 2162, backward nfe 1340, Train: 0.1000, Val: 0.0162, Test: 0.0329\n",
      "Epoch: 003, Runtime 1.921703, Loss 2.436288, forward nfe 4206, backward nfe 2364, Train: 0.1000, Val: 0.1554, Test: 0.1479\n",
      "Epoch: 004, Runtime 1.921011, Loss 2.282495, forward nfe 6250, backward nfe 3388, Train: 0.1900, Val: 0.1215, Test: 0.1250\n",
      "Epoch: 005, Runtime 1.923130, Loss 2.266297, forward nfe 8295, backward nfe 4412, Train: 0.2350, Val: 0.1508, Test: 0.1565\n",
      "Epoch: 006, Runtime 1.927247, Loss 2.243196, forward nfe 10348, backward nfe 5438, Train: 0.2000, Val: 0.1662, Test: 0.1596\n",
      "Epoch: 007, Runtime 1.930201, Loss 2.185733, forward nfe 12394, backward nfe 6468, Train: 0.3500, Val: 0.2415, Test: 0.2433\n",
      "Epoch: 008, Runtime 1.931024, Loss 2.138750, forward nfe 14443, backward nfe 7500, Train: 0.3850, Val: 0.1492, Test: 0.1738\n",
      "Epoch: 009, Runtime 1.938966, Loss 2.110629, forward nfe 16497, backward nfe 8530, Train: 0.3300, Val: 0.0885, Test: 0.1103\n",
      "Epoch: 010, Runtime 1.968307, Loss 2.099342, forward nfe 18586, backward nfe 9590, Train: 0.3950, Val: 0.1115, Test: 0.1397\n",
      "Epoch: 011, Runtime 1.939663, Loss 2.065043, forward nfe 20645, backward nfe 10627, Train: 0.4350, Val: 0.2015, Test: 0.2185\n",
      "Epoch: 012, Runtime 1.996788, Loss 2.017078, forward nfe 22745, backward nfe 11657, Train: 0.4250, Val: 0.2615, Test: 0.2732\n",
      "Epoch: 013, Runtime 2.036736, Loss 1.987220, forward nfe 24983, backward nfe 12711, Train: 0.3900, Val: 0.2438, Test: 0.2463\n",
      "Epoch: 014, Runtime 1.973905, Loss 1.971259, forward nfe 27207, backward nfe 13750, Train: 0.3700, Val: 0.2085, Test: 0.2246\n",
      "Epoch: 015, Runtime 1.984489, Loss 1.933022, forward nfe 29349, backward nfe 14776, Train: 0.4400, Val: 0.2223, Test: 0.2453\n",
      "Epoch: 016, Runtime 2.038471, Loss 1.890455, forward nfe 31528, backward nfe 15832, Train: 0.4800, Val: 0.2354, Test: 0.2616\n",
      "Epoch: 017, Runtime 2.017173, Loss 1.855244, forward nfe 33731, backward nfe 16881, Train: 0.4700, Val: 0.2392, Test: 0.2680\n",
      "Epoch: 018, Runtime 1.995586, Loss 1.826155, forward nfe 35969, backward nfe 17913, Train: 0.5050, Val: 0.2492, Test: 0.2777\n",
      "Epoch: 019, Runtime 1.998307, Loss 1.793383, forward nfe 38158, backward nfe 18961, Train: 0.5200, Val: 0.2669, Test: 0.2933\n",
      "Epoch: 020, Runtime 2.038136, Loss 1.759624, forward nfe 40324, backward nfe 20014, Train: 0.5250, Val: 0.2777, Test: 0.3038\n",
      "Epoch: 021, Runtime 2.003944, Loss 1.716592, forward nfe 42562, backward nfe 21039, Train: 0.5200, Val: 0.2831, Test: 0.3122\n",
      "Epoch: 022, Runtime 1.968940, Loss 1.683323, forward nfe 44706, backward nfe 22097, Train: 0.5250, Val: 0.2808, Test: 0.3143\n",
      "Epoch: 023, Runtime 1.946579, Loss 1.648888, forward nfe 46811, backward nfe 23124, Train: 0.5650, Val: 0.2885, Test: 0.3192\n",
      "Epoch: 024, Runtime 1.998470, Loss 1.618686, forward nfe 48960, backward nfe 24181, Train: 0.5800, Val: 0.2969, Test: 0.3295\n",
      "Epoch: 025, Runtime 2.014338, Loss 1.578943, forward nfe 51102, backward nfe 25210, Train: 0.5800, Val: 0.3123, Test: 0.3416\n",
      "Epoch: 026, Runtime 2.020013, Loss 1.542652, forward nfe 53340, backward nfe 26243, Train: 0.5950, Val: 0.3215, Test: 0.3484\n",
      "Epoch: 027, Runtime 2.085208, Loss 1.509266, forward nfe 55568, backward nfe 27353, Train: 0.6250, Val: 0.3208, Test: 0.3505\n",
      "Epoch: 028, Runtime 1.981903, Loss 1.467773, forward nfe 57805, backward nfe 28384, Train: 0.6200, Val: 0.3162, Test: 0.3503\n",
      "Epoch: 029, Runtime 2.027965, Loss 1.439616, forward nfe 59965, backward nfe 29425, Train: 0.6400, Val: 0.3200, Test: 0.3540\n",
      "Epoch: 030, Runtime 2.000995, Loss 1.398081, forward nfe 62203, backward nfe 30461, Train: 0.6550, Val: 0.3162, Test: 0.3537\n",
      "Epoch: 031, Runtime 1.985586, Loss 1.347194, forward nfe 64397, backward nfe 31495, Train: 0.6650, Val: 0.3308, Test: 0.3573\n",
      "Epoch: 032, Runtime 1.986288, Loss 1.319344, forward nfe 66564, backward nfe 32530, Train: 0.6900, Val: 0.3700, Test: 0.4015\n",
      "Epoch: 033, Runtime 2.009325, Loss 1.278648, forward nfe 68683, backward nfe 33564, Train: 0.6900, Val: 0.4354, Test: 0.4641\n",
      "Epoch: 034, Runtime 1.969756, Loss 1.247653, forward nfe 70825, backward nfe 34592, Train: 0.7300, Val: 0.5769, Test: 0.5906\n",
      "Epoch: 035, Runtime 1.951813, Loss 1.210926, forward nfe 73015, backward nfe 35621, Train: 0.7400, Val: 0.6031, Test: 0.6204\n",
      "Epoch: 036, Runtime 1.983547, Loss 1.185082, forward nfe 75151, backward nfe 36670, Train: 0.7250, Val: 0.5608, Test: 0.5766\n",
      "Epoch: 037, Runtime 1.958606, Loss 1.140903, forward nfe 77232, backward nfe 37695, Train: 0.7050, Val: 0.4585, Test: 0.4873\n",
      "Epoch: 038, Runtime 1.988885, Loss 1.104988, forward nfe 79381, backward nfe 38755, Train: 0.7150, Val: 0.4685, Test: 0.4952\n",
      "Epoch: 039, Runtime 1.971344, Loss 1.080007, forward nfe 81442, backward nfe 39785, Train: 0.7700, Val: 0.5954, Test: 0.6145\n",
      "Epoch: 040, Runtime 1.943539, Loss 1.049057, forward nfe 83625, backward nfe 40809, Train: 0.7900, Val: 0.6362, Test: 0.6525\n",
      "Epoch: 041, Runtime 1.951858, Loss 1.017698, forward nfe 85692, backward nfe 41843, Train: 0.8000, Val: 0.6469, Test: 0.6652\n",
      "Epoch: 042, Runtime 1.972341, Loss 0.993957, forward nfe 87817, backward nfe 42872, Train: 0.8050, Val: 0.6454, Test: 0.6752\n",
      "Epoch: 043, Runtime 1.937883, Loss 0.966421, forward nfe 89925, backward nfe 43908, Train: 0.8000, Val: 0.6531, Test: 0.6813\n",
      "Epoch: 044, Runtime 1.976783, Loss 0.941232, forward nfe 92042, backward nfe 44940, Train: 0.8250, Val: 0.6723, Test: 0.6965\n",
      "Epoch: 045, Runtime 1.932286, Loss 0.914483, forward nfe 94120, backward nfe 45969, Train: 0.8250, Val: 0.6892, Test: 0.7113\n",
      "Epoch: 046, Runtime 1.992601, Loss 0.892478, forward nfe 96254, backward nfe 46994, Train: 0.8100, Val: 0.6838, Test: 0.7111\n",
      "Epoch: 047, Runtime 1.965274, Loss 0.857734, forward nfe 98420, backward nfe 48037, Train: 0.8200, Val: 0.6792, Test: 0.7070\n",
      "Epoch: 048, Runtime 1.954268, Loss 0.845902, forward nfe 100515, backward nfe 49066, Train: 0.8150, Val: 0.6815, Test: 0.7080\n",
      "Epoch: 049, Runtime 1.943549, Loss 0.821825, forward nfe 102594, backward nfe 50099, Train: 0.8250, Val: 0.6908, Test: 0.7161\n",
      "Epoch: 050, Runtime 1.931757, Loss 0.798006, forward nfe 104675, backward nfe 51123, Train: 0.8250, Val: 0.7000, Test: 0.7238\n",
      "Epoch: 051, Runtime 1.929210, Loss 0.769814, forward nfe 106739, backward nfe 52147, Train: 0.8250, Val: 0.7000, Test: 0.7300\n",
      "Epoch: 052, Runtime 1.930130, Loss 0.751275, forward nfe 108798, backward nfe 53174, Train: 0.8250, Val: 0.7046, Test: 0.7346\n",
      "Epoch: 053, Runtime 1.931334, Loss 0.736349, forward nfe 110858, backward nfe 54200, Train: 0.8350, Val: 0.7092, Test: 0.7355\n",
      "Epoch: 054, Runtime 1.933735, Loss 0.718138, forward nfe 112925, backward nfe 55224, Train: 0.8350, Val: 0.7100, Test: 0.7358\n",
      "Epoch: 055, Runtime 1.922248, Loss 0.699918, forward nfe 114995, backward nfe 56249, Train: 0.8300, Val: 0.7131, Test: 0.7361\n",
      "Epoch: 056, Runtime 1.935958, Loss 0.680058, forward nfe 117046, backward nfe 57284, Train: 0.8450, Val: 0.7069, Test: 0.7377\n",
      "Epoch: 057, Runtime 1.935596, Loss 0.669214, forward nfe 119110, backward nfe 58317, Train: 0.8550, Val: 0.7146, Test: 0.7432\n",
      "Epoch: 058, Runtime 1.927047, Loss 0.649507, forward nfe 121156, backward nfe 59343, Train: 0.8450, Val: 0.7208, Test: 0.7479\n",
      "Epoch: 059, Runtime 1.926493, Loss 0.637215, forward nfe 123210, backward nfe 60372, Train: 0.8500, Val: 0.7185, Test: 0.7461\n",
      "Epoch: 060, Runtime 1.934013, Loss 0.614091, forward nfe 125258, backward nfe 61401, Train: 0.8450, Val: 0.7177, Test: 0.7440\n",
      "Epoch: 061, Runtime 1.923957, Loss 0.601743, forward nfe 127318, backward nfe 62425, Train: 0.8500, Val: 0.7208, Test: 0.7486\n",
      "Epoch: 062, Runtime 1.930901, Loss 0.591092, forward nfe 129370, backward nfe 63450, Train: 0.8500, Val: 0.7269, Test: 0.7533\n",
      "Epoch: 063, Runtime 1.931647, Loss 0.571034, forward nfe 131438, backward nfe 64482, Train: 0.8500, Val: 0.7269, Test: 0.7553\n",
      "Epoch: 064, Runtime 1.934182, Loss 0.563576, forward nfe 133489, backward nfe 65509, Train: 0.8500, Val: 0.7277, Test: 0.7507\n",
      "Epoch: 065, Runtime 1.931266, Loss 0.545114, forward nfe 135537, backward nfe 66537, Train: 0.8500, Val: 0.7269, Test: 0.7552\n",
      "Epoch: 066, Runtime 1.925885, Loss 0.542858, forward nfe 137599, backward nfe 67563, Train: 0.8550, Val: 0.7338, Test: 0.7575\n",
      "Epoch: 067, Runtime 1.927577, Loss 0.528511, forward nfe 139656, backward nfe 68589, Train: 0.8600, Val: 0.7338, Test: 0.7573\n",
      "Epoch: 068, Runtime 1.927167, Loss 0.520940, forward nfe 141703, backward nfe 69618, Train: 0.8650, Val: 0.7331, Test: 0.7584\n",
      "Epoch: 069, Runtime 1.923565, Loss 0.506892, forward nfe 143748, backward nfe 70645, Train: 0.8600, Val: 0.7385, Test: 0.7610\n",
      "Epoch: 070, Runtime 1.923993, Loss 0.496886, forward nfe 145796, backward nfe 71669, Train: 0.8550, Val: 0.7385, Test: 0.7647\n",
      "Epoch: 071, Runtime 1.928629, Loss 0.492155, forward nfe 147849, backward nfe 72697, Train: 0.8600, Val: 0.7385, Test: 0.7618\n",
      "Epoch: 072, Runtime 1.922352, Loss 0.486066, forward nfe 149895, backward nfe 73722, Train: 0.8600, Val: 0.7438, Test: 0.7669\n",
      "Epoch: 073, Runtime 1.924751, Loss 0.469714, forward nfe 151941, backward nfe 74750, Train: 0.8650, Val: 0.7500, Test: 0.7687\n",
      "Epoch: 074, Runtime 1.921492, Loss 0.460132, forward nfe 153986, backward nfe 75774, Train: 0.8650, Val: 0.7423, Test: 0.7683\n",
      "Epoch: 075, Runtime 1.922409, Loss 0.461454, forward nfe 156030, backward nfe 76798, Train: 0.8650, Val: 0.7477, Test: 0.7698\n",
      "Epoch: 076, Runtime 1.923716, Loss 0.448045, forward nfe 158076, backward nfe 77823, Train: 0.8600, Val: 0.7531, Test: 0.7721\n",
      "Epoch: 077, Runtime 1.923538, Loss 0.445407, forward nfe 160124, backward nfe 78849, Train: 0.8600, Val: 0.7531, Test: 0.7741\n",
      "Epoch: 078, Runtime 1.921454, Loss 0.433588, forward nfe 162169, backward nfe 79873, Train: 0.8650, Val: 0.7515, Test: 0.7779\n",
      "Epoch: 079, Runtime 1.920879, Loss 0.417277, forward nfe 164213, backward nfe 80897, Train: 0.8650, Val: 0.7508, Test: 0.7790\n",
      "Epoch: 080, Runtime 1.924120, Loss 0.418066, forward nfe 166258, backward nfe 81924, Train: 0.8600, Val: 0.7577, Test: 0.7797\n",
      "Epoch: 081, Runtime 1.923355, Loss 0.401375, forward nfe 168307, backward nfe 82948, Train: 0.8650, Val: 0.7592, Test: 0.7783\n",
      "Epoch: 082, Runtime 1.925001, Loss 0.406045, forward nfe 170353, backward nfe 83976, Train: 0.8600, Val: 0.7592, Test: 0.7793\n",
      "Epoch: 083, Runtime 1.919703, Loss 0.393588, forward nfe 172399, backward nfe 85000, Train: 0.8650, Val: 0.7562, Test: 0.7837\n",
      "Epoch: 084, Runtime 1.924789, Loss 0.378601, forward nfe 174444, backward nfe 86027, Train: 0.8700, Val: 0.7577, Test: 0.7867\n",
      "Epoch: 085, Runtime 1.921258, Loss 0.384578, forward nfe 176492, backward nfe 87052, Train: 0.8700, Val: 0.7554, Test: 0.7835\n",
      "Epoch: 086, Runtime 1.921480, Loss 0.377231, forward nfe 178536, backward nfe 88078, Train: 0.8750, Val: 0.7654, Test: 0.7850\n",
      "Epoch: 087, Runtime 1.927046, Loss 0.365251, forward nfe 180583, backward nfe 89106, Train: 0.8800, Val: 0.7654, Test: 0.7864\n",
      "Epoch: 088, Runtime 1.923709, Loss 0.363279, forward nfe 182633, backward nfe 90132, Train: 0.8750, Val: 0.7638, Test: 0.7879\n",
      "Epoch: 089, Runtime 1.923969, Loss 0.351074, forward nfe 184678, backward nfe 91158, Train: 0.8850, Val: 0.7615, Test: 0.7885\n",
      "Epoch: 090, Runtime 1.922452, Loss 0.357401, forward nfe 186726, backward nfe 92181, Train: 0.8900, Val: 0.7715, Test: 0.7940\n",
      "Epoch: 091, Runtime 1.922266, Loss 0.343334, forward nfe 188776, backward nfe 93206, Train: 0.8850, Val: 0.7715, Test: 0.7929\n",
      "Epoch: 092, Runtime 1.921574, Loss 0.339663, forward nfe 190822, backward nfe 94230, Train: 0.8900, Val: 0.7692, Test: 0.7900\n",
      "Epoch: 093, Runtime 1.921503, Loss 0.344012, forward nfe 192867, backward nfe 95254, Train: 0.8850, Val: 0.7754, Test: 0.7960\n",
      "Epoch: 094, Runtime 1.923280, Loss 0.332701, forward nfe 194913, backward nfe 96278, Train: 0.8800, Val: 0.7754, Test: 0.7948\n",
      "Epoch: 095, Runtime 1.930733, Loss 0.320167, forward nfe 196964, backward nfe 97303, Train: 0.9000, Val: 0.7715, Test: 0.7915\n",
      "Epoch: 096, Runtime 1.921538, Loss 0.313076, forward nfe 199011, backward nfe 98327, Train: 0.8900, Val: 0.7762, Test: 0.7967\n",
      "Epoch: 097, Runtime 1.922048, Loss 0.313445, forward nfe 201056, backward nfe 99351, Train: 0.9000, Val: 0.7777, Test: 0.7999\n",
      "Epoch: 098, Runtime 1.922231, Loss 0.306665, forward nfe 203102, backward nfe 100376, Train: 0.9000, Val: 0.7808, Test: 0.8001\n",
      "Epoch: 099, Runtime 1.924956, Loss 0.297926, forward nfe 205147, backward nfe 101403, Train: 0.9050, Val: 0.7769, Test: 0.7962\n",
      "best val accuracy 0.780769 with test accuracy 0.800114 at epoch 98\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 1.0, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 1.0, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.826622, Loss 2.333534, forward nfe 118, backward nfe 290, Train: 0.1150, Val: 0.3000, Test: 0.2757\n",
      "Epoch: 002, Runtime 1.920373, Loss 2.431937, forward nfe 2162, backward nfe 1314, Train: 0.1000, Val: 0.0223, Test: 0.0357\n",
      "Epoch: 003, Runtime 1.918595, Loss 2.337613, forward nfe 4206, backward nfe 2339, Train: 0.1000, Val: 0.0223, Test: 0.0357\n",
      "Epoch: 004, Runtime 1.920127, Loss 2.308376, forward nfe 6250, backward nfe 3363, Train: 0.1850, Val: 0.0338, Test: 0.0544\n",
      "Epoch: 005, Runtime 1.922659, Loss 2.302625, forward nfe 8300, backward nfe 4389, Train: 0.1150, Val: 0.0231, Test: 0.0366\n",
      "Epoch: 006, Runtime 1.927866, Loss 2.289293, forward nfe 10344, backward nfe 5419, Train: 0.1050, Val: 0.0223, Test: 0.0360\n",
      "Epoch: 007, Runtime 2.026200, Loss 2.283893, forward nfe 12464, backward nfe 6471, Train: 0.1050, Val: 0.0223, Test: 0.0360\n",
      "Epoch: 008, Runtime 2.095922, Loss 2.278990, forward nfe 14702, backward nfe 7590, Train: 0.1200, Val: 0.0223, Test: 0.0362\n",
      "Epoch: 009, Runtime 2.015208, Loss 2.271937, forward nfe 16940, backward nfe 8620, Train: 0.1200, Val: 0.0231, Test: 0.0373\n",
      "Epoch: 010, Runtime 2.097597, Loss 2.260770, forward nfe 19178, backward nfe 9739, Train: 0.1500, Val: 0.0254, Test: 0.0405\n",
      "Epoch: 011, Runtime 2.096179, Loss 2.252088, forward nfe 21416, backward nfe 10858, Train: 0.1550, Val: 0.0285, Test: 0.0443\n",
      "Epoch: 012, Runtime 2.098123, Loss 2.243818, forward nfe 23654, backward nfe 11977, Train: 0.1550, Val: 0.0400, Test: 0.0596\n",
      "Epoch: 013, Runtime 2.095722, Loss 2.230660, forward nfe 25892, backward nfe 13096, Train: 0.1600, Val: 0.0685, Test: 0.0807\n",
      "Epoch: 014, Runtime 2.099288, Loss 2.225031, forward nfe 28130, backward nfe 14215, Train: 0.1550, Val: 0.0762, Test: 0.0797\n",
      "Epoch: 015, Runtime 2.095488, Loss 2.213080, forward nfe 30368, backward nfe 15334, Train: 0.1650, Val: 0.0808, Test: 0.0841\n",
      "Epoch: 016, Runtime 2.098563, Loss 2.201428, forward nfe 32606, backward nfe 16453, Train: 0.1650, Val: 0.0769, Test: 0.0894\n",
      "Epoch: 017, Runtime 2.096966, Loss 2.193244, forward nfe 34844, backward nfe 17572, Train: 0.1900, Val: 0.0769, Test: 0.0939\n",
      "Epoch: 018, Runtime 2.070674, Loss 2.182261, forward nfe 37082, backward nfe 18660, Train: 0.2000, Val: 0.0900, Test: 0.0984\n",
      "Epoch: 019, Runtime 2.095537, Loss 2.171168, forward nfe 39320, backward nfe 19779, Train: 0.1650, Val: 0.0885, Test: 0.1007\n",
      "Epoch: 020, Runtime 2.099927, Loss 2.160075, forward nfe 41558, backward nfe 20898, Train: 0.1600, Val: 0.0915, Test: 0.1017\n",
      "Epoch: 021, Runtime 2.095615, Loss 2.153187, forward nfe 43796, backward nfe 22017, Train: 0.2050, Val: 0.0954, Test: 0.1110\n",
      "Epoch: 022, Runtime 2.099792, Loss 2.138338, forward nfe 46034, backward nfe 23136, Train: 0.1800, Val: 0.0569, Test: 0.0713\n",
      "Epoch: 023, Runtime 2.098894, Loss 2.136036, forward nfe 48272, backward nfe 24255, Train: 0.2000, Val: 0.0885, Test: 0.0988\n",
      "Epoch: 024, Runtime 2.097369, Loss 2.115087, forward nfe 50510, backward nfe 25374, Train: 0.2150, Val: 0.0908, Test: 0.1071\n",
      "Epoch: 025, Runtime 2.099270, Loss 2.103651, forward nfe 52748, backward nfe 26493, Train: 0.2150, Val: 0.0723, Test: 0.0926\n",
      "Epoch: 026, Runtime 2.018386, Loss 2.084177, forward nfe 54986, backward nfe 27528, Train: 0.1850, Val: 0.0592, Test: 0.0762\n",
      "Epoch: 027, Runtime 2.098365, Loss 2.068345, forward nfe 57224, backward nfe 28647, Train: 0.1950, Val: 0.0600, Test: 0.0751\n",
      "Epoch: 028, Runtime 2.097955, Loss 2.062126, forward nfe 59462, backward nfe 29766, Train: 0.2150, Val: 0.0608, Test: 0.0810\n",
      "Epoch: 029, Runtime 2.106052, Loss 2.038933, forward nfe 61700, backward nfe 30885, Train: 0.2850, Val: 0.0723, Test: 0.0997\n",
      "Epoch: 030, Runtime 2.078584, Loss 2.033405, forward nfe 63938, backward nfe 31983, Train: 0.2300, Val: 0.0608, Test: 0.0831\n",
      "Epoch: 031, Runtime 2.077713, Loss 2.010698, forward nfe 66176, backward nfe 33081, Train: 0.2200, Val: 0.0615, Test: 0.0792\n",
      "Epoch: 032, Runtime 2.100169, Loss 1.999068, forward nfe 68414, backward nfe 34200, Train: 0.2300, Val: 0.0615, Test: 0.0821\n",
      "Epoch: 033, Runtime 2.095704, Loss 1.985758, forward nfe 70652, backward nfe 35319, Train: 0.3050, Val: 0.0869, Test: 0.1103\n",
      "Epoch: 034, Runtime 2.099132, Loss 1.969569, forward nfe 72890, backward nfe 36438, Train: 0.3250, Val: 0.1008, Test: 0.1185\n",
      "Epoch: 035, Runtime 2.092192, Loss 1.957527, forward nfe 75128, backward nfe 37551, Train: 0.2400, Val: 0.0638, Test: 0.0866\n",
      "Epoch: 036, Runtime 2.036580, Loss 1.945189, forward nfe 77366, backward nfe 38606, Train: 0.2300, Val: 0.0638, Test: 0.0838\n",
      "Epoch: 037, Runtime 2.098797, Loss 1.935032, forward nfe 79604, backward nfe 39725, Train: 0.2550, Val: 0.0831, Test: 0.1045\n",
      "Epoch: 038, Runtime 2.071888, Loss 1.919178, forward nfe 81842, backward nfe 40816, Train: 0.3900, Val: 0.1485, Test: 0.1695\n",
      "Epoch: 039, Runtime 2.097244, Loss 1.905448, forward nfe 84080, backward nfe 41935, Train: 0.3600, Val: 0.1331, Test: 0.1568\n",
      "Epoch: 040, Runtime 2.091142, Loss 1.889601, forward nfe 86318, backward nfe 43046, Train: 0.3200, Val: 0.1192, Test: 0.1450\n",
      "Epoch: 041, Runtime 2.071488, Loss 1.881691, forward nfe 88556, backward nfe 44136, Train: 0.4150, Val: 0.1792, Test: 0.1985\n",
      "Epoch: 042, Runtime 2.031729, Loss 1.857998, forward nfe 90794, backward nfe 45183, Train: 0.4000, Val: 0.1946, Test: 0.2103\n",
      "Epoch: 043, Runtime 2.096981, Loss 1.843407, forward nfe 93032, backward nfe 46302, Train: 0.3300, Val: 0.1354, Test: 0.1636\n",
      "Epoch: 044, Runtime 2.083038, Loss 1.832332, forward nfe 95270, backward nfe 47421, Train: 0.3450, Val: 0.1515, Test: 0.1794\n",
      "Epoch: 045, Runtime 1.994060, Loss 1.815357, forward nfe 97476, backward nfe 48454, Train: 0.4500, Val: 0.2069, Test: 0.2352\n",
      "Epoch: 046, Runtime 2.043445, Loss 1.798343, forward nfe 99661, backward nfe 49513, Train: 0.4250, Val: 0.1915, Test: 0.2238\n",
      "Epoch: 047, Runtime 1.984090, Loss 1.785051, forward nfe 101808, backward nfe 50554, Train: 0.4250, Val: 0.1831, Test: 0.2109\n",
      "Epoch: 048, Runtime 2.031943, Loss 1.770596, forward nfe 104046, backward nfe 51601, Train: 0.4800, Val: 0.2231, Test: 0.2528\n",
      "Epoch: 049, Runtime 2.064593, Loss 1.761472, forward nfe 106284, backward nfe 52685, Train: 0.4650, Val: 0.2115, Test: 0.2358\n",
      "Epoch: 050, Runtime 2.039585, Loss 1.740154, forward nfe 108481, backward nfe 53758, Train: 0.4300, Val: 0.1946, Test: 0.2205\n",
      "Epoch: 051, Runtime 2.005285, Loss 1.731579, forward nfe 110719, backward nfe 54782, Train: 0.5150, Val: 0.2454, Test: 0.2689\n",
      "Epoch: 052, Runtime 1.962910, Loss 1.710446, forward nfe 112891, backward nfe 55807, Train: 0.5350, Val: 0.2515, Test: 0.2691\n",
      "Epoch: 053, Runtime 1.991829, Loss 1.682292, forward nfe 115022, backward nfe 56874, Train: 0.5150, Val: 0.2362, Test: 0.2583\n",
      "Epoch: 054, Runtime 1.935890, Loss 1.677960, forward nfe 117122, backward nfe 57905, Train: 0.5600, Val: 0.2600, Test: 0.2847\n",
      "Epoch: 055, Runtime 1.959779, Loss 1.658593, forward nfe 119232, backward nfe 58931, Train: 0.5550, Val: 0.2777, Test: 0.3067\n",
      "Epoch: 056, Runtime 1.977941, Loss 1.643670, forward nfe 121311, backward nfe 59997, Train: 0.5350, Val: 0.2715, Test: 0.2935\n",
      "Epoch: 057, Runtime 1.979772, Loss 1.621779, forward nfe 123387, backward nfe 61031, Train: 0.5050, Val: 0.2854, Test: 0.3004\n",
      "Epoch: 058, Runtime 1.966805, Loss 1.620844, forward nfe 125614, backward nfe 62058, Train: 0.5750, Val: 0.3146, Test: 0.3386\n",
      "Epoch: 059, Runtime 1.966240, Loss 1.595577, forward nfe 127725, backward nfe 63084, Train: 0.5450, Val: 0.3331, Test: 0.3447\n",
      "Epoch: 060, Runtime 1.940483, Loss 1.573779, forward nfe 129808, backward nfe 64115, Train: 0.5600, Val: 0.3562, Test: 0.3569\n",
      "Epoch: 061, Runtime 1.924342, Loss 1.561352, forward nfe 131865, backward nfe 65141, Train: 0.5500, Val: 0.3654, Test: 0.3682\n",
      "Epoch: 062, Runtime 1.930365, Loss 1.545145, forward nfe 133914, backward nfe 66165, Train: 0.5600, Val: 0.3800, Test: 0.3844\n",
      "Epoch: 063, Runtime 1.929427, Loss 1.532434, forward nfe 135997, backward nfe 67190, Train: 0.5750, Val: 0.3969, Test: 0.3998\n",
      "Epoch: 064, Runtime 1.943779, Loss 1.519591, forward nfe 138053, backward nfe 68214, Train: 0.5650, Val: 0.4215, Test: 0.4167\n",
      "Epoch: 065, Runtime 1.942787, Loss 1.519152, forward nfe 140146, backward nfe 69249, Train: 0.5650, Val: 0.4377, Test: 0.4297\n",
      "Epoch: 066, Runtime 1.936673, Loss 1.496152, forward nfe 142225, backward nfe 70275, Train: 0.5750, Val: 0.4392, Test: 0.4341\n",
      "Epoch: 067, Runtime 1.927871, Loss 1.492338, forward nfe 144291, backward nfe 71300, Train: 0.5800, Val: 0.4385, Test: 0.4349\n",
      "Epoch: 068, Runtime 1.944707, Loss 1.462271, forward nfe 146350, backward nfe 72330, Train: 0.5700, Val: 0.4200, Test: 0.4235\n",
      "Epoch: 069, Runtime 1.923085, Loss 1.460176, forward nfe 148431, backward nfe 73355, Train: 0.5800, Val: 0.4069, Test: 0.4198\n",
      "Epoch: 070, Runtime 1.924530, Loss 1.441374, forward nfe 150482, backward nfe 74382, Train: 0.5750, Val: 0.4108, Test: 0.4154\n",
      "Epoch: 071, Runtime 1.932395, Loss 1.431185, forward nfe 152546, backward nfe 75411, Train: 0.5550, Val: 0.3962, Test: 0.4056\n",
      "Epoch: 072, Runtime 1.923668, Loss 1.427358, forward nfe 154594, backward nfe 76436, Train: 0.5950, Val: 0.4008, Test: 0.4148\n",
      "Epoch: 073, Runtime 1.922537, Loss 1.406367, forward nfe 156641, backward nfe 77462, Train: 0.6100, Val: 0.4000, Test: 0.4145\n",
      "Epoch: 074, Runtime 1.926113, Loss 1.386216, forward nfe 158690, backward nfe 78490, Train: 0.5700, Val: 0.3938, Test: 0.4084\n",
      "Epoch: 075, Runtime 1.929959, Loss 1.390407, forward nfe 160744, backward nfe 79517, Train: 0.5900, Val: 0.3977, Test: 0.4099\n",
      "Epoch: 076, Runtime 1.922280, Loss 1.374121, forward nfe 162797, backward nfe 80541, Train: 0.5900, Val: 0.4062, Test: 0.4126\n",
      "Epoch: 077, Runtime 1.921116, Loss 1.369599, forward nfe 164846, backward nfe 81565, Train: 0.6050, Val: 0.4146, Test: 0.4204\n",
      "Epoch: 078, Runtime 1.921644, Loss 1.355646, forward nfe 166895, backward nfe 82590, Train: 0.5850, Val: 0.4331, Test: 0.4385\n",
      "Epoch: 079, Runtime 1.920078, Loss 1.351284, forward nfe 168943, backward nfe 83614, Train: 0.6100, Val: 0.4554, Test: 0.4467\n",
      "Epoch: 080, Runtime 1.922458, Loss 1.332194, forward nfe 170990, backward nfe 84637, Train: 0.6350, Val: 0.4469, Test: 0.4520\n",
      "Epoch: 081, Runtime 1.923151, Loss 1.341467, forward nfe 173040, backward nfe 85661, Train: 0.6250, Val: 0.4562, Test: 0.4538\n",
      "Epoch: 082, Runtime 1.920912, Loss 1.318915, forward nfe 175090, backward nfe 86686, Train: 0.6050, Val: 0.4454, Test: 0.4469\n",
      "Epoch: 083, Runtime 1.920285, Loss 1.304150, forward nfe 177138, backward nfe 87710, Train: 0.6250, Val: 0.4415, Test: 0.4422\n",
      "Epoch: 084, Runtime 1.922515, Loss 1.311316, forward nfe 179184, backward nfe 88734, Train: 0.6450, Val: 0.4392, Test: 0.4529\n",
      "Epoch: 085, Runtime 1.921194, Loss 1.285757, forward nfe 181231, backward nfe 89758, Train: 0.5950, Val: 0.4469, Test: 0.4491\n",
      "Epoch: 086, Runtime 1.922550, Loss 1.289547, forward nfe 183276, backward nfe 90783, Train: 0.6450, Val: 0.4531, Test: 0.4618\n",
      "Epoch: 087, Runtime 1.920367, Loss 1.265410, forward nfe 185324, backward nfe 91806, Train: 0.6500, Val: 0.4508, Test: 0.4585\n",
      "Epoch: 088, Runtime 1.921987, Loss 1.268347, forward nfe 187369, backward nfe 92830, Train: 0.6550, Val: 0.4546, Test: 0.4589\n",
      "Epoch: 089, Runtime 1.922783, Loss 1.250991, forward nfe 189415, backward nfe 93855, Train: 0.6050, Val: 0.4362, Test: 0.4456\n",
      "Epoch: 090, Runtime 1.922602, Loss 1.261976, forward nfe 191462, backward nfe 94879, Train: 0.6400, Val: 0.4385, Test: 0.4438\n",
      "Epoch: 091, Runtime 1.920518, Loss 1.253038, forward nfe 193510, backward nfe 95903, Train: 0.6550, Val: 0.4346, Test: 0.4483\n",
      "Epoch: 092, Runtime 1.921391, Loss 1.243592, forward nfe 195556, backward nfe 96926, Train: 0.6550, Val: 0.4500, Test: 0.4560\n",
      "Epoch: 093, Runtime 1.929573, Loss 1.232780, forward nfe 197604, backward nfe 97949, Train: 0.6250, Val: 0.4554, Test: 0.4629\n",
      "Epoch: 094, Runtime 1.922154, Loss 1.231783, forward nfe 199651, backward nfe 98973, Train: 0.6250, Val: 0.4569, Test: 0.4677\n",
      "Epoch: 095, Runtime 1.922179, Loss 1.231943, forward nfe 201696, backward nfe 99996, Train: 0.6650, Val: 0.4708, Test: 0.4732\n",
      "Epoch: 096, Runtime 1.920684, Loss 1.210281, forward nfe 203742, backward nfe 101019, Train: 0.6650, Val: 0.4600, Test: 0.4727\n",
      "Epoch: 097, Runtime 1.917475, Loss 1.207750, forward nfe 205787, backward nfe 102042, Train: 0.6350, Val: 0.4708, Test: 0.4691\n",
      "Epoch: 098, Runtime 1.919396, Loss 1.197831, forward nfe 207832, backward nfe 103065, Train: 0.6300, Val: 0.4646, Test: 0.4653\n",
      "Epoch: 099, Runtime 1.921408, Loss 1.200092, forward nfe 209876, backward nfe 104088, Train: 0.6600, Val: 0.4585, Test: 0.4686\n",
      "best val accuracy 0.470769 with test accuracy 0.473229 at epoch 95\n",
      "*** Doing stepsize 0.5 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.444043, Loss 2.325633, forward nfe 169, backward nfe 318, Train: 0.0850, Val: 0.0162, Test: 0.0236\n",
      "Epoch: 002, Runtime 3.047717, Loss 2.551157, forward nfe 4637, backward nfe 1342, Train: 0.0800, Val: 0.0954, Test: 0.0948\n",
      "Epoch: 003, Runtime 3.049946, Loss 2.333307, forward nfe 9105, backward nfe 2366, Train: 0.0800, Val: 0.1254, Test: 0.1170\n",
      "Epoch: 004, Runtime 3.057848, Loss 2.327431, forward nfe 13574, backward nfe 3393, Train: 0.1550, Val: 0.0915, Test: 0.0979\n",
      "Epoch: 005, Runtime 3.054183, Loss 2.280384, forward nfe 18051, backward nfe 4417, Train: 0.1350, Val: 0.0408, Test: 0.0535\n",
      "Epoch: 006, Runtime 3.064787, Loss 2.270484, forward nfe 22533, backward nfe 5444, Train: 0.1750, Val: 0.1015, Test: 0.1082\n",
      "Epoch: 007, Runtime 3.067234, Loss 2.256804, forward nfe 27027, backward nfe 6473, Train: 0.1650, Val: 0.1731, Test: 0.1632\n",
      "Epoch: 008, Runtime 3.068995, Loss 2.240337, forward nfe 31551, backward nfe 7498, Train: 0.2250, Val: 0.1815, Test: 0.1810\n",
      "Epoch: 009, Runtime 3.122152, Loss 2.227605, forward nfe 36082, backward nfe 8534, Train: 0.2500, Val: 0.1823, Test: 0.1827\n",
      "Epoch: 010, Runtime 3.219767, Loss 2.214687, forward nfe 40729, backward nfe 9653, Train: 0.2100, Val: 0.1377, Test: 0.1396\n",
      "Epoch: 011, Runtime 3.187005, Loss 2.196570, forward nfe 45376, backward nfe 10733, Train: 0.1950, Val: 0.1392, Test: 0.1450\n",
      "Epoch: 012, Runtime 3.142873, Loss 2.178004, forward nfe 50038, backward nfe 11762, Train: 0.2800, Val: 0.2185, Test: 0.2252\n",
      "Epoch: 013, Runtime 3.134124, Loss 2.164365, forward nfe 54700, backward nfe 12817, Train: 0.3250, Val: 0.2508, Test: 0.2501\n",
      "Epoch: 014, Runtime 3.153377, Loss 2.139671, forward nfe 59291, backward nfe 13858, Train: 0.2650, Val: 0.2200, Test: 0.2261\n",
      "Epoch: 015, Runtime 3.159945, Loss 2.119772, forward nfe 63953, backward nfe 14906, Train: 0.2000, Val: 0.1569, Test: 0.1596\n",
      "Epoch: 016, Runtime 3.165065, Loss 2.099501, forward nfe 68615, backward nfe 15961, Train: 0.2000, Val: 0.1592, Test: 0.1636\n",
      "Epoch: 017, Runtime 3.232896, Loss 2.080271, forward nfe 73277, backward nfe 17080, Train: 0.2850, Val: 0.2369, Test: 0.2396\n",
      "Epoch: 018, Runtime 3.226653, Loss 2.055550, forward nfe 77939, backward nfe 18199, Train: 0.3000, Val: 0.2562, Test: 0.2551\n",
      "Epoch: 019, Runtime 3.166474, Loss 2.034381, forward nfe 82601, backward nfe 19255, Train: 0.2550, Val: 0.2154, Test: 0.2190\n",
      "Epoch: 020, Runtime 3.149656, Loss 2.014216, forward nfe 87263, backward nfe 20291, Train: 0.2550, Val: 0.2154, Test: 0.2137\n",
      "Epoch: 021, Runtime 3.224447, Loss 1.994357, forward nfe 91925, backward nfe 21410, Train: 0.2950, Val: 0.2492, Test: 0.2521\n",
      "Epoch: 022, Runtime 3.227171, Loss 1.974706, forward nfe 96587, backward nfe 22529, Train: 0.2900, Val: 0.2469, Test: 0.2518\n",
      "Epoch: 023, Runtime 3.226851, Loss 1.954465, forward nfe 101249, backward nfe 23648, Train: 0.2950, Val: 0.2300, Test: 0.2273\n",
      "Epoch: 024, Runtime 3.225825, Loss 1.935061, forward nfe 105911, backward nfe 24767, Train: 0.3050, Val: 0.2392, Test: 0.2441\n",
      "Epoch: 025, Runtime 3.227434, Loss 1.916950, forward nfe 110573, backward nfe 25886, Train: 0.3300, Val: 0.2392, Test: 0.2478\n",
      "Epoch: 026, Runtime 3.170403, Loss 1.894013, forward nfe 115235, backward nfe 26945, Train: 0.3050, Val: 0.2615, Test: 0.2674\n",
      "Epoch: 027, Runtime 3.224558, Loss 1.878085, forward nfe 119897, backward nfe 28064, Train: 0.3150, Val: 0.2654, Test: 0.2722\n",
      "Epoch: 028, Runtime 3.226424, Loss 1.858448, forward nfe 124559, backward nfe 29183, Train: 0.3300, Val: 0.2762, Test: 0.2871\n",
      "Epoch: 029, Runtime 3.227252, Loss 1.846211, forward nfe 129221, backward nfe 30302, Train: 0.3350, Val: 0.2985, Test: 0.3066\n",
      "Epoch: 030, Runtime 3.226297, Loss 1.822814, forward nfe 133883, backward nfe 31421, Train: 0.3450, Val: 0.3085, Test: 0.3194\n",
      "Epoch: 031, Runtime 3.227171, Loss 1.806405, forward nfe 138545, backward nfe 32540, Train: 0.3500, Val: 0.3092, Test: 0.3222\n",
      "Epoch: 032, Runtime 3.208752, Loss 1.794769, forward nfe 143172, backward nfe 33659, Train: 0.3500, Val: 0.3146, Test: 0.3186\n",
      "Epoch: 033, Runtime 3.186124, Loss 1.775892, forward nfe 147834, backward nfe 34736, Train: 0.3700, Val: 0.3138, Test: 0.3221\n",
      "Epoch: 034, Runtime 3.226124, Loss 1.762691, forward nfe 152496, backward nfe 35855, Train: 0.3650, Val: 0.3238, Test: 0.3332\n",
      "Epoch: 035, Runtime 3.236493, Loss 1.749221, forward nfe 157158, backward nfe 36974, Train: 0.3750, Val: 0.3138, Test: 0.3316\n",
      "Epoch: 036, Runtime 3.226589, Loss 1.731396, forward nfe 161820, backward nfe 38093, Train: 0.3800, Val: 0.2992, Test: 0.3218\n",
      "Epoch: 037, Runtime 3.170255, Loss 1.717155, forward nfe 166482, backward nfe 39199, Train: 0.3950, Val: 0.2915, Test: 0.3137\n",
      "Epoch: 038, Runtime 3.136296, Loss 1.706552, forward nfe 171051, backward nfe 40223, Train: 0.4050, Val: 0.2838, Test: 0.3041\n",
      "Epoch: 039, Runtime 3.181738, Loss 1.693216, forward nfe 175647, backward nfe 41327, Train: 0.4000, Val: 0.2869, Test: 0.3065\n",
      "Epoch: 040, Runtime 3.099892, Loss 1.678261, forward nfe 180309, backward nfe 42353, Train: 0.4100, Val: 0.2938, Test: 0.3124\n",
      "Epoch: 041, Runtime 3.117447, Loss 1.672474, forward nfe 184791, backward nfe 43401, Train: 0.4150, Val: 0.3000, Test: 0.3195\n",
      "Epoch: 042, Runtime 3.163177, Loss 1.655705, forward nfe 189453, backward nfe 44452, Train: 0.4200, Val: 0.3062, Test: 0.3293\n",
      "Epoch: 043, Runtime 3.149241, Loss 1.640408, forward nfe 194115, backward nfe 45487, Train: 0.4200, Val: 0.3146, Test: 0.3399\n",
      "Epoch: 044, Runtime 3.071070, Loss 1.630402, forward nfe 198684, backward nfe 46516, Train: 0.4350, Val: 0.3285, Test: 0.3495\n",
      "Epoch: 045, Runtime 3.158374, Loss 1.621472, forward nfe 203213, backward nfe 47598, Train: 0.4600, Val: 0.3300, Test: 0.3538\n",
      "Epoch: 046, Runtime 3.179317, Loss 1.607181, forward nfe 207875, backward nfe 48665, Train: 0.4850, Val: 0.3400, Test: 0.3629\n",
      "Epoch: 047, Runtime 3.141259, Loss 1.587685, forward nfe 212537, backward nfe 49714, Train: 0.4800, Val: 0.3515, Test: 0.3670\n",
      "Epoch: 048, Runtime 3.136687, Loss 1.576866, forward nfe 217156, backward nfe 50739, Train: 0.4750, Val: 0.3554, Test: 0.3743\n",
      "Epoch: 049, Runtime 3.132324, Loss 1.573718, forward nfe 221818, backward nfe 51770, Train: 0.4800, Val: 0.3615, Test: 0.3812\n",
      "Epoch: 050, Runtime 3.135967, Loss 1.562231, forward nfe 226427, backward nfe 52806, Train: 0.4750, Val: 0.3831, Test: 0.3965\n",
      "Epoch: 051, Runtime 3.112819, Loss 1.547420, forward nfe 230999, backward nfe 53845, Train: 0.4700, Val: 0.3977, Test: 0.4056\n",
      "Epoch: 052, Runtime 3.121809, Loss 1.538166, forward nfe 235624, backward nfe 54878, Train: 0.4650, Val: 0.3962, Test: 0.4164\n",
      "Epoch: 053, Runtime 3.064795, Loss 1.524926, forward nfe 240172, backward nfe 55908, Train: 0.4800, Val: 0.3900, Test: 0.4134\n",
      "Epoch: 054, Runtime 3.098283, Loss 1.519299, forward nfe 244688, backward nfe 56935, Train: 0.4750, Val: 0.3954, Test: 0.4197\n",
      "Epoch: 055, Runtime 3.060595, Loss 1.509096, forward nfe 249231, backward nfe 57965, Train: 0.4800, Val: 0.4038, Test: 0.4164\n",
      "Epoch: 056, Runtime 3.069168, Loss 1.491463, forward nfe 253740, backward nfe 58993, Train: 0.4800, Val: 0.3954, Test: 0.4180\n",
      "Epoch: 057, Runtime 3.071714, Loss 1.482591, forward nfe 258242, backward nfe 60024, Train: 0.5150, Val: 0.3892, Test: 0.4120\n",
      "Epoch: 058, Runtime 3.077955, Loss 1.470857, forward nfe 262734, backward nfe 61055, Train: 0.5200, Val: 0.4015, Test: 0.4235\n",
      "Epoch: 059, Runtime 3.062877, Loss 1.460093, forward nfe 267263, backward nfe 62083, Train: 0.4800, Val: 0.4215, Test: 0.4337\n",
      "Epoch: 060, Runtime 3.079507, Loss 1.457116, forward nfe 271769, backward nfe 63108, Train: 0.4950, Val: 0.4277, Test: 0.4398\n",
      "Epoch: 061, Runtime 3.056333, Loss 1.444741, forward nfe 276276, backward nfe 64132, Train: 0.5350, Val: 0.4269, Test: 0.4351\n",
      "Epoch: 062, Runtime 3.063137, Loss 1.438215, forward nfe 280758, backward nfe 65167, Train: 0.5400, Val: 0.4277, Test: 0.4373\n",
      "Epoch: 063, Runtime 3.077622, Loss 1.417662, forward nfe 285257, backward nfe 66194, Train: 0.5150, Val: 0.4315, Test: 0.4457\n",
      "Epoch: 064, Runtime 3.060515, Loss 1.416657, forward nfe 289760, backward nfe 67218, Train: 0.5250, Val: 0.4246, Test: 0.4376\n",
      "Epoch: 065, Runtime 3.058572, Loss 1.387956, forward nfe 294251, backward nfe 68245, Train: 0.5450, Val: 0.4238, Test: 0.4329\n",
      "Epoch: 066, Runtime 3.056125, Loss 1.382823, forward nfe 298724, backward nfe 69272, Train: 0.5300, Val: 0.4269, Test: 0.4394\n",
      "Epoch: 067, Runtime 3.060855, Loss 1.369613, forward nfe 303202, backward nfe 70296, Train: 0.5300, Val: 0.4362, Test: 0.4447\n",
      "Epoch: 068, Runtime 3.054145, Loss 1.373151, forward nfe 307701, backward nfe 71321, Train: 0.5300, Val: 0.4423, Test: 0.4501\n",
      "Epoch: 069, Runtime 3.053661, Loss 1.361154, forward nfe 312177, backward nfe 72348, Train: 0.5300, Val: 0.4485, Test: 0.4547\n",
      "Epoch: 070, Runtime 3.057840, Loss 1.346617, forward nfe 316657, backward nfe 73374, Train: 0.5350, Val: 0.4554, Test: 0.4575\n",
      "Epoch: 071, Runtime 3.048186, Loss 1.332819, forward nfe 321140, backward nfe 74398, Train: 0.5350, Val: 0.4508, Test: 0.4530\n",
      "Epoch: 072, Runtime 3.048428, Loss 1.356316, forward nfe 325610, backward nfe 75422, Train: 0.5300, Val: 0.4323, Test: 0.4423\n",
      "Epoch: 073, Runtime 3.052240, Loss 1.326033, forward nfe 330078, backward nfe 76449, Train: 0.5350, Val: 0.4362, Test: 0.4467\n",
      "Epoch: 074, Runtime 3.047794, Loss 1.318308, forward nfe 334550, backward nfe 77473, Train: 0.5400, Val: 0.4577, Test: 0.4602\n",
      "Epoch: 075, Runtime 3.046245, Loss 1.311779, forward nfe 339022, backward nfe 78497, Train: 0.5400, Val: 0.4723, Test: 0.4728\n",
      "Epoch: 076, Runtime 3.048182, Loss 1.292173, forward nfe 343492, backward nfe 79522, Train: 0.5400, Val: 0.4738, Test: 0.4752\n",
      "Epoch: 077, Runtime 3.046399, Loss 1.289133, forward nfe 347962, backward nfe 80546, Train: 0.5400, Val: 0.4654, Test: 0.4700\n",
      "Epoch: 078, Runtime 3.045444, Loss 1.290544, forward nfe 352434, backward nfe 81570, Train: 0.5350, Val: 0.4638, Test: 0.4625\n",
      "Epoch: 079, Runtime 3.047020, Loss 1.276575, forward nfe 356902, backward nfe 82594, Train: 0.5350, Val: 0.4608, Test: 0.4638\n",
      "Epoch: 080, Runtime 3.046470, Loss 1.276782, forward nfe 361372, backward nfe 83618, Train: 0.5450, Val: 0.4762, Test: 0.4760\n",
      "Epoch: 081, Runtime 3.057767, Loss 1.261420, forward nfe 365840, backward nfe 84644, Train: 0.5450, Val: 0.4777, Test: 0.4784\n",
      "Epoch: 082, Runtime 3.046259, Loss 1.266076, forward nfe 370310, backward nfe 85668, Train: 0.5450, Val: 0.4777, Test: 0.4751\n",
      "Epoch: 083, Runtime 3.046469, Loss 1.249464, forward nfe 374780, backward nfe 86692, Train: 0.5550, Val: 0.4654, Test: 0.4701\n",
      "Epoch: 084, Runtime 3.048550, Loss 1.238100, forward nfe 379250, backward nfe 87716, Train: 0.5550, Val: 0.4738, Test: 0.4739\n",
      "Epoch: 085, Runtime 3.049020, Loss 1.231734, forward nfe 383719, backward nfe 88740, Train: 0.5500, Val: 0.4900, Test: 0.4928\n",
      "Epoch: 086, Runtime 3.048927, Loss 1.230369, forward nfe 388188, backward nfe 89764, Train: 0.5550, Val: 0.4877, Test: 0.4931\n",
      "Epoch: 087, Runtime 3.047855, Loss 1.230460, forward nfe 392657, backward nfe 90788, Train: 0.5650, Val: 0.4823, Test: 0.4773\n",
      "Epoch: 088, Runtime 3.049046, Loss 1.222724, forward nfe 397126, backward nfe 91812, Train: 0.5550, Val: 0.4723, Test: 0.4722\n",
      "Epoch: 089, Runtime 3.050219, Loss 1.208283, forward nfe 401600, backward nfe 92837, Train: 0.5550, Val: 0.5008, Test: 0.4969\n",
      "Epoch: 090, Runtime 3.048924, Loss 1.199761, forward nfe 406071, backward nfe 93860, Train: 0.5400, Val: 0.4946, Test: 0.5044\n",
      "Epoch: 091, Runtime 3.046944, Loss 1.217261, forward nfe 410541, backward nfe 94884, Train: 0.5550, Val: 0.4977, Test: 0.4963\n",
      "Epoch: 092, Runtime 3.047501, Loss 1.186277, forward nfe 415009, backward nfe 95908, Train: 0.5650, Val: 0.4762, Test: 0.4761\n",
      "Epoch: 093, Runtime 3.047721, Loss 1.200964, forward nfe 419482, backward nfe 96933, Train: 0.5700, Val: 0.4831, Test: 0.4795\n",
      "Epoch: 094, Runtime 3.045838, Loss 1.180678, forward nfe 423952, backward nfe 97957, Train: 0.5650, Val: 0.5008, Test: 0.5030\n",
      "Epoch: 095, Runtime 3.045709, Loss 1.176165, forward nfe 428420, backward nfe 98980, Train: 0.5600, Val: 0.5131, Test: 0.5162\n",
      "Epoch: 096, Runtime 3.046635, Loss 1.168823, forward nfe 432888, backward nfe 100003, Train: 0.5550, Val: 0.5046, Test: 0.5084\n",
      "Epoch: 097, Runtime 3.046841, Loss 1.166699, forward nfe 437356, backward nfe 101026, Train: 0.5850, Val: 0.4777, Test: 0.4811\n",
      "Epoch: 098, Runtime 3.056204, Loss 1.156448, forward nfe 441824, backward nfe 102050, Train: 0.5850, Val: 0.4700, Test: 0.4718\n",
      "Epoch: 099, Runtime 3.048359, Loss 1.147288, forward nfe 446293, backward nfe 103073, Train: 0.5650, Val: 0.5015, Test: 0.5076\n",
      "best val accuracy 0.513077 with test accuracy 0.516161 at epoch 95\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.910578, Loss 2.314560, forward nfe 166, backward nfe 308, Train: 0.1450, Val: 0.0223, Test: 0.0393\n",
      "Epoch: 002, Runtime 2.564189, Loss 2.621233, forward nfe 2448, backward nfe 1332, Train: 0.1200, Val: 0.2215, Test: 0.2265\n",
      "Epoch: 003, Runtime 3.051247, Loss 2.414627, forward nfe 6916, backward nfe 2358, Train: 0.1000, Val: 0.0223, Test: 0.0358\n",
      "Epoch: 004, Runtime 3.049057, Loss 2.420461, forward nfe 11384, backward nfe 3382, Train: 0.1000, Val: 0.0223, Test: 0.0359\n",
      "Epoch: 005, Runtime 3.053767, Loss 2.317528, forward nfe 15852, backward nfe 4407, Train: 0.0800, Val: 0.0685, Test: 0.0762\n",
      "Epoch: 006, Runtime 3.053391, Loss 2.254331, forward nfe 20326, backward nfe 5432, Train: 0.2100, Val: 0.0662, Test: 0.0860\n",
      "Epoch: 007, Runtime 3.051163, Loss 2.237412, forward nfe 24802, backward nfe 6457, Train: 0.2400, Val: 0.0515, Test: 0.0781\n",
      "Epoch: 008, Runtime 3.057735, Loss 2.234280, forward nfe 29282, backward nfe 7482, Train: 0.2000, Val: 0.0415, Test: 0.0628\n",
      "Epoch: 009, Runtime 3.085090, Loss 2.232495, forward nfe 33772, backward nfe 8509, Train: 0.1900, Val: 0.0569, Test: 0.0707\n",
      "Epoch: 010, Runtime 3.095123, Loss 2.198154, forward nfe 38294, backward nfe 9535, Train: 0.1900, Val: 0.0385, Test: 0.0565\n",
      "Epoch: 011, Runtime 3.111213, Loss 2.183626, forward nfe 42956, backward nfe 10571, Train: 0.1900, Val: 0.0423, Test: 0.0596\n",
      "Epoch: 012, Runtime 3.117776, Loss 2.176907, forward nfe 47449, backward nfe 11661, Train: 0.2000, Val: 0.0500, Test: 0.0708\n",
      "Epoch: 013, Runtime 3.140550, Loss 2.166352, forward nfe 52012, backward nfe 12709, Train: 0.2100, Val: 0.0792, Test: 0.0924\n",
      "Epoch: 014, Runtime 3.209225, Loss 2.143536, forward nfe 56624, backward nfe 13816, Train: 0.1800, Val: 0.1054, Test: 0.1128\n",
      "Epoch: 015, Runtime 3.194527, Loss 2.110977, forward nfe 61286, backward nfe 14900, Train: 0.1950, Val: 0.1338, Test: 0.1367\n",
      "Epoch: 016, Runtime 3.234952, Loss 2.091607, forward nfe 65948, backward nfe 16019, Train: 0.2150, Val: 0.1677, Test: 0.1694\n",
      "Epoch: 017, Runtime 3.228677, Loss 2.073630, forward nfe 70610, backward nfe 17138, Train: 0.2550, Val: 0.1885, Test: 0.1939\n",
      "Epoch: 018, Runtime 3.161630, Loss 2.052293, forward nfe 75272, backward nfe 18192, Train: 0.2550, Val: 0.1954, Test: 0.1965\n",
      "Epoch: 019, Runtime 3.183291, Loss 2.028444, forward nfe 79836, backward nfe 19311, Train: 0.2950, Val: 0.2146, Test: 0.2181\n",
      "Epoch: 020, Runtime 3.166143, Loss 2.003829, forward nfe 84498, backward nfe 20366, Train: 0.2500, Val: 0.2015, Test: 0.1969\n",
      "Epoch: 021, Runtime 3.195520, Loss 1.985127, forward nfe 89160, backward nfe 21452, Train: 0.2350, Val: 0.1885, Test: 0.1877\n",
      "Epoch: 022, Runtime 3.206826, Loss 1.965339, forward nfe 93784, backward nfe 22571, Train: 0.2800, Val: 0.2131, Test: 0.2127\n",
      "Epoch: 023, Runtime 3.192187, Loss 1.939419, forward nfe 98446, backward nfe 23652, Train: 0.3050, Val: 0.2400, Test: 0.2362\n",
      "Epoch: 024, Runtime 3.196836, Loss 1.919375, forward nfe 103042, backward nfe 24771, Train: 0.3250, Val: 0.2646, Test: 0.2637\n",
      "Epoch: 025, Runtime 3.190146, Loss 1.894057, forward nfe 107704, backward nfe 25849, Train: 0.3400, Val: 0.2754, Test: 0.2834\n",
      "Epoch: 026, Runtime 3.195024, Loss 1.864813, forward nfe 112366, backward nfe 26934, Train: 0.3700, Val: 0.2931, Test: 0.2973\n",
      "Epoch: 027, Runtime 3.226053, Loss 1.840248, forward nfe 117028, backward nfe 28053, Train: 0.3700, Val: 0.2923, Test: 0.3040\n",
      "Epoch: 028, Runtime 3.170528, Loss 1.824069, forward nfe 121690, backward nfe 29112, Train: 0.3550, Val: 0.2885, Test: 0.3001\n",
      "Epoch: 029, Runtime 3.165761, Loss 1.790380, forward nfe 126352, backward nfe 30203, Train: 0.4250, Val: 0.2331, Test: 0.2631\n",
      "Epoch: 030, Runtime 3.226227, Loss 1.757554, forward nfe 130936, backward nfe 31322, Train: 0.4000, Val: 0.1923, Test: 0.2269\n",
      "Epoch: 031, Runtime 3.165754, Loss 1.734775, forward nfe 135598, backward nfe 32376, Train: 0.3650, Val: 0.2115, Test: 0.2423\n",
      "Epoch: 032, Runtime 3.163055, Loss 1.709557, forward nfe 140260, backward nfe 33425, Train: 0.3800, Val: 0.2062, Test: 0.2409\n",
      "Epoch: 033, Runtime 3.239415, Loss 1.687606, forward nfe 144922, backward nfe 34544, Train: 0.4350, Val: 0.2169, Test: 0.2542\n",
      "Epoch: 034, Runtime 3.222727, Loss 1.652840, forward nfe 149584, backward nfe 35660, Train: 0.4850, Val: 0.2308, Test: 0.2739\n",
      "Epoch: 035, Runtime 3.192251, Loss 1.634797, forward nfe 154175, backward nfe 36779, Train: 0.5250, Val: 0.3215, Test: 0.3555\n",
      "Epoch: 036, Runtime 3.136589, Loss 1.599173, forward nfe 158748, backward nfe 37846, Train: 0.5600, Val: 0.3515, Test: 0.3844\n",
      "Epoch: 037, Runtime 3.181203, Loss 1.578894, forward nfe 163410, backward nfe 38918, Train: 0.5450, Val: 0.3515, Test: 0.3869\n",
      "Epoch: 038, Runtime 3.199267, Loss 1.556515, forward nfe 168072, backward nfe 40036, Train: 0.5250, Val: 0.3285, Test: 0.3696\n",
      "Epoch: 039, Runtime 3.110989, Loss 1.527864, forward nfe 172591, backward nfe 41100, Train: 0.5500, Val: 0.3569, Test: 0.3900\n",
      "Epoch: 040, Runtime 3.164380, Loss 1.492052, forward nfe 177195, backward nfe 42153, Train: 0.5900, Val: 0.3846, Test: 0.4167\n",
      "Epoch: 041, Runtime 3.194526, Loss 1.473156, forward nfe 181850, backward nfe 43238, Train: 0.6000, Val: 0.3862, Test: 0.4298\n",
      "Epoch: 042, Runtime 3.207182, Loss 1.442033, forward nfe 186512, backward nfe 44336, Train: 0.5950, Val: 0.3854, Test: 0.4285\n",
      "Epoch: 043, Runtime 3.077729, Loss 1.423233, forward nfe 191105, backward nfe 45369, Train: 0.5800, Val: 0.3992, Test: 0.4335\n",
      "Epoch: 044, Runtime 3.095944, Loss 1.385949, forward nfe 195600, backward nfe 46421, Train: 0.5600, Val: 0.4008, Test: 0.4278\n",
      "Epoch: 045, Runtime 3.111389, Loss 1.373519, forward nfe 200199, backward nfe 47451, Train: 0.5950, Val: 0.4100, Test: 0.4469\n",
      "Epoch: 046, Runtime 3.143403, Loss 1.344841, forward nfe 204788, backward nfe 48509, Train: 0.6250, Val: 0.4154, Test: 0.4579\n",
      "Epoch: 047, Runtime 3.128149, Loss 1.315972, forward nfe 209312, backward nfe 49568, Train: 0.6400, Val: 0.4200, Test: 0.4611\n",
      "Epoch: 048, Runtime 3.110940, Loss 1.289900, forward nfe 213952, backward nfe 50608, Train: 0.6500, Val: 0.4246, Test: 0.4626\n",
      "Epoch: 049, Runtime 3.074182, Loss 1.267938, forward nfe 218428, backward nfe 51639, Train: 0.6700, Val: 0.4269, Test: 0.4625\n",
      "Epoch: 050, Runtime 3.065406, Loss 1.242244, forward nfe 222923, backward nfe 52668, Train: 0.6550, Val: 0.4415, Test: 0.4793\n",
      "Epoch: 051, Runtime 3.110896, Loss 1.220559, forward nfe 227407, backward nfe 53709, Train: 0.6900, Val: 0.4700, Test: 0.5035\n",
      "Epoch: 052, Runtime 3.058191, Loss 1.194063, forward nfe 231965, backward nfe 54733, Train: 0.7100, Val: 0.4900, Test: 0.5169\n",
      "Epoch: 053, Runtime 3.101147, Loss 1.165105, forward nfe 236540, backward nfe 55764, Train: 0.7250, Val: 0.5169, Test: 0.5389\n",
      "Epoch: 054, Runtime 3.110106, Loss 1.145516, forward nfe 241021, backward nfe 56821, Train: 0.7100, Val: 0.5246, Test: 0.5524\n",
      "Epoch: 055, Runtime 3.063090, Loss 1.122499, forward nfe 245554, backward nfe 57854, Train: 0.7350, Val: 0.5508, Test: 0.5767\n",
      "Epoch: 056, Runtime 3.109196, Loss 1.096843, forward nfe 250115, backward nfe 58886, Train: 0.7550, Val: 0.5715, Test: 0.5942\n",
      "Epoch: 057, Runtime 3.079736, Loss 1.063040, forward nfe 254622, backward nfe 59928, Train: 0.7500, Val: 0.5800, Test: 0.6050\n",
      "Epoch: 058, Runtime 3.088584, Loss 1.052177, forward nfe 259107, backward nfe 60977, Train: 0.7550, Val: 0.5808, Test: 0.6087\n",
      "Epoch: 059, Runtime 3.087183, Loss 1.024199, forward nfe 263688, backward nfe 62002, Train: 0.7600, Val: 0.5877, Test: 0.6139\n",
      "Epoch: 060, Runtime 3.109080, Loss 1.005356, forward nfe 268195, backward nfe 63053, Train: 0.7800, Val: 0.5800, Test: 0.6141\n",
      "Epoch: 061, Runtime 3.057723, Loss 0.978460, forward nfe 272707, backward nfe 64081, Train: 0.7850, Val: 0.5900, Test: 0.6232\n",
      "Epoch: 062, Runtime 3.069551, Loss 0.943783, forward nfe 277214, backward nfe 65107, Train: 0.8000, Val: 0.6038, Test: 0.6348\n",
      "Epoch: 063, Runtime 3.074819, Loss 0.924812, forward nfe 281702, backward nfe 66138, Train: 0.7950, Val: 0.6346, Test: 0.6639\n",
      "Epoch: 064, Runtime 3.059969, Loss 0.882946, forward nfe 286201, backward nfe 67162, Train: 0.8000, Val: 0.6631, Test: 0.6860\n",
      "Epoch: 065, Runtime 3.076682, Loss 0.848655, forward nfe 290700, backward nfe 68198, Train: 0.8200, Val: 0.6723, Test: 0.7000\n",
      "Epoch: 066, Runtime 3.065109, Loss 0.819491, forward nfe 295179, backward nfe 69231, Train: 0.8300, Val: 0.6562, Test: 0.6863\n",
      "Epoch: 067, Runtime 3.055017, Loss 0.788532, forward nfe 299653, backward nfe 70258, Train: 0.8300, Val: 0.6554, Test: 0.6883\n",
      "Epoch: 068, Runtime 3.057627, Loss 0.764941, forward nfe 304134, backward nfe 71285, Train: 0.8300, Val: 0.6631, Test: 0.7001\n",
      "Epoch: 069, Runtime 3.065511, Loss 0.731113, forward nfe 308626, backward nfe 72311, Train: 0.8350, Val: 0.6777, Test: 0.7094\n",
      "Epoch: 070, Runtime 3.057080, Loss 0.719978, forward nfe 313108, backward nfe 73338, Train: 0.8350, Val: 0.6738, Test: 0.7093\n",
      "Epoch: 071, Runtime 3.059002, Loss 0.698745, forward nfe 317586, backward nfe 74366, Train: 0.8500, Val: 0.6746, Test: 0.7046\n",
      "Epoch: 072, Runtime 3.058589, Loss 0.665412, forward nfe 322070, backward nfe 75392, Train: 0.8500, Val: 0.6846, Test: 0.7143\n",
      "Epoch: 073, Runtime 3.072532, Loss 0.650313, forward nfe 326552, backward nfe 76441, Train: 0.8350, Val: 0.7062, Test: 0.7291\n",
      "Epoch: 074, Runtime 3.055606, Loss 0.629285, forward nfe 331028, backward nfe 77468, Train: 0.8450, Val: 0.7038, Test: 0.7294\n",
      "Epoch: 075, Runtime 3.079172, Loss 0.616465, forward nfe 335501, backward nfe 78499, Train: 0.8600, Val: 0.6892, Test: 0.7228\n",
      "Epoch: 076, Runtime 3.052541, Loss 0.611510, forward nfe 340016, backward nfe 79525, Train: 0.8550, Val: 0.6985, Test: 0.7243\n",
      "Epoch: 077, Runtime 3.057152, Loss 0.581363, forward nfe 344495, backward nfe 80551, Train: 0.8400, Val: 0.7162, Test: 0.7347\n",
      "Epoch: 078, Runtime 3.061467, Loss 0.569657, forward nfe 348973, backward nfe 81583, Train: 0.8550, Val: 0.7185, Test: 0.7426\n",
      "Epoch: 079, Runtime 3.050188, Loss 0.547631, forward nfe 353447, backward nfe 82609, Train: 0.8500, Val: 0.7138, Test: 0.7403\n",
      "Epoch: 080, Runtime 3.067887, Loss 0.527775, forward nfe 357922, backward nfe 83634, Train: 0.8500, Val: 0.7123, Test: 0.7397\n",
      "Epoch: 081, Runtime 3.050982, Loss 0.516808, forward nfe 362404, backward nfe 84658, Train: 0.8500, Val: 0.7100, Test: 0.7415\n",
      "Epoch: 082, Runtime 3.052245, Loss 0.508623, forward nfe 366883, backward nfe 85682, Train: 0.8500, Val: 0.7038, Test: 0.7385\n",
      "Epoch: 083, Runtime 3.050679, Loss 0.497993, forward nfe 371351, backward nfe 86708, Train: 0.8550, Val: 0.7138, Test: 0.7431\n",
      "Epoch: 084, Runtime 3.052246, Loss 0.477007, forward nfe 375822, backward nfe 87732, Train: 0.8600, Val: 0.7262, Test: 0.7519\n",
      "Epoch: 085, Runtime 3.052634, Loss 0.477154, forward nfe 380297, backward nfe 88756, Train: 0.8650, Val: 0.7392, Test: 0.7641\n",
      "Epoch: 086, Runtime 3.053369, Loss 0.474490, forward nfe 384781, backward nfe 89780, Train: 0.8750, Val: 0.7477, Test: 0.7689\n",
      "Epoch: 087, Runtime 3.051467, Loss 0.455130, forward nfe 389252, backward nfe 90804, Train: 0.8750, Val: 0.7346, Test: 0.7631\n",
      "Epoch: 088, Runtime 3.054792, Loss 0.435343, forward nfe 393728, backward nfe 91829, Train: 0.8750, Val: 0.7277, Test: 0.7525\n",
      "Epoch: 089, Runtime 3.057698, Loss 0.437589, forward nfe 398204, backward nfe 92858, Train: 0.8750, Val: 0.7300, Test: 0.7585\n",
      "Epoch: 090, Runtime 3.053511, Loss 0.427357, forward nfe 402675, backward nfe 93884, Train: 0.8800, Val: 0.7415, Test: 0.7726\n",
      "Epoch: 091, Runtime 3.051170, Loss 0.410892, forward nfe 407149, backward nfe 94911, Train: 0.8800, Val: 0.7546, Test: 0.7790\n",
      "Epoch: 092, Runtime 3.053002, Loss 0.420888, forward nfe 411619, backward nfe 95937, Train: 0.8750, Val: 0.7585, Test: 0.7794\n",
      "Epoch: 093, Runtime 3.050534, Loss 0.412577, forward nfe 416090, backward nfe 96962, Train: 0.8900, Val: 0.7492, Test: 0.7755\n",
      "Epoch: 094, Runtime 3.050250, Loss 0.386879, forward nfe 420560, backward nfe 97986, Train: 0.8800, Val: 0.7462, Test: 0.7703\n",
      "Epoch: 095, Runtime 3.056777, Loss 0.391901, forward nfe 425032, backward nfe 99012, Train: 0.8900, Val: 0.7523, Test: 0.7782\n",
      "Epoch: 096, Runtime 3.060490, Loss 0.383832, forward nfe 429511, backward nfe 100038, Train: 0.8900, Val: 0.7638, Test: 0.7836\n",
      "Epoch: 097, Runtime 3.051083, Loss 0.365908, forward nfe 433981, backward nfe 101062, Train: 0.8950, Val: 0.7638, Test: 0.7843\n",
      "Epoch: 098, Runtime 3.056432, Loss 0.367003, forward nfe 438453, backward nfe 102089, Train: 0.8950, Val: 0.7685, Test: 0.7853\n",
      "Epoch: 099, Runtime 3.049824, Loss 0.365837, forward nfe 442926, backward nfe 103113, Train: 0.8950, Val: 0.7608, Test: 0.7840\n",
      "best val accuracy 0.768462 with test accuracy 0.785260 at epoch 98\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.446316, Loss 2.310613, forward nfe 164, backward nfe 323, Train: 0.0800, Val: 0.0362, Test: 0.0434\n",
      "Epoch: 002, Runtime 3.048487, Loss 2.523327, forward nfe 4632, backward nfe 1347, Train: 0.1050, Val: 0.0592, Test: 0.0634\n",
      "Epoch: 003, Runtime 3.049589, Loss 2.503672, forward nfe 9100, backward nfe 2371, Train: 0.1000, Val: 0.0585, Test: 0.0615\n",
      "Epoch: 004, Runtime 3.050424, Loss 2.479398, forward nfe 13568, backward nfe 3395, Train: 0.1650, Val: 0.0862, Test: 0.0875\n",
      "Epoch: 005, Runtime 3.048677, Loss 2.313076, forward nfe 18038, backward nfe 4419, Train: 0.1150, Val: 0.1069, Test: 0.1072\n",
      "Epoch: 006, Runtime 3.051605, Loss 2.297935, forward nfe 22506, backward nfe 5445, Train: 0.1050, Val: 0.1069, Test: 0.1063\n",
      "Epoch: 007, Runtime 3.051917, Loss 2.278252, forward nfe 26978, backward nfe 6471, Train: 0.1650, Val: 0.1600, Test: 0.1516\n",
      "Epoch: 008, Runtime 3.074612, Loss 2.239067, forward nfe 31483, backward nfe 7503, Train: 0.1600, Val: 0.1685, Test: 0.1621\n",
      "Epoch: 009, Runtime 3.148970, Loss 2.250400, forward nfe 35997, backward nfe 8565, Train: 0.1300, Val: 0.1292, Test: 0.1339\n",
      "Epoch: 010, Runtime 3.150324, Loss 2.255197, forward nfe 40659, backward nfe 9629, Train: 0.1600, Val: 0.1931, Test: 0.1891\n",
      "Epoch: 011, Runtime 3.151936, Loss 2.248838, forward nfe 45264, backward nfe 10668, Train: 0.1600, Val: 0.1885, Test: 0.1815\n",
      "Epoch: 012, Runtime 3.168439, Loss 2.241534, forward nfe 49853, backward nfe 11759, Train: 0.1100, Val: 0.1208, Test: 0.1193\n",
      "Epoch: 013, Runtime 3.225899, Loss 2.237098, forward nfe 54515, backward nfe 12878, Train: 0.1650, Val: 0.1785, Test: 0.1660\n",
      "Epoch: 014, Runtime 3.150905, Loss 2.224433, forward nfe 59177, backward nfe 13915, Train: 0.1950, Val: 0.2108, Test: 0.2078\n",
      "Epoch: 015, Runtime 3.206255, Loss 2.219119, forward nfe 63839, backward nfe 15014, Train: 0.2000, Val: 0.2108, Test: 0.2054\n",
      "Epoch: 016, Runtime 3.234939, Loss 2.205772, forward nfe 68501, backward nfe 16133, Train: 0.1700, Val: 0.1692, Test: 0.1617\n",
      "Epoch: 017, Runtime 3.208161, Loss 2.197057, forward nfe 73163, backward nfe 17252, Train: 0.1750, Val: 0.1762, Test: 0.1662\n",
      "Epoch: 018, Runtime 3.158429, Loss 2.187927, forward nfe 77786, backward nfe 18299, Train: 0.2000, Val: 0.2123, Test: 0.2067\n",
      "Epoch: 019, Runtime 3.177177, Loss 2.176881, forward nfe 82448, backward nfe 19367, Train: 0.2000, Val: 0.2131, Test: 0.2076\n",
      "Epoch: 020, Runtime 3.217388, Loss 2.171213, forward nfe 87110, backward nfe 20479, Train: 0.2000, Val: 0.2085, Test: 0.1961\n",
      "Epoch: 021, Runtime 3.224157, Loss 2.155185, forward nfe 91772, backward nfe 21598, Train: 0.1650, Val: 0.1569, Test: 0.1543\n",
      "Epoch: 022, Runtime 3.196514, Loss 2.154898, forward nfe 96434, backward nfe 22686, Train: 0.1950, Val: 0.2031, Test: 0.1880\n",
      "Epoch: 023, Runtime 3.223884, Loss 2.134969, forward nfe 101096, backward nfe 23805, Train: 0.2050, Val: 0.2146, Test: 0.2075\n",
      "Epoch: 024, Runtime 3.187870, Loss 2.132120, forward nfe 105758, backward nfe 24883, Train: 0.2050, Val: 0.2138, Test: 0.2085\n",
      "Epoch: 025, Runtime 3.225773, Loss 2.126747, forward nfe 110420, backward nfe 26002, Train: 0.2000, Val: 0.2131, Test: 0.2049\n",
      "Epoch: 026, Runtime 3.225453, Loss 2.111226, forward nfe 115082, backward nfe 27121, Train: 0.2000, Val: 0.2069, Test: 0.1923\n",
      "Epoch: 027, Runtime 3.202628, Loss 2.101148, forward nfe 119744, backward nfe 28215, Train: 0.1950, Val: 0.2046, Test: 0.1888\n",
      "Epoch: 028, Runtime 3.224972, Loss 2.094539, forward nfe 124406, backward nfe 29334, Train: 0.2000, Val: 0.2131, Test: 0.2047\n",
      "Epoch: 029, Runtime 3.156188, Loss 2.084221, forward nfe 129068, backward nfe 30377, Train: 0.2050, Val: 0.2154, Test: 0.2086\n",
      "Epoch: 030, Runtime 3.207441, Loss 2.067361, forward nfe 133730, backward nfe 31477, Train: 0.2200, Val: 0.2192, Test: 0.2124\n",
      "Epoch: 031, Runtime 3.226821, Loss 2.045165, forward nfe 138392, backward nfe 32596, Train: 0.2300, Val: 0.1200, Test: 0.1232\n",
      "Epoch: 032, Runtime 3.234564, Loss 2.038639, forward nfe 143054, backward nfe 33715, Train: 0.2000, Val: 0.0992, Test: 0.1025\n",
      "Epoch: 033, Runtime 3.228543, Loss 2.041030, forward nfe 147716, backward nfe 34834, Train: 0.2150, Val: 0.1046, Test: 0.1086\n",
      "Epoch: 034, Runtime 3.227638, Loss 2.021748, forward nfe 152378, backward nfe 35953, Train: 0.2300, Val: 0.1423, Test: 0.1476\n",
      "Epoch: 035, Runtime 3.168028, Loss 1.986673, forward nfe 157040, backward nfe 37010, Train: 0.2700, Val: 0.2254, Test: 0.2237\n",
      "Epoch: 036, Runtime 3.227496, Loss 1.975281, forward nfe 161702, backward nfe 38129, Train: 0.2900, Val: 0.2246, Test: 0.2276\n",
      "Epoch: 037, Runtime 3.226229, Loss 1.969568, forward nfe 166364, backward nfe 39248, Train: 0.3400, Val: 0.2277, Test: 0.2276\n",
      "Epoch: 038, Runtime 3.118206, Loss 1.939669, forward nfe 170978, backward nfe 40274, Train: 0.2700, Val: 0.1531, Test: 0.1633\n",
      "Epoch: 039, Runtime 3.207023, Loss 1.925545, forward nfe 175640, backward nfe 41372, Train: 0.2500, Val: 0.1285, Test: 0.1404\n",
      "Epoch: 040, Runtime 3.099820, Loss 1.912564, forward nfe 180231, backward nfe 42402, Train: 0.2600, Val: 0.1292, Test: 0.1427\n",
      "Epoch: 041, Runtime 3.192275, Loss 1.896820, forward nfe 184867, backward nfe 43508, Train: 0.2900, Val: 0.1546, Test: 0.1705\n",
      "Epoch: 042, Runtime 3.144361, Loss 1.868802, forward nfe 189485, backward nfe 44537, Train: 0.3550, Val: 0.2385, Test: 0.2509\n",
      "Epoch: 043, Runtime 3.178776, Loss 1.849232, forward nfe 194147, backward nfe 45605, Train: 0.4150, Val: 0.2969, Test: 0.2942\n",
      "Epoch: 044, Runtime 3.105932, Loss 1.847081, forward nfe 198729, backward nfe 46642, Train: 0.4050, Val: 0.2815, Test: 0.2879\n",
      "Epoch: 045, Runtime 3.145055, Loss 1.817615, forward nfe 203374, backward nfe 47673, Train: 0.3000, Val: 0.1723, Test: 0.1849\n",
      "Epoch: 046, Runtime 3.099062, Loss 1.804099, forward nfe 208036, backward nfe 48697, Train: 0.2700, Val: 0.1431, Test: 0.1602\n",
      "Epoch: 047, Runtime 3.126802, Loss 1.789358, forward nfe 212607, backward nfe 49737, Train: 0.2900, Val: 0.1685, Test: 0.1824\n",
      "Epoch: 048, Runtime 3.113928, Loss 1.774530, forward nfe 217161, backward nfe 50790, Train: 0.4050, Val: 0.2662, Test: 0.2724\n",
      "Epoch: 049, Runtime 3.104638, Loss 1.749178, forward nfe 221747, backward nfe 51826, Train: 0.4350, Val: 0.2969, Test: 0.3056\n",
      "Epoch: 050, Runtime 3.093430, Loss 1.739529, forward nfe 226236, backward nfe 52868, Train: 0.4150, Val: 0.2638, Test: 0.2654\n",
      "Epoch: 051, Runtime 3.090904, Loss 1.721827, forward nfe 230804, backward nfe 53901, Train: 0.3500, Val: 0.1900, Test: 0.1981\n",
      "Epoch: 052, Runtime 3.121409, Loss 1.704310, forward nfe 235306, backward nfe 54956, Train: 0.3550, Val: 0.2008, Test: 0.2100\n",
      "Epoch: 053, Runtime 3.076212, Loss 1.690094, forward nfe 239862, backward nfe 55984, Train: 0.3950, Val: 0.2400, Test: 0.2532\n",
      "Epoch: 054, Runtime 3.096380, Loss 1.680080, forward nfe 244369, backward nfe 57029, Train: 0.4300, Val: 0.2577, Test: 0.2667\n",
      "Epoch: 055, Runtime 3.098310, Loss 1.668898, forward nfe 248843, backward nfe 58053, Train: 0.3950, Val: 0.2415, Test: 0.2478\n",
      "Epoch: 056, Runtime 3.074898, Loss 1.648992, forward nfe 253399, backward nfe 59086, Train: 0.3950, Val: 0.2292, Test: 0.2372\n",
      "Epoch: 057, Runtime 3.089588, Loss 1.644747, forward nfe 257911, backward nfe 60110, Train: 0.4350, Val: 0.2554, Test: 0.2680\n",
      "Epoch: 058, Runtime 3.076096, Loss 1.622219, forward nfe 262450, backward nfe 61136, Train: 0.4700, Val: 0.2954, Test: 0.3102\n",
      "Epoch: 059, Runtime 3.066322, Loss 1.607862, forward nfe 266929, backward nfe 62164, Train: 0.4850, Val: 0.3069, Test: 0.3246\n",
      "Epoch: 060, Runtime 3.073094, Loss 1.597079, forward nfe 271446, backward nfe 63189, Train: 0.4900, Val: 0.2838, Test: 0.3050\n",
      "Epoch: 061, Runtime 3.068571, Loss 1.581837, forward nfe 275936, backward nfe 64216, Train: 0.5000, Val: 0.3015, Test: 0.3151\n",
      "Epoch: 062, Runtime 3.075113, Loss 1.571166, forward nfe 280445, backward nfe 65243, Train: 0.5050, Val: 0.3277, Test: 0.3399\n",
      "Epoch: 063, Runtime 3.087995, Loss 1.554895, forward nfe 284924, backward nfe 66277, Train: 0.5100, Val: 0.3438, Test: 0.3508\n",
      "Epoch: 064, Runtime 3.088449, Loss 1.545901, forward nfe 289407, backward nfe 67303, Train: 0.5150, Val: 0.3269, Test: 0.3386\n",
      "Epoch: 065, Runtime 3.077361, Loss 1.533241, forward nfe 293901, backward nfe 68332, Train: 0.5050, Val: 0.3077, Test: 0.3241\n",
      "Epoch: 066, Runtime 3.084153, Loss 1.522800, forward nfe 298391, backward nfe 69358, Train: 0.5250, Val: 0.3292, Test: 0.3434\n",
      "Epoch: 067, Runtime 3.079552, Loss 1.513136, forward nfe 302864, backward nfe 70382, Train: 0.5150, Val: 0.3485, Test: 0.3592\n",
      "Epoch: 068, Runtime 3.067723, Loss 1.498676, forward nfe 307335, backward nfe 71411, Train: 0.5200, Val: 0.3446, Test: 0.3599\n",
      "Epoch: 069, Runtime 3.057841, Loss 1.489350, forward nfe 311824, backward nfe 72438, Train: 0.5350, Val: 0.3392, Test: 0.3534\n",
      "Epoch: 070, Runtime 3.053047, Loss 1.482677, forward nfe 316294, backward nfe 73462, Train: 0.5450, Val: 0.3515, Test: 0.3671\n",
      "Epoch: 071, Runtime 3.061455, Loss 1.464101, forward nfe 320774, backward nfe 74486, Train: 0.5300, Val: 0.3600, Test: 0.3719\n",
      "Epoch: 072, Runtime 3.055084, Loss 1.460763, forward nfe 325252, backward nfe 75510, Train: 0.5450, Val: 0.3546, Test: 0.3697\n",
      "Epoch: 073, Runtime 3.055997, Loss 1.445089, forward nfe 329721, backward nfe 76534, Train: 0.5250, Val: 0.3323, Test: 0.3524\n",
      "Epoch: 074, Runtime 3.057787, Loss 1.437091, forward nfe 334193, backward nfe 77558, Train: 0.5400, Val: 0.3646, Test: 0.3825\n",
      "Epoch: 075, Runtime 3.055401, Loss 1.417135, forward nfe 338667, backward nfe 78582, Train: 0.5600, Val: 0.4392, Test: 0.4634\n",
      "Epoch: 076, Runtime 3.056173, Loss 1.408676, forward nfe 343138, backward nfe 79606, Train: 0.5550, Val: 0.4577, Test: 0.4825\n",
      "Epoch: 077, Runtime 3.062697, Loss 1.379407, forward nfe 347613, backward nfe 80636, Train: 0.5450, Val: 0.4115, Test: 0.4487\n",
      "Epoch: 078, Runtime 3.056772, Loss 1.384819, forward nfe 352087, backward nfe 81660, Train: 0.5500, Val: 0.4254, Test: 0.4624\n",
      "Epoch: 079, Runtime 3.055591, Loss 1.366914, forward nfe 356557, backward nfe 82686, Train: 0.5600, Val: 0.4600, Test: 0.4862\n",
      "Epoch: 080, Runtime 3.055152, Loss 1.350035, forward nfe 361029, backward nfe 83710, Train: 0.4350, Val: 0.2408, Test: 0.2551\n",
      "Epoch: 081, Runtime 3.066843, Loss 1.448408, forward nfe 365500, backward nfe 84734, Train: 0.5600, Val: 0.5369, Test: 0.5366\n",
      "Epoch: 082, Runtime 3.055140, Loss 1.335045, forward nfe 369971, backward nfe 85758, Train: 0.5150, Val: 0.4008, Test: 0.4088\n",
      "Epoch: 083, Runtime 3.055685, Loss 1.368638, forward nfe 374440, backward nfe 86782, Train: 0.5150, Val: 0.3908, Test: 0.4117\n",
      "Epoch: 084, Runtime 3.056292, Loss 1.349119, forward nfe 378911, backward nfe 87807, Train: 0.5350, Val: 0.4569, Test: 0.4816\n",
      "Epoch: 085, Runtime 3.055049, Loss 1.327034, forward nfe 383382, backward nfe 88831, Train: 0.5550, Val: 0.5077, Test: 0.5176\n",
      "Epoch: 086, Runtime 3.054533, Loss 1.276034, forward nfe 387855, backward nfe 89856, Train: 0.5350, Val: 0.4731, Test: 0.4936\n",
      "Epoch: 087, Runtime 3.054211, Loss 1.269716, forward nfe 392325, backward nfe 90880, Train: 0.4950, Val: 0.4408, Test: 0.4574\n",
      "Epoch: 088, Runtime 3.056100, Loss 1.248393, forward nfe 396793, backward nfe 91905, Train: 0.4900, Val: 0.4346, Test: 0.4490\n",
      "Epoch: 089, Runtime 3.055764, Loss 1.259731, forward nfe 401264, backward nfe 92929, Train: 0.5400, Val: 0.4292, Test: 0.4517\n",
      "Epoch: 090, Runtime 3.054188, Loss 1.233859, forward nfe 405736, backward nfe 93953, Train: 0.5550, Val: 0.3985, Test: 0.4413\n",
      "Epoch: 091, Runtime 3.056742, Loss 1.191163, forward nfe 410207, backward nfe 94977, Train: 0.5550, Val: 0.3108, Test: 0.3457\n",
      "Epoch: 092, Runtime 3.055736, Loss 1.185921, forward nfe 414677, backward nfe 96001, Train: 0.5600, Val: 0.3031, Test: 0.3388\n",
      "Epoch: 093, Runtime 3.056239, Loss 1.166592, forward nfe 419147, backward nfe 97025, Train: 0.5900, Val: 0.3015, Test: 0.3373\n",
      "Epoch: 094, Runtime 3.054813, Loss 1.147154, forward nfe 423616, backward nfe 98049, Train: 0.5850, Val: 0.3015, Test: 0.3373\n",
      "Epoch: 095, Runtime 3.054365, Loss 1.134854, forward nfe 428085, backward nfe 99073, Train: 0.5850, Val: 0.3000, Test: 0.3359\n",
      "Epoch: 096, Runtime 3.055306, Loss 1.122702, forward nfe 432554, backward nfe 100096, Train: 0.6100, Val: 0.3046, Test: 0.3390\n",
      "Epoch: 097, Runtime 3.053267, Loss 1.105733, forward nfe 437024, backward nfe 101119, Train: 0.6100, Val: 0.3069, Test: 0.3410\n",
      "Epoch: 098, Runtime 3.053719, Loss 1.069919, forward nfe 441493, backward nfe 102143, Train: 0.6200, Val: 0.3023, Test: 0.3435\n",
      "Epoch: 099, Runtime 3.061635, Loss 1.066299, forward nfe 445963, backward nfe 103166, Train: 0.6200, Val: 0.3023, Test: 0.3411\n",
      "best val accuracy 0.536923 with test accuracy 0.536647 at epoch 81\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.419309, Loss 2.320800, forward nfe 164, backward nfe 292, Train: 0.1000, Val: 0.1269, Test: 0.1286\n",
      "Epoch: 002, Runtime 3.055317, Loss 2.537109, forward nfe 4632, backward nfe 1316, Train: 0.1400, Val: 0.1100, Test: 0.1069\n",
      "Epoch: 003, Runtime 3.054595, Loss 2.252418, forward nfe 9100, backward nfe 2340, Train: 0.1400, Val: 0.0185, Test: 0.0286\n",
      "Epoch: 004, Runtime 3.056370, Loss 2.351321, forward nfe 13568, backward nfe 3365, Train: 0.1950, Val: 0.0623, Test: 0.0775\n",
      "Epoch: 005, Runtime 3.056475, Loss 2.274406, forward nfe 18037, backward nfe 4390, Train: 0.1050, Val: 0.0562, Test: 0.0597\n",
      "Epoch: 006, Runtime 3.065446, Loss 2.265000, forward nfe 22508, backward nfe 5418, Train: 0.1050, Val: 0.0562, Test: 0.0597\n",
      "Epoch: 007, Runtime 3.067847, Loss 2.277126, forward nfe 26995, backward nfe 6452, Train: 0.1100, Val: 0.0562, Test: 0.0601\n",
      "Epoch: 008, Runtime 3.176780, Loss 2.256487, forward nfe 31562, backward nfe 7509, Train: 0.1350, Val: 0.0577, Test: 0.0621\n",
      "Epoch: 009, Runtime 3.152950, Loss 2.231393, forward nfe 36223, backward nfe 8541, Train: 0.1950, Val: 0.0615, Test: 0.0772\n",
      "Epoch: 010, Runtime 3.233011, Loss 2.240384, forward nfe 40885, backward nfe 9660, Train: 0.1950, Val: 0.0623, Test: 0.0772\n",
      "Epoch: 011, Runtime 3.181288, Loss 2.236280, forward nfe 45547, backward nfe 10726, Train: 0.1650, Val: 0.0623, Test: 0.0678\n",
      "Epoch: 012, Runtime 3.232656, Loss 2.205641, forward nfe 50209, backward nfe 11845, Train: 0.1150, Val: 0.0577, Test: 0.0609\n",
      "Epoch: 013, Runtime 3.225063, Loss 2.204509, forward nfe 54871, backward nfe 12956, Train: 0.1100, Val: 0.0569, Test: 0.0604\n",
      "Epoch: 014, Runtime 3.231721, Loss 2.209700, forward nfe 59533, backward nfe 14075, Train: 0.1150, Val: 0.0577, Test: 0.0608\n",
      "Epoch: 015, Runtime 3.233181, Loss 2.195816, forward nfe 64195, backward nfe 15194, Train: 0.1400, Val: 0.0577, Test: 0.0632\n",
      "Epoch: 016, Runtime 3.234822, Loss 2.171779, forward nfe 68857, backward nfe 16313, Train: 0.1900, Val: 0.0677, Test: 0.0774\n",
      "Epoch: 017, Runtime 3.240491, Loss 2.168559, forward nfe 73519, backward nfe 17432, Train: 0.1950, Val: 0.0700, Test: 0.0788\n",
      "Epoch: 018, Runtime 3.233349, Loss 2.168547, forward nfe 78181, backward nfe 18551, Train: 0.2050, Val: 0.0715, Test: 0.0816\n",
      "Epoch: 019, Runtime 3.232762, Loss 2.149175, forward nfe 82843, backward nfe 19670, Train: 0.1650, Val: 0.0677, Test: 0.0754\n",
      "Epoch: 020, Runtime 3.231050, Loss 2.135014, forward nfe 87505, backward nfe 20789, Train: 0.1600, Val: 0.0646, Test: 0.0695\n",
      "Epoch: 021, Runtime 3.234598, Loss 2.134342, forward nfe 92167, backward nfe 21908, Train: 0.1600, Val: 0.0669, Test: 0.0727\n",
      "Epoch: 022, Runtime 3.231228, Loss 2.124192, forward nfe 96829, backward nfe 23027, Train: 0.1950, Val: 0.0708, Test: 0.0780\n",
      "Epoch: 023, Runtime 3.233695, Loss 2.107736, forward nfe 101491, backward nfe 24146, Train: 0.2000, Val: 0.0715, Test: 0.0790\n",
      "Epoch: 024, Runtime 3.234194, Loss 2.105840, forward nfe 106153, backward nfe 25265, Train: 0.2000, Val: 0.0715, Test: 0.0790\n",
      "Epoch: 025, Runtime 3.235614, Loss 2.098436, forward nfe 110815, backward nfe 26384, Train: 0.1950, Val: 0.0708, Test: 0.0784\n",
      "Epoch: 026, Runtime 3.235178, Loss 2.083883, forward nfe 115477, backward nfe 27503, Train: 0.1800, Val: 0.0700, Test: 0.0762\n",
      "Epoch: 027, Runtime 3.235904, Loss 2.078032, forward nfe 120139, backward nfe 28622, Train: 0.1800, Val: 0.0700, Test: 0.0764\n",
      "Epoch: 028, Runtime 3.232344, Loss 2.073095, forward nfe 124801, backward nfe 29741, Train: 0.1950, Val: 0.0708, Test: 0.0784\n",
      "Epoch: 029, Runtime 3.232448, Loss 2.058997, forward nfe 129463, backward nfe 30860, Train: 0.2000, Val: 0.0723, Test: 0.0789\n",
      "Epoch: 030, Runtime 3.234561, Loss 2.054393, forward nfe 134125, backward nfe 31979, Train: 0.2000, Val: 0.0723, Test: 0.0792\n",
      "Epoch: 031, Runtime 3.235667, Loss 2.050417, forward nfe 138787, backward nfe 33098, Train: 0.2000, Val: 0.0723, Test: 0.0788\n",
      "Epoch: 032, Runtime 3.236046, Loss 2.039342, forward nfe 143449, backward nfe 34217, Train: 0.1900, Val: 0.0708, Test: 0.0779\n",
      "Epoch: 033, Runtime 3.243088, Loss 2.030730, forward nfe 148111, backward nfe 35336, Train: 0.1950, Val: 0.0708, Test: 0.0776\n",
      "Epoch: 034, Runtime 3.231380, Loss 2.026341, forward nfe 152773, backward nfe 36455, Train: 0.1900, Val: 0.0708, Test: 0.0777\n",
      "Epoch: 035, Runtime 3.234555, Loss 2.022631, forward nfe 157435, backward nfe 37574, Train: 0.1950, Val: 0.0715, Test: 0.0779\n",
      "Epoch: 036, Runtime 3.234540, Loss 2.011826, forward nfe 162097, backward nfe 38693, Train: 0.2000, Val: 0.0731, Test: 0.0790\n",
      "Epoch: 037, Runtime 3.232858, Loss 2.007755, forward nfe 166759, backward nfe 39812, Train: 0.2000, Val: 0.0731, Test: 0.0789\n",
      "Epoch: 038, Runtime 3.232532, Loss 2.003901, forward nfe 171421, backward nfe 40931, Train: 0.2000, Val: 0.0731, Test: 0.0789\n",
      "Epoch: 039, Runtime 3.233649, Loss 1.994372, forward nfe 176083, backward nfe 42050, Train: 0.1950, Val: 0.0715, Test: 0.0780\n",
      "Epoch: 040, Runtime 3.234278, Loss 1.988050, forward nfe 180745, backward nfe 43169, Train: 0.1900, Val: 0.0715, Test: 0.0780\n",
      "Epoch: 041, Runtime 3.200176, Loss 1.983437, forward nfe 185407, backward nfe 44251, Train: 0.1900, Val: 0.0715, Test: 0.0777\n",
      "Epoch: 042, Runtime 3.232365, Loss 1.981842, forward nfe 190069, backward nfe 45370, Train: 0.2000, Val: 0.0723, Test: 0.0788\n",
      "Epoch: 043, Runtime 3.234661, Loss 1.970719, forward nfe 194731, backward nfe 46489, Train: 0.2000, Val: 0.0738, Test: 0.0802\n",
      "Epoch: 044, Runtime 3.230770, Loss 1.967706, forward nfe 199393, backward nfe 47605, Train: 0.2000, Val: 0.0738, Test: 0.0807\n",
      "Epoch: 045, Runtime 3.197925, Loss 1.964389, forward nfe 204055, backward nfe 48724, Train: 0.2150, Val: 0.0731, Test: 0.0818\n",
      "Epoch: 046, Runtime 3.230371, Loss 1.957970, forward nfe 208643, backward nfe 49843, Train: 0.2050, Val: 0.0723, Test: 0.0829\n",
      "Epoch: 047, Runtime 3.233383, Loss 1.952064, forward nfe 213305, backward nfe 50962, Train: 0.2100, Val: 0.0723, Test: 0.0841\n",
      "Epoch: 048, Runtime 3.234649, Loss 1.951199, forward nfe 217967, backward nfe 52081, Train: 0.2200, Val: 0.0723, Test: 0.0851\n",
      "Epoch: 049, Runtime 3.199520, Loss 1.945431, forward nfe 222629, backward nfe 53167, Train: 0.2050, Val: 0.0723, Test: 0.0845\n",
      "Epoch: 050, Runtime 3.173681, Loss 1.936755, forward nfe 227291, backward nfe 54209, Train: 0.2150, Val: 0.0708, Test: 0.0846\n",
      "Epoch: 051, Runtime 3.232745, Loss 1.935809, forward nfe 231953, backward nfe 55325, Train: 0.2000, Val: 0.0700, Test: 0.0842\n",
      "Epoch: 052, Runtime 3.201808, Loss 1.934099, forward nfe 236615, backward nfe 56412, Train: 0.2050, Val: 0.0700, Test: 0.0850\n",
      "Epoch: 053, Runtime 3.154188, Loss 1.929688, forward nfe 241277, backward nfe 57445, Train: 0.2150, Val: 0.0708, Test: 0.0856\n",
      "Epoch: 054, Runtime 3.168437, Loss 1.924260, forward nfe 245939, backward nfe 58512, Train: 0.2300, Val: 0.0723, Test: 0.0868\n",
      "Epoch: 055, Runtime 3.117300, Loss 1.918906, forward nfe 250565, backward nfe 59542, Train: 0.2500, Val: 0.0769, Test: 0.0961\n",
      "Epoch: 056, Runtime 3.160177, Loss 1.915195, forward nfe 255156, backward nfe 60607, Train: 0.2000, Val: 0.0769, Test: 0.0957\n",
      "Epoch: 057, Runtime 3.170550, Loss 1.906804, forward nfe 259768, backward nfe 61657, Train: 0.2000, Val: 0.0777, Test: 0.0939\n",
      "Epoch: 058, Runtime 3.173249, Loss 1.893696, forward nfe 264430, backward nfe 62718, Train: 0.2000, Val: 0.0777, Test: 0.0941\n",
      "Epoch: 059, Runtime 3.202919, Loss 1.883740, forward nfe 269079, backward nfe 63805, Train: 0.2000, Val: 0.0754, Test: 0.0943\n",
      "Epoch: 060, Runtime 3.179413, Loss 1.867192, forward nfe 273741, backward nfe 64865, Train: 0.2050, Val: 0.0769, Test: 0.0959\n",
      "Epoch: 061, Runtime 3.109130, Loss 1.853558, forward nfe 278403, backward nfe 65891, Train: 0.2150, Val: 0.0769, Test: 0.0973\n",
      "Epoch: 062, Runtime 3.156383, Loss 1.836121, forward nfe 282983, backward nfe 66925, Train: 0.2050, Val: 0.0746, Test: 0.0983\n",
      "Epoch: 063, Runtime 3.124716, Loss 1.819936, forward nfe 287559, backward nfe 67980, Train: 0.2000, Val: 0.0931, Test: 0.1133\n",
      "Epoch: 064, Runtime 3.146809, Loss 1.809355, forward nfe 292149, backward nfe 69029, Train: 0.1900, Val: 0.1408, Test: 0.1471\n",
      "Epoch: 065, Runtime 3.137381, Loss 1.802420, forward nfe 296731, backward nfe 70089, Train: 0.1950, Val: 0.1269, Test: 0.1392\n",
      "Epoch: 066, Runtime 3.091860, Loss 1.799866, forward nfe 301349, backward nfe 71118, Train: 0.2550, Val: 0.0854, Test: 0.1169\n",
      "Epoch: 067, Runtime 3.152748, Loss 1.787008, forward nfe 305919, backward nfe 72162, Train: 0.2550, Val: 0.0808, Test: 0.1143\n",
      "Epoch: 068, Runtime 3.147593, Loss 1.786020, forward nfe 310534, backward nfe 73186, Train: 0.2500, Val: 0.0800, Test: 0.1093\n",
      "Epoch: 069, Runtime 3.151573, Loss 1.773711, forward nfe 315196, backward nfe 74235, Train: 0.2450, Val: 0.0777, Test: 0.1082\n",
      "Epoch: 070, Runtime 3.138796, Loss 1.767587, forward nfe 319791, backward nfe 75270, Train: 0.2700, Val: 0.0854, Test: 0.1195\n",
      "Epoch: 071, Runtime 3.117036, Loss 1.757585, forward nfe 324357, backward nfe 76323, Train: 0.2800, Val: 0.1115, Test: 0.1379\n",
      "Epoch: 072, Runtime 3.079589, Loss 1.748537, forward nfe 328941, backward nfe 77347, Train: 0.2450, Val: 0.1292, Test: 0.1484\n",
      "Epoch: 073, Runtime 3.083268, Loss 1.728912, forward nfe 333428, backward nfe 78379, Train: 0.2650, Val: 0.1369, Test: 0.1525\n",
      "Epoch: 074, Runtime 3.076749, Loss 1.729627, forward nfe 337932, backward nfe 79409, Train: 0.2700, Val: 0.1431, Test: 0.1645\n",
      "Epoch: 075, Runtime 3.077076, Loss 1.718360, forward nfe 342460, backward nfe 80437, Train: 0.2850, Val: 0.1562, Test: 0.1724\n",
      "Epoch: 076, Runtime 3.077402, Loss 1.709245, forward nfe 346969, backward nfe 81464, Train: 0.2900, Val: 0.1608, Test: 0.1733\n",
      "Epoch: 077, Runtime 3.061411, Loss 1.701868, forward nfe 351454, backward nfe 82488, Train: 0.3150, Val: 0.1569, Test: 0.1757\n",
      "Epoch: 078, Runtime 3.058111, Loss 1.686973, forward nfe 355927, backward nfe 83514, Train: 0.3100, Val: 0.1669, Test: 0.1750\n",
      "Epoch: 079, Runtime 3.062988, Loss 1.676785, forward nfe 360400, backward nfe 84542, Train: 0.3300, Val: 0.1631, Test: 0.1779\n",
      "Epoch: 080, Runtime 3.056624, Loss 1.671626, forward nfe 364874, backward nfe 85566, Train: 0.3350, Val: 0.1600, Test: 0.1800\n",
      "Epoch: 081, Runtime 3.074422, Loss 1.664188, forward nfe 369358, backward nfe 86593, Train: 0.3350, Val: 0.1723, Test: 0.1900\n",
      "Epoch: 082, Runtime 3.061065, Loss 1.653955, forward nfe 373853, backward nfe 87617, Train: 0.3600, Val: 0.1900, Test: 0.2076\n",
      "Epoch: 083, Runtime 3.062357, Loss 1.637487, forward nfe 378333, backward nfe 88641, Train: 0.3650, Val: 0.1908, Test: 0.2156\n",
      "Epoch: 084, Runtime 3.076153, Loss 1.625238, forward nfe 382809, backward nfe 89668, Train: 0.3700, Val: 0.1915, Test: 0.2178\n",
      "Epoch: 085, Runtime 3.059873, Loss 1.614340, forward nfe 387295, backward nfe 90692, Train: 0.3800, Val: 0.2108, Test: 0.2294\n",
      "Epoch: 086, Runtime 3.088523, Loss 1.602487, forward nfe 391823, backward nfe 91716, Train: 0.4000, Val: 0.2138, Test: 0.2364\n",
      "Epoch: 087, Runtime 3.063227, Loss 1.597879, forward nfe 396303, backward nfe 92746, Train: 0.4150, Val: 0.2169, Test: 0.2424\n",
      "Epoch: 088, Runtime 3.057385, Loss 1.575447, forward nfe 400776, backward nfe 93771, Train: 0.4100, Val: 0.2223, Test: 0.2445\n",
      "Epoch: 089, Runtime 3.073687, Loss 1.572021, forward nfe 405258, backward nfe 94805, Train: 0.4200, Val: 0.2254, Test: 0.2441\n",
      "Epoch: 090, Runtime 3.055469, Loss 1.566009, forward nfe 409732, backward nfe 95829, Train: 0.4250, Val: 0.2192, Test: 0.2414\n",
      "Epoch: 091, Runtime 3.059612, Loss 1.542027, forward nfe 414203, backward nfe 96856, Train: 0.4150, Val: 0.2269, Test: 0.2447\n",
      "Epoch: 092, Runtime 3.059063, Loss 1.529616, forward nfe 418675, backward nfe 97880, Train: 0.4100, Val: 0.2300, Test: 0.2487\n",
      "Epoch: 093, Runtime 3.061090, Loss 1.533010, forward nfe 423148, backward nfe 98908, Train: 0.4050, Val: 0.2262, Test: 0.2502\n",
      "Epoch: 094, Runtime 3.057664, Loss 1.508741, forward nfe 427618, backward nfe 99934, Train: 0.4250, Val: 0.2323, Test: 0.2499\n",
      "Epoch: 095, Runtime 3.057202, Loss 1.497291, forward nfe 432089, backward nfe 100958, Train: 0.4400, Val: 0.2308, Test: 0.2591\n",
      "Epoch: 096, Runtime 3.060009, Loss 1.498111, forward nfe 436560, backward nfe 101982, Train: 0.4450, Val: 0.2154, Test: 0.2467\n",
      "Epoch: 097, Runtime 3.062396, Loss 1.494303, forward nfe 441040, backward nfe 103008, Train: 0.4500, Val: 0.2292, Test: 0.2572\n",
      "Epoch: 098, Runtime 3.059445, Loss 1.474977, forward nfe 445513, backward nfe 104032, Train: 0.5050, Val: 0.2815, Test: 0.2880\n",
      "Epoch: 099, Runtime 3.060994, Loss 1.473508, forward nfe 449981, backward nfe 105060, Train: 0.5000, Val: 0.2831, Test: 0.3017\n",
      "best val accuracy 0.283077 with test accuracy 0.301747 at epoch 99\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.516194, Loss 2.334697, forward nfe 169, backward nfe 390, Train: 0.0750, Val: 0.0908, Test: 0.0908\n",
      "Epoch: 002, Runtime 3.054395, Loss 2.617846, forward nfe 4637, backward nfe 1414, Train: 0.0850, Val: 0.0608, Test: 0.0671\n",
      "Epoch: 003, Runtime 3.055057, Loss 2.311870, forward nfe 9106, backward nfe 2439, Train: 0.1100, Val: 0.1777, Test: 0.1540\n",
      "Epoch: 004, Runtime 3.056096, Loss 2.311297, forward nfe 13575, backward nfe 3463, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 005, Runtime 3.055219, Loss 2.310992, forward nfe 18052, backward nfe 4489, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 006, Runtime 3.056330, Loss 2.309324, forward nfe 22526, backward nfe 5515, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 007, Runtime 3.128439, Loss 2.306899, forward nfe 27020, backward nfe 6564, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 008, Runtime 3.132395, Loss 2.306281, forward nfe 31623, backward nfe 7607, Train: 0.1050, Val: 0.1685, Test: 0.1487\n",
      "Epoch: 009, Runtime 3.151181, Loss 2.304731, forward nfe 36285, backward nfe 8643, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 010, Runtime 3.147826, Loss 2.301643, forward nfe 40947, backward nfe 9673, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 011, Runtime 3.218386, Loss 2.299151, forward nfe 45609, backward nfe 10779, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 012, Runtime 3.228125, Loss 2.295264, forward nfe 50271, backward nfe 11898, Train: 0.1100, Val: 0.1777, Test: 0.1544\n",
      "Epoch: 013, Runtime 3.227006, Loss 2.294912, forward nfe 54933, backward nfe 13017, Train: 0.1100, Val: 0.1777, Test: 0.1544\n",
      "Epoch: 014, Runtime 3.226985, Loss 2.291711, forward nfe 59595, backward nfe 14136, Train: 0.1100, Val: 0.1777, Test: 0.1544\n",
      "Epoch: 015, Runtime 3.227530, Loss 2.291490, forward nfe 64257, backward nfe 15255, Train: 0.1100, Val: 0.1777, Test: 0.1544\n",
      "Epoch: 016, Runtime 3.228650, Loss 2.289738, forward nfe 68919, backward nfe 16374, Train: 0.1100, Val: 0.1777, Test: 0.1544\n",
      "Epoch: 017, Runtime 3.238712, Loss 2.287195, forward nfe 73581, backward nfe 17493, Train: 0.1100, Val: 0.1777, Test: 0.1544\n",
      "Epoch: 018, Runtime 3.227044, Loss 2.285998, forward nfe 78243, backward nfe 18612, Train: 0.1100, Val: 0.1777, Test: 0.1544\n",
      "Epoch: 019, Runtime 3.226439, Loss 2.285090, forward nfe 82905, backward nfe 19731, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 020, Runtime 3.226050, Loss 2.283670, forward nfe 87567, backward nfe 20850, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 021, Runtime 3.226682, Loss 2.282722, forward nfe 92229, backward nfe 21969, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 022, Runtime 3.226427, Loss 2.282546, forward nfe 96891, backward nfe 23088, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 023, Runtime 3.227943, Loss 2.281677, forward nfe 101553, backward nfe 24207, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 024, Runtime 3.232261, Loss 2.281172, forward nfe 106215, backward nfe 25326, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 025, Runtime 3.226368, Loss 2.280058, forward nfe 110877, backward nfe 26445, Train: 0.1100, Val: 0.1777, Test: 0.1543\n",
      "Epoch: 026, Runtime 3.225759, Loss 2.279630, forward nfe 115539, backward nfe 27564, Train: 0.1100, Val: 0.1777, Test: 0.1540\n",
      "Epoch: 027, Runtime 3.226692, Loss 2.278445, forward nfe 120201, backward nfe 28683, Train: 0.1100, Val: 0.1777, Test: 0.1540\n",
      "Epoch: 028, Runtime 3.226820, Loss 2.278144, forward nfe 124863, backward nfe 29802, Train: 0.1100, Val: 0.1777, Test: 0.1541\n",
      "Epoch: 029, Runtime 3.230374, Loss 2.276367, forward nfe 129525, backward nfe 30921, Train: 0.1150, Val: 0.1769, Test: 0.1539\n",
      "Epoch: 030, Runtime 3.229145, Loss 2.276509, forward nfe 134187, backward nfe 32040, Train: 0.1100, Val: 0.1762, Test: 0.1537\n",
      "Epoch: 031, Runtime 3.227723, Loss 2.274418, forward nfe 138849, backward nfe 33159, Train: 0.1500, Val: 0.1646, Test: 0.1563\n",
      "Epoch: 032, Runtime 3.227850, Loss 2.250043, forward nfe 143511, backward nfe 34278, Train: 0.1150, Val: 0.0269, Test: 0.0300\n",
      "Epoch: 033, Runtime 3.232457, Loss 2.283569, forward nfe 148173, backward nfe 35397, Train: 0.1450, Val: 0.0677, Test: 0.0679\n",
      "Epoch: 034, Runtime 3.229081, Loss 2.252910, forward nfe 152835, backward nfe 36516, Train: 0.1150, Val: 0.1746, Test: 0.1528\n",
      "Epoch: 035, Runtime 3.241969, Loss 2.247602, forward nfe 157497, backward nfe 37635, Train: 0.1250, Val: 0.1762, Test: 0.1574\n",
      "Epoch: 036, Runtime 3.228235, Loss 2.265708, forward nfe 162159, backward nfe 38754, Train: 0.1250, Val: 0.0169, Test: 0.0222\n",
      "Epoch: 037, Runtime 3.227028, Loss 2.269965, forward nfe 166821, backward nfe 39873, Train: 0.1200, Val: 0.0169, Test: 0.0220\n",
      "Epoch: 038, Runtime 3.228917, Loss 2.273740, forward nfe 171483, backward nfe 40992, Train: 0.1150, Val: 0.0154, Test: 0.0220\n",
      "Epoch: 039, Runtime 3.231041, Loss 2.272007, forward nfe 176145, backward nfe 42111, Train: 0.1150, Val: 0.0154, Test: 0.0218\n",
      "Epoch: 040, Runtime 3.228273, Loss 2.270384, forward nfe 180807, backward nfe 43230, Train: 0.1150, Val: 0.0154, Test: 0.0216\n",
      "Epoch: 041, Runtime 3.232422, Loss 2.269510, forward nfe 185469, backward nfe 44349, Train: 0.1150, Val: 0.0154, Test: 0.0219\n",
      "Epoch: 042, Runtime 3.229549, Loss 2.269144, forward nfe 190131, backward nfe 45468, Train: 0.1150, Val: 0.0162, Test: 0.0219\n",
      "Epoch: 043, Runtime 3.232261, Loss 2.269636, forward nfe 194793, backward nfe 46587, Train: 0.1150, Val: 0.0162, Test: 0.0219\n",
      "Epoch: 044, Runtime 3.228477, Loss 2.269857, forward nfe 199455, backward nfe 47706, Train: 0.1150, Val: 0.0162, Test: 0.0219\n",
      "Epoch: 045, Runtime 3.227932, Loss 2.269096, forward nfe 204117, backward nfe 48825, Train: 0.1150, Val: 0.0162, Test: 0.0219\n",
      "Epoch: 046, Runtime 3.230609, Loss 2.269190, forward nfe 208779, backward nfe 49944, Train: 0.1150, Val: 0.0162, Test: 0.0219\n",
      "Epoch: 047, Runtime 3.230193, Loss 2.268981, forward nfe 213441, backward nfe 51063, Train: 0.1150, Val: 0.0162, Test: 0.0220\n",
      "Epoch: 048, Runtime 3.229562, Loss 2.269123, forward nfe 218103, backward nfe 52182, Train: 0.1150, Val: 0.0162, Test: 0.0220\n",
      "Epoch: 049, Runtime 3.230191, Loss 2.269177, forward nfe 222765, backward nfe 53301, Train: 0.1150, Val: 0.0162, Test: 0.0220\n",
      "Epoch: 050, Runtime 3.224856, Loss 2.269627, forward nfe 227427, backward nfe 54415, Train: 0.1150, Val: 0.0162, Test: 0.0220\n",
      "Epoch: 051, Runtime 3.229113, Loss 2.269011, forward nfe 232089, backward nfe 55534, Train: 0.1200, Val: 0.0169, Test: 0.0224\n",
      "Epoch: 052, Runtime 3.237243, Loss 2.269157, forward nfe 236751, backward nfe 56653, Train: 0.1200, Val: 0.0177, Test: 0.0224\n",
      "Epoch: 053, Runtime 3.229911, Loss 2.268805, forward nfe 241413, backward nfe 57772, Train: 0.1200, Val: 0.0177, Test: 0.0224\n",
      "Epoch: 054, Runtime 3.229419, Loss 2.268160, forward nfe 246075, backward nfe 58891, Train: 0.1200, Val: 0.0177, Test: 0.0226\n",
      "Epoch: 055, Runtime 3.227368, Loss 2.268623, forward nfe 250737, backward nfe 60010, Train: 0.1250, Val: 0.0185, Test: 0.0229\n",
      "Epoch: 056, Runtime 3.230698, Loss 2.266964, forward nfe 255399, backward nfe 61129, Train: 0.1600, Val: 0.4077, Test: 0.3824\n",
      "Epoch: 057, Runtime 3.229226, Loss 2.258571, forward nfe 260061, backward nfe 62248, Train: 0.1850, Val: 0.4323, Test: 0.4070\n",
      "Epoch: 058, Runtime 3.225616, Loss 2.221900, forward nfe 264723, backward nfe 63367, Train: 0.2000, Val: 0.4323, Test: 0.4161\n",
      "Epoch: 059, Runtime 3.228158, Loss 2.159689, forward nfe 269385, backward nfe 64486, Train: 0.1250, Val: 0.1338, Test: 0.1327\n",
      "Epoch: 060, Runtime 3.232212, Loss 2.237543, forward nfe 274047, backward nfe 65605, Train: 0.1450, Val: 0.1685, Test: 0.1780\n",
      "Epoch: 061, Runtime 3.171185, Loss 2.203604, forward nfe 278709, backward nfe 66658, Train: 0.2100, Val: 0.4331, Test: 0.4174\n",
      "Epoch: 062, Runtime 3.229355, Loss 2.128552, forward nfe 283371, backward nfe 67777, Train: 0.2150, Val: 0.0808, Test: 0.0970\n",
      "Epoch: 063, Runtime 3.227428, Loss 2.168308, forward nfe 288033, backward nfe 68896, Train: 0.1900, Val: 0.0708, Test: 0.0866\n",
      "Epoch: 064, Runtime 3.226778, Loss 2.181122, forward nfe 292695, backward nfe 70015, Train: 0.1900, Val: 0.0715, Test: 0.0876\n",
      "Epoch: 065, Runtime 3.231877, Loss 2.170965, forward nfe 297357, backward nfe 71134, Train: 0.2150, Val: 0.0900, Test: 0.1055\n",
      "Epoch: 066, Runtime 3.168493, Loss 2.148470, forward nfe 302019, backward nfe 72188, Train: 0.2200, Val: 0.0862, Test: 0.1065\n",
      "Epoch: 067, Runtime 3.228267, Loss 2.111310, forward nfe 306681, backward nfe 73307, Train: 0.1200, Val: 0.0623, Test: 0.0649\n",
      "Epoch: 068, Runtime 3.230880, Loss 2.149152, forward nfe 311343, backward nfe 74426, Train: 0.1250, Val: 0.0638, Test: 0.0668\n",
      "Epoch: 069, Runtime 3.239525, Loss 2.142614, forward nfe 316005, backward nfe 75545, Train: 0.2200, Val: 0.0892, Test: 0.1047\n",
      "Epoch: 070, Runtime 3.227546, Loss 2.094321, forward nfe 320667, backward nfe 76664, Train: 0.2200, Val: 0.0923, Test: 0.1130\n",
      "Epoch: 071, Runtime 3.229998, Loss 2.114383, forward nfe 325329, backward nfe 77783, Train: 0.2350, Val: 0.0885, Test: 0.1087\n",
      "Epoch: 072, Runtime 3.215129, Loss 2.125531, forward nfe 329991, backward nfe 78886, Train: 0.1950, Val: 0.0669, Test: 0.0840\n",
      "Epoch: 073, Runtime 3.229149, Loss 2.317390, forward nfe 334653, backward nfe 80005, Train: 0.2150, Val: 0.0885, Test: 0.1081\n",
      "Epoch: 074, Runtime 3.146659, Loss 2.077775, forward nfe 339315, backward nfe 81036, Train: 0.1650, Val: 0.0738, Test: 0.0852\n",
      "Epoch: 075, Runtime 3.229414, Loss 2.089261, forward nfe 343977, backward nfe 82155, Train: 0.1800, Val: 0.0731, Test: 0.0856\n",
      "Epoch: 076, Runtime 3.230474, Loss 2.080157, forward nfe 348639, backward nfe 83274, Train: 0.2200, Val: 0.1046, Test: 0.1192\n",
      "Epoch: 077, Runtime 3.174180, Loss 2.049122, forward nfe 353301, backward nfe 84333, Train: 0.2900, Val: 0.1500, Test: 0.1623\n",
      "Epoch: 078, Runtime 3.229551, Loss 2.043967, forward nfe 357963, backward nfe 85452, Train: 0.2900, Val: 0.1746, Test: 0.1803\n",
      "Epoch: 079, Runtime 3.166452, Loss 2.036943, forward nfe 362625, backward nfe 86503, Train: 0.3050, Val: 0.2315, Test: 0.2303\n",
      "Epoch: 080, Runtime 3.229965, Loss 2.000098, forward nfe 367287, backward nfe 87622, Train: 0.3000, Val: 0.2000, Test: 0.2098\n",
      "Epoch: 081, Runtime 3.231113, Loss 1.987164, forward nfe 371949, backward nfe 88741, Train: 0.2950, Val: 0.1900, Test: 0.1994\n",
      "Epoch: 082, Runtime 3.228321, Loss 1.979261, forward nfe 376611, backward nfe 89860, Train: 0.3000, Val: 0.1792, Test: 0.1906\n",
      "Epoch: 083, Runtime 3.164588, Loss 1.952497, forward nfe 381273, backward nfe 90908, Train: 0.3100, Val: 0.1785, Test: 0.1936\n",
      "Epoch: 084, Runtime 3.230545, Loss 1.941170, forward nfe 385935, backward nfe 92027, Train: 0.3450, Val: 0.2077, Test: 0.2010\n",
      "Epoch: 085, Runtime 3.237938, Loss 1.915657, forward nfe 390597, backward nfe 93146, Train: 0.3000, Val: 0.1462, Test: 0.1512\n",
      "Epoch: 086, Runtime 3.228783, Loss 1.906569, forward nfe 395259, backward nfe 94265, Train: 0.2950, Val: 0.1485, Test: 0.1583\n",
      "Epoch: 087, Runtime 3.200679, Loss 1.889400, forward nfe 399921, backward nfe 95351, Train: 0.2950, Val: 0.1700, Test: 0.1758\n",
      "Epoch: 088, Runtime 3.230328, Loss 1.880640, forward nfe 404583, backward nfe 96470, Train: 0.2950, Val: 0.2215, Test: 0.2205\n",
      "Epoch: 089, Runtime 3.217183, Loss 1.851021, forward nfe 409224, backward nfe 97589, Train: 0.3300, Val: 0.2046, Test: 0.2176\n",
      "Epoch: 090, Runtime 3.227795, Loss 1.830110, forward nfe 413886, backward nfe 98708, Train: 0.3600, Val: 0.1846, Test: 0.2017\n",
      "Epoch: 091, Runtime 3.231737, Loss 1.821769, forward nfe 418548, backward nfe 99827, Train: 0.3650, Val: 0.2300, Test: 0.2403\n",
      "Epoch: 092, Runtime 3.227431, Loss 1.796138, forward nfe 423210, backward nfe 100946, Train: 0.3750, Val: 0.2446, Test: 0.2587\n",
      "Epoch: 093, Runtime 3.231995, Loss 1.777542, forward nfe 427872, backward nfe 102065, Train: 0.3750, Val: 0.2338, Test: 0.2446\n",
      "Epoch: 094, Runtime 3.183388, Loss 1.769283, forward nfe 432534, backward nfe 103136, Train: 0.3550, Val: 0.2808, Test: 0.2767\n",
      "Epoch: 095, Runtime 3.220867, Loss 1.755651, forward nfe 437196, backward nfe 104245, Train: 0.3250, Val: 0.2938, Test: 0.2864\n",
      "Epoch: 096, Runtime 3.226346, Loss 1.738958, forward nfe 441858, backward nfe 105364, Train: 0.3350, Val: 0.2931, Test: 0.2857\n",
      "Epoch: 097, Runtime 3.228798, Loss 1.725710, forward nfe 446520, backward nfe 106483, Train: 0.3750, Val: 0.2600, Test: 0.2740\n",
      "Epoch: 098, Runtime 3.209423, Loss 1.698366, forward nfe 451164, backward nfe 107588, Train: 0.3800, Val: 0.2262, Test: 0.2393\n",
      "Epoch: 099, Runtime 3.186854, Loss 1.687298, forward nfe 455733, backward nfe 108707, Train: 0.3900, Val: 0.2054, Test: 0.2271\n",
      "best val accuracy 0.433077 with test accuracy 0.417401 at epoch 61\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.478577, Loss 2.314827, forward nfe 167, backward nfe 353, Train: 0.1000, Val: 0.0569, Test: 0.0594\n",
      "Epoch: 002, Runtime 3.041111, Loss 2.705899, forward nfe 4603, backward nfe 1377, Train: 0.0950, Val: 0.0585, Test: 0.0629\n",
      "Epoch: 003, Runtime 3.058978, Loss 2.273075, forward nfe 9071, backward nfe 2403, Train: 0.0750, Val: 0.2646, Test: 0.2443\n",
      "Epoch: 004, Runtime 3.056841, Loss 2.377267, forward nfe 13540, backward nfe 3427, Train: 0.0700, Val: 0.2208, Test: 0.1995\n",
      "Epoch: 005, Runtime 3.058147, Loss 2.317916, forward nfe 18010, backward nfe 4452, Train: 0.1050, Val: 0.0169, Test: 0.0327\n",
      "Epoch: 006, Runtime 3.055462, Loss 2.304377, forward nfe 22482, backward nfe 5477, Train: 0.1100, Val: 0.0485, Test: 0.0611\n",
      "Epoch: 007, Runtime 3.062603, Loss 2.288343, forward nfe 26955, backward nfe 6508, Train: 0.1050, Val: 0.0200, Test: 0.0342\n",
      "Epoch: 008, Runtime 3.063092, Loss 2.292007, forward nfe 31439, backward nfe 7532, Train: 0.1200, Val: 0.1277, Test: 0.1336\n",
      "Epoch: 009, Runtime 3.112319, Loss 2.281758, forward nfe 35959, backward nfe 8561, Train: 0.1050, Val: 0.0208, Test: 0.0344\n",
      "Epoch: 010, Runtime 3.164790, Loss 2.287537, forward nfe 40587, backward nfe 9606, Train: 0.1050, Val: 0.0208, Test: 0.0340\n",
      "Epoch: 011, Runtime 3.173582, Loss 2.285838, forward nfe 45249, backward nfe 10661, Train: 0.1050, Val: 0.0362, Test: 0.0508\n",
      "Epoch: 012, Runtime 3.172453, Loss 2.275157, forward nfe 49911, backward nfe 11712, Train: 0.1150, Val: 0.1992, Test: 0.1961\n",
      "Epoch: 013, Runtime 3.194869, Loss 2.259438, forward nfe 54536, backward nfe 12831, Train: 0.1000, Val: 0.2300, Test: 0.2151\n",
      "Epoch: 014, Runtime 3.232678, Loss 2.264463, forward nfe 59149, backward nfe 13950, Train: 0.1200, Val: 0.0938, Test: 0.1048\n",
      "Epoch: 015, Runtime 3.232622, Loss 2.249728, forward nfe 63811, backward nfe 15069, Train: 0.1200, Val: 0.0400, Test: 0.0584\n",
      "Epoch: 016, Runtime 3.234532, Loss 2.251603, forward nfe 68473, backward nfe 16188, Train: 0.1400, Val: 0.0362, Test: 0.0523\n",
      "Epoch: 017, Runtime 3.242919, Loss 2.236594, forward nfe 73135, backward nfe 17307, Train: 0.1300, Val: 0.0200, Test: 0.0318\n",
      "Epoch: 018, Runtime 3.230594, Loss 2.229503, forward nfe 77797, backward nfe 18426, Train: 0.1450, Val: 0.0208, Test: 0.0340\n",
      "Epoch: 019, Runtime 3.234486, Loss 2.219993, forward nfe 82459, backward nfe 19545, Train: 0.1800, Val: 0.0269, Test: 0.0471\n",
      "Epoch: 020, Runtime 3.235542, Loss 2.203429, forward nfe 87121, backward nfe 20664, Train: 0.1950, Val: 0.0300, Test: 0.0514\n",
      "Epoch: 021, Runtime 3.235436, Loss 2.195150, forward nfe 91783, backward nfe 21783, Train: 0.1900, Val: 0.0292, Test: 0.0495\n",
      "Epoch: 022, Runtime 3.235314, Loss 2.178056, forward nfe 96445, backward nfe 22902, Train: 0.1650, Val: 0.0223, Test: 0.0408\n",
      "Epoch: 023, Runtime 3.236184, Loss 2.156502, forward nfe 101107, backward nfe 24021, Train: 0.1600, Val: 0.0208, Test: 0.0363\n",
      "Epoch: 024, Runtime 3.235658, Loss 2.141647, forward nfe 105769, backward nfe 25140, Train: 0.2200, Val: 0.0985, Test: 0.1091\n",
      "Epoch: 025, Runtime 3.235635, Loss 2.121186, forward nfe 110431, backward nfe 26259, Train: 0.2700, Val: 0.1292, Test: 0.1449\n",
      "Epoch: 026, Runtime 3.235182, Loss 2.107254, forward nfe 115093, backward nfe 27378, Train: 0.2950, Val: 0.1238, Test: 0.1394\n",
      "Epoch: 027, Runtime 3.234843, Loss 2.086712, forward nfe 119755, backward nfe 28497, Train: 0.3350, Val: 0.1354, Test: 0.1525\n",
      "Epoch: 028, Runtime 3.234922, Loss 2.068395, forward nfe 124417, backward nfe 29616, Train: 0.3050, Val: 0.1377, Test: 0.1481\n",
      "Epoch: 029, Runtime 3.235626, Loss 2.053841, forward nfe 129079, backward nfe 30735, Train: 0.2450, Val: 0.0723, Test: 0.0850\n",
      "Epoch: 030, Runtime 3.228913, Loss 2.037594, forward nfe 133733, backward nfe 31854, Train: 0.2350, Val: 0.0708, Test: 0.0835\n",
      "Epoch: 031, Runtime 3.233918, Loss 2.025573, forward nfe 138395, backward nfe 32973, Train: 0.2500, Val: 0.0723, Test: 0.0856\n",
      "Epoch: 032, Runtime 3.234965, Loss 2.005860, forward nfe 143057, backward nfe 34092, Train: 0.2650, Val: 0.0800, Test: 0.0923\n",
      "Epoch: 033, Runtime 3.244061, Loss 1.991203, forward nfe 147719, backward nfe 35211, Train: 0.2500, Val: 0.0777, Test: 0.0915\n",
      "Epoch: 034, Runtime 3.234796, Loss 1.969211, forward nfe 152381, backward nfe 36330, Train: 0.2400, Val: 0.0785, Test: 0.0928\n",
      "Epoch: 035, Runtime 3.231990, Loss 1.955428, forward nfe 157043, backward nfe 37449, Train: 0.2550, Val: 0.0869, Test: 0.1015\n",
      "Epoch: 036, Runtime 3.233444, Loss 1.937655, forward nfe 161705, backward nfe 38568, Train: 0.3300, Val: 0.1408, Test: 0.1534\n",
      "Epoch: 037, Runtime 3.235299, Loss 1.927387, forward nfe 166367, backward nfe 39687, Train: 0.3000, Val: 0.1300, Test: 0.1502\n",
      "Epoch: 038, Runtime 3.235481, Loss 1.905091, forward nfe 171029, backward nfe 40806, Train: 0.3050, Val: 0.1377, Test: 0.1548\n",
      "Epoch: 039, Runtime 3.234321, Loss 1.891628, forward nfe 175691, backward nfe 41925, Train: 0.3450, Val: 0.1808, Test: 0.1967\n",
      "Epoch: 040, Runtime 3.236192, Loss 1.870678, forward nfe 180353, backward nfe 43044, Train: 0.3600, Val: 0.1885, Test: 0.2112\n",
      "Epoch: 041, Runtime 3.208001, Loss 1.861283, forward nfe 185015, backward nfe 44134, Train: 0.3550, Val: 0.1969, Test: 0.2146\n",
      "Epoch: 042, Runtime 3.195258, Loss 1.839415, forward nfe 189677, backward nfe 45213, Train: 0.3650, Val: 0.2062, Test: 0.2214\n",
      "Epoch: 043, Runtime 3.213367, Loss 1.830328, forward nfe 194339, backward nfe 46310, Train: 0.3900, Val: 0.2100, Test: 0.2268\n",
      "Epoch: 044, Runtime 3.205137, Loss 1.808384, forward nfe 199001, backward nfe 47398, Train: 0.3850, Val: 0.2169, Test: 0.2271\n",
      "Epoch: 045, Runtime 3.158113, Loss 1.801617, forward nfe 203621, backward nfe 48474, Train: 0.3900, Val: 0.2154, Test: 0.2307\n",
      "Epoch: 046, Runtime 3.233586, Loss 1.786536, forward nfe 208247, backward nfe 49593, Train: 0.3900, Val: 0.2208, Test: 0.2342\n",
      "Epoch: 047, Runtime 3.148262, Loss 1.775009, forward nfe 212909, backward nfe 50620, Train: 0.3850, Val: 0.2277, Test: 0.2453\n",
      "Epoch: 048, Runtime 3.197359, Loss 1.758265, forward nfe 217571, backward nfe 51701, Train: 0.3950, Val: 0.2354, Test: 0.2562\n",
      "Epoch: 049, Runtime 3.152176, Loss 1.746214, forward nfe 222233, backward nfe 52732, Train: 0.4100, Val: 0.2492, Test: 0.2653\n",
      "Epoch: 050, Runtime 3.207095, Loss 1.730634, forward nfe 226895, backward nfe 53812, Train: 0.4250, Val: 0.2554, Test: 0.2724\n",
      "Epoch: 051, Runtime 3.204456, Loss 1.716946, forward nfe 231557, backward nfe 54901, Train: 0.4200, Val: 0.2615, Test: 0.2797\n",
      "Epoch: 052, Runtime 3.163966, Loss 1.702001, forward nfe 236219, backward nfe 55944, Train: 0.4300, Val: 0.2731, Test: 0.2849\n",
      "Epoch: 053, Runtime 3.177301, Loss 1.687987, forward nfe 240881, backward nfe 57002, Train: 0.4300, Val: 0.2769, Test: 0.2953\n",
      "Epoch: 054, Runtime 3.107340, Loss 1.679915, forward nfe 245543, backward nfe 58028, Train: 0.4300, Val: 0.2923, Test: 0.3093\n",
      "Epoch: 055, Runtime 3.152912, Loss 1.655272, forward nfe 250116, backward nfe 59076, Train: 0.4550, Val: 0.3069, Test: 0.3178\n",
      "Epoch: 056, Runtime 3.130210, Loss 1.652934, forward nfe 254674, backward nfe 60118, Train: 0.4350, Val: 0.3000, Test: 0.3225\n",
      "Epoch: 057, Runtime 3.149910, Loss 1.631605, forward nfe 259336, backward nfe 61148, Train: 0.4500, Val: 0.2962, Test: 0.3212\n",
      "Epoch: 058, Runtime 3.173517, Loss 1.626343, forward nfe 263998, backward nfe 62201, Train: 0.4450, Val: 0.3054, Test: 0.3211\n",
      "Epoch: 059, Runtime 3.103214, Loss 1.603190, forward nfe 268577, backward nfe 63269, Train: 0.4450, Val: 0.3069, Test: 0.3219\n",
      "Epoch: 060, Runtime 3.147096, Loss 1.588723, forward nfe 273142, backward nfe 64298, Train: 0.4450, Val: 0.3100, Test: 0.3296\n",
      "Epoch: 061, Runtime 3.127556, Loss 1.578563, forward nfe 277704, backward nfe 65352, Train: 0.4450, Val: 0.3138, Test: 0.3319\n",
      "Epoch: 062, Runtime 3.100448, Loss 1.564533, forward nfe 282301, backward nfe 66385, Train: 0.4400, Val: 0.3108, Test: 0.3262\n",
      "Epoch: 063, Runtime 3.088236, Loss 1.560410, forward nfe 286827, backward nfe 67416, Train: 0.4450, Val: 0.3162, Test: 0.3290\n",
      "Epoch: 064, Runtime 3.102027, Loss 1.532002, forward nfe 291404, backward nfe 68444, Train: 0.4850, Val: 0.3146, Test: 0.3408\n",
      "Epoch: 065, Runtime 3.100360, Loss 1.521153, forward nfe 295908, backward nfe 69477, Train: 0.4700, Val: 0.3162, Test: 0.3434\n",
      "Epoch: 066, Runtime 3.115806, Loss 1.509792, forward nfe 300446, backward nfe 70522, Train: 0.4650, Val: 0.3285, Test: 0.3367\n",
      "Epoch: 067, Runtime 3.075012, Loss 1.502076, forward nfe 305019, backward nfe 71554, Train: 0.4850, Val: 0.3277, Test: 0.3379\n",
      "Epoch: 068, Runtime 3.112224, Loss 1.487365, forward nfe 309593, backward nfe 72578, Train: 0.4850, Val: 0.3100, Test: 0.3413\n",
      "Epoch: 069, Runtime 3.089072, Loss 1.480797, forward nfe 314115, backward nfe 73604, Train: 0.5100, Val: 0.3169, Test: 0.3442\n",
      "Epoch: 070, Runtime 3.075737, Loss 1.442797, forward nfe 318617, backward nfe 74631, Train: 0.5500, Val: 0.3408, Test: 0.3627\n",
      "Epoch: 071, Runtime 3.064324, Loss 1.413272, forward nfe 323116, backward nfe 75656, Train: 0.5800, Val: 0.3669, Test: 0.3825\n",
      "Epoch: 072, Runtime 3.088871, Loss 1.368859, forward nfe 327592, backward nfe 76689, Train: 0.5750, Val: 0.3985, Test: 0.4281\n",
      "Epoch: 073, Runtime 3.058508, Loss 1.339197, forward nfe 332104, backward nfe 77714, Train: 0.6100, Val: 0.4331, Test: 0.4515\n",
      "Epoch: 074, Runtime 3.087306, Loss 1.301812, forward nfe 336591, backward nfe 78746, Train: 0.6050, Val: 0.4092, Test: 0.4178\n",
      "Epoch: 075, Runtime 3.083211, Loss 1.289843, forward nfe 341102, backward nfe 79778, Train: 0.6100, Val: 0.4023, Test: 0.4219\n",
      "Epoch: 076, Runtime 3.089565, Loss 1.247604, forward nfe 345608, backward nfe 80803, Train: 0.6150, Val: 0.4077, Test: 0.4252\n",
      "Epoch: 077, Runtime 3.078226, Loss 1.231227, forward nfe 350161, backward nfe 81834, Train: 0.6250, Val: 0.4523, Test: 0.4889\n",
      "Epoch: 078, Runtime 3.072032, Loss 1.190057, forward nfe 354637, backward nfe 82859, Train: 0.5950, Val: 0.4969, Test: 0.5181\n",
      "Epoch: 079, Runtime 3.114555, Loss 1.171759, forward nfe 359163, backward nfe 83893, Train: 0.6150, Val: 0.4708, Test: 0.4948\n",
      "Epoch: 080, Runtime 3.058472, Loss 1.164708, forward nfe 363730, backward nfe 84917, Train: 0.6500, Val: 0.3985, Test: 0.4354\n",
      "Epoch: 081, Runtime 3.089316, Loss 1.161969, forward nfe 368212, backward nfe 85944, Train: 0.6600, Val: 0.4085, Test: 0.4332\n",
      "Epoch: 082, Runtime 3.078929, Loss 1.115466, forward nfe 372749, backward nfe 86971, Train: 0.6750, Val: 0.4131, Test: 0.4443\n",
      "Epoch: 083, Runtime 3.060525, Loss 1.093449, forward nfe 377270, backward nfe 88000, Train: 0.6550, Val: 0.4515, Test: 0.4790\n",
      "Epoch: 084, Runtime 3.070560, Loss 1.073668, forward nfe 381753, backward nfe 89028, Train: 0.6650, Val: 0.4985, Test: 0.5215\n",
      "Epoch: 085, Runtime 3.076528, Loss 1.072352, forward nfe 386247, backward nfe 90052, Train: 0.6300, Val: 0.5023, Test: 0.5269\n",
      "Epoch: 086, Runtime 3.063880, Loss 1.036975, forward nfe 390759, backward nfe 91083, Train: 0.6650, Val: 0.5192, Test: 0.5387\n",
      "Epoch: 087, Runtime 3.080074, Loss 1.020780, forward nfe 395275, backward nfe 92110, Train: 0.6900, Val: 0.4592, Test: 0.4845\n",
      "Epoch: 088, Runtime 3.070499, Loss 1.025668, forward nfe 399752, backward nfe 93141, Train: 0.7000, Val: 0.4254, Test: 0.4529\n",
      "Epoch: 089, Runtime 3.062375, Loss 0.999705, forward nfe 404222, backward nfe 94169, Train: 0.7000, Val: 0.4700, Test: 0.4811\n",
      "Epoch: 090, Runtime 3.064706, Loss 0.971046, forward nfe 408705, backward nfe 95193, Train: 0.7100, Val: 0.5208, Test: 0.5447\n",
      "Epoch: 091, Runtime 3.060204, Loss 0.973159, forward nfe 413196, backward nfe 96221, Train: 0.7100, Val: 0.5338, Test: 0.5517\n",
      "Epoch: 092, Runtime 3.058670, Loss 0.956480, forward nfe 417668, backward nfe 97245, Train: 0.7150, Val: 0.5362, Test: 0.5537\n",
      "Epoch: 093, Runtime 3.065802, Loss 0.945919, forward nfe 422154, backward nfe 98274, Train: 0.7250, Val: 0.4946, Test: 0.5211\n",
      "Epoch: 094, Runtime 3.073791, Loss 0.927695, forward nfe 426670, backward nfe 99298, Train: 0.7250, Val: 0.4800, Test: 0.4968\n",
      "Epoch: 095, Runtime 3.057194, Loss 0.917934, forward nfe 431151, backward nfe 100321, Train: 0.7300, Val: 0.5046, Test: 0.5153\n",
      "Epoch: 096, Runtime 3.060752, Loss 0.894617, forward nfe 435627, backward nfe 101347, Train: 0.7450, Val: 0.5623, Test: 0.5657\n",
      "Epoch: 097, Runtime 3.064149, Loss 0.892431, forward nfe 440101, backward nfe 102379, Train: 0.7250, Val: 0.5646, Test: 0.5769\n",
      "Epoch: 098, Runtime 3.055576, Loss 0.892763, forward nfe 444571, backward nfe 103404, Train: 0.7300, Val: 0.5700, Test: 0.5779\n",
      "Epoch: 099, Runtime 3.054595, Loss 0.878083, forward nfe 449046, backward nfe 104428, Train: 0.7500, Val: 0.5531, Test: 0.5609\n",
      "best val accuracy 0.570000 with test accuracy 0.577946 at epoch 98\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.233162, Loss 2.320840, forward nfe 165, backward nfe 293, Train: 0.1000, Val: 0.0162, Test: 0.0219\n",
      "Epoch: 002, Runtime 2.439799, Loss 2.370907, forward nfe 2903, backward nfe 1317, Train: 0.1800, Val: 0.1285, Test: 0.1268\n",
      "Epoch: 003, Runtime 3.056029, Loss 2.336900, forward nfe 7371, backward nfe 2341, Train: 0.1400, Val: 0.0631, Test: 0.0731\n",
      "Epoch: 004, Runtime 3.055935, Loss 2.212757, forward nfe 11839, backward nfe 3366, Train: 0.0950, Val: 0.0708, Test: 0.0790\n",
      "Epoch: 005, Runtime 3.053741, Loss 2.221449, forward nfe 16308, backward nfe 4390, Train: 0.1650, Val: 0.0569, Test: 0.0733\n",
      "Epoch: 006, Runtime 3.055659, Loss 2.190540, forward nfe 20777, backward nfe 5414, Train: 0.3300, Val: 0.1477, Test: 0.1673\n",
      "Epoch: 007, Runtime 3.058525, Loss 2.138075, forward nfe 25256, backward nfe 6438, Train: 0.3800, Val: 0.1592, Test: 0.1877\n",
      "Epoch: 008, Runtime 3.059315, Loss 2.100240, forward nfe 29726, backward nfe 7465, Train: 0.3350, Val: 0.1546, Test: 0.1818\n",
      "Epoch: 009, Runtime 3.059788, Loss 2.075666, forward nfe 34207, backward nfe 8493, Train: 0.3050, Val: 0.1623, Test: 0.1858\n",
      "Epoch: 010, Runtime 3.058004, Loss 2.049022, forward nfe 38678, backward nfe 9524, Train: 0.2700, Val: 0.1877, Test: 0.2095\n",
      "Epoch: 011, Runtime 3.054381, Loss 2.005925, forward nfe 43146, backward nfe 10551, Train: 0.3550, Val: 0.2077, Test: 0.2301\n",
      "Epoch: 012, Runtime 3.054666, Loss 1.965166, forward nfe 47614, backward nfe 11576, Train: 0.4450, Val: 0.1815, Test: 0.2131\n",
      "Epoch: 013, Runtime 3.064518, Loss 1.920073, forward nfe 52093, backward nfe 12606, Train: 0.4250, Val: 0.1723, Test: 0.2049\n",
      "Epoch: 014, Runtime 3.071063, Loss 1.893702, forward nfe 56567, backward nfe 13637, Train: 0.4150, Val: 0.1738, Test: 0.2058\n",
      "Epoch: 015, Runtime 3.078974, Loss 1.855558, forward nfe 61086, backward nfe 14668, Train: 0.4350, Val: 0.1815, Test: 0.2125\n",
      "Epoch: 016, Runtime 3.122571, Loss 1.812247, forward nfe 65604, backward nfe 15693, Train: 0.4500, Val: 0.1900, Test: 0.2235\n",
      "Epoch: 017, Runtime 3.080031, Loss 1.766163, forward nfe 70182, backward nfe 16738, Train: 0.4550, Val: 0.1946, Test: 0.2308\n",
      "Epoch: 018, Runtime 3.066676, Loss 1.725784, forward nfe 74660, backward nfe 17764, Train: 0.4800, Val: 0.1954, Test: 0.2352\n",
      "Epoch: 019, Runtime 3.146260, Loss 1.699001, forward nfe 79231, backward nfe 18792, Train: 0.4700, Val: 0.1969, Test: 0.2319\n",
      "Epoch: 020, Runtime 3.091693, Loss 1.649213, forward nfe 83801, backward nfe 19817, Train: 0.4750, Val: 0.1954, Test: 0.2266\n",
      "Epoch: 021, Runtime 3.128723, Loss 1.606694, forward nfe 88352, backward nfe 20876, Train: 0.4700, Val: 0.1938, Test: 0.2275\n",
      "Epoch: 022, Runtime 3.099426, Loss 1.572461, forward nfe 92925, backward nfe 21915, Train: 0.4750, Val: 0.1992, Test: 0.2322\n",
      "Epoch: 023, Runtime 3.081942, Loss 1.533219, forward nfe 97461, backward nfe 22947, Train: 0.4850, Val: 0.2362, Test: 0.2710\n",
      "Epoch: 024, Runtime 3.093627, Loss 1.500875, forward nfe 101981, backward nfe 23978, Train: 0.5600, Val: 0.3800, Test: 0.4088\n",
      "Epoch: 025, Runtime 3.138211, Loss 1.464024, forward nfe 106594, backward nfe 25022, Train: 0.6300, Val: 0.4538, Test: 0.4796\n",
      "Epoch: 026, Runtime 3.066267, Loss 1.428021, forward nfe 111119, backward nfe 26054, Train: 0.6650, Val: 0.4923, Test: 0.5234\n",
      "Epoch: 027, Runtime 3.120002, Loss 1.398927, forward nfe 115625, backward nfe 27087, Train: 0.7050, Val: 0.5277, Test: 0.5628\n",
      "Epoch: 028, Runtime 3.059331, Loss 1.366597, forward nfe 120191, backward nfe 28113, Train: 0.7000, Val: 0.5531, Test: 0.5819\n",
      "Epoch: 029, Runtime 3.139889, Loss 1.333988, forward nfe 124666, backward nfe 29205, Train: 0.6800, Val: 0.5715, Test: 0.5841\n",
      "Epoch: 030, Runtime 3.096087, Loss 1.306723, forward nfe 129233, backward nfe 30239, Train: 0.6850, Val: 0.5762, Test: 0.5879\n",
      "Epoch: 031, Runtime 3.096693, Loss 1.284075, forward nfe 133710, backward nfe 31303, Train: 0.6800, Val: 0.5738, Test: 0.5890\n",
      "Epoch: 032, Runtime 3.075702, Loss 1.255045, forward nfe 138227, backward nfe 32329, Train: 0.6800, Val: 0.5746, Test: 0.5958\n",
      "Epoch: 033, Runtime 3.064811, Loss 1.228334, forward nfe 142711, backward nfe 33358, Train: 0.7000, Val: 0.5885, Test: 0.6055\n",
      "Epoch: 034, Runtime 3.132480, Loss 1.205210, forward nfe 147264, backward nfe 34411, Train: 0.7200, Val: 0.5969, Test: 0.6145\n",
      "Epoch: 035, Runtime 3.072827, Loss 1.179377, forward nfe 151761, backward nfe 35441, Train: 0.7250, Val: 0.6054, Test: 0.6228\n",
      "Epoch: 036, Runtime 3.065494, Loss 1.159833, forward nfe 156255, backward nfe 36469, Train: 0.7150, Val: 0.6069, Test: 0.6251\n",
      "Epoch: 037, Runtime 3.063567, Loss 1.130047, forward nfe 160749, backward nfe 37496, Train: 0.7150, Val: 0.6092, Test: 0.6274\n",
      "Epoch: 038, Runtime 3.068073, Loss 1.111836, forward nfe 165230, backward nfe 38529, Train: 0.7450, Val: 0.6100, Test: 0.6380\n",
      "Epoch: 039, Runtime 3.056757, Loss 1.094909, forward nfe 169709, backward nfe 39556, Train: 0.7750, Val: 0.6215, Test: 0.6460\n",
      "Epoch: 040, Runtime 3.062865, Loss 1.073527, forward nfe 174184, backward nfe 40581, Train: 0.7700, Val: 0.6108, Test: 0.6459\n",
      "Epoch: 041, Runtime 3.063448, Loss 1.049405, forward nfe 178678, backward nfe 41611, Train: 0.7650, Val: 0.6115, Test: 0.6451\n",
      "Epoch: 042, Runtime 3.060375, Loss 1.030431, forward nfe 183161, backward nfe 42636, Train: 0.7600, Val: 0.6208, Test: 0.6441\n",
      "Epoch: 043, Runtime 3.051451, Loss 1.000642, forward nfe 187640, backward nfe 43660, Train: 0.7450, Val: 0.6238, Test: 0.6513\n",
      "Epoch: 044, Runtime 3.066925, Loss 0.986719, forward nfe 192120, backward nfe 44692, Train: 0.7600, Val: 0.6469, Test: 0.6732\n",
      "Epoch: 045, Runtime 3.082322, Loss 0.973966, forward nfe 196619, backward nfe 45735, Train: 0.7800, Val: 0.6508, Test: 0.6777\n",
      "Epoch: 046, Runtime 3.063003, Loss 0.938298, forward nfe 201088, backward nfe 46769, Train: 0.7900, Val: 0.6369, Test: 0.6694\n",
      "Epoch: 047, Runtime 3.062021, Loss 0.915779, forward nfe 205567, backward nfe 47798, Train: 0.7700, Val: 0.6238, Test: 0.6580\n",
      "Epoch: 048, Runtime 3.064234, Loss 0.904335, forward nfe 210043, backward nfe 48836, Train: 0.7700, Val: 0.6362, Test: 0.6663\n",
      "Epoch: 049, Runtime 3.054389, Loss 0.881733, forward nfe 214513, backward nfe 49861, Train: 0.7900, Val: 0.6585, Test: 0.6858\n",
      "Epoch: 050, Runtime 3.058578, Loss 0.857151, forward nfe 218992, backward nfe 50885, Train: 0.7950, Val: 0.6700, Test: 0.6969\n",
      "Epoch: 051, Runtime 3.066674, Loss 0.837780, forward nfe 223470, backward nfe 51909, Train: 0.8050, Val: 0.6669, Test: 0.6969\n",
      "Epoch: 052, Runtime 3.054193, Loss 0.821013, forward nfe 227940, backward nfe 52934, Train: 0.7950, Val: 0.6569, Test: 0.6907\n",
      "Epoch: 053, Runtime 3.057767, Loss 0.789497, forward nfe 232411, backward nfe 53963, Train: 0.8000, Val: 0.6692, Test: 0.7015\n",
      "Epoch: 054, Runtime 3.060158, Loss 0.784968, forward nfe 236886, backward nfe 54990, Train: 0.8000, Val: 0.6800, Test: 0.7108\n",
      "Epoch: 055, Runtime 3.055620, Loss 0.754321, forward nfe 241362, backward nfe 56015, Train: 0.8150, Val: 0.6831, Test: 0.7128\n",
      "Epoch: 056, Runtime 3.059871, Loss 0.729713, forward nfe 245845, backward nfe 57042, Train: 0.8150, Val: 0.6754, Test: 0.7069\n",
      "Epoch: 057, Runtime 3.057562, Loss 0.710508, forward nfe 250318, backward nfe 58066, Train: 0.8150, Val: 0.6777, Test: 0.7093\n",
      "Epoch: 058, Runtime 3.057363, Loss 0.689832, forward nfe 254802, backward nfe 59093, Train: 0.8200, Val: 0.6977, Test: 0.7209\n",
      "Epoch: 059, Runtime 3.055112, Loss 0.671726, forward nfe 259276, backward nfe 60118, Train: 0.8200, Val: 0.7115, Test: 0.7319\n",
      "Epoch: 060, Runtime 3.051697, Loss 0.664916, forward nfe 263749, backward nfe 61142, Train: 0.8250, Val: 0.7000, Test: 0.7275\n",
      "Epoch: 061, Runtime 3.058771, Loss 0.648804, forward nfe 268231, backward nfe 62166, Train: 0.8300, Val: 0.7000, Test: 0.7267\n",
      "Epoch: 062, Runtime 3.058323, Loss 0.617628, forward nfe 272701, backward nfe 63194, Train: 0.8200, Val: 0.7046, Test: 0.7281\n",
      "Epoch: 063, Runtime 3.064130, Loss 0.597564, forward nfe 277175, backward nfe 64230, Train: 0.8450, Val: 0.7115, Test: 0.7365\n",
      "Epoch: 064, Runtime 3.058166, Loss 0.587763, forward nfe 281646, backward nfe 65257, Train: 0.8500, Val: 0.7346, Test: 0.7491\n",
      "Epoch: 065, Runtime 3.058732, Loss 0.573870, forward nfe 286127, backward nfe 66281, Train: 0.8450, Val: 0.7369, Test: 0.7507\n",
      "Epoch: 066, Runtime 3.056337, Loss 0.551872, forward nfe 290606, backward nfe 67306, Train: 0.8450, Val: 0.7262, Test: 0.7453\n",
      "Epoch: 067, Runtime 3.052501, Loss 0.553505, forward nfe 295074, backward nfe 68330, Train: 0.8550, Val: 0.7254, Test: 0.7447\n",
      "Epoch: 068, Runtime 3.067460, Loss 0.528754, forward nfe 299552, backward nfe 69356, Train: 0.8450, Val: 0.7346, Test: 0.7489\n",
      "Epoch: 069, Runtime 3.051354, Loss 0.520743, forward nfe 304021, backward nfe 70381, Train: 0.8550, Val: 0.7362, Test: 0.7503\n",
      "Epoch: 070, Runtime 3.057587, Loss 0.501417, forward nfe 308493, backward nfe 71409, Train: 0.8500, Val: 0.7238, Test: 0.7495\n",
      "Epoch: 071, Runtime 3.059595, Loss 0.499228, forward nfe 312963, backward nfe 72439, Train: 0.8650, Val: 0.7438, Test: 0.7556\n",
      "Epoch: 072, Runtime 3.063093, Loss 0.494225, forward nfe 317444, backward nfe 73467, Train: 0.8600, Val: 0.7531, Test: 0.7620\n",
      "Epoch: 073, Runtime 3.052331, Loss 0.475824, forward nfe 321918, backward nfe 74491, Train: 0.8600, Val: 0.7500, Test: 0.7631\n",
      "Epoch: 074, Runtime 3.054702, Loss 0.461113, forward nfe 326387, backward nfe 75515, Train: 0.8650, Val: 0.7392, Test: 0.7577\n",
      "Epoch: 075, Runtime 3.050817, Loss 0.459094, forward nfe 330858, backward nfe 76539, Train: 0.8600, Val: 0.7485, Test: 0.7590\n",
      "Epoch: 076, Runtime 3.054783, Loss 0.449198, forward nfe 335329, backward nfe 77564, Train: 0.8650, Val: 0.7508, Test: 0.7631\n",
      "Epoch: 077, Runtime 3.054496, Loss 0.449727, forward nfe 339799, backward nfe 78588, Train: 0.8650, Val: 0.7438, Test: 0.7640\n",
      "Epoch: 078, Runtime 3.054475, Loss 0.430968, forward nfe 344271, backward nfe 79613, Train: 0.8600, Val: 0.7531, Test: 0.7652\n",
      "Epoch: 079, Runtime 3.065204, Loss 0.418205, forward nfe 348753, backward nfe 80642, Train: 0.8750, Val: 0.7546, Test: 0.7680\n",
      "Epoch: 080, Runtime 3.050489, Loss 0.422673, forward nfe 353224, backward nfe 81666, Train: 0.8900, Val: 0.7485, Test: 0.7635\n",
      "Epoch: 081, Runtime 3.056426, Loss 0.407469, forward nfe 357695, backward nfe 82692, Train: 0.8800, Val: 0.7500, Test: 0.7640\n",
      "Epoch: 082, Runtime 3.054665, Loss 0.390878, forward nfe 362164, backward nfe 83717, Train: 0.8850, Val: 0.7462, Test: 0.7645\n",
      "Epoch: 083, Runtime 3.056221, Loss 0.390427, forward nfe 366641, backward nfe 84741, Train: 0.8900, Val: 0.7569, Test: 0.7712\n",
      "Epoch: 084, Runtime 3.062655, Loss 0.398390, forward nfe 371110, backward nfe 85766, Train: 0.8800, Val: 0.7585, Test: 0.7752\n",
      "Epoch: 085, Runtime 3.060538, Loss 0.382386, forward nfe 375581, backward nfe 86799, Train: 0.8900, Val: 0.7508, Test: 0.7742\n",
      "Epoch: 086, Runtime 3.061351, Loss 0.368869, forward nfe 380053, backward nfe 87827, Train: 0.8900, Val: 0.7462, Test: 0.7716\n",
      "Epoch: 087, Runtime 3.054440, Loss 0.374873, forward nfe 384525, backward nfe 88851, Train: 0.8900, Val: 0.7538, Test: 0.7755\n",
      "Epoch: 088, Runtime 3.051118, Loss 0.368433, forward nfe 389001, backward nfe 89874, Train: 0.8900, Val: 0.7554, Test: 0.7773\n",
      "Epoch: 089, Runtime 3.054234, Loss 0.363029, forward nfe 393470, backward nfe 90898, Train: 0.8900, Val: 0.7500, Test: 0.7738\n",
      "Epoch: 090, Runtime 3.050938, Loss 0.342547, forward nfe 397938, backward nfe 91922, Train: 0.8950, Val: 0.7500, Test: 0.7758\n",
      "Epoch: 091, Runtime 3.053462, Loss 0.345707, forward nfe 402407, backward nfe 92946, Train: 0.8850, Val: 0.7562, Test: 0.7800\n",
      "Epoch: 092, Runtime 3.054318, Loss 0.344735, forward nfe 406876, backward nfe 93970, Train: 0.8900, Val: 0.7600, Test: 0.7820\n",
      "Epoch: 093, Runtime 3.055429, Loss 0.330862, forward nfe 411351, backward nfe 94994, Train: 0.8900, Val: 0.7523, Test: 0.7792\n",
      "Epoch: 094, Runtime 3.056278, Loss 0.334272, forward nfe 415823, backward nfe 96018, Train: 0.8950, Val: 0.7485, Test: 0.7773\n",
      "Epoch: 095, Runtime 3.052936, Loss 0.322690, forward nfe 420294, backward nfe 97044, Train: 0.8950, Val: 0.7531, Test: 0.7807\n",
      "Epoch: 096, Runtime 3.052644, Loss 0.312669, forward nfe 424763, backward nfe 98069, Train: 0.8950, Val: 0.7592, Test: 0.7843\n",
      "Epoch: 097, Runtime 3.053697, Loss 0.308173, forward nfe 429232, backward nfe 99092, Train: 0.8950, Val: 0.7662, Test: 0.7872\n",
      "Epoch: 098, Runtime 3.053450, Loss 0.317572, forward nfe 433700, backward nfe 100117, Train: 0.8900, Val: 0.7662, Test: 0.7885\n",
      "Epoch: 099, Runtime 3.050411, Loss 0.306662, forward nfe 438169, backward nfe 101140, Train: 0.8900, Val: 0.7723, Test: 0.7924\n",
      "best val accuracy 0.772308 with test accuracy 0.792442 at epoch 99\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.176018, Loss 2.314054, forward nfe 164, backward nfe 327, Train: 0.1000, Val: 0.0562, Test: 0.0593\n",
      "Epoch: 002, Runtime 3.054495, Loss 2.540217, forward nfe 4039, backward nfe 1351, Train: 0.1000, Val: 0.4023, Test: 0.3767\n",
      "Epoch: 003, Runtime 3.054541, Loss 2.407554, forward nfe 8507, backward nfe 2375, Train: 0.1100, Val: 0.4031, Test: 0.3777\n",
      "Epoch: 004, Runtime 3.056627, Loss 2.348633, forward nfe 12977, backward nfe 3399, Train: 0.1050, Val: 0.1169, Test: 0.1196\n",
      "Epoch: 005, Runtime 3.052318, Loss 2.244112, forward nfe 17448, backward nfe 4423, Train: 0.1100, Val: 0.0638, Test: 0.0637\n",
      "Epoch: 006, Runtime 3.058086, Loss 2.236591, forward nfe 21925, backward nfe 5447, Train: 0.2450, Val: 0.1231, Test: 0.1288\n",
      "Epoch: 007, Runtime 3.057167, Loss 2.194718, forward nfe 26399, backward nfe 6473, Train: 0.2700, Val: 0.2100, Test: 0.2175\n",
      "Epoch: 008, Runtime 3.100951, Loss 2.149092, forward nfe 30869, backward nfe 7499, Train: 0.2750, Val: 0.2231, Test: 0.2321\n",
      "Epoch: 009, Runtime 3.138178, Loss 2.113672, forward nfe 35494, backward nfe 8571, Train: 0.2250, Val: 0.1846, Test: 0.1969\n",
      "Epoch: 010, Runtime 3.165259, Loss 2.106944, forward nfe 39997, backward nfe 9665, Train: 0.2950, Val: 0.2277, Test: 0.2441\n",
      "Epoch: 011, Runtime 3.153395, Loss 2.066878, forward nfe 44659, backward nfe 10698, Train: 0.3800, Val: 0.3585, Test: 0.3776\n",
      "Epoch: 012, Runtime 3.230922, Loss 2.032953, forward nfe 49321, backward nfe 11817, Train: 0.2700, Val: 0.2469, Test: 0.2627\n",
      "Epoch: 013, Runtime 3.195759, Loss 2.024187, forward nfe 53983, backward nfe 12896, Train: 0.2850, Val: 0.2577, Test: 0.2765\n",
      "Epoch: 014, Runtime 3.208776, Loss 1.994481, forward nfe 58645, backward nfe 13993, Train: 0.2900, Val: 0.2385, Test: 0.2585\n",
      "Epoch: 015, Runtime 3.240050, Loss 1.953218, forward nfe 63307, backward nfe 15112, Train: 0.3350, Val: 0.2062, Test: 0.2184\n",
      "Epoch: 016, Runtime 3.171720, Loss 1.935626, forward nfe 67969, backward nfe 16167, Train: 0.3050, Val: 0.2123, Test: 0.2217\n",
      "Epoch: 017, Runtime 3.232579, Loss 1.908365, forward nfe 72631, backward nfe 17286, Train: 0.3350, Val: 0.2392, Test: 0.2481\n",
      "Epoch: 018, Runtime 3.229453, Loss 1.875174, forward nfe 77293, backward nfe 18405, Train: 0.4100, Val: 0.2692, Test: 0.2783\n",
      "Epoch: 019, Runtime 3.186371, Loss 1.843765, forward nfe 81955, backward nfe 19477, Train: 0.4250, Val: 0.2715, Test: 0.2973\n",
      "Epoch: 020, Runtime 3.228668, Loss 1.820174, forward nfe 86617, backward nfe 20596, Train: 0.4550, Val: 0.2600, Test: 0.2826\n",
      "Epoch: 021, Runtime 3.231951, Loss 1.784350, forward nfe 91279, backward nfe 21715, Train: 0.4500, Val: 0.2446, Test: 0.2726\n",
      "Epoch: 022, Runtime 3.232390, Loss 1.762917, forward nfe 95941, backward nfe 22834, Train: 0.4500, Val: 0.2523, Test: 0.2827\n",
      "Epoch: 023, Runtime 3.202191, Loss 1.727205, forward nfe 100603, backward nfe 23953, Train: 0.4500, Val: 0.2838, Test: 0.3191\n",
      "Epoch: 024, Runtime 3.231827, Loss 1.695005, forward nfe 105198, backward nfe 25072, Train: 0.4700, Val: 0.3362, Test: 0.3627\n",
      "Epoch: 025, Runtime 3.234486, Loss 1.666233, forward nfe 109860, backward nfe 26191, Train: 0.5050, Val: 0.4062, Test: 0.4162\n",
      "Epoch: 026, Runtime 3.228218, Loss 1.633024, forward nfe 114522, backward nfe 27310, Train: 0.5650, Val: 0.4138, Test: 0.4363\n",
      "Epoch: 027, Runtime 3.233644, Loss 1.600356, forward nfe 119184, backward nfe 28429, Train: 0.5650, Val: 0.4062, Test: 0.4305\n",
      "Epoch: 028, Runtime 3.206792, Loss 1.570555, forward nfe 123846, backward nfe 29515, Train: 0.5950, Val: 0.4415, Test: 0.4686\n",
      "Epoch: 029, Runtime 3.229890, Loss 1.533603, forward nfe 128508, backward nfe 30634, Train: 0.6100, Val: 0.4992, Test: 0.5238\n",
      "Epoch: 030, Runtime 3.234230, Loss 1.503460, forward nfe 133170, backward nfe 31753, Train: 0.6150, Val: 0.5308, Test: 0.5603\n",
      "Epoch: 031, Runtime 3.243782, Loss 1.473341, forward nfe 137832, backward nfe 32872, Train: 0.6250, Val: 0.5500, Test: 0.5788\n",
      "Epoch: 032, Runtime 3.230911, Loss 1.451208, forward nfe 142494, backward nfe 33991, Train: 0.6350, Val: 0.5569, Test: 0.5838\n",
      "Epoch: 033, Runtime 3.235278, Loss 1.410368, forward nfe 147156, backward nfe 35109, Train: 0.6250, Val: 0.5454, Test: 0.5784\n",
      "Epoch: 034, Runtime 3.231993, Loss 1.382514, forward nfe 151818, backward nfe 36228, Train: 0.6650, Val: 0.5500, Test: 0.5772\n",
      "Epoch: 035, Runtime 3.230355, Loss 1.348848, forward nfe 156480, backward nfe 37347, Train: 0.6800, Val: 0.5662, Test: 0.5899\n",
      "Epoch: 036, Runtime 3.187961, Loss 1.329693, forward nfe 161142, backward nfe 38422, Train: 0.6900, Val: 0.5915, Test: 0.6079\n",
      "Epoch: 037, Runtime 3.200748, Loss 1.295371, forward nfe 165804, backward nfe 39505, Train: 0.6800, Val: 0.6000, Test: 0.6220\n",
      "Epoch: 038, Runtime 3.209337, Loss 1.276006, forward nfe 170466, backward nfe 40601, Train: 0.7000, Val: 0.6123, Test: 0.6269\n",
      "Epoch: 039, Runtime 3.232202, Loss 1.241190, forward nfe 175128, backward nfe 41720, Train: 0.7000, Val: 0.6092, Test: 0.6267\n",
      "Epoch: 040, Runtime 3.230270, Loss 1.221509, forward nfe 179790, backward nfe 42839, Train: 0.6900, Val: 0.6085, Test: 0.6256\n",
      "Epoch: 041, Runtime 3.229188, Loss 1.196309, forward nfe 184452, backward nfe 43958, Train: 0.7000, Val: 0.6015, Test: 0.6247\n",
      "Epoch: 042, Runtime 3.232280, Loss 1.165972, forward nfe 189114, backward nfe 45077, Train: 0.6850, Val: 0.6062, Test: 0.6284\n",
      "Epoch: 043, Runtime 3.190102, Loss 1.145236, forward nfe 193776, backward nfe 46151, Train: 0.6850, Val: 0.6208, Test: 0.6332\n",
      "Epoch: 044, Runtime 3.142544, Loss 1.129150, forward nfe 198438, backward nfe 47175, Train: 0.6750, Val: 0.6292, Test: 0.6370\n",
      "Epoch: 045, Runtime 3.162547, Loss 1.108355, forward nfe 203100, backward nfe 48219, Train: 0.6800, Val: 0.6285, Test: 0.6390\n",
      "Epoch: 046, Runtime 3.195380, Loss 1.082647, forward nfe 207762, backward nfe 49290, Train: 0.7000, Val: 0.6231, Test: 0.6385\n",
      "Epoch: 047, Runtime 3.195869, Loss 1.056259, forward nfe 212424, backward nfe 50372, Train: 0.7150, Val: 0.6208, Test: 0.6395\n",
      "Epoch: 048, Runtime 3.124514, Loss 1.050360, forward nfe 217086, backward nfe 51405, Train: 0.7200, Val: 0.6377, Test: 0.6471\n",
      "Epoch: 049, Runtime 3.158080, Loss 1.019158, forward nfe 221692, backward nfe 52445, Train: 0.7150, Val: 0.6362, Test: 0.6540\n",
      "Epoch: 050, Runtime 3.127609, Loss 1.007061, forward nfe 226354, backward nfe 53479, Train: 0.7200, Val: 0.6400, Test: 0.6547\n",
      "Epoch: 051, Runtime 3.148866, Loss 0.992433, forward nfe 230943, backward nfe 54530, Train: 0.7400, Val: 0.6377, Test: 0.6502\n",
      "Epoch: 052, Runtime 3.133260, Loss 0.971319, forward nfe 235543, backward nfe 55567, Train: 0.7550, Val: 0.6362, Test: 0.6468\n",
      "Epoch: 053, Runtime 3.127622, Loss 0.957288, forward nfe 240205, backward nfe 56593, Train: 0.7550, Val: 0.6462, Test: 0.6512\n",
      "Epoch: 054, Runtime 3.123011, Loss 0.934542, forward nfe 244832, backward nfe 57629, Train: 0.7450, Val: 0.6500, Test: 0.6582\n",
      "Epoch: 055, Runtime 3.120743, Loss 0.920556, forward nfe 249426, backward nfe 58671, Train: 0.7400, Val: 0.6546, Test: 0.6659\n",
      "Epoch: 056, Runtime 3.150464, Loss 0.903225, forward nfe 254008, backward nfe 59708, Train: 0.7450, Val: 0.6623, Test: 0.6685\n",
      "Epoch: 057, Runtime 3.115488, Loss 0.883741, forward nfe 258579, backward nfe 60740, Train: 0.7600, Val: 0.6654, Test: 0.6670\n",
      "Epoch: 058, Runtime 3.136372, Loss 0.870675, forward nfe 263241, backward nfe 61768, Train: 0.7650, Val: 0.6662, Test: 0.6678\n",
      "Epoch: 059, Runtime 3.121593, Loss 0.859593, forward nfe 267826, backward nfe 62801, Train: 0.7650, Val: 0.6662, Test: 0.6701\n",
      "Epoch: 060, Runtime 3.119763, Loss 0.841270, forward nfe 272411, backward nfe 63830, Train: 0.7650, Val: 0.6669, Test: 0.6741\n",
      "Epoch: 061, Runtime 3.115788, Loss 0.829421, forward nfe 277005, backward nfe 64869, Train: 0.7750, Val: 0.6746, Test: 0.6815\n",
      "Epoch: 062, Runtime 3.095896, Loss 0.817758, forward nfe 281609, backward nfe 65899, Train: 0.7750, Val: 0.6785, Test: 0.6817\n",
      "Epoch: 063, Runtime 3.085468, Loss 0.811999, forward nfe 286079, backward nfe 66943, Train: 0.7850, Val: 0.6823, Test: 0.6832\n",
      "Epoch: 064, Runtime 3.076977, Loss 0.793039, forward nfe 290581, backward nfe 67970, Train: 0.8000, Val: 0.6831, Test: 0.6835\n",
      "Epoch: 065, Runtime 3.073316, Loss 0.765624, forward nfe 295113, backward nfe 68995, Train: 0.7950, Val: 0.6846, Test: 0.6881\n",
      "Epoch: 066, Runtime 3.061533, Loss 0.755860, forward nfe 299610, backward nfe 70019, Train: 0.8050, Val: 0.6923, Test: 0.6937\n",
      "Epoch: 067, Runtime 3.070734, Loss 0.754634, forward nfe 304116, backward nfe 71043, Train: 0.8150, Val: 0.6992, Test: 0.6962\n",
      "Epoch: 068, Runtime 3.087189, Loss 0.740869, forward nfe 308620, backward nfe 72081, Train: 0.8250, Val: 0.7015, Test: 0.6978\n",
      "Epoch: 069, Runtime 3.056873, Loss 0.728864, forward nfe 313118, backward nfe 73105, Train: 0.8250, Val: 0.7008, Test: 0.6992\n",
      "Epoch: 070, Runtime 3.055515, Loss 0.710164, forward nfe 317592, backward nfe 74132, Train: 0.8200, Val: 0.7031, Test: 0.7028\n",
      "Epoch: 071, Runtime 3.065677, Loss 0.712532, forward nfe 322076, backward nfe 75157, Train: 0.8300, Val: 0.7038, Test: 0.7014\n",
      "Epoch: 072, Runtime 3.076543, Loss 0.684921, forward nfe 326595, backward nfe 76182, Train: 0.8350, Val: 0.6992, Test: 0.7034\n",
      "Epoch: 073, Runtime 3.071087, Loss 0.676968, forward nfe 331081, backward nfe 77212, Train: 0.8350, Val: 0.7054, Test: 0.7120\n",
      "Epoch: 074, Runtime 3.061257, Loss 0.672721, forward nfe 335567, backward nfe 78241, Train: 0.8350, Val: 0.7092, Test: 0.7114\n",
      "Epoch: 075, Runtime 3.059046, Loss 0.652916, forward nfe 340042, backward nfe 79268, Train: 0.8400, Val: 0.7046, Test: 0.7081\n",
      "Epoch: 076, Runtime 3.074289, Loss 0.647151, forward nfe 344531, backward nfe 80310, Train: 0.8450, Val: 0.7108, Test: 0.7144\n",
      "Epoch: 077, Runtime 3.062956, Loss 0.632969, forward nfe 349014, backward nfe 81338, Train: 0.8600, Val: 0.7192, Test: 0.7210\n",
      "Epoch: 078, Runtime 3.060774, Loss 0.629626, forward nfe 353483, backward nfe 82362, Train: 0.8500, Val: 0.7123, Test: 0.7148\n",
      "Epoch: 079, Runtime 3.065753, Loss 0.612678, forward nfe 357955, backward nfe 83396, Train: 0.8600, Val: 0.6985, Test: 0.7099\n",
      "Epoch: 080, Runtime 3.053865, Loss 0.602131, forward nfe 362433, backward nfe 84420, Train: 0.8600, Val: 0.7115, Test: 0.7180\n",
      "Epoch: 081, Runtime 3.065017, Loss 0.587941, forward nfe 366912, backward nfe 85447, Train: 0.8650, Val: 0.7200, Test: 0.7271\n",
      "Epoch: 082, Runtime 3.057837, Loss 0.579032, forward nfe 371395, backward nfe 86472, Train: 0.8650, Val: 0.7185, Test: 0.7238\n",
      "Epoch: 083, Runtime 3.057726, Loss 0.569295, forward nfe 375870, backward nfe 87496, Train: 0.8650, Val: 0.7062, Test: 0.7119\n",
      "Epoch: 084, Runtime 3.059779, Loss 0.579708, forward nfe 380350, backward nfe 88524, Train: 0.8600, Val: 0.7177, Test: 0.7270\n",
      "Epoch: 085, Runtime 3.054263, Loss 0.548977, forward nfe 384821, backward nfe 89548, Train: 0.8750, Val: 0.7338, Test: 0.7399\n",
      "Epoch: 086, Runtime 3.055521, Loss 0.541283, forward nfe 389290, backward nfe 90574, Train: 0.8700, Val: 0.7300, Test: 0.7342\n",
      "Epoch: 087, Runtime 3.054675, Loss 0.547743, forward nfe 393760, backward nfe 91598, Train: 0.8650, Val: 0.7146, Test: 0.7195\n",
      "Epoch: 088, Runtime 3.056546, Loss 0.529257, forward nfe 398233, backward nfe 92624, Train: 0.8700, Val: 0.7223, Test: 0.7286\n",
      "Epoch: 089, Runtime 3.053689, Loss 0.528488, forward nfe 402704, backward nfe 93648, Train: 0.8750, Val: 0.7292, Test: 0.7365\n",
      "Epoch: 090, Runtime 3.055997, Loss 0.512036, forward nfe 407176, backward nfe 94673, Train: 0.8750, Val: 0.7315, Test: 0.7389\n",
      "Epoch: 091, Runtime 3.055460, Loss 0.493790, forward nfe 411644, backward nfe 95699, Train: 0.8850, Val: 0.7231, Test: 0.7324\n",
      "Epoch: 092, Runtime 3.056196, Loss 0.502549, forward nfe 416119, backward nfe 96724, Train: 0.8850, Val: 0.7315, Test: 0.7397\n",
      "Epoch: 093, Runtime 3.057624, Loss 0.477234, forward nfe 420591, backward nfe 97751, Train: 0.8750, Val: 0.7369, Test: 0.7475\n",
      "Epoch: 094, Runtime 3.063458, Loss 0.492642, forward nfe 425060, backward nfe 98777, Train: 0.8700, Val: 0.7323, Test: 0.7401\n",
      "Epoch: 095, Runtime 3.054450, Loss 0.487461, forward nfe 429530, backward nfe 99802, Train: 0.8800, Val: 0.7223, Test: 0.7302\n",
      "Epoch: 096, Runtime 3.052230, Loss 0.462738, forward nfe 433998, backward nfe 100826, Train: 0.8850, Val: 0.7331, Test: 0.7396\n",
      "Epoch: 097, Runtime 3.053257, Loss 0.450728, forward nfe 438468, backward nfe 101850, Train: 0.8850, Val: 0.7485, Test: 0.7516\n",
      "Epoch: 098, Runtime 3.055470, Loss 0.449693, forward nfe 442938, backward nfe 102874, Train: 0.8850, Val: 0.7538, Test: 0.7533\n",
      "Epoch: 099, Runtime 3.055009, Loss 0.449334, forward nfe 447411, backward nfe 103898, Train: 0.8900, Val: 0.7400, Test: 0.7448\n",
      "best val accuracy 0.753846 with test accuracy 0.753265 at epoch 98\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.397440, Loss 2.312509, forward nfe 161, backward nfe 292, Train: 0.1000, Val: 0.0292, Test: 0.0395\n",
      "Epoch: 002, Runtime 3.051778, Loss 2.490463, forward nfe 4593, backward nfe 1316, Train: 0.1750, Val: 0.1377, Test: 0.1308\n",
      "Epoch: 003, Runtime 3.051079, Loss 2.379136, forward nfe 9061, backward nfe 2340, Train: 0.1200, Val: 0.1131, Test: 0.1146\n",
      "Epoch: 004, Runtime 3.052378, Loss 2.272323, forward nfe 13529, backward nfe 3364, Train: 0.2000, Val: 0.2869, Test: 0.2853\n",
      "Epoch: 005, Runtime 3.052058, Loss 2.249440, forward nfe 18004, backward nfe 4389, Train: 0.2100, Val: 0.0654, Test: 0.0748\n",
      "Epoch: 006, Runtime 3.052375, Loss 2.202671, forward nfe 22475, backward nfe 5413, Train: 0.2500, Val: 0.0762, Test: 0.0985\n",
      "Epoch: 007, Runtime 3.066368, Loss 2.136486, forward nfe 26951, backward nfe 6440, Train: 0.3100, Val: 0.1654, Test: 0.1839\n",
      "Epoch: 008, Runtime 3.066087, Loss 2.124120, forward nfe 31462, backward nfe 7464, Train: 0.3050, Val: 0.1900, Test: 0.1916\n",
      "Epoch: 009, Runtime 3.085685, Loss 2.067596, forward nfe 35977, backward nfe 8490, Train: 0.2950, Val: 0.1792, Test: 0.1996\n",
      "Epoch: 010, Runtime 3.066821, Loss 2.031049, forward nfe 40488, backward nfe 9515, Train: 0.3150, Val: 0.2831, Test: 0.2903\n",
      "Epoch: 011, Runtime 3.102602, Loss 2.025240, forward nfe 45035, backward nfe 10557, Train: 0.3200, Val: 0.2931, Test: 0.3056\n",
      "Epoch: 012, Runtime 3.105532, Loss 2.002938, forward nfe 49538, backward nfe 11586, Train: 0.3500, Val: 0.2223, Test: 0.2472\n",
      "Epoch: 013, Runtime 3.142401, Loss 1.959681, forward nfe 54200, backward nfe 12614, Train: 0.4450, Val: 0.3538, Test: 0.3688\n",
      "Epoch: 014, Runtime 3.160959, Loss 1.910165, forward nfe 58862, backward nfe 13660, Train: 0.4750, Val: 0.3815, Test: 0.4013\n",
      "Epoch: 015, Runtime 3.130594, Loss 1.882344, forward nfe 63524, backward nfe 14693, Train: 0.4700, Val: 0.2177, Test: 0.2749\n",
      "Epoch: 016, Runtime 3.124189, Loss 1.863394, forward nfe 68128, backward nfe 15724, Train: 0.5000, Val: 0.2462, Test: 0.2804\n",
      "Epoch: 017, Runtime 3.139393, Loss 1.839699, forward nfe 72657, backward nfe 16787, Train: 0.5100, Val: 0.2400, Test: 0.2760\n",
      "Epoch: 018, Runtime 3.107958, Loss 1.808594, forward nfe 77237, backward nfe 17822, Train: 0.5600, Val: 0.3392, Test: 0.3855\n",
      "Epoch: 019, Runtime 3.154720, Loss 1.765071, forward nfe 81893, backward nfe 18862, Train: 0.5100, Val: 0.3946, Test: 0.4322\n",
      "Epoch: 020, Runtime 3.225551, Loss 1.731264, forward nfe 86555, backward nfe 19975, Train: 0.5200, Val: 0.4038, Test: 0.4348\n",
      "Epoch: 021, Runtime 3.177140, Loss 1.693102, forward nfe 91217, backward nfe 21034, Train: 0.5600, Val: 0.4085, Test: 0.4385\n",
      "Epoch: 022, Runtime 3.146633, Loss 1.655474, forward nfe 95843, backward nfe 22082, Train: 0.5400, Val: 0.3615, Test: 0.4036\n",
      "Epoch: 023, Runtime 3.138474, Loss 1.608541, forward nfe 100505, backward nfe 23114, Train: 0.5500, Val: 0.3123, Test: 0.3656\n",
      "Epoch: 024, Runtime 3.167975, Loss 1.570306, forward nfe 105142, backward nfe 24168, Train: 0.6250, Val: 0.4562, Test: 0.4925\n",
      "Epoch: 025, Runtime 3.227222, Loss 1.533505, forward nfe 109804, backward nfe 25287, Train: 0.6450, Val: 0.5115, Test: 0.5400\n",
      "Epoch: 026, Runtime 3.137683, Loss 1.490536, forward nfe 114466, backward nfe 26345, Train: 0.6500, Val: 0.5285, Test: 0.5504\n",
      "Epoch: 027, Runtime 3.142841, Loss 1.451344, forward nfe 119053, backward nfe 27370, Train: 0.6500, Val: 0.5185, Test: 0.5433\n",
      "Epoch: 028, Runtime 3.184722, Loss 1.417310, forward nfe 123700, backward nfe 28449, Train: 0.6600, Val: 0.5431, Test: 0.5558\n",
      "Epoch: 029, Runtime 3.160680, Loss 1.380864, forward nfe 128362, backward nfe 29495, Train: 0.6500, Val: 0.5654, Test: 0.5824\n",
      "Epoch: 030, Runtime 3.168483, Loss 1.347066, forward nfe 133024, backward nfe 30549, Train: 0.6400, Val: 0.5962, Test: 0.6077\n",
      "Epoch: 031, Runtime 3.151963, Loss 1.323539, forward nfe 137686, backward nfe 31600, Train: 0.6500, Val: 0.5877, Test: 0.5938\n",
      "Epoch: 032, Runtime 3.154329, Loss 1.284508, forward nfe 142322, backward nfe 32665, Train: 0.6750, Val: 0.5831, Test: 0.5908\n",
      "Epoch: 033, Runtime 3.158252, Loss 1.255416, forward nfe 146894, backward nfe 33713, Train: 0.6900, Val: 0.5992, Test: 0.6215\n",
      "Epoch: 034, Runtime 3.109172, Loss 1.224741, forward nfe 151556, backward nfe 34748, Train: 0.7350, Val: 0.6246, Test: 0.6445\n",
      "Epoch: 035, Runtime 3.113810, Loss 1.193795, forward nfe 156034, backward nfe 35787, Train: 0.7800, Val: 0.6423, Test: 0.6650\n",
      "Epoch: 036, Runtime 3.117619, Loss 1.159127, forward nfe 160614, backward nfe 36827, Train: 0.7850, Val: 0.6500, Test: 0.6711\n",
      "Epoch: 037, Runtime 3.109075, Loss 1.134484, forward nfe 165188, backward nfe 37861, Train: 0.7900, Val: 0.6446, Test: 0.6715\n",
      "Epoch: 038, Runtime 3.114606, Loss 1.112390, forward nfe 169778, backward nfe 38892, Train: 0.8000, Val: 0.6323, Test: 0.6675\n",
      "Epoch: 039, Runtime 3.126855, Loss 1.075923, forward nfe 174375, backward nfe 39941, Train: 0.8050, Val: 0.6315, Test: 0.6639\n",
      "Epoch: 040, Runtime 3.103397, Loss 1.053804, forward nfe 179016, backward nfe 40976, Train: 0.7900, Val: 0.6500, Test: 0.6779\n",
      "Epoch: 041, Runtime 3.135960, Loss 1.020209, forward nfe 183585, backward nfe 42015, Train: 0.7950, Val: 0.6554, Test: 0.6820\n",
      "Epoch: 042, Runtime 3.134210, Loss 0.996378, forward nfe 188157, backward nfe 43052, Train: 0.8050, Val: 0.6608, Test: 0.6881\n",
      "Epoch: 043, Runtime 3.094943, Loss 0.968289, forward nfe 192804, backward nfe 44081, Train: 0.8150, Val: 0.6554, Test: 0.6823\n",
      "Epoch: 044, Runtime 3.111910, Loss 0.947168, forward nfe 197373, backward nfe 45113, Train: 0.8050, Val: 0.6654, Test: 0.6897\n",
      "Epoch: 045, Runtime 3.127239, Loss 0.916075, forward nfe 201919, backward nfe 46161, Train: 0.8050, Val: 0.6738, Test: 0.6994\n",
      "Epoch: 046, Runtime 3.088448, Loss 0.902991, forward nfe 206474, backward nfe 47189, Train: 0.8000, Val: 0.6800, Test: 0.7047\n",
      "Epoch: 047, Runtime 3.064139, Loss 0.888504, forward nfe 211005, backward nfe 48213, Train: 0.8050, Val: 0.6831, Test: 0.7066\n",
      "Epoch: 048, Runtime 3.089089, Loss 0.863406, forward nfe 215516, backward nfe 49240, Train: 0.8150, Val: 0.6800, Test: 0.7053\n",
      "Epoch: 049, Runtime 3.106282, Loss 0.838208, forward nfe 220059, backward nfe 50275, Train: 0.8250, Val: 0.6754, Test: 0.7076\n",
      "Epoch: 050, Runtime 3.110096, Loss 0.821990, forward nfe 224652, backward nfe 51324, Train: 0.8200, Val: 0.6762, Test: 0.7083\n",
      "Epoch: 051, Runtime 3.094854, Loss 0.807863, forward nfe 229194, backward nfe 52355, Train: 0.8150, Val: 0.6846, Test: 0.7057\n",
      "Epoch: 052, Runtime 3.075594, Loss 0.782018, forward nfe 233717, backward nfe 53379, Train: 0.8250, Val: 0.6915, Test: 0.7120\n",
      "Epoch: 053, Runtime 3.069667, Loss 0.772524, forward nfe 238217, backward nfe 54405, Train: 0.8250, Val: 0.7115, Test: 0.7331\n",
      "Epoch: 054, Runtime 3.102959, Loss 0.753071, forward nfe 242797, backward nfe 55433, Train: 0.8150, Val: 0.7077, Test: 0.7328\n",
      "Epoch: 055, Runtime 3.070149, Loss 0.729498, forward nfe 247315, backward nfe 56463, Train: 0.8400, Val: 0.7008, Test: 0.7225\n",
      "Epoch: 056, Runtime 3.065668, Loss 0.714309, forward nfe 251800, backward nfe 57498, Train: 0.8400, Val: 0.7062, Test: 0.7271\n",
      "Epoch: 057, Runtime 3.081643, Loss 0.697443, forward nfe 256317, backward nfe 58527, Train: 0.8150, Val: 0.7115, Test: 0.7336\n",
      "Epoch: 058, Runtime 3.074090, Loss 0.682229, forward nfe 260798, backward nfe 59553, Train: 0.8350, Val: 0.7062, Test: 0.7249\n",
      "Epoch: 059, Runtime 3.075586, Loss 0.660456, forward nfe 265326, backward nfe 60577, Train: 0.8250, Val: 0.7177, Test: 0.7387\n",
      "Epoch: 060, Runtime 3.106865, Loss 0.646763, forward nfe 269923, backward nfe 61605, Train: 0.8300, Val: 0.7223, Test: 0.7397\n",
      "Epoch: 061, Runtime 3.091437, Loss 0.632365, forward nfe 274483, backward nfe 62633, Train: 0.8400, Val: 0.7192, Test: 0.7400\n",
      "Epoch: 062, Runtime 3.092171, Loss 0.623480, forward nfe 279002, backward nfe 63664, Train: 0.8350, Val: 0.7146, Test: 0.7411\n",
      "Epoch: 063, Runtime 3.065648, Loss 0.602315, forward nfe 283520, backward nfe 64689, Train: 0.8300, Val: 0.7185, Test: 0.7429\n",
      "Epoch: 064, Runtime 3.049444, Loss 0.596356, forward nfe 287999, backward nfe 65713, Train: 0.8300, Val: 0.7238, Test: 0.7477\n",
      "Epoch: 065, Runtime 3.058662, Loss 0.582922, forward nfe 292469, backward nfe 66743, Train: 0.8400, Val: 0.7200, Test: 0.7469\n",
      "Epoch: 066, Runtime 3.065396, Loss 0.567166, forward nfe 296957, backward nfe 67769, Train: 0.8400, Val: 0.7215, Test: 0.7461\n",
      "Epoch: 067, Runtime 3.059711, Loss 0.561608, forward nfe 301432, backward nfe 68794, Train: 0.8350, Val: 0.7269, Test: 0.7529\n",
      "Epoch: 068, Runtime 3.059194, Loss 0.545670, forward nfe 305919, backward nfe 69822, Train: 0.8350, Val: 0.7262, Test: 0.7533\n",
      "Epoch: 069, Runtime 3.060584, Loss 0.535920, forward nfe 310396, backward nfe 70848, Train: 0.8550, Val: 0.7338, Test: 0.7572\n",
      "Epoch: 070, Runtime 3.060212, Loss 0.532701, forward nfe 314883, backward nfe 71872, Train: 0.8400, Val: 0.7285, Test: 0.7542\n",
      "Epoch: 071, Runtime 3.052610, Loss 0.515280, forward nfe 319354, backward nfe 72896, Train: 0.8400, Val: 0.7315, Test: 0.7566\n",
      "Epoch: 072, Runtime 3.063029, Loss 0.514057, forward nfe 323829, backward nfe 73920, Train: 0.8600, Val: 0.7385, Test: 0.7603\n",
      "Epoch: 073, Runtime 3.061225, Loss 0.497434, forward nfe 328320, backward nfe 74954, Train: 0.8600, Val: 0.7354, Test: 0.7564\n",
      "Epoch: 074, Runtime 3.054200, Loss 0.482753, forward nfe 332793, backward nfe 75979, Train: 0.8650, Val: 0.7354, Test: 0.7568\n",
      "Epoch: 075, Runtime 3.067888, Loss 0.469886, forward nfe 337291, backward nfe 77005, Train: 0.8700, Val: 0.7446, Test: 0.7654\n",
      "Epoch: 076, Runtime 3.053451, Loss 0.460272, forward nfe 341766, backward nfe 78029, Train: 0.8700, Val: 0.7438, Test: 0.7675\n",
      "Epoch: 077, Runtime 3.055143, Loss 0.450293, forward nfe 346238, backward nfe 79054, Train: 0.8800, Val: 0.7438, Test: 0.7641\n",
      "Epoch: 078, Runtime 3.053220, Loss 0.456215, forward nfe 350716, backward nfe 80078, Train: 0.8700, Val: 0.7523, Test: 0.7735\n",
      "Epoch: 079, Runtime 3.050932, Loss 0.452235, forward nfe 355187, backward nfe 81102, Train: 0.8700, Val: 0.7554, Test: 0.7711\n",
      "Epoch: 080, Runtime 3.057545, Loss 0.432337, forward nfe 359657, backward nfe 82132, Train: 0.8750, Val: 0.7531, Test: 0.7706\n",
      "Epoch: 081, Runtime 3.055259, Loss 0.417562, forward nfe 364125, backward nfe 83160, Train: 0.8800, Val: 0.7554, Test: 0.7720\n",
      "Epoch: 082, Runtime 3.052167, Loss 0.427891, forward nfe 368600, backward nfe 84185, Train: 0.8850, Val: 0.7562, Test: 0.7740\n",
      "Epoch: 083, Runtime 3.052810, Loss 0.409582, forward nfe 373074, backward nfe 85210, Train: 0.8850, Val: 0.7554, Test: 0.7724\n",
      "Epoch: 084, Runtime 3.052377, Loss 0.403891, forward nfe 377545, backward nfe 86235, Train: 0.8800, Val: 0.7608, Test: 0.7782\n",
      "Epoch: 085, Runtime 3.051608, Loss 0.399816, forward nfe 382013, backward nfe 87259, Train: 0.8900, Val: 0.7562, Test: 0.7786\n",
      "Epoch: 086, Runtime 3.051952, Loss 0.384534, forward nfe 386486, backward nfe 88283, Train: 0.8850, Val: 0.7692, Test: 0.7831\n",
      "Epoch: 087, Runtime 3.058223, Loss 0.384449, forward nfe 390954, backward nfe 89313, Train: 0.8900, Val: 0.7723, Test: 0.7859\n",
      "Epoch: 088, Runtime 3.059303, Loss 0.391848, forward nfe 395426, backward nfe 90337, Train: 0.8950, Val: 0.7646, Test: 0.7800\n",
      "Epoch: 089, Runtime 3.053320, Loss 0.362017, forward nfe 399897, backward nfe 91364, Train: 0.8950, Val: 0.7708, Test: 0.7887\n",
      "Epoch: 090, Runtime 3.052794, Loss 0.376644, forward nfe 404365, backward nfe 92389, Train: 0.8900, Val: 0.7723, Test: 0.7886\n",
      "Epoch: 091, Runtime 3.054925, Loss 0.365807, forward nfe 408835, backward nfe 93416, Train: 0.8950, Val: 0.7746, Test: 0.7902\n",
      "Epoch: 092, Runtime 3.064791, Loss 0.357846, forward nfe 413304, backward nfe 94452, Train: 0.9000, Val: 0.7715, Test: 0.7890\n",
      "Epoch: 093, Runtime 3.054101, Loss 0.338934, forward nfe 417776, backward nfe 95478, Train: 0.9000, Val: 0.7762, Test: 0.7927\n",
      "Epoch: 094, Runtime 3.053289, Loss 0.346435, forward nfe 422244, backward nfe 96505, Train: 0.8950, Val: 0.7785, Test: 0.7977\n",
      "Epoch: 095, Runtime 3.052512, Loss 0.348675, forward nfe 426714, backward nfe 97530, Train: 0.9000, Val: 0.7546, Test: 0.7718\n",
      "Epoch: 096, Runtime 3.053753, Loss 0.357747, forward nfe 431185, backward nfe 98555, Train: 0.9050, Val: 0.7769, Test: 0.8005\n",
      "Epoch: 097, Runtime 3.050773, Loss 0.320897, forward nfe 435653, backward nfe 99578, Train: 0.8900, Val: 0.7931, Test: 0.8030\n",
      "Epoch: 098, Runtime 3.054084, Loss 0.356304, forward nfe 440124, backward nfe 100603, Train: 0.9150, Val: 0.7600, Test: 0.7760\n",
      "Epoch: 099, Runtime 3.052059, Loss 0.343746, forward nfe 444594, backward nfe 101627, Train: 0.9200, Val: 0.7762, Test: 0.7928\n",
      "best val accuracy 0.793077 with test accuracy 0.802971 at epoch 97\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.5, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.5, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 1.386414, Loss 2.329310, forward nfe 167, backward nfe 300, Train: 0.1050, Val: 0.1338, Test: 0.1210\n",
      "Epoch: 002, Runtime 3.054211, Loss 2.452289, forward nfe 4551, backward nfe 1324, Train: 0.1450, Val: 0.0385, Test: 0.0530\n",
      "Epoch: 003, Runtime 3.051403, Loss 2.471391, forward nfe 9019, backward nfe 2348, Train: 0.1100, Val: 0.0815, Test: 0.0833\n",
      "Epoch: 004, Runtime 3.049762, Loss 2.568023, forward nfe 13487, backward nfe 3373, Train: 0.0700, Val: 0.0138, Test: 0.0297\n",
      "Epoch: 005, Runtime 3.056440, Loss 2.343092, forward nfe 17957, backward nfe 4400, Train: 0.1100, Val: 0.0215, Test: 0.0406\n",
      "Epoch: 006, Runtime 3.055775, Loss 2.297146, forward nfe 22425, backward nfe 5426, Train: 0.1700, Val: 0.0446, Test: 0.0586\n",
      "Epoch: 007, Runtime 3.054770, Loss 2.276042, forward nfe 26899, backward nfe 6451, Train: 0.2100, Val: 0.1354, Test: 0.1374\n",
      "Epoch: 008, Runtime 3.074548, Loss 2.229186, forward nfe 31410, backward nfe 7480, Train: 0.1800, Val: 0.1162, Test: 0.1152\n",
      "Epoch: 009, Runtime 3.104912, Loss 2.166329, forward nfe 35897, backward nfe 8511, Train: 0.1900, Val: 0.4108, Test: 0.3848\n",
      "Epoch: 010, Runtime 3.077563, Loss 2.172069, forward nfe 40472, backward nfe 9549, Train: 0.2650, Val: 0.4446, Test: 0.4169\n",
      "Epoch: 011, Runtime 3.143636, Loss 2.166218, forward nfe 44984, backward nfe 10613, Train: 0.2800, Val: 0.2562, Test: 0.2644\n",
      "Epoch: 012, Runtime 3.120653, Loss 2.103761, forward nfe 49646, backward nfe 11646, Train: 0.2400, Val: 0.1538, Test: 0.1651\n",
      "Epoch: 013, Runtime 3.101700, Loss 2.097699, forward nfe 54255, backward nfe 12672, Train: 0.2100, Val: 0.0762, Test: 0.0990\n",
      "Epoch: 014, Runtime 3.200199, Loss 2.107832, forward nfe 58830, backward nfe 13760, Train: 0.2200, Val: 0.0885, Test: 0.1098\n",
      "Epoch: 015, Runtime 3.116395, Loss 2.080486, forward nfe 63432, backward nfe 14823, Train: 0.2950, Val: 0.1608, Test: 0.1804\n",
      "Epoch: 016, Runtime 3.135188, Loss 2.040843, forward nfe 67947, backward nfe 15870, Train: 0.3600, Val: 0.2215, Test: 0.2423\n",
      "Epoch: 017, Runtime 3.160417, Loss 2.018002, forward nfe 72609, backward nfe 16913, Train: 0.3650, Val: 0.2608, Test: 0.2733\n",
      "Epoch: 018, Runtime 3.197274, Loss 2.001292, forward nfe 77271, backward nfe 17998, Train: 0.3750, Val: 0.2777, Test: 0.2847\n",
      "Epoch: 019, Runtime 3.197448, Loss 1.977543, forward nfe 81933, backward nfe 19086, Train: 0.3800, Val: 0.2685, Test: 0.2834\n",
      "Epoch: 020, Runtime 3.183980, Loss 1.948676, forward nfe 86595, backward nfe 20156, Train: 0.3750, Val: 0.2554, Test: 0.2736\n",
      "Epoch: 021, Runtime 3.161544, Loss 1.933489, forward nfe 91257, backward nfe 21201, Train: 0.3850, Val: 0.2485, Test: 0.2663\n",
      "Epoch: 022, Runtime 3.159518, Loss 1.916071, forward nfe 95919, backward nfe 22245, Train: 0.4650, Val: 0.2577, Test: 0.2827\n",
      "Epoch: 023, Runtime 3.187697, Loss 1.885736, forward nfe 100581, backward nfe 23318, Train: 0.4350, Val: 0.1931, Test: 0.2244\n",
      "Epoch: 024, Runtime 3.228945, Loss 1.862426, forward nfe 105243, backward nfe 24437, Train: 0.4300, Val: 0.2085, Test: 0.2356\n",
      "Epoch: 025, Runtime 3.225847, Loss 1.841420, forward nfe 109905, backward nfe 25556, Train: 0.4550, Val: 0.2731, Test: 0.2970\n",
      "Epoch: 026, Runtime 3.153693, Loss 1.810467, forward nfe 114567, backward nfe 26593, Train: 0.4600, Val: 0.2992, Test: 0.3122\n",
      "Epoch: 027, Runtime 3.181862, Loss 1.789637, forward nfe 119229, backward nfe 27661, Train: 0.4600, Val: 0.3031, Test: 0.3172\n",
      "Epoch: 028, Runtime 3.224408, Loss 1.759870, forward nfe 123891, backward nfe 28780, Train: 0.4650, Val: 0.2992, Test: 0.3186\n",
      "Epoch: 029, Runtime 3.184727, Loss 1.741046, forward nfe 128553, backward nfe 29852, Train: 0.5000, Val: 0.2954, Test: 0.3247\n",
      "Epoch: 030, Runtime 3.217784, Loss 1.706127, forward nfe 133215, backward nfe 30959, Train: 0.5450, Val: 0.2677, Test: 0.3129\n",
      "Epoch: 031, Runtime 3.225903, Loss 1.674022, forward nfe 137877, backward nfe 32078, Train: 0.5700, Val: 0.3177, Test: 0.3624\n",
      "Epoch: 032, Runtime 3.146751, Loss 1.654503, forward nfe 142539, backward nfe 33108, Train: 0.6350, Val: 0.4231, Test: 0.4460\n",
      "Epoch: 033, Runtime 3.237989, Loss 1.628967, forward nfe 147201, backward nfe 34227, Train: 0.6950, Val: 0.5185, Test: 0.5562\n",
      "Epoch: 034, Runtime 3.165388, Loss 1.599646, forward nfe 151863, backward nfe 35281, Train: 0.6850, Val: 0.5954, Test: 0.6121\n",
      "Epoch: 035, Runtime 3.200856, Loss 1.565196, forward nfe 156525, backward nfe 36400, Train: 0.6550, Val: 0.6023, Test: 0.6166\n",
      "Epoch: 036, Runtime 3.206713, Loss 1.540858, forward nfe 161083, backward nfe 37519, Train: 0.6550, Val: 0.6077, Test: 0.6177\n",
      "Epoch: 037, Runtime 3.108649, Loss 1.515737, forward nfe 165745, backward nfe 38558, Train: 0.6850, Val: 0.6054, Test: 0.6239\n",
      "Epoch: 038, Runtime 3.225253, Loss 1.484125, forward nfe 170301, backward nfe 39677, Train: 0.7000, Val: 0.6092, Test: 0.6269\n",
      "Epoch: 039, Runtime 3.166417, Loss 1.459698, forward nfe 174963, backward nfe 40727, Train: 0.7150, Val: 0.6115, Test: 0.6272\n",
      "Epoch: 040, Runtime 3.226741, Loss 1.438859, forward nfe 179625, backward nfe 41846, Train: 0.7250, Val: 0.6185, Test: 0.6355\n",
      "Epoch: 041, Runtime 3.229320, Loss 1.411544, forward nfe 184287, backward nfe 42965, Train: 0.7350, Val: 0.6223, Test: 0.6470\n",
      "Epoch: 042, Runtime 3.216993, Loss 1.377375, forward nfe 188926, backward nfe 44084, Train: 0.7450, Val: 0.6269, Test: 0.6525\n",
      "Epoch: 043, Runtime 3.142406, Loss 1.351374, forward nfe 193588, backward nfe 45111, Train: 0.7450, Val: 0.6308, Test: 0.6586\n",
      "Epoch: 044, Runtime 3.181947, Loss 1.327006, forward nfe 198250, backward nfe 46178, Train: 0.7550, Val: 0.6308, Test: 0.6639\n",
      "Epoch: 045, Runtime 3.167374, Loss 1.295519, forward nfe 202845, backward nfe 47266, Train: 0.7700, Val: 0.6354, Test: 0.6651\n",
      "Epoch: 046, Runtime 3.169153, Loss 1.269481, forward nfe 207417, backward nfe 48368, Train: 0.7900, Val: 0.6377, Test: 0.6666\n",
      "Epoch: 047, Runtime 3.084349, Loss 1.244384, forward nfe 211983, backward nfe 49407, Train: 0.7850, Val: 0.6385, Test: 0.6721\n",
      "Epoch: 048, Runtime 3.107519, Loss 1.208276, forward nfe 216519, backward nfe 50431, Train: 0.8050, Val: 0.6385, Test: 0.6763\n",
      "Epoch: 049, Runtime 3.131207, Loss 1.188896, forward nfe 221165, backward nfe 51474, Train: 0.8050, Val: 0.6423, Test: 0.6758\n",
      "Epoch: 050, Runtime 3.084944, Loss 1.164235, forward nfe 225696, backward nfe 52501, Train: 0.8050, Val: 0.6438, Test: 0.6717\n",
      "Epoch: 051, Runtime 3.088619, Loss 1.145073, forward nfe 230248, backward nfe 53528, Train: 0.8000, Val: 0.6423, Test: 0.6694\n",
      "Epoch: 052, Runtime 3.103270, Loss 1.116371, forward nfe 234829, backward nfe 54560, Train: 0.8000, Val: 0.6415, Test: 0.6706\n",
      "Epoch: 053, Runtime 3.144568, Loss 1.089555, forward nfe 239403, backward nfe 55618, Train: 0.7950, Val: 0.6469, Test: 0.6782\n",
      "Epoch: 054, Runtime 3.126954, Loss 1.061712, forward nfe 243961, backward nfe 56654, Train: 0.8100, Val: 0.6592, Test: 0.6890\n",
      "Epoch: 055, Runtime 3.110335, Loss 1.035759, forward nfe 248555, backward nfe 57688, Train: 0.8150, Val: 0.6754, Test: 0.6962\n",
      "Epoch: 056, Runtime 3.111782, Loss 1.013078, forward nfe 253181, backward nfe 58717, Train: 0.8050, Val: 0.6800, Test: 0.6978\n",
      "Epoch: 057, Runtime 3.120510, Loss 0.992553, forward nfe 257769, backward nfe 59749, Train: 0.8150, Val: 0.6800, Test: 0.6985\n",
      "Epoch: 058, Runtime 3.106715, Loss 0.965643, forward nfe 262391, backward nfe 60775, Train: 0.8150, Val: 0.6831, Test: 0.7032\n",
      "Epoch: 059, Runtime 3.084205, Loss 0.948406, forward nfe 266912, backward nfe 61800, Train: 0.8350, Val: 0.6869, Test: 0.7108\n",
      "Epoch: 060, Runtime 3.077177, Loss 0.917623, forward nfe 271425, backward nfe 62835, Train: 0.8400, Val: 0.6931, Test: 0.7167\n",
      "Epoch: 061, Runtime 3.113798, Loss 0.897270, forward nfe 275980, backward nfe 63861, Train: 0.8400, Val: 0.6923, Test: 0.7188\n",
      "Epoch: 062, Runtime 3.070571, Loss 0.881157, forward nfe 280518, backward nfe 64897, Train: 0.8300, Val: 0.6915, Test: 0.7176\n",
      "Epoch: 063, Runtime 3.055094, Loss 0.856107, forward nfe 284992, backward nfe 65921, Train: 0.8350, Val: 0.6892, Test: 0.7156\n",
      "Epoch: 064, Runtime 3.077573, Loss 0.840868, forward nfe 289487, backward nfe 66946, Train: 0.8350, Val: 0.6938, Test: 0.7178\n",
      "Epoch: 065, Runtime 3.060915, Loss 0.822297, forward nfe 293978, backward nfe 67977, Train: 0.8350, Val: 0.6969, Test: 0.7216\n",
      "Epoch: 066, Runtime 3.081384, Loss 0.810269, forward nfe 298466, backward nfe 69001, Train: 0.8350, Val: 0.7054, Test: 0.7240\n",
      "Epoch: 067, Runtime 3.051765, Loss 0.783768, forward nfe 302994, backward nfe 70025, Train: 0.8450, Val: 0.7046, Test: 0.7259\n",
      "Epoch: 068, Runtime 3.055361, Loss 0.773256, forward nfe 307475, backward nfe 71050, Train: 0.8500, Val: 0.7046, Test: 0.7302\n",
      "Epoch: 069, Runtime 3.061141, Loss 0.754580, forward nfe 311952, backward nfe 72077, Train: 0.8450, Val: 0.7062, Test: 0.7343\n",
      "Epoch: 070, Runtime 3.067113, Loss 0.738938, forward nfe 316443, backward nfe 73106, Train: 0.8600, Val: 0.7100, Test: 0.7371\n",
      "Epoch: 071, Runtime 3.063509, Loss 0.728856, forward nfe 320924, backward nfe 74133, Train: 0.8550, Val: 0.7108, Test: 0.7360\n",
      "Epoch: 072, Runtime 3.068771, Loss 0.715109, forward nfe 325405, backward nfe 75174, Train: 0.8550, Val: 0.7092, Test: 0.7327\n",
      "Epoch: 073, Runtime 3.060094, Loss 0.706450, forward nfe 329882, backward nfe 76201, Train: 0.8550, Val: 0.7131, Test: 0.7365\n",
      "Epoch: 074, Runtime 3.058832, Loss 0.689783, forward nfe 334360, backward nfe 77227, Train: 0.8500, Val: 0.7223, Test: 0.7427\n",
      "Epoch: 075, Runtime 3.066167, Loss 0.663113, forward nfe 338843, backward nfe 78257, Train: 0.8500, Val: 0.7215, Test: 0.7459\n",
      "Epoch: 076, Runtime 3.053864, Loss 0.645407, forward nfe 343313, backward nfe 79283, Train: 0.8550, Val: 0.7200, Test: 0.7485\n",
      "Epoch: 077, Runtime 3.057708, Loss 0.646989, forward nfe 347790, backward nfe 80309, Train: 0.8650, Val: 0.7238, Test: 0.7477\n",
      "Epoch: 078, Runtime 3.055938, Loss 0.630856, forward nfe 352261, backward nfe 81337, Train: 0.8650, Val: 0.7200, Test: 0.7447\n",
      "Epoch: 079, Runtime 3.056677, Loss 0.620501, forward nfe 356731, backward nfe 82365, Train: 0.8600, Val: 0.7223, Test: 0.7436\n",
      "Epoch: 080, Runtime 3.067289, Loss 0.610191, forward nfe 361204, backward nfe 83395, Train: 0.8650, Val: 0.7231, Test: 0.7452\n",
      "Epoch: 081, Runtime 3.052383, Loss 0.598626, forward nfe 365674, backward nfe 84421, Train: 0.8600, Val: 0.7277, Test: 0.7458\n",
      "Epoch: 082, Runtime 3.051769, Loss 0.589773, forward nfe 370143, backward nfe 85447, Train: 0.8700, Val: 0.7292, Test: 0.7517\n",
      "Epoch: 083, Runtime 3.059598, Loss 0.588859, forward nfe 374620, backward nfe 86472, Train: 0.8800, Val: 0.7369, Test: 0.7557\n",
      "Epoch: 084, Runtime 3.060151, Loss 0.567941, forward nfe 379093, backward nfe 87503, Train: 0.8800, Val: 0.7369, Test: 0.7584\n",
      "Epoch: 085, Runtime 3.054350, Loss 0.559738, forward nfe 383567, backward nfe 88529, Train: 0.8800, Val: 0.7392, Test: 0.7580\n",
      "Epoch: 086, Runtime 3.057467, Loss 0.555685, forward nfe 388037, backward nfe 89559, Train: 0.8800, Val: 0.7362, Test: 0.7573\n",
      "Epoch: 087, Runtime 3.054733, Loss 0.535879, forward nfe 392508, backward nfe 90586, Train: 0.8800, Val: 0.7400, Test: 0.7573\n",
      "Epoch: 088, Runtime 3.057305, Loss 0.523697, forward nfe 396983, backward nfe 91612, Train: 0.8750, Val: 0.7415, Test: 0.7582\n",
      "Epoch: 089, Runtime 3.063424, Loss 0.532128, forward nfe 401459, backward nfe 92645, Train: 0.8800, Val: 0.7415, Test: 0.7613\n",
      "Epoch: 090, Runtime 3.053609, Loss 0.509824, forward nfe 405933, backward nfe 93669, Train: 0.8850, Val: 0.7431, Test: 0.7608\n",
      "Epoch: 091, Runtime 3.050677, Loss 0.515072, forward nfe 410403, backward nfe 94694, Train: 0.8800, Val: 0.7438, Test: 0.7597\n",
      "Epoch: 092, Runtime 3.052722, Loss 0.492407, forward nfe 414874, backward nfe 95718, Train: 0.8750, Val: 0.7485, Test: 0.7645\n",
      "Epoch: 093, Runtime 3.052008, Loss 0.491420, forward nfe 419343, backward nfe 96742, Train: 0.8750, Val: 0.7531, Test: 0.7699\n",
      "Epoch: 094, Runtime 3.062230, Loss 0.482982, forward nfe 423819, backward nfe 97766, Train: 0.8750, Val: 0.7538, Test: 0.7714\n",
      "Epoch: 095, Runtime 3.055771, Loss 0.481096, forward nfe 428293, backward nfe 98790, Train: 0.8750, Val: 0.7485, Test: 0.7698\n",
      "Epoch: 096, Runtime 3.055507, Loss 0.460941, forward nfe 432764, backward nfe 99815, Train: 0.8800, Val: 0.7454, Test: 0.7660\n",
      "Epoch: 097, Runtime 3.048522, Loss 0.459733, forward nfe 437240, backward nfe 100839, Train: 0.8850, Val: 0.7485, Test: 0.7653\n",
      "Epoch: 098, Runtime 3.055500, Loss 0.446617, forward nfe 441711, backward nfe 101863, Train: 0.8800, Val: 0.7546, Test: 0.7698\n",
      "Epoch: 099, Runtime 3.053862, Loss 0.446037, forward nfe 446187, backward nfe 102887, Train: 0.8800, Val: 0.7569, Test: 0.7717\n",
      "best val accuracy 0.756923 with test accuracy 0.771711 at epoch 99\n",
      "*** Doing stepsize 0.1 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.594352, Loss 2.315887, forward nfe 445, backward nfe 68, Train: 0.1000, Val: 0.0638, Test: 0.0646\n",
      "Epoch: 002, Runtime 1.485472, Loss 2.475800, forward nfe 1402, backward nfe 1092, Train: 0.1000, Val: 0.0838, Test: 0.0931\n",
      "Epoch: 003, Runtime 1.494129, Loss 2.343059, forward nfe 2361, backward nfe 2116, Train: 0.1050, Val: 0.4023, Test: 0.3763\n",
      "Epoch: 004, Runtime 1.502920, Loss 2.289215, forward nfe 3336, backward nfe 3140, Train: 0.0900, Val: 0.1215, Test: 0.1228\n",
      "Epoch: 005, Runtime 1.503824, Loss 2.287310, forward nfe 4329, backward nfe 4164, Train: 0.0950, Val: 0.1138, Test: 0.1177\n",
      "Epoch: 006, Runtime 1.528422, Loss 2.277813, forward nfe 5340, backward nfe 5194, Train: 0.1150, Val: 0.1715, Test: 0.1665\n",
      "Epoch: 007, Runtime 1.560856, Loss 2.270446, forward nfe 6382, backward nfe 6221, Train: 0.1200, Val: 0.1408, Test: 0.1403\n",
      "Epoch: 008, Runtime 1.650603, Loss 2.263730, forward nfe 7530, backward nfe 7253, Train: 0.1050, Val: 0.0746, Test: 0.0808\n",
      "Epoch: 009, Runtime 1.749353, Loss 2.251304, forward nfe 8913, backward nfe 8281, Train: 0.1200, Val: 0.0569, Test: 0.0680\n",
      "Epoch: 010, Runtime 1.857800, Loss 2.244728, forward nfe 10561, backward nfe 9370, Train: 0.1450, Val: 0.0508, Test: 0.0701\n",
      "Epoch: 011, Runtime 1.933378, Loss 2.235125, forward nfe 12345, backward nfe 10408, Train: 0.1450, Val: 0.0323, Test: 0.0561\n",
      "Epoch: 012, Runtime 2.034086, Loss 2.227666, forward nfe 14213, backward nfe 11442, Train: 0.1450, Val: 0.0231, Test: 0.0415\n",
      "Epoch: 013, Runtime 2.388504, Loss 2.217013, forward nfe 16757, backward nfe 12509, Train: 0.1450, Val: 0.0185, Test: 0.0324\n",
      "Epoch: 014, Runtime 2.561176, Loss 2.208748, forward nfe 19500, backward nfe 13621, Train: 0.1500, Val: 0.0185, Test: 0.0336\n",
      "Epoch: 015, Runtime 2.928279, Loss 2.198898, forward nfe 23130, backward nfe 14740, Train: 0.1600, Val: 0.0185, Test: 0.0357\n",
      "Epoch: 016, Runtime 3.508959, Loss 2.188316, forward nfe 27477, backward nfe 15859, Train: 0.1750, Val: 0.0223, Test: 0.0419\n",
      "Epoch: 017, Runtime 3.407540, Loss 2.179149, forward nfe 32982, backward nfe 16936, Train: 0.2000, Val: 0.0238, Test: 0.0442\n",
      "Epoch: 018, Runtime 3.457668, Loss 2.168597, forward nfe 37814, backward nfe 18055, Train: 0.1850, Val: 0.0215, Test: 0.0420\n",
      "Epoch: 019, Runtime 3.651049, Loss 2.158017, forward nfe 43072, backward nfe 19116, Train: 0.1350, Val: 0.0285, Test: 0.0412\n",
      "Epoch: 020, Runtime 3.935316, Loss 2.111008, forward nfe 48778, backward nfe 20212, Train: 0.1600, Val: 0.0485, Test: 0.0575\n",
      "Epoch: 021, Runtime 4.141518, Loss 2.162624, forward nfe 55008, backward nfe 21258, Train: 0.1750, Val: 0.0292, Test: 0.0445\n",
      "Epoch: 022, Runtime 4.033112, Loss 2.101537, forward nfe 61323, backward nfe 22366, Train: 0.1750, Val: 0.0485, Test: 0.0549\n",
      "Epoch: 023, Runtime 3.796074, Loss 2.094778, forward nfe 67887, backward nfe 23443, Train: 0.2050, Val: 0.0531, Test: 0.0721\n",
      "Epoch: 024, Runtime 3.813604, Loss 2.099484, forward nfe 73850, backward nfe 24467, Train: 0.2300, Val: 0.0769, Test: 0.0883\n",
      "Epoch: 025, Runtime 3.240459, Loss 2.085541, forward nfe 79119, backward nfe 25495, Train: 0.2150, Val: 0.0662, Test: 0.0793\n",
      "Epoch: 026, Runtime 3.275576, Loss 2.048851, forward nfe 83831, backward nfe 26528, Train: 0.1900, Val: 0.0477, Test: 0.0661\n",
      "Epoch: 027, Runtime 2.842229, Loss 2.028932, forward nfe 88235, backward nfe 27588, Train: 0.1800, Val: 0.0454, Test: 0.0648\n",
      "Epoch: 028, Runtime 2.706129, Loss 2.032056, forward nfe 91632, backward nfe 28631, Train: 0.1850, Val: 0.0562, Test: 0.0717\n",
      "Epoch: 029, Runtime 2.932261, Loss 2.015466, forward nfe 95495, backward nfe 29744, Train: 0.2200, Val: 0.0946, Test: 0.1081\n",
      "Epoch: 030, Runtime 2.734282, Loss 1.994144, forward nfe 99134, backward nfe 30863, Train: 0.2650, Val: 0.1438, Test: 0.1578\n",
      "Epoch: 031, Runtime 2.737696, Loss 1.970283, forward nfe 102684, backward nfe 31890, Train: 0.3050, Val: 0.1662, Test: 0.1809\n",
      "Epoch: 032, Runtime 2.501004, Loss 1.968363, forward nfe 105934, backward nfe 32926, Train: 0.2900, Val: 0.1815, Test: 0.1943\n",
      "Epoch: 033, Runtime 2.773908, Loss 1.957605, forward nfe 109429, backward nfe 33989, Train: 0.2750, Val: 0.1731, Test: 0.1890\n",
      "Epoch: 034, Runtime 2.585776, Loss 1.936392, forward nfe 113047, backward nfe 35019, Train: 0.2650, Val: 0.1608, Test: 0.1753\n",
      "Epoch: 035, Runtime 2.511568, Loss 1.913856, forward nfe 115979, backward nfe 36138, Train: 0.2550, Val: 0.1623, Test: 0.1751\n",
      "Epoch: 036, Runtime 2.574971, Loss 1.905757, forward nfe 119145, backward nfe 37257, Train: 0.2600, Val: 0.1654, Test: 0.1809\n",
      "Epoch: 037, Runtime 2.327579, Loss 1.897440, forward nfe 121951, backward nfe 38317, Train: 0.2700, Val: 0.1731, Test: 0.1878\n",
      "Epoch: 038, Runtime 2.227010, Loss 1.877063, forward nfe 124600, backward nfe 39351, Train: 0.3000, Val: 0.2008, Test: 0.2089\n",
      "Epoch: 039, Runtime 2.238440, Loss 1.860912, forward nfe 127080, backward nfe 40379, Train: 0.3300, Val: 0.2369, Test: 0.2458\n",
      "Epoch: 040, Runtime 2.223210, Loss 1.850808, forward nfe 129611, backward nfe 41424, Train: 0.3250, Val: 0.2654, Test: 0.2720\n",
      "Epoch: 041, Runtime 2.271589, Loss 1.834792, forward nfe 132145, backward nfe 42520, Train: 0.3350, Val: 0.2738, Test: 0.2796\n",
      "Epoch: 042, Runtime 2.155954, Loss 1.821299, forward nfe 134489, backward nfe 43579, Train: 0.3300, Val: 0.2623, Test: 0.2791\n",
      "Epoch: 043, Runtime 2.093405, Loss 1.790697, forward nfe 136790, backward nfe 44609, Train: 0.3750, Val: 0.2938, Test: 0.3146\n",
      "Epoch: 044, Runtime 2.012168, Loss 1.766792, forward nfe 139117, backward nfe 45644, Train: 0.4200, Val: 0.3062, Test: 0.3270\n",
      "Epoch: 045, Runtime 2.041171, Loss 1.739809, forward nfe 141129, backward nfe 46705, Train: 0.3300, Val: 0.2700, Test: 0.3075\n",
      "Epoch: 046, Runtime 1.956254, Loss 1.711265, forward nfe 142993, backward nfe 47783, Train: 0.3300, Val: 0.2900, Test: 0.3102\n",
      "Epoch: 047, Runtime 1.931753, Loss 1.694001, forward nfe 144942, backward nfe 48814, Train: 0.4200, Val: 0.3162, Test: 0.3349\n",
      "Epoch: 048, Runtime 1.983953, Loss 1.659048, forward nfe 146765, backward nfe 49894, Train: 0.4750, Val: 0.3808, Test: 0.3945\n",
      "Epoch: 049, Runtime 1.924558, Loss 1.634840, forward nfe 148671, backward nfe 50922, Train: 0.4300, Val: 0.4054, Test: 0.4203\n",
      "Epoch: 050, Runtime 1.930442, Loss 1.620063, forward nfe 150563, backward nfe 51954, Train: 0.4750, Val: 0.4523, Test: 0.4708\n",
      "Epoch: 051, Runtime 1.917601, Loss 1.590438, forward nfe 152462, backward nfe 52981, Train: 0.4850, Val: 0.4323, Test: 0.4599\n",
      "Epoch: 052, Runtime 1.858167, Loss 1.565624, forward nfe 154289, backward nfe 54040, Train: 0.5100, Val: 0.4062, Test: 0.4349\n",
      "Epoch: 053, Runtime 1.792993, Loss 1.528787, forward nfe 155823, backward nfe 55066, Train: 0.5000, Val: 0.3723, Test: 0.3938\n",
      "Epoch: 054, Runtime 1.783727, Loss 1.496980, forward nfe 157450, backward nfe 56095, Train: 0.5000, Val: 0.3592, Test: 0.3896\n",
      "Epoch: 055, Runtime 1.801948, Loss 1.476420, forward nfe 159084, backward nfe 57123, Train: 0.5200, Val: 0.3700, Test: 0.3910\n",
      "Epoch: 056, Runtime 1.731782, Loss 1.459415, forward nfe 160580, backward nfe 58167, Train: 0.5350, Val: 0.3762, Test: 0.4030\n",
      "Epoch: 057, Runtime 1.759408, Loss 1.427064, forward nfe 162070, backward nfe 59193, Train: 0.5150, Val: 0.3669, Test: 0.3917\n",
      "Epoch: 058, Runtime 1.740909, Loss 1.393406, forward nfe 163591, backward nfe 60220, Train: 0.5650, Val: 0.3785, Test: 0.4129\n",
      "Epoch: 059, Runtime 1.691446, Loss 1.375898, forward nfe 165045, backward nfe 61265, Train: 0.5600, Val: 0.4515, Test: 0.4755\n",
      "Epoch: 060, Runtime 1.668600, Loss 1.346874, forward nfe 166368, backward nfe 62296, Train: 0.5700, Val: 0.4715, Test: 0.5016\n",
      "Epoch: 061, Runtime 1.652831, Loss 1.315214, forward nfe 167696, backward nfe 63320, Train: 0.5750, Val: 0.4908, Test: 0.5169\n",
      "Epoch: 062, Runtime 1.651663, Loss 1.289646, forward nfe 168971, backward nfe 64349, Train: 0.6100, Val: 0.4877, Test: 0.5248\n",
      "Epoch: 063, Runtime 1.642169, Loss 1.269850, forward nfe 170284, backward nfe 65377, Train: 0.6100, Val: 0.4923, Test: 0.5082\n",
      "Epoch: 064, Runtime 1.622873, Loss 1.226847, forward nfe 171534, backward nfe 66414, Train: 0.6300, Val: 0.4815, Test: 0.4992\n",
      "Epoch: 065, Runtime 1.611631, Loss 1.189008, forward nfe 172729, backward nfe 67440, Train: 0.6800, Val: 0.5223, Test: 0.5516\n",
      "Epoch: 066, Runtime 1.619490, Loss 1.155913, forward nfe 173949, backward nfe 68467, Train: 0.7000, Val: 0.5669, Test: 0.5931\n",
      "Epoch: 067, Runtime 1.630422, Loss 1.125096, forward nfe 175222, backward nfe 69494, Train: 0.7000, Val: 0.5923, Test: 0.6145\n",
      "Epoch: 068, Runtime 1.628697, Loss 1.083747, forward nfe 176448, backward nfe 70532, Train: 0.6950, Val: 0.5877, Test: 0.6165\n",
      "Epoch: 069, Runtime 1.627351, Loss 1.037103, forward nfe 177662, backward nfe 71589, Train: 0.7150, Val: 0.5877, Test: 0.6247\n",
      "Epoch: 070, Runtime 1.629214, Loss 1.005237, forward nfe 178878, backward nfe 72614, Train: 0.7250, Val: 0.6123, Test: 0.6427\n",
      "Epoch: 071, Runtime 1.613043, Loss 0.972093, forward nfe 180105, backward nfe 73640, Train: 0.7250, Val: 0.6208, Test: 0.6486\n",
      "Epoch: 072, Runtime 1.634366, Loss 0.942503, forward nfe 181352, backward nfe 74672, Train: 0.7450, Val: 0.6315, Test: 0.6505\n",
      "Epoch: 073, Runtime 1.599591, Loss 0.897673, forward nfe 182567, backward nfe 75701, Train: 0.7450, Val: 0.6400, Test: 0.6565\n",
      "Epoch: 074, Runtime 1.614617, Loss 0.889620, forward nfe 183792, backward nfe 76728, Train: 0.7550, Val: 0.6331, Test: 0.6574\n",
      "Epoch: 075, Runtime 1.619663, Loss 0.842896, forward nfe 185001, backward nfe 77757, Train: 0.7600, Val: 0.6292, Test: 0.6601\n",
      "Epoch: 076, Runtime 1.626657, Loss 0.824051, forward nfe 186255, backward nfe 78782, Train: 0.7750, Val: 0.6362, Test: 0.6653\n",
      "Epoch: 077, Runtime 1.619517, Loss 0.806737, forward nfe 187477, backward nfe 79809, Train: 0.7650, Val: 0.6508, Test: 0.6772\n",
      "Epoch: 078, Runtime 1.640418, Loss 0.768573, forward nfe 188731, backward nfe 80833, Train: 0.7750, Val: 0.6577, Test: 0.6840\n",
      "Epoch: 079, Runtime 1.612633, Loss 0.747486, forward nfe 189984, backward nfe 81858, Train: 0.7850, Val: 0.6600, Test: 0.6831\n",
      "Epoch: 080, Runtime 1.604826, Loss 0.726900, forward nfe 191220, backward nfe 82883, Train: 0.7800, Val: 0.6569, Test: 0.6836\n",
      "Epoch: 081, Runtime 1.588904, Loss 0.721668, forward nfe 192381, backward nfe 83919, Train: 0.7900, Val: 0.6492, Test: 0.6785\n",
      "Epoch: 082, Runtime 1.590485, Loss 0.697492, forward nfe 193541, backward nfe 84943, Train: 0.7950, Val: 0.6446, Test: 0.6764\n",
      "Epoch: 083, Runtime 1.588213, Loss 0.696939, forward nfe 194684, backward nfe 85969, Train: 0.7950, Val: 0.6531, Test: 0.6830\n",
      "Epoch: 084, Runtime 1.601027, Loss 0.671420, forward nfe 195861, backward nfe 87000, Train: 0.8000, Val: 0.6692, Test: 0.6935\n",
      "Epoch: 085, Runtime 1.574809, Loss 0.642542, forward nfe 197008, backward nfe 88025, Train: 0.8100, Val: 0.6754, Test: 0.6998\n",
      "Epoch: 086, Runtime 1.569442, Loss 0.629211, forward nfe 198150, backward nfe 89051, Train: 0.7850, Val: 0.6700, Test: 0.7040\n",
      "Epoch: 087, Runtime 1.569994, Loss 0.625536, forward nfe 199295, backward nfe 90077, Train: 0.8000, Val: 0.6692, Test: 0.6964\n",
      "Epoch: 088, Runtime 1.579307, Loss 0.597889, forward nfe 200413, backward nfe 91103, Train: 0.8350, Val: 0.6723, Test: 0.6982\n",
      "Epoch: 089, Runtime 1.559285, Loss 0.600317, forward nfe 201535, backward nfe 92127, Train: 0.8250, Val: 0.6815, Test: 0.7079\n",
      "Epoch: 090, Runtime 1.553021, Loss 0.586793, forward nfe 202640, backward nfe 93153, Train: 0.8150, Val: 0.6754, Test: 0.7089\n",
      "Epoch: 091, Runtime 1.547040, Loss 0.578123, forward nfe 203715, backward nfe 94178, Train: 0.8350, Val: 0.6808, Test: 0.7061\n",
      "Epoch: 092, Runtime 1.559905, Loss 0.548109, forward nfe 204799, backward nfe 95204, Train: 0.8250, Val: 0.6892, Test: 0.7178\n",
      "Epoch: 093, Runtime 1.535633, Loss 0.558275, forward nfe 205875, backward nfe 96229, Train: 0.8400, Val: 0.6946, Test: 0.7191\n",
      "Epoch: 094, Runtime 1.552378, Loss 0.526822, forward nfe 206938, backward nfe 97254, Train: 0.8350, Val: 0.6977, Test: 0.7239\n",
      "Epoch: 095, Runtime 1.545845, Loss 0.535844, forward nfe 208025, backward nfe 98283, Train: 0.8250, Val: 0.6877, Test: 0.7160\n",
      "Epoch: 096, Runtime 1.559455, Loss 0.518775, forward nfe 209115, backward nfe 99306, Train: 0.8300, Val: 0.6838, Test: 0.7105\n",
      "Epoch: 097, Runtime 1.549667, Loss 0.511934, forward nfe 210190, backward nfe 100333, Train: 0.8450, Val: 0.6992, Test: 0.7221\n",
      "Epoch: 098, Runtime 1.533514, Loss 0.502865, forward nfe 211263, backward nfe 101357, Train: 0.8400, Val: 0.7023, Test: 0.7263\n",
      "Epoch: 099, Runtime 1.550539, Loss 0.486840, forward nfe 212333, backward nfe 102380, Train: 0.8500, Val: 0.7046, Test: 0.7323\n",
      "best val accuracy 0.704615 with test accuracy 0.732289 at epoch 99\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.591262, Loss 2.334567, forward nfe 442, backward nfe 68, Train: 0.1000, Val: 0.1015, Test: 0.1049\n",
      "Epoch: 002, Runtime 1.489907, Loss 2.689482, forward nfe 1399, backward nfe 1092, Train: 0.1100, Val: 0.1377, Test: 0.1379\n",
      "Epoch: 003, Runtime 1.501587, Loss 2.373035, forward nfe 2360, backward nfe 2116, Train: 0.1000, Val: 0.4023, Test: 0.3767\n",
      "Epoch: 004, Runtime 1.514024, Loss 2.423819, forward nfe 3365, backward nfe 3141, Train: 0.1000, Val: 0.3923, Test: 0.3687\n",
      "Epoch: 005, Runtime 1.524366, Loss 2.340263, forward nfe 4384, backward nfe 4165, Train: 0.1400, Val: 0.1262, Test: 0.1234\n",
      "Epoch: 006, Runtime 1.539248, Loss 2.289405, forward nfe 5434, backward nfe 5191, Train: 0.1100, Val: 0.0838, Test: 0.0842\n",
      "Epoch: 007, Runtime 1.596306, Loss 2.279935, forward nfe 6515, backward nfe 6216, Train: 0.1100, Val: 0.0177, Test: 0.0333\n",
      "Epoch: 008, Runtime 1.772558, Loss 2.287684, forward nfe 7813, backward nfe 7249, Train: 0.1750, Val: 0.0985, Test: 0.1040\n",
      "Epoch: 009, Runtime 1.912732, Loss 2.261149, forward nfe 9588, backward nfe 8294, Train: 0.1150, Val: 0.0923, Test: 0.0947\n",
      "Epoch: 010, Runtime 2.076288, Loss 2.273996, forward nfe 11516, backward nfe 9346, Train: 0.1100, Val: 0.0900, Test: 0.0913\n",
      "Epoch: 011, Runtime 2.330402, Loss 2.250320, forward nfe 13928, backward nfe 10406, Train: 0.1100, Val: 0.0231, Test: 0.0390\n",
      "Epoch: 012, Runtime 2.586102, Loss 2.250574, forward nfe 16822, backward nfe 11508, Train: 0.1250, Val: 0.0338, Test: 0.0477\n",
      "Epoch: 013, Runtime 2.775058, Loss 2.241070, forward nfe 20222, backward nfe 12615, Train: 0.1350, Val: 0.0985, Test: 0.0979\n",
      "Epoch: 014, Runtime 3.024412, Loss 2.223572, forward nfe 24163, backward nfe 13734, Train: 0.1450, Val: 0.1592, Test: 0.1487\n",
      "Epoch: 015, Runtime 3.207600, Loss 2.220779, forward nfe 28228, backward nfe 14760, Train: 0.1800, Val: 0.2038, Test: 0.1970\n",
      "Epoch: 016, Runtime 3.361997, Loss 2.209594, forward nfe 33193, backward nfe 15879, Train: 0.1950, Val: 0.2092, Test: 0.2127\n",
      "Epoch: 017, Runtime 3.506846, Loss 2.198513, forward nfe 38244, backward nfe 16998, Train: 0.2700, Val: 0.2008, Test: 0.2168\n",
      "Epoch: 018, Runtime 3.412709, Loss 2.195384, forward nfe 43367, backward nfe 18117, Train: 0.3000, Val: 0.2708, Test: 0.2680\n",
      "Epoch: 019, Runtime 3.397710, Loss 2.183033, forward nfe 48326, backward nfe 19236, Train: 0.2700, Val: 0.3085, Test: 0.3091\n",
      "Epoch: 020, Runtime 3.165486, Loss 2.161895, forward nfe 52754, backward nfe 20344, Train: 0.2800, Val: 0.2438, Test: 0.2542\n",
      "Epoch: 021, Runtime 3.539616, Loss 2.153497, forward nfe 57307, backward nfe 21463, Train: 0.3100, Val: 0.3108, Test: 0.3156\n",
      "Epoch: 022, Runtime 3.449813, Loss 2.127491, forward nfe 62484, backward nfe 22533, Train: 0.3000, Val: 0.3277, Test: 0.3230\n",
      "Epoch: 023, Runtime 3.622742, Loss 2.115707, forward nfe 67996, backward nfe 23579, Train: 0.3150, Val: 0.3154, Test: 0.3222\n",
      "Epoch: 024, Runtime 3.866415, Loss 2.098776, forward nfe 73814, backward nfe 24698, Train: 0.3000, Val: 0.2654, Test: 0.2764\n",
      "Epoch: 025, Runtime 4.091419, Loss 2.082183, forward nfe 80189, backward nfe 25817, Train: 0.3250, Val: 0.3377, Test: 0.3329\n",
      "Epoch: 026, Runtime 4.363757, Loss 2.061125, forward nfe 86646, backward nfe 26936, Train: 0.3250, Val: 0.3415, Test: 0.3347\n",
      "Epoch: 027, Runtime 4.326944, Loss 2.048000, forward nfe 93484, backward nfe 28033, Train: 0.3300, Val: 0.3523, Test: 0.3457\n",
      "Epoch: 028, Runtime 4.546106, Loss 2.019410, forward nfe 100235, backward nfe 29152, Train: 0.3350, Val: 0.3008, Test: 0.3128\n",
      "Epoch: 029, Runtime 4.476654, Loss 2.005746, forward nfe 108263, backward nfe 30271, Train: 0.3500, Val: 0.3454, Test: 0.3479\n",
      "Epoch: 030, Runtime 4.418001, Loss 1.977241, forward nfe 115054, backward nfe 31390, Train: 0.3950, Val: 0.3538, Test: 0.3544\n",
      "Epoch: 031, Runtime 4.054641, Loss 1.960590, forward nfe 121893, backward nfe 32441, Train: 0.3900, Val: 0.3477, Test: 0.3538\n",
      "Epoch: 032, Runtime 3.882487, Loss 1.932968, forward nfe 127948, backward nfe 33560, Train: 0.4250, Val: 0.3423, Test: 0.3506\n",
      "Epoch: 033, Runtime 3.762825, Loss 1.909922, forward nfe 134365, backward nfe 34633, Train: 0.4750, Val: 0.3546, Test: 0.3712\n",
      "Epoch: 034, Runtime 2.921132, Loss 1.881250, forward nfe 138694, backward nfe 35682, Train: 0.4700, Val: 0.3531, Test: 0.3632\n",
      "Epoch: 035, Runtime 2.959564, Loss 1.862841, forward nfe 142614, backward nfe 36721, Train: 0.5050, Val: 0.3554, Test: 0.3729\n",
      "Epoch: 036, Runtime 3.221163, Loss 1.833228, forward nfe 147108, backward nfe 37840, Train: 0.5050, Val: 0.3531, Test: 0.3719\n",
      "Epoch: 037, Runtime 2.987795, Loss 1.806785, forward nfe 151140, backward nfe 38959, Train: 0.5350, Val: 0.3600, Test: 0.3754\n",
      "Epoch: 038, Runtime 3.088975, Loss 1.773960, forward nfe 155366, backward nfe 40078, Train: 0.5350, Val: 0.3638, Test: 0.3799\n",
      "Epoch: 039, Runtime 3.135300, Loss 1.753994, forward nfe 159456, backward nfe 41190, Train: 0.5400, Val: 0.3600, Test: 0.3859\n",
      "Epoch: 040, Runtime 2.851589, Loss 1.716793, forward nfe 163610, backward nfe 42309, Train: 0.5300, Val: 0.3631, Test: 0.3818\n",
      "Epoch: 041, Runtime 2.761911, Loss 1.693632, forward nfe 167266, backward nfe 43428, Train: 0.5400, Val: 0.3815, Test: 0.3997\n",
      "Epoch: 042, Runtime 2.503650, Loss 1.657049, forward nfe 170659, backward nfe 44464, Train: 0.5500, Val: 0.3892, Test: 0.4096\n",
      "Epoch: 043, Runtime 2.439982, Loss 1.633282, forward nfe 173599, backward nfe 45583, Train: 0.5400, Val: 0.3762, Test: 0.4166\n",
      "Epoch: 044, Runtime 2.123068, Loss 1.596307, forward nfe 176078, backward nfe 46617, Train: 0.5400, Val: 0.3815, Test: 0.4186\n",
      "Epoch: 045, Runtime 2.267619, Loss 1.563642, forward nfe 178454, backward nfe 47690, Train: 0.5550, Val: 0.4038, Test: 0.4284\n",
      "Epoch: 046, Runtime 2.151811, Loss 1.528046, forward nfe 181022, backward nfe 48737, Train: 0.5650, Val: 0.4008, Test: 0.4345\n",
      "Epoch: 047, Runtime 2.108524, Loss 1.502735, forward nfe 183264, backward nfe 49798, Train: 0.5450, Val: 0.3969, Test: 0.4410\n",
      "Epoch: 048, Runtime 2.076413, Loss 1.479453, forward nfe 185441, backward nfe 50856, Train: 0.5600, Val: 0.4085, Test: 0.4451\n",
      "Epoch: 049, Runtime 2.009554, Loss 1.443566, forward nfe 187506, backward nfe 51901, Train: 0.5700, Val: 0.4154, Test: 0.4541\n",
      "Epoch: 050, Runtime 2.017920, Loss 1.410744, forward nfe 189533, backward nfe 53009, Train: 0.5750, Val: 0.4215, Test: 0.4572\n",
      "Epoch: 051, Runtime 1.949054, Loss 1.378499, forward nfe 191438, backward nfe 54047, Train: 0.5550, Val: 0.4262, Test: 0.4654\n",
      "Epoch: 052, Runtime 1.831545, Loss 1.353147, forward nfe 193226, backward nfe 55082, Train: 0.5750, Val: 0.4246, Test: 0.4693\n",
      "Epoch: 053, Runtime 1.846371, Loss 1.332222, forward nfe 194930, backward nfe 56128, Train: 0.5650, Val: 0.4377, Test: 0.4736\n",
      "Epoch: 054, Runtime 1.854439, Loss 1.300504, forward nfe 196561, backward nfe 57226, Train: 0.5850, Val: 0.4431, Test: 0.4776\n",
      "Epoch: 055, Runtime 1.784152, Loss 1.281066, forward nfe 198182, backward nfe 58250, Train: 0.5800, Val: 0.4438, Test: 0.4811\n",
      "Epoch: 056, Runtime 1.808600, Loss 1.246899, forward nfe 199798, backward nfe 59283, Train: 0.5700, Val: 0.4246, Test: 0.4763\n",
      "Epoch: 057, Runtime 1.754398, Loss 1.230053, forward nfe 201365, backward nfe 60314, Train: 0.5850, Val: 0.4462, Test: 0.4850\n",
      "Epoch: 058, Runtime 1.676482, Loss 1.214494, forward nfe 202781, backward nfe 61340, Train: 0.6100, Val: 0.4546, Test: 0.4858\n",
      "Epoch: 059, Runtime 1.767942, Loss 1.191297, forward nfe 204140, backward nfe 62426, Train: 0.6000, Val: 0.4508, Test: 0.4879\n",
      "Epoch: 060, Runtime 1.727442, Loss 1.167224, forward nfe 205585, backward nfe 63491, Train: 0.5850, Val: 0.4377, Test: 0.4868\n",
      "Epoch: 061, Runtime 1.691797, Loss 1.151371, forward nfe 206913, backward nfe 64519, Train: 0.6000, Val: 0.4523, Test: 0.4967\n",
      "Epoch: 062, Runtime 1.664787, Loss 1.139180, forward nfe 208288, backward nfe 65553, Train: 0.6250, Val: 0.4723, Test: 0.5060\n",
      "Epoch: 063, Runtime 1.662846, Loss 1.125277, forward nfe 209596, backward nfe 66583, Train: 0.6400, Val: 0.4738, Test: 0.5113\n",
      "Epoch: 064, Runtime 1.640798, Loss 1.093599, forward nfe 210944, backward nfe 67610, Train: 0.6150, Val: 0.4754, Test: 0.5151\n",
      "Epoch: 065, Runtime 1.619856, Loss 1.086928, forward nfe 212184, backward nfe 68637, Train: 0.6150, Val: 0.4854, Test: 0.5212\n",
      "Epoch: 066, Runtime 1.607334, Loss 1.055331, forward nfe 213387, backward nfe 69661, Train: 0.6350, Val: 0.4962, Test: 0.5306\n",
      "Epoch: 067, Runtime 1.595209, Loss 1.051717, forward nfe 214613, backward nfe 70688, Train: 0.6500, Val: 0.5023, Test: 0.5351\n",
      "Epoch: 068, Runtime 1.599495, Loss 1.036979, forward nfe 215784, backward nfe 71716, Train: 0.6700, Val: 0.5177, Test: 0.5437\n",
      "Epoch: 069, Runtime 1.597596, Loss 1.017876, forward nfe 216987, backward nfe 72740, Train: 0.6500, Val: 0.5262, Test: 0.5495\n",
      "Epoch: 070, Runtime 1.584970, Loss 1.018884, forward nfe 218138, backward nfe 73765, Train: 0.6700, Val: 0.5262, Test: 0.5529\n",
      "Epoch: 071, Runtime 1.584127, Loss 0.982607, forward nfe 219325, backward nfe 74789, Train: 0.6850, Val: 0.5123, Test: 0.5492\n",
      "Epoch: 072, Runtime 1.574764, Loss 0.983114, forward nfe 220431, backward nfe 75822, Train: 0.7050, Val: 0.5146, Test: 0.5477\n",
      "Epoch: 073, Runtime 1.573760, Loss 0.968643, forward nfe 221587, backward nfe 76854, Train: 0.7100, Val: 0.5354, Test: 0.5602\n",
      "Epoch: 074, Runtime 1.565144, Loss 0.968041, forward nfe 222681, backward nfe 77880, Train: 0.7050, Val: 0.5677, Test: 0.5840\n",
      "Epoch: 075, Runtime 1.557795, Loss 0.926782, forward nfe 223792, backward nfe 78906, Train: 0.7000, Val: 0.5908, Test: 0.6114\n",
      "Epoch: 076, Runtime 1.546764, Loss 0.917871, forward nfe 224877, backward nfe 79932, Train: 0.7200, Val: 0.5885, Test: 0.6046\n",
      "Epoch: 077, Runtime 1.549734, Loss 0.901303, forward nfe 225967, backward nfe 80961, Train: 0.7200, Val: 0.5631, Test: 0.5863\n",
      "Epoch: 078, Runtime 1.553345, Loss 0.896402, forward nfe 227033, backward nfe 81991, Train: 0.7200, Val: 0.5677, Test: 0.5957\n",
      "Epoch: 079, Runtime 1.549116, Loss 0.882788, forward nfe 228104, backward nfe 83016, Train: 0.7300, Val: 0.6023, Test: 0.6244\n",
      "Epoch: 080, Runtime 1.545634, Loss 0.865093, forward nfe 229188, backward nfe 84041, Train: 0.7350, Val: 0.6354, Test: 0.6517\n",
      "Epoch: 081, Runtime 1.544472, Loss 0.881883, forward nfe 230256, backward nfe 85068, Train: 0.7500, Val: 0.6354, Test: 0.6506\n",
      "Epoch: 082, Runtime 1.530647, Loss 0.869648, forward nfe 231305, backward nfe 86094, Train: 0.7350, Val: 0.6185, Test: 0.6402\n",
      "Epoch: 083, Runtime 1.538382, Loss 0.836598, forward nfe 232356, backward nfe 87119, Train: 0.7350, Val: 0.6246, Test: 0.6420\n",
      "Epoch: 084, Runtime 1.530297, Loss 0.836464, forward nfe 233411, backward nfe 88144, Train: 0.7400, Val: 0.6446, Test: 0.6659\n",
      "Epoch: 085, Runtime 1.521329, Loss 0.824649, forward nfe 234455, backward nfe 89169, Train: 0.7550, Val: 0.6592, Test: 0.6748\n",
      "Epoch: 086, Runtime 1.540197, Loss 0.799535, forward nfe 235488, backward nfe 90193, Train: 0.7650, Val: 0.6669, Test: 0.6763\n",
      "Epoch: 087, Runtime 1.528662, Loss 0.811303, forward nfe 236544, backward nfe 91217, Train: 0.7550, Val: 0.6469, Test: 0.6725\n",
      "Epoch: 088, Runtime 1.524773, Loss 0.802792, forward nfe 237573, backward nfe 92242, Train: 0.7500, Val: 0.6515, Test: 0.6726\n",
      "Epoch: 089, Runtime 1.520080, Loss 0.790896, forward nfe 238605, backward nfe 93267, Train: 0.7650, Val: 0.6838, Test: 0.6860\n",
      "Epoch: 090, Runtime 1.519765, Loss 0.776575, forward nfe 239618, backward nfe 94291, Train: 0.7750, Val: 0.6808, Test: 0.6859\n",
      "Epoch: 091, Runtime 1.510752, Loss 0.765583, forward nfe 240630, backward nfe 95315, Train: 0.7600, Val: 0.6577, Test: 0.6773\n",
      "Epoch: 092, Runtime 1.517826, Loss 0.774169, forward nfe 241643, backward nfe 96340, Train: 0.7750, Val: 0.6677, Test: 0.6859\n",
      "Epoch: 093, Runtime 1.504395, Loss 0.755827, forward nfe 242639, backward nfe 97364, Train: 0.7750, Val: 0.6915, Test: 0.6947\n",
      "Epoch: 094, Runtime 1.507451, Loss 0.747825, forward nfe 243641, backward nfe 98389, Train: 0.7750, Val: 0.6769, Test: 0.6916\n",
      "Epoch: 095, Runtime 1.520792, Loss 0.731182, forward nfe 244626, backward nfe 99417, Train: 0.7850, Val: 0.6669, Test: 0.6823\n",
      "Epoch: 096, Runtime 1.507211, Loss 0.719448, forward nfe 245628, backward nfe 100442, Train: 0.7850, Val: 0.6723, Test: 0.6856\n",
      "Epoch: 097, Runtime 1.501768, Loss 0.723989, forward nfe 246618, backward nfe 101467, Train: 0.7750, Val: 0.6746, Test: 0.6922\n",
      "Epoch: 098, Runtime 1.505484, Loss 0.708415, forward nfe 247598, backward nfe 102491, Train: 0.7800, Val: 0.6785, Test: 0.6943\n",
      "Epoch: 099, Runtime 1.507432, Loss 0.702376, forward nfe 248594, backward nfe 103517, Train: 0.7950, Val: 0.6692, Test: 0.6885\n",
      "best val accuracy 0.691538 with test accuracy 0.694662 at epoch 93\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.570420, Loss 2.315168, forward nfe 396, backward nfe 68, Train: 0.1200, Val: 0.0992, Test: 0.1008\n",
      "Epoch: 002, Runtime 1.492707, Loss 2.383670, forward nfe 1355, backward nfe 1092, Train: 0.1000, Val: 0.0223, Test: 0.0358\n",
      "Epoch: 003, Runtime 1.500543, Loss 2.516679, forward nfe 2315, backward nfe 2116, Train: 0.1000, Val: 0.0208, Test: 0.0358\n",
      "Epoch: 004, Runtime 1.506163, Loss 2.358468, forward nfe 3296, backward nfe 3140, Train: 0.1600, Val: 0.0315, Test: 0.0393\n",
      "Epoch: 005, Runtime 1.520661, Loss 2.251015, forward nfe 4309, backward nfe 4164, Train: 0.1550, Val: 0.0985, Test: 0.0966\n",
      "Epoch: 006, Runtime 1.555714, Loss 2.276353, forward nfe 5362, backward nfe 5192, Train: 0.1550, Val: 0.0985, Test: 0.0982\n",
      "Epoch: 007, Runtime 1.596006, Loss 2.270131, forward nfe 6469, backward nfe 6216, Train: 0.2350, Val: 0.1146, Test: 0.1182\n",
      "Epoch: 008, Runtime 1.721764, Loss 2.226402, forward nfe 7777, backward nfe 7257, Train: 0.2050, Val: 0.0608, Test: 0.0679\n",
      "Epoch: 009, Runtime 1.871542, Loss 2.200035, forward nfe 9330, backward nfe 8283, Train: 0.1900, Val: 0.0608, Test: 0.0701\n",
      "Epoch: 010, Runtime 2.163740, Loss 2.189430, forward nfe 11285, backward nfe 9328, Train: 0.1850, Val: 0.0623, Test: 0.0714\n",
      "Epoch: 011, Runtime 2.361780, Loss 2.175221, forward nfe 13957, backward nfe 10407, Train: 0.1900, Val: 0.0638, Test: 0.0732\n",
      "Epoch: 012, Runtime 2.634087, Loss 2.162057, forward nfe 17133, backward nfe 11438, Train: 0.2000, Val: 0.0631, Test: 0.0745\n",
      "Epoch: 013, Runtime 3.051331, Loss 2.125937, forward nfe 21062, backward nfe 12557, Train: 0.2050, Val: 0.0715, Test: 0.0794\n",
      "Epoch: 014, Runtime 3.506907, Loss 2.097304, forward nfe 25327, backward nfe 13676, Train: 0.2650, Val: 0.1354, Test: 0.1457\n",
      "Epoch: 015, Runtime 3.472657, Loss 2.089824, forward nfe 30537, backward nfe 14712, Train: 0.2750, Val: 0.1462, Test: 0.1545\n",
      "Epoch: 016, Runtime 3.783291, Loss 2.073495, forward nfe 36233, backward nfe 15831, Train: 0.2950, Val: 0.1523, Test: 0.1641\n",
      "Epoch: 017, Runtime 4.289528, Loss 2.039282, forward nfe 42465, backward nfe 16950, Train: 0.3200, Val: 0.1823, Test: 0.1952\n",
      "Epoch: 018, Runtime 4.940522, Loss 2.008475, forward nfe 50307, backward nfe 18014, Train: 0.3300, Val: 0.1923, Test: 0.2024\n",
      "Epoch: 019, Runtime 4.595746, Loss 1.993086, forward nfe 57732, backward nfe 19067, Train: 0.3300, Val: 0.2138, Test: 0.2149\n",
      "Epoch: 020, Runtime 4.817636, Loss 1.974422, forward nfe 65911, backward nfe 20182, Train: 0.3650, Val: 0.2408, Test: 0.2500\n",
      "Epoch: 021, Runtime 5.076402, Loss 1.950469, forward nfe 73802, backward nfe 21301, Train: 0.3900, Val: 0.2592, Test: 0.2706\n",
      "Epoch: 022, Runtime 5.189302, Loss 1.928249, forward nfe 82337, backward nfe 22420, Train: 0.3750, Val: 0.2608, Test: 0.2604\n",
      "Epoch: 023, Runtime 4.789418, Loss 1.901873, forward nfe 91005, backward nfe 23539, Train: 0.3700, Val: 0.2400, Test: 0.2428\n",
      "Epoch: 024, Runtime 3.769765, Loss 1.887010, forward nfe 97780, backward nfe 24606, Train: 0.3750, Val: 0.2400, Test: 0.2409\n",
      "Epoch: 025, Runtime 3.761690, Loss 1.864227, forward nfe 103295, backward nfe 25685, Train: 0.3750, Val: 0.2569, Test: 0.2538\n",
      "Epoch: 026, Runtime 3.670833, Loss 1.837029, forward nfe 109041, backward nfe 26724, Train: 0.3800, Val: 0.2777, Test: 0.2790\n",
      "Epoch: 027, Runtime 3.784593, Loss 1.811858, forward nfe 114746, backward nfe 27839, Train: 0.4150, Val: 0.2931, Test: 0.2964\n",
      "Epoch: 028, Runtime 3.817075, Loss 1.787133, forward nfe 120517, backward nfe 28958, Train: 0.4200, Val: 0.2977, Test: 0.3034\n",
      "Epoch: 029, Runtime 3.350007, Loss 1.768831, forward nfe 125798, backward nfe 30077, Train: 0.4150, Val: 0.3008, Test: 0.3113\n",
      "Epoch: 030, Runtime 3.330141, Loss 1.741147, forward nfe 130584, backward nfe 31196, Train: 0.4100, Val: 0.3131, Test: 0.3213\n",
      "Epoch: 031, Runtime 3.552532, Loss 1.715639, forward nfe 135476, backward nfe 32315, Train: 0.4150, Val: 0.3262, Test: 0.3352\n",
      "Epoch: 032, Runtime 3.177488, Loss 1.696023, forward nfe 140498, backward nfe 33361, Train: 0.4250, Val: 0.3508, Test: 0.3584\n",
      "Epoch: 033, Runtime 3.018643, Loss 1.667447, forward nfe 144706, backward nfe 34480, Train: 0.4600, Val: 0.3577, Test: 0.3692\n",
      "Epoch: 034, Runtime 2.888556, Loss 1.641144, forward nfe 148949, backward nfe 35520, Train: 0.4850, Val: 0.3669, Test: 0.3750\n",
      "Epoch: 035, Runtime 2.591016, Loss 1.621936, forward nfe 152434, backward nfe 36639, Train: 0.4800, Val: 0.3569, Test: 0.3760\n",
      "Epoch: 036, Runtime 2.568430, Loss 1.592833, forward nfe 155335, backward nfe 37757, Train: 0.5000, Val: 0.3415, Test: 0.3665\n",
      "Epoch: 037, Runtime 2.383378, Loss 1.570622, forward nfe 158363, backward nfe 38837, Train: 0.5300, Val: 0.3392, Test: 0.3739\n",
      "Epoch: 038, Runtime 2.492028, Loss 1.548219, forward nfe 161271, backward nfe 39934, Train: 0.5350, Val: 0.3562, Test: 0.3814\n",
      "Epoch: 039, Runtime 2.296688, Loss 1.526459, forward nfe 164104, backward nfe 40966, Train: 0.5300, Val: 0.3538, Test: 0.3902\n",
      "Epoch: 040, Runtime 2.296395, Loss 1.495773, forward nfe 166544, backward nfe 42085, Train: 0.5500, Val: 0.3562, Test: 0.3945\n",
      "Epoch: 041, Runtime 2.222598, Loss 1.477431, forward nfe 169122, backward nfe 43110, Train: 0.5600, Val: 0.3615, Test: 0.3979\n",
      "Epoch: 042, Runtime 2.181557, Loss 1.451692, forward nfe 171626, backward nfe 44193, Train: 0.5650, Val: 0.3685, Test: 0.4018\n",
      "Epoch: 043, Runtime 2.158938, Loss 1.428814, forward nfe 173915, backward nfe 45272, Train: 0.5750, Val: 0.3662, Test: 0.4033\n",
      "Epoch: 044, Runtime 1.894701, Loss 1.401179, forward nfe 175949, backward nfe 46302, Train: 0.5700, Val: 0.3738, Test: 0.4103\n",
      "Epoch: 045, Runtime 1.865686, Loss 1.392331, forward nfe 177718, backward nfe 47326, Train: 0.5800, Val: 0.3838, Test: 0.4145\n",
      "Epoch: 046, Runtime 1.884084, Loss 1.363795, forward nfe 179499, backward nfe 48356, Train: 0.5850, Val: 0.3846, Test: 0.4203\n",
      "Epoch: 047, Runtime 1.910431, Loss 1.325413, forward nfe 181349, backward nfe 49407, Train: 0.5900, Val: 0.3938, Test: 0.4290\n",
      "Epoch: 048, Runtime 1.849335, Loss 1.326739, forward nfe 183143, backward nfe 50440, Train: 0.5800, Val: 0.3877, Test: 0.4141\n",
      "Epoch: 049, Runtime 1.794479, Loss 1.287400, forward nfe 184730, backward nfe 51485, Train: 0.5350, Val: 0.3885, Test: 0.4087\n",
      "Epoch: 050, Runtime 1.774220, Loss 1.275167, forward nfe 186326, backward nfe 52523, Train: 0.5700, Val: 0.4162, Test: 0.4306\n",
      "Epoch: 051, Runtime 1.744990, Loss 1.255081, forward nfe 187837, backward nfe 53562, Train: 0.5850, Val: 0.4415, Test: 0.4556\n",
      "Epoch: 052, Runtime 1.694816, Loss 1.236041, forward nfe 189203, backward nfe 54628, Train: 0.5950, Val: 0.4208, Test: 0.4475\n",
      "Epoch: 053, Runtime 1.687473, Loss 1.212064, forward nfe 190534, backward nfe 55655, Train: 0.6150, Val: 0.4046, Test: 0.4411\n",
      "Epoch: 054, Runtime 1.666945, Loss 1.191298, forward nfe 191901, backward nfe 56679, Train: 0.6200, Val: 0.4038, Test: 0.4326\n",
      "Epoch: 055, Runtime 1.668290, Loss 1.172627, forward nfe 193216, backward nfe 57715, Train: 0.6050, Val: 0.3985, Test: 0.4238\n",
      "Epoch: 056, Runtime 1.649123, Loss 1.157901, forward nfe 194526, backward nfe 58744, Train: 0.6350, Val: 0.4146, Test: 0.4420\n",
      "Epoch: 057, Runtime 1.645743, Loss 1.134601, forward nfe 195848, backward nfe 59774, Train: 0.6400, Val: 0.4262, Test: 0.4593\n",
      "Epoch: 058, Runtime 1.606417, Loss 1.109943, forward nfe 197075, backward nfe 60799, Train: 0.6450, Val: 0.4308, Test: 0.4634\n",
      "Epoch: 059, Runtime 1.603259, Loss 1.111905, forward nfe 198255, backward nfe 61830, Train: 0.6450, Val: 0.4315, Test: 0.4646\n",
      "Epoch: 060, Runtime 1.599396, Loss 1.067616, forward nfe 199442, backward nfe 62857, Train: 0.6400, Val: 0.4215, Test: 0.4540\n",
      "Epoch: 061, Runtime 1.603930, Loss 1.063512, forward nfe 200636, backward nfe 63888, Train: 0.6650, Val: 0.4185, Test: 0.4577\n",
      "Epoch: 062, Runtime 1.590426, Loss 1.039668, forward nfe 201828, backward nfe 64917, Train: 0.6550, Val: 0.4254, Test: 0.4577\n",
      "Epoch: 063, Runtime 1.584112, Loss 1.019708, forward nfe 202992, backward nfe 65946, Train: 0.6800, Val: 0.4354, Test: 0.4706\n",
      "Epoch: 064, Runtime 1.556444, Loss 1.001030, forward nfe 204102, backward nfe 66971, Train: 0.6800, Val: 0.4331, Test: 0.4699\n",
      "Epoch: 065, Runtime 1.557009, Loss 0.985737, forward nfe 205193, backward nfe 67996, Train: 0.6450, Val: 0.4246, Test: 0.4676\n",
      "Epoch: 066, Runtime 1.558922, Loss 0.970880, forward nfe 206308, backward nfe 69020, Train: 0.6650, Val: 0.4292, Test: 0.4729\n",
      "Epoch: 067, Runtime 1.546610, Loss 0.954352, forward nfe 207397, backward nfe 70044, Train: 0.6850, Val: 0.4438, Test: 0.4882\n",
      "Epoch: 068, Runtime 1.540304, Loss 0.936137, forward nfe 208452, backward nfe 71072, Train: 0.7000, Val: 0.4515, Test: 0.5009\n",
      "Epoch: 069, Runtime 1.541039, Loss 0.919963, forward nfe 209523, backward nfe 72097, Train: 0.7000, Val: 0.4546, Test: 0.4998\n",
      "Epoch: 070, Runtime 1.524185, Loss 0.910036, forward nfe 210568, backward nfe 73121, Train: 0.7100, Val: 0.4592, Test: 0.5034\n",
      "Epoch: 071, Runtime 1.545343, Loss 0.881090, forward nfe 211618, backward nfe 74154, Train: 0.7200, Val: 0.4685, Test: 0.5068\n",
      "Epoch: 072, Runtime 1.529251, Loss 0.865407, forward nfe 212668, backward nfe 75178, Train: 0.7300, Val: 0.4646, Test: 0.5083\n",
      "Epoch: 073, Runtime 1.525652, Loss 0.845643, forward nfe 213706, backward nfe 76202, Train: 0.7100, Val: 0.4585, Test: 0.5066\n",
      "Epoch: 074, Runtime 1.524419, Loss 0.849674, forward nfe 214734, backward nfe 77228, Train: 0.7400, Val: 0.4731, Test: 0.5162\n",
      "Epoch: 075, Runtime 1.523772, Loss 0.820481, forward nfe 215764, backward nfe 78255, Train: 0.7600, Val: 0.4823, Test: 0.5241\n",
      "Epoch: 076, Runtime 1.516517, Loss 0.803250, forward nfe 216770, backward nfe 79279, Train: 0.7400, Val: 0.4769, Test: 0.5210\n",
      "Epoch: 077, Runtime 1.518117, Loss 0.789042, forward nfe 217797, backward nfe 80303, Train: 0.7400, Val: 0.4808, Test: 0.5262\n",
      "Epoch: 078, Runtime 1.511856, Loss 0.783744, forward nfe 218802, backward nfe 81327, Train: 0.7450, Val: 0.5054, Test: 0.5480\n",
      "Epoch: 079, Runtime 1.509114, Loss 0.765790, forward nfe 219808, backward nfe 82351, Train: 0.7600, Val: 0.5177, Test: 0.5628\n",
      "Epoch: 080, Runtime 1.515061, Loss 0.751864, forward nfe 220817, backward nfe 83380, Train: 0.7600, Val: 0.5200, Test: 0.5604\n",
      "Epoch: 081, Runtime 1.503401, Loss 0.744140, forward nfe 221803, backward nfe 84405, Train: 0.7700, Val: 0.5062, Test: 0.5470\n",
      "Epoch: 082, Runtime 1.507563, Loss 0.720598, forward nfe 222791, backward nfe 85430, Train: 0.7650, Val: 0.5146, Test: 0.5552\n",
      "Epoch: 083, Runtime 1.501014, Loss 0.720296, forward nfe 223780, backward nfe 86454, Train: 0.7800, Val: 0.5277, Test: 0.5692\n",
      "Epoch: 084, Runtime 1.512967, Loss 0.695319, forward nfe 224775, backward nfe 87479, Train: 0.7750, Val: 0.5469, Test: 0.5763\n",
      "Epoch: 085, Runtime 1.505551, Loss 0.682124, forward nfe 225777, backward nfe 88505, Train: 0.7850, Val: 0.5769, Test: 0.5977\n",
      "Epoch: 086, Runtime 1.508996, Loss 0.679187, forward nfe 226760, backward nfe 89533, Train: 0.7950, Val: 0.5954, Test: 0.6157\n",
      "Epoch: 087, Runtime 1.500921, Loss 0.662823, forward nfe 227737, backward nfe 90558, Train: 0.7900, Val: 0.5985, Test: 0.6161\n",
      "Epoch: 088, Runtime 1.499904, Loss 0.650855, forward nfe 228715, backward nfe 91582, Train: 0.8000, Val: 0.5823, Test: 0.6074\n",
      "Epoch: 089, Runtime 1.500076, Loss 0.645253, forward nfe 229694, backward nfe 92606, Train: 0.8050, Val: 0.6077, Test: 0.6348\n",
      "Epoch: 090, Runtime 1.502670, Loss 0.626859, forward nfe 230682, backward nfe 93629, Train: 0.8200, Val: 0.6538, Test: 0.6715\n",
      "Epoch: 091, Runtime 1.500131, Loss 0.620144, forward nfe 231660, backward nfe 94654, Train: 0.8300, Val: 0.6585, Test: 0.6758\n",
      "Epoch: 092, Runtime 1.496722, Loss 0.622350, forward nfe 232639, backward nfe 95678, Train: 0.8250, Val: 0.6423, Test: 0.6599\n",
      "Epoch: 093, Runtime 1.497698, Loss 0.598781, forward nfe 233607, backward nfe 96702, Train: 0.8300, Val: 0.6623, Test: 0.6762\n",
      "Epoch: 094, Runtime 1.495053, Loss 0.586203, forward nfe 234579, backward nfe 97726, Train: 0.8350, Val: 0.6754, Test: 0.6920\n",
      "Epoch: 095, Runtime 1.495537, Loss 0.564876, forward nfe 235548, backward nfe 98749, Train: 0.8500, Val: 0.7085, Test: 0.7172\n",
      "Epoch: 096, Runtime 1.493824, Loss 0.544467, forward nfe 236517, backward nfe 99772, Train: 0.8550, Val: 0.7092, Test: 0.7188\n",
      "Epoch: 097, Runtime 1.489284, Loss 0.555026, forward nfe 237479, backward nfe 100796, Train: 0.8500, Val: 0.7085, Test: 0.7187\n",
      "Epoch: 098, Runtime 1.492362, Loss 0.532009, forward nfe 238441, backward nfe 101819, Train: 0.8500, Val: 0.7108, Test: 0.7224\n",
      "Epoch: 099, Runtime 1.495463, Loss 0.524842, forward nfe 239408, backward nfe 102842, Train: 0.8500, Val: 0.7154, Test: 0.7271\n",
      "best val accuracy 0.715385 with test accuracy 0.727147 at epoch 99\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.582229, Loss 2.313485, forward nfe 422, backward nfe 68, Train: 0.1000, Val: 0.0162, Test: 0.0218\n",
      "Epoch: 002, Runtime 1.486365, Loss 2.699940, forward nfe 1380, backward nfe 1092, Train: 0.1000, Val: 0.0162, Test: 0.0218\n",
      "Epoch: 003, Runtime 1.493901, Loss 2.333807, forward nfe 2344, backward nfe 2116, Train: 0.1050, Val: 0.0162, Test: 0.0218\n",
      "Epoch: 004, Runtime 1.508179, Loss 2.302051, forward nfe 3339, backward nfe 3142, Train: 0.1050, Val: 0.0192, Test: 0.0327\n",
      "Epoch: 005, Runtime 1.519022, Loss 2.288880, forward nfe 4346, backward nfe 4166, Train: 0.1400, Val: 0.0385, Test: 0.0520\n",
      "Epoch: 006, Runtime 1.540561, Loss 2.268724, forward nfe 5381, backward nfe 5190, Train: 0.1600, Val: 0.0469, Test: 0.0608\n",
      "Epoch: 007, Runtime 1.594346, Loss 2.236792, forward nfe 6484, backward nfe 6217, Train: 0.1600, Val: 0.0546, Test: 0.0628\n",
      "Epoch: 008, Runtime 1.750364, Loss 2.260332, forward nfe 7767, backward nfe 7264, Train: 0.1800, Val: 0.0600, Test: 0.0699\n",
      "Epoch: 009, Runtime 1.860135, Loss 2.233161, forward nfe 9379, backward nfe 8299, Train: 0.1900, Val: 0.0623, Test: 0.0747\n",
      "Epoch: 010, Runtime 2.131715, Loss 2.218086, forward nfe 11338, backward nfe 9344, Train: 0.1850, Val: 0.0623, Test: 0.0733\n",
      "Epoch: 011, Runtime 2.408912, Loss 2.216680, forward nfe 13910, backward nfe 10371, Train: 0.1900, Val: 0.0615, Test: 0.0766\n",
      "Epoch: 012, Runtime 2.886301, Loss 2.200161, forward nfe 17239, backward nfe 11461, Train: 0.1850, Val: 0.0615, Test: 0.0744\n",
      "Epoch: 013, Runtime 3.444248, Loss 2.182195, forward nfe 21475, backward nfe 12580, Train: 0.1800, Val: 0.0638, Test: 0.0724\n",
      "Epoch: 014, Runtime 4.244580, Loss 2.173416, forward nfe 27513, backward nfe 13699, Train: 0.1800, Val: 0.0646, Test: 0.0740\n",
      "Epoch: 015, Runtime 4.968001, Loss 2.158704, forward nfe 34567, backward nfe 14818, Train: 0.1950, Val: 0.0677, Test: 0.0808\n",
      "Epoch: 016, Runtime 5.270780, Loss 2.139406, forward nfe 43442, backward nfe 15864, Train: 0.2050, Val: 0.0823, Test: 0.0912\n",
      "Epoch: 017, Runtime 6.146042, Loss 2.127839, forward nfe 53119, backward nfe 16942, Train: 0.2050, Val: 0.0785, Test: 0.0876\n",
      "Epoch: 018, Runtime 6.063869, Loss 2.112903, forward nfe 64286, backward nfe 18061, Train: 0.1950, Val: 0.0677, Test: 0.0776\n",
      "Epoch: 019, Runtime 6.389600, Loss 2.101043, forward nfe 74954, backward nfe 19180, Train: 0.1950, Val: 0.0677, Test: 0.0778\n",
      "Epoch: 020, Runtime 5.552108, Loss 2.082910, forward nfe 86509, backward nfe 20299, Train: 0.2050, Val: 0.0785, Test: 0.0882\n",
      "Epoch: 021, Runtime 5.634232, Loss 2.067524, forward nfe 95364, backward nfe 21418, Train: 0.2050, Val: 0.0777, Test: 0.0904\n",
      "Epoch: 022, Runtime 5.632815, Loss 2.056437, forward nfe 104710, backward nfe 22537, Train: 0.2050, Val: 0.0738, Test: 0.0855\n",
      "Epoch: 023, Runtime 8.201956, Loss 2.034410, forward nfe 120835, backward nfe 23656, Train: 0.2050, Val: 0.0685, Test: 0.0805\n",
      "Epoch: 024, Runtime 5.431233, Loss 2.027079, forward nfe 130217, backward nfe 24775, Train: 0.2050, Val: 0.0792, Test: 0.0920\n",
      "Epoch: 025, Runtime 5.781885, Loss 2.006804, forward nfe 140220, backward nfe 25894, Train: 0.2350, Val: 0.0892, Test: 0.1050\n",
      "Epoch: 026, Runtime 5.243722, Loss 1.988722, forward nfe 148525, backward nfe 27013, Train: 0.2500, Val: 0.0962, Test: 0.1192\n",
      "Epoch: 027, Runtime 5.013388, Loss 1.970111, forward nfe 157561, backward nfe 28132, Train: 0.2350, Val: 0.0869, Test: 0.1062\n",
      "Epoch: 028, Runtime 5.158442, Loss 1.958559, forward nfe 166052, backward nfe 29251, Train: 0.2700, Val: 0.1092, Test: 0.1357\n",
      "Epoch: 029, Runtime 5.167124, Loss 1.933865, forward nfe 174666, backward nfe 30292, Train: 0.3000, Val: 0.1223, Test: 0.1465\n",
      "Epoch: 030, Runtime 4.855611, Loss 1.919409, forward nfe 183407, backward nfe 31411, Train: 0.3050, Val: 0.1208, Test: 0.1468\n",
      "Epoch: 031, Runtime 5.202715, Loss 1.896505, forward nfe 191496, backward nfe 32530, Train: 0.3000, Val: 0.1085, Test: 0.1369\n",
      "Epoch: 032, Runtime 5.228832, Loss 1.880282, forward nfe 200500, backward nfe 33649, Train: 0.3500, Val: 0.1285, Test: 0.1596\n",
      "Epoch: 033, Runtime 4.995314, Loss 1.863902, forward nfe 209199, backward nfe 34768, Train: 0.3600, Val: 0.1392, Test: 0.1725\n",
      "Epoch: 034, Runtime 4.522828, Loss 1.842010, forward nfe 216660, backward nfe 35887, Train: 0.3650, Val: 0.1408, Test: 0.1734\n",
      "Epoch: 035, Runtime 3.985798, Loss 1.820551, forward nfe 223748, backward nfe 37006, Train: 0.3550, Val: 0.1423, Test: 0.1662\n",
      "Epoch: 036, Runtime 3.773889, Loss 1.805655, forward nfe 229417, backward nfe 38125, Train: 0.3650, Val: 0.1508, Test: 0.1729\n",
      "Epoch: 037, Runtime 3.815989, Loss 1.786510, forward nfe 235402, backward nfe 39244, Train: 0.3850, Val: 0.1654, Test: 0.1898\n",
      "Epoch: 038, Runtime 3.203448, Loss 1.772362, forward nfe 240154, backward nfe 40295, Train: 0.3800, Val: 0.1669, Test: 0.1932\n",
      "Epoch: 039, Runtime 3.401277, Loss 1.754224, forward nfe 245153, backward nfe 41324, Train: 0.3450, Val: 0.1646, Test: 0.1871\n",
      "Epoch: 040, Runtime 3.445139, Loss 1.736941, forward nfe 250101, backward nfe 42443, Train: 0.3550, Val: 0.1646, Test: 0.1858\n",
      "Epoch: 041, Runtime 3.286198, Loss 1.724980, forward nfe 255097, backward nfe 43509, Train: 0.3600, Val: 0.1700, Test: 0.1924\n",
      "Epoch: 042, Runtime 3.089361, Loss 1.708716, forward nfe 259286, backward nfe 44628, Train: 0.3700, Val: 0.1746, Test: 0.1903\n",
      "Epoch: 043, Runtime 2.922619, Loss 1.687960, forward nfe 263769, backward nfe 45717, Train: 0.3500, Val: 0.1685, Test: 0.1853\n",
      "Epoch: 044, Runtime 3.109315, Loss 1.666672, forward nfe 267965, backward nfe 46836, Train: 0.3500, Val: 0.1623, Test: 0.1849\n",
      "Epoch: 045, Runtime 2.607157, Loss 1.652880, forward nfe 271523, backward nfe 47886, Train: 0.3650, Val: 0.1608, Test: 0.1875\n",
      "Epoch: 046, Runtime 2.430436, Loss 1.636762, forward nfe 274290, backward nfe 49005, Train: 0.3800, Val: 0.1623, Test: 0.1889\n",
      "Epoch: 047, Runtime 2.405347, Loss 1.622413, forward nfe 277417, backward nfe 50056, Train: 0.3750, Val: 0.1923, Test: 0.2068\n",
      "Epoch: 048, Runtime 2.519522, Loss 1.599782, forward nfe 280213, backward nfe 51175, Train: 0.4550, Val: 0.2369, Test: 0.2569\n",
      "Epoch: 049, Runtime 2.396780, Loss 1.590286, forward nfe 283052, backward nfe 52294, Train: 0.4850, Val: 0.2846, Test: 0.2991\n",
      "Epoch: 050, Runtime 2.207936, Loss 1.546654, forward nfe 285734, backward nfe 53329, Train: 0.5050, Val: 0.3000, Test: 0.3153\n",
      "Epoch: 051, Runtime 2.302540, Loss 1.521568, forward nfe 288199, backward nfe 54355, Train: 0.5050, Val: 0.2885, Test: 0.3050\n",
      "Epoch: 052, Runtime 2.349421, Loss 1.477894, forward nfe 290952, backward nfe 55468, Train: 0.5100, Val: 0.2877, Test: 0.3078\n",
      "Epoch: 053, Runtime 2.338466, Loss 1.442890, forward nfe 293837, backward nfe 56550, Train: 0.4700, Val: 0.2508, Test: 0.2800\n",
      "Epoch: 054, Runtime 2.124605, Loss 1.422894, forward nfe 296061, backward nfe 57669, Train: 0.5650, Val: 0.2846, Test: 0.3114\n",
      "Epoch: 055, Runtime 2.173831, Loss 1.372021, forward nfe 298126, backward nfe 58788, Train: 0.5700, Val: 0.4177, Test: 0.4422\n",
      "Epoch: 056, Runtime 2.096649, Loss 1.343777, forward nfe 300418, backward nfe 59847, Train: 0.6750, Val: 0.6285, Test: 0.6483\n",
      "Epoch: 057, Runtime 2.085956, Loss 1.296198, forward nfe 302569, backward nfe 60956, Train: 0.6700, Val: 0.6477, Test: 0.6615\n",
      "Epoch: 058, Runtime 2.117057, Loss 1.266595, forward nfe 304747, backward nfe 62000, Train: 0.6650, Val: 0.6462, Test: 0.6508\n",
      "Epoch: 059, Runtime 2.249793, Loss 1.241116, forward nfe 306939, backward nfe 63119, Train: 0.6700, Val: 0.6538, Test: 0.6445\n",
      "Epoch: 060, Runtime 2.185374, Loss 1.211241, forward nfe 309617, backward nfe 64190, Train: 0.6700, Val: 0.6338, Test: 0.6340\n",
      "Epoch: 061, Runtime 2.075094, Loss 1.170930, forward nfe 311743, backward nfe 65215, Train: 0.6750, Val: 0.6469, Test: 0.6397\n",
      "Epoch: 062, Runtime 2.110133, Loss 1.153476, forward nfe 313850, backward nfe 66334, Train: 0.7200, Val: 0.6608, Test: 0.6611\n",
      "Epoch: 063, Runtime 2.208218, Loss 1.124544, forward nfe 316101, backward nfe 67407, Train: 0.7600, Val: 0.6846, Test: 0.6907\n",
      "Epoch: 064, Runtime 2.181668, Loss 1.083181, forward nfe 318516, backward nfe 68435, Train: 0.7450, Val: 0.7000, Test: 0.7052\n",
      "Epoch: 065, Runtime 2.057426, Loss 1.059095, forward nfe 320928, backward nfe 69467, Train: 0.7750, Val: 0.6885, Test: 0.6916\n",
      "Epoch: 066, Runtime 2.140113, Loss 1.021733, forward nfe 323105, backward nfe 70522, Train: 0.7850, Val: 0.6854, Test: 0.6895\n",
      "Epoch: 067, Runtime 2.071535, Loss 0.992553, forward nfe 325365, backward nfe 71579, Train: 0.7750, Val: 0.6854, Test: 0.6898\n",
      "Epoch: 068, Runtime 2.029458, Loss 0.970112, forward nfe 327414, backward nfe 72628, Train: 0.7800, Val: 0.6885, Test: 0.6924\n",
      "Epoch: 069, Runtime 1.998499, Loss 0.942123, forward nfe 329437, backward nfe 73671, Train: 0.7750, Val: 0.7000, Test: 0.6966\n",
      "Epoch: 070, Runtime 1.982244, Loss 0.914505, forward nfe 331447, backward nfe 74735, Train: 0.7800, Val: 0.7100, Test: 0.7098\n",
      "Epoch: 071, Runtime 1.916236, Loss 0.873479, forward nfe 333358, backward nfe 75770, Train: 0.7900, Val: 0.7177, Test: 0.7179\n",
      "Epoch: 072, Runtime 1.918940, Loss 0.842266, forward nfe 335176, backward nfe 76817, Train: 0.8200, Val: 0.7131, Test: 0.7143\n",
      "Epoch: 073, Runtime 1.916252, Loss 0.827509, forward nfe 337127, backward nfe 77852, Train: 0.8150, Val: 0.7092, Test: 0.7088\n",
      "Epoch: 074, Runtime 1.892704, Loss 0.809012, forward nfe 338890, backward nfe 78899, Train: 0.8250, Val: 0.7154, Test: 0.7133\n",
      "Epoch: 075, Runtime 1.808421, Loss 0.768297, forward nfe 340627, backward nfe 79935, Train: 0.8350, Val: 0.7285, Test: 0.7316\n",
      "Epoch: 076, Runtime 1.783061, Loss 0.737948, forward nfe 342196, backward nfe 80961, Train: 0.8350, Val: 0.7323, Test: 0.7383\n",
      "Epoch: 077, Runtime 1.843199, Loss 0.710918, forward nfe 343861, backward nfe 81987, Train: 0.8450, Val: 0.7323, Test: 0.7431\n",
      "Epoch: 078, Runtime 1.767980, Loss 0.699766, forward nfe 345470, backward nfe 83029, Train: 0.8500, Val: 0.7269, Test: 0.7399\n",
      "Epoch: 079, Runtime 1.760895, Loss 0.658054, forward nfe 347004, backward nfe 84073, Train: 0.8600, Val: 0.7323, Test: 0.7418\n",
      "Epoch: 080, Runtime 1.763937, Loss 0.638592, forward nfe 348563, backward nfe 85107, Train: 0.8700, Val: 0.7346, Test: 0.7442\n",
      "Epoch: 081, Runtime 1.778730, Loss 0.617623, forward nfe 350059, backward nfe 86137, Train: 0.8750, Val: 0.7369, Test: 0.7517\n",
      "Epoch: 082, Runtime 1.754473, Loss 0.609712, forward nfe 351601, backward nfe 87188, Train: 0.8800, Val: 0.7308, Test: 0.7496\n",
      "Epoch: 083, Runtime 1.704675, Loss 0.575646, forward nfe 353084, backward nfe 88234, Train: 0.8750, Val: 0.7315, Test: 0.7507\n",
      "Epoch: 084, Runtime 1.713395, Loss 0.554898, forward nfe 354454, backward nfe 89261, Train: 0.8650, Val: 0.7369, Test: 0.7573\n",
      "Epoch: 085, Runtime 1.714576, Loss 0.540003, forward nfe 355869, backward nfe 90293, Train: 0.8750, Val: 0.7392, Test: 0.7582\n",
      "Epoch: 086, Runtime 1.684051, Loss 0.516276, forward nfe 357285, backward nfe 91322, Train: 0.8800, Val: 0.7323, Test: 0.7524\n",
      "Epoch: 087, Runtime 1.689187, Loss 0.509943, forward nfe 358667, backward nfe 92352, Train: 0.8750, Val: 0.7362, Test: 0.7543\n",
      "Epoch: 088, Runtime 1.675596, Loss 0.499062, forward nfe 360035, backward nfe 93384, Train: 0.8750, Val: 0.7492, Test: 0.7655\n",
      "Epoch: 089, Runtime 1.668662, Loss 0.489899, forward nfe 361380, backward nfe 94412, Train: 0.8800, Val: 0.7508, Test: 0.7687\n",
      "Epoch: 090, Runtime 1.650746, Loss 0.479654, forward nfe 362670, backward nfe 95447, Train: 0.8800, Val: 0.7485, Test: 0.7666\n",
      "Epoch: 091, Runtime 1.679338, Loss 0.456874, forward nfe 363972, backward nfe 96499, Train: 0.8850, Val: 0.7431, Test: 0.7608\n",
      "Epoch: 092, Runtime 1.636235, Loss 0.450408, forward nfe 365285, backward nfe 97523, Train: 0.8850, Val: 0.7454, Test: 0.7693\n",
      "Epoch: 093, Runtime 1.665604, Loss 0.436085, forward nfe 366590, backward nfe 98552, Train: 0.8750, Val: 0.7546, Test: 0.7761\n",
      "Epoch: 094, Runtime 1.651690, Loss 0.431857, forward nfe 367903, backward nfe 99588, Train: 0.8800, Val: 0.7523, Test: 0.7734\n",
      "Epoch: 095, Runtime 1.608359, Loss 0.432398, forward nfe 369154, backward nfe 100612, Train: 0.8950, Val: 0.7454, Test: 0.7672\n",
      "Epoch: 096, Runtime 1.663106, Loss 0.413440, forward nfe 370402, backward nfe 101640, Train: 0.8850, Val: 0.7485, Test: 0.7699\n",
      "Epoch: 097, Runtime 1.643056, Loss 0.400370, forward nfe 371730, backward nfe 102669, Train: 0.8800, Val: 0.7546, Test: 0.7781\n",
      "Epoch: 098, Runtime 1.667837, Loss 0.397597, forward nfe 373059, backward nfe 103694, Train: 0.8850, Val: 0.7585, Test: 0.7797\n",
      "Epoch: 099, Runtime 1.630615, Loss 0.388959, forward nfe 374313, backward nfe 104720, Train: 0.8900, Val: 0.7631, Test: 0.7828\n",
      "best val accuracy 0.763077 with test accuracy 0.782811 at epoch 99\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.565933, Loss 2.312523, forward nfe 390, backward nfe 67, Train: 0.1150, Val: 0.1169, Test: 0.1106\n",
      "Epoch: 002, Runtime 1.492145, Loss 2.512814, forward nfe 1350, backward nfe 1091, Train: 0.1150, Val: 0.0131, Test: 0.0233\n",
      "Epoch: 003, Runtime 1.494967, Loss 2.332627, forward nfe 2313, backward nfe 2115, Train: 0.1450, Val: 0.1700, Test: 0.1643\n",
      "Epoch: 004, Runtime 1.502766, Loss 2.294734, forward nfe 3303, backward nfe 3139, Train: 0.1650, Val: 0.2892, Test: 0.2795\n",
      "Epoch: 005, Runtime 1.516580, Loss 2.210050, forward nfe 4305, backward nfe 4165, Train: 0.1900, Val: 0.3131, Test: 0.3149\n",
      "Epoch: 006, Runtime 1.530054, Loss 2.208198, forward nfe 5316, backward nfe 5191, Train: 0.2250, Val: 0.4185, Test: 0.4146\n",
      "Epoch: 007, Runtime 1.580223, Loss 2.197992, forward nfe 6397, backward nfe 6215, Train: 0.2500, Val: 0.4262, Test: 0.4163\n",
      "Epoch: 008, Runtime 1.637632, Loss 2.155524, forward nfe 7608, backward nfe 7254, Train: 0.3950, Val: 0.3315, Test: 0.3546\n",
      "Epoch: 009, Runtime 1.697965, Loss 2.105747, forward nfe 8949, backward nfe 8291, Train: 0.2850, Val: 0.1585, Test: 0.1730\n",
      "Epoch: 010, Runtime 1.738121, Loss 2.093302, forward nfe 10372, backward nfe 9317, Train: 0.2450, Val: 0.1377, Test: 0.1474\n",
      "Epoch: 011, Runtime 1.801272, Loss 2.065517, forward nfe 11942, backward nfe 10350, Train: 0.3900, Val: 0.1885, Test: 0.2173\n",
      "Epoch: 012, Runtime 1.866720, Loss 2.014071, forward nfe 13579, backward nfe 11391, Train: 0.3700, Val: 0.2277, Test: 0.2507\n",
      "Epoch: 013, Runtime 2.087225, Loss 1.983231, forward nfe 15513, backward nfe 12418, Train: 0.4550, Val: 0.2762, Test: 0.3015\n",
      "Epoch: 014, Runtime 2.297134, Loss 1.952884, forward nfe 17946, backward nfe 13537, Train: 0.4950, Val: 0.2662, Test: 0.2988\n",
      "Epoch: 015, Runtime 2.350972, Loss 1.903938, forward nfe 20493, backward nfe 14582, Train: 0.4600, Val: 0.2185, Test: 0.2611\n",
      "Epoch: 016, Runtime 2.450053, Loss 1.891863, forward nfe 23495, backward nfe 15688, Train: 0.5150, Val: 0.2323, Test: 0.2689\n",
      "Epoch: 017, Runtime 2.548329, Loss 1.837724, forward nfe 26412, backward nfe 16719, Train: 0.5250, Val: 0.2262, Test: 0.2631\n",
      "Epoch: 018, Runtime 2.666910, Loss 1.801816, forward nfe 29768, backward nfe 17838, Train: 0.5650, Val: 0.2754, Test: 0.3188\n",
      "Epoch: 019, Runtime 2.663002, Loss 1.738338, forward nfe 33156, backward nfe 18867, Train: 0.5900, Val: 0.2915, Test: 0.3355\n",
      "Epoch: 020, Runtime 2.560649, Loss 1.715913, forward nfe 36582, backward nfe 19896, Train: 0.6400, Val: 0.3292, Test: 0.3773\n",
      "Epoch: 021, Runtime 2.745606, Loss 1.653949, forward nfe 39982, backward nfe 20946, Train: 0.6250, Val: 0.3238, Test: 0.3769\n",
      "Epoch: 022, Runtime 2.727410, Loss 1.631457, forward nfe 43526, backward nfe 22023, Train: 0.6400, Val: 0.4008, Test: 0.4403\n",
      "Epoch: 023, Runtime 2.461821, Loss 1.583202, forward nfe 46894, backward nfe 23051, Train: 0.6750, Val: 0.4900, Test: 0.5302\n",
      "Epoch: 024, Runtime 2.305885, Loss 1.535447, forward nfe 49790, backward nfe 24087, Train: 0.6700, Val: 0.4808, Test: 0.5167\n",
      "Epoch: 025, Runtime 2.212209, Loss 1.505890, forward nfe 52356, backward nfe 25123, Train: 0.6950, Val: 0.4146, Test: 0.4660\n",
      "Epoch: 026, Runtime 2.254460, Loss 1.456080, forward nfe 54761, backward nfe 26210, Train: 0.7400, Val: 0.4008, Test: 0.4604\n",
      "Epoch: 027, Runtime 2.213090, Loss 1.429518, forward nfe 57134, backward nfe 27329, Train: 0.7300, Val: 0.4031, Test: 0.4525\n",
      "Epoch: 028, Runtime 2.188135, Loss 1.380019, forward nfe 59703, backward nfe 28354, Train: 0.7050, Val: 0.4077, Test: 0.4651\n",
      "Epoch: 029, Runtime 2.228771, Loss 1.335534, forward nfe 62112, backward nfe 29401, Train: 0.6650, Val: 0.4392, Test: 0.4873\n",
      "Epoch: 030, Runtime 2.147158, Loss 1.307413, forward nfe 64470, backward nfe 30471, Train: 0.6650, Val: 0.4500, Test: 0.5007\n",
      "Epoch: 031, Runtime 2.210339, Loss 1.277513, forward nfe 66919, backward nfe 31534, Train: 0.7100, Val: 0.4108, Test: 0.4683\n",
      "Epoch: 032, Runtime 2.186552, Loss 1.238763, forward nfe 69297, backward nfe 32603, Train: 0.7500, Val: 0.4169, Test: 0.4798\n",
      "Epoch: 033, Runtime 2.098943, Loss 1.200179, forward nfe 71601, backward nfe 33639, Train: 0.7400, Val: 0.4385, Test: 0.4961\n",
      "Epoch: 034, Runtime 2.020458, Loss 1.174666, forward nfe 73769, backward nfe 34670, Train: 0.7350, Val: 0.4585, Test: 0.5170\n",
      "Epoch: 035, Runtime 1.936514, Loss 1.142680, forward nfe 75669, backward nfe 35700, Train: 0.7500, Val: 0.5108, Test: 0.5544\n",
      "Epoch: 036, Runtime 1.937481, Loss 1.117888, forward nfe 77652, backward nfe 36729, Train: 0.7700, Val: 0.5654, Test: 0.6070\n",
      "Epoch: 037, Runtime 1.920536, Loss 1.080467, forward nfe 79507, backward nfe 37785, Train: 0.7900, Val: 0.6285, Test: 0.6710\n",
      "Epoch: 038, Runtime 1.915625, Loss 1.044368, forward nfe 81394, backward nfe 38817, Train: 0.8050, Val: 0.6531, Test: 0.6934\n",
      "Epoch: 039, Runtime 1.911496, Loss 1.029454, forward nfe 83196, backward nfe 39861, Train: 0.8150, Val: 0.6592, Test: 0.6981\n",
      "Epoch: 040, Runtime 1.892704, Loss 0.996280, forward nfe 85016, backward nfe 40888, Train: 0.8100, Val: 0.6554, Test: 0.7017\n",
      "Epoch: 041, Runtime 1.852429, Loss 0.970195, forward nfe 86833, backward nfe 41914, Train: 0.8100, Val: 0.6569, Test: 0.6991\n",
      "Epoch: 042, Runtime 1.815982, Loss 0.945411, forward nfe 88582, backward nfe 42953, Train: 0.8050, Val: 0.6538, Test: 0.6930\n",
      "Epoch: 043, Runtime 1.817535, Loss 0.927245, forward nfe 90216, backward nfe 43982, Train: 0.8100, Val: 0.6692, Test: 0.7080\n",
      "Epoch: 044, Runtime 1.772732, Loss 0.903323, forward nfe 91759, backward nfe 45017, Train: 0.8300, Val: 0.6708, Test: 0.7112\n",
      "Epoch: 045, Runtime 1.781076, Loss 0.890584, forward nfe 93259, backward nfe 46064, Train: 0.8300, Val: 0.6754, Test: 0.7161\n",
      "Epoch: 046, Runtime 1.784895, Loss 0.866246, forward nfe 94811, backward nfe 47110, Train: 0.8200, Val: 0.6777, Test: 0.7162\n",
      "Epoch: 047, Runtime 1.788804, Loss 0.839759, forward nfe 96325, backward nfe 48139, Train: 0.8300, Val: 0.6846, Test: 0.7220\n",
      "Epoch: 048, Runtime 1.744798, Loss 0.828137, forward nfe 97945, backward nfe 49165, Train: 0.8300, Val: 0.6846, Test: 0.7280\n",
      "Epoch: 049, Runtime 1.745227, Loss 0.810389, forward nfe 99451, backward nfe 50189, Train: 0.8350, Val: 0.6900, Test: 0.7276\n",
      "Epoch: 050, Runtime 1.724741, Loss 0.788995, forward nfe 100944, backward nfe 51213, Train: 0.8250, Val: 0.6862, Test: 0.7254\n",
      "Epoch: 051, Runtime 1.701769, Loss 0.768663, forward nfe 102446, backward nfe 52242, Train: 0.8400, Val: 0.6977, Test: 0.7362\n",
      "Epoch: 052, Runtime 1.676020, Loss 0.752354, forward nfe 103771, backward nfe 53268, Train: 0.8350, Val: 0.7054, Test: 0.7417\n",
      "Epoch: 053, Runtime 1.678715, Loss 0.734421, forward nfe 105122, backward nfe 54297, Train: 0.8400, Val: 0.7023, Test: 0.7391\n",
      "Epoch: 054, Runtime 1.660159, Loss 0.722028, forward nfe 106465, backward nfe 55324, Train: 0.8400, Val: 0.7031, Test: 0.7400\n",
      "Epoch: 055, Runtime 1.676625, Loss 0.702910, forward nfe 107783, backward nfe 56348, Train: 0.8350, Val: 0.7046, Test: 0.7445\n",
      "Epoch: 056, Runtime 1.680244, Loss 0.689815, forward nfe 109166, backward nfe 57380, Train: 0.8400, Val: 0.7100, Test: 0.7461\n",
      "Epoch: 057, Runtime 1.679418, Loss 0.673198, forward nfe 110541, backward nfe 58404, Train: 0.8400, Val: 0.7108, Test: 0.7461\n",
      "Epoch: 058, Runtime 1.623957, Loss 0.662083, forward nfe 111835, backward nfe 59428, Train: 0.8500, Val: 0.7185, Test: 0.7474\n",
      "Epoch: 059, Runtime 1.596024, Loss 0.646174, forward nfe 113059, backward nfe 60452, Train: 0.8550, Val: 0.7200, Test: 0.7565\n",
      "Epoch: 060, Runtime 1.606873, Loss 0.629144, forward nfe 114236, backward nfe 61477, Train: 0.8500, Val: 0.7300, Test: 0.7583\n",
      "Epoch: 061, Runtime 1.624959, Loss 0.616049, forward nfe 115457, backward nfe 62501, Train: 0.8450, Val: 0.7277, Test: 0.7547\n",
      "Epoch: 062, Runtime 1.605787, Loss 0.599645, forward nfe 116685, backward nfe 63525, Train: 0.8600, Val: 0.7277, Test: 0.7613\n",
      "Epoch: 063, Runtime 1.614488, Loss 0.595822, forward nfe 117884, backward nfe 64554, Train: 0.8600, Val: 0.7338, Test: 0.7638\n",
      "Epoch: 064, Runtime 1.581321, Loss 0.575167, forward nfe 119089, backward nfe 65582, Train: 0.8700, Val: 0.7415, Test: 0.7646\n",
      "Epoch: 065, Runtime 1.560890, Loss 0.563226, forward nfe 120205, backward nfe 66607, Train: 0.8500, Val: 0.7446, Test: 0.7658\n",
      "Epoch: 066, Runtime 1.564192, Loss 0.564082, forward nfe 121321, backward nfe 67632, Train: 0.8700, Val: 0.7462, Test: 0.7694\n",
      "Epoch: 067, Runtime 1.572224, Loss 0.539254, forward nfe 122477, backward nfe 68657, Train: 0.8750, Val: 0.7438, Test: 0.7708\n",
      "Epoch: 068, Runtime 1.560686, Loss 0.532454, forward nfe 123588, backward nfe 69681, Train: 0.8850, Val: 0.7492, Test: 0.7760\n",
      "Epoch: 069, Runtime 1.567132, Loss 0.526229, forward nfe 124715, backward nfe 70705, Train: 0.8800, Val: 0.7508, Test: 0.7747\n",
      "Epoch: 070, Runtime 1.556452, Loss 0.509960, forward nfe 125811, backward nfe 71732, Train: 0.8800, Val: 0.7500, Test: 0.7746\n",
      "Epoch: 071, Runtime 1.540779, Loss 0.499783, forward nfe 126906, backward nfe 72757, Train: 0.8850, Val: 0.7585, Test: 0.7799\n",
      "Epoch: 072, Runtime 1.537694, Loss 0.486359, forward nfe 127972, backward nfe 73782, Train: 0.8800, Val: 0.7646, Test: 0.7862\n",
      "Epoch: 073, Runtime 1.546169, Loss 0.488649, forward nfe 129035, backward nfe 74812, Train: 0.8750, Val: 0.7631, Test: 0.7861\n",
      "Epoch: 074, Runtime 1.551481, Loss 0.470334, forward nfe 130124, backward nfe 75836, Train: 0.8800, Val: 0.7638, Test: 0.7865\n",
      "Epoch: 075, Runtime 1.543826, Loss 0.471879, forward nfe 131207, backward nfe 76860, Train: 0.8850, Val: 0.7708, Test: 0.7900\n",
      "Epoch: 076, Runtime 1.534838, Loss 0.452004, forward nfe 132266, backward nfe 77890, Train: 0.8850, Val: 0.7700, Test: 0.7894\n",
      "Epoch: 077, Runtime 1.523163, Loss 0.443576, forward nfe 133304, backward nfe 78914, Train: 0.8850, Val: 0.7662, Test: 0.7857\n",
      "Epoch: 078, Runtime 1.522861, Loss 0.437152, forward nfe 134335, backward nfe 79938, Train: 0.8800, Val: 0.7723, Test: 0.7897\n",
      "Epoch: 079, Runtime 1.517994, Loss 0.422943, forward nfe 135361, backward nfe 80963, Train: 0.8850, Val: 0.7792, Test: 0.7966\n",
      "Epoch: 080, Runtime 1.549686, Loss 0.419463, forward nfe 136399, backward nfe 82001, Train: 0.8900, Val: 0.7808, Test: 0.7962\n",
      "Epoch: 081, Runtime 1.519869, Loss 0.416329, forward nfe 137425, backward nfe 83025, Train: 0.8900, Val: 0.7815, Test: 0.7978\n",
      "Epoch: 082, Runtime 1.518543, Loss 0.404048, forward nfe 138452, backward nfe 84052, Train: 0.8850, Val: 0.7823, Test: 0.8000\n",
      "Epoch: 083, Runtime 1.527931, Loss 0.389655, forward nfe 139475, backward nfe 85076, Train: 0.8850, Val: 0.7869, Test: 0.8013\n",
      "Epoch: 084, Runtime 1.522352, Loss 0.390308, forward nfe 140511, backward nfe 86104, Train: 0.8900, Val: 0.7877, Test: 0.8034\n",
      "Epoch: 085, Runtime 1.515122, Loss 0.376667, forward nfe 141542, backward nfe 87128, Train: 0.8900, Val: 0.7938, Test: 0.8073\n",
      "Epoch: 086, Runtime 1.515571, Loss 0.377399, forward nfe 142548, backward nfe 88151, Train: 0.8850, Val: 0.7954, Test: 0.8065\n",
      "Epoch: 087, Runtime 1.515068, Loss 0.378369, forward nfe 143566, backward nfe 89175, Train: 0.8850, Val: 0.7900, Test: 0.8025\n",
      "Epoch: 088, Runtime 1.502080, Loss 0.366336, forward nfe 144564, backward nfe 90199, Train: 0.8900, Val: 0.7923, Test: 0.8054\n",
      "Epoch: 089, Runtime 1.508690, Loss 0.356311, forward nfe 145566, backward nfe 91225, Train: 0.8950, Val: 0.7869, Test: 0.8067\n",
      "Epoch: 090, Runtime 1.509192, Loss 0.347581, forward nfe 146557, backward nfe 92249, Train: 0.8950, Val: 0.8015, Test: 0.8126\n",
      "Epoch: 091, Runtime 1.515633, Loss 0.347200, forward nfe 147561, backward nfe 93275, Train: 0.8900, Val: 0.7938, Test: 0.8097\n",
      "Epoch: 092, Runtime 1.515695, Loss 0.336638, forward nfe 148576, backward nfe 94302, Train: 0.9000, Val: 0.7946, Test: 0.8071\n",
      "Epoch: 093, Runtime 1.510513, Loss 0.322046, forward nfe 149582, backward nfe 95326, Train: 0.9000, Val: 0.7931, Test: 0.8111\n",
      "Epoch: 094, Runtime 1.507665, Loss 0.313490, forward nfe 150575, backward nfe 96350, Train: 0.8950, Val: 0.8062, Test: 0.8159\n",
      "Epoch: 095, Runtime 1.503392, Loss 0.327990, forward nfe 151571, backward nfe 97373, Train: 0.8950, Val: 0.8054, Test: 0.8154\n",
      "Epoch: 096, Runtime 1.504836, Loss 0.310807, forward nfe 152562, backward nfe 98398, Train: 0.9100, Val: 0.7969, Test: 0.8103\n",
      "Epoch: 097, Runtime 1.503132, Loss 0.296269, forward nfe 153557, backward nfe 99422, Train: 0.9050, Val: 0.7969, Test: 0.8124\n",
      "Epoch: 098, Runtime 1.500905, Loss 0.294000, forward nfe 154538, backward nfe 100446, Train: 0.9000, Val: 0.8046, Test: 0.8173\n",
      "Epoch: 099, Runtime 1.506311, Loss 0.307814, forward nfe 155530, backward nfe 101471, Train: 0.9050, Val: 0.8023, Test: 0.8176\n",
      "best val accuracy 0.806154 with test accuracy 0.815948 at epoch 94\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.586051, Loss 2.331545, forward nfe 422, backward nfe 71, Train: 0.0950, Val: 0.0538, Test: 0.0579\n",
      "Epoch: 002, Runtime 1.492224, Loss 2.408909, forward nfe 1381, backward nfe 1095, Train: 0.1150, Val: 0.0731, Test: 0.0684\n",
      "Epoch: 003, Runtime 1.495982, Loss 2.346505, forward nfe 2342, backward nfe 2119, Train: 0.2050, Val: 0.2331, Test: 0.2294\n",
      "Epoch: 004, Runtime 1.503108, Loss 2.246048, forward nfe 3324, backward nfe 3143, Train: 0.1050, Val: 0.1038, Test: 0.1034\n",
      "Epoch: 005, Runtime 1.513298, Loss 2.252384, forward nfe 4324, backward nfe 4168, Train: 0.2100, Val: 0.1354, Test: 0.1441\n",
      "Epoch: 006, Runtime 1.522075, Loss 2.211878, forward nfe 5357, backward nfe 5192, Train: 0.1950, Val: 0.1000, Test: 0.1086\n",
      "Epoch: 007, Runtime 1.559599, Loss 2.199056, forward nfe 6404, backward nfe 6217, Train: 0.2200, Val: 0.1377, Test: 0.1485\n",
      "Epoch: 008, Runtime 1.638263, Loss 2.153227, forward nfe 7566, backward nfe 7243, Train: 0.2550, Val: 0.2008, Test: 0.2075\n",
      "Epoch: 009, Runtime 1.786039, Loss 2.152411, forward nfe 9016, backward nfe 8272, Train: 0.2750, Val: 0.2131, Test: 0.2280\n",
      "Epoch: 010, Runtime 1.903408, Loss 2.148896, forward nfe 10728, backward nfe 9309, Train: 0.3200, Val: 0.2100, Test: 0.2261\n",
      "Epoch: 011, Runtime 1.997921, Loss 2.113667, forward nfe 12567, backward nfe 10346, Train: 0.2650, Val: 0.1938, Test: 0.2025\n",
      "Epoch: 012, Runtime 2.161526, Loss 2.069939, forward nfe 14763, backward nfe 11420, Train: 0.2750, Val: 0.1600, Test: 0.1765\n",
      "Epoch: 013, Runtime 2.232474, Loss 2.040365, forward nfe 17191, backward nfe 12448, Train: 0.2350, Val: 0.1346, Test: 0.1511\n",
      "Epoch: 014, Runtime 2.402174, Loss 2.041753, forward nfe 19869, backward nfe 13567, Train: 0.2450, Val: 0.1454, Test: 0.1610\n",
      "Epoch: 015, Runtime 2.594973, Loss 2.015842, forward nfe 23016, backward nfe 14686, Train: 0.3350, Val: 0.1862, Test: 0.1992\n",
      "Epoch: 016, Runtime 2.469312, Loss 1.971972, forward nfe 26030, backward nfe 15713, Train: 0.3850, Val: 0.2077, Test: 0.2258\n",
      "Epoch: 017, Runtime 2.662939, Loss 1.935100, forward nfe 29344, backward nfe 16769, Train: 0.3900, Val: 0.2215, Test: 0.2328\n",
      "Epoch: 018, Runtime 2.518900, Loss 1.918540, forward nfe 32555, backward nfe 17813, Train: 0.3950, Val: 0.2277, Test: 0.2400\n",
      "Epoch: 019, Runtime 2.729886, Loss 1.903522, forward nfe 35833, backward nfe 18837, Train: 0.4150, Val: 0.2362, Test: 0.2528\n",
      "Epoch: 020, Runtime 2.786189, Loss 1.859247, forward nfe 39478, backward nfe 19883, Train: 0.4100, Val: 0.2338, Test: 0.2573\n",
      "Epoch: 021, Runtime 2.723435, Loss 1.823783, forward nfe 43152, backward nfe 20935, Train: 0.4400, Val: 0.2292, Test: 0.2580\n",
      "Epoch: 022, Runtime 2.851397, Loss 1.789720, forward nfe 47130, backward nfe 22002, Train: 0.4600, Val: 0.2346, Test: 0.2622\n",
      "Epoch: 023, Runtime 2.826719, Loss 1.766410, forward nfe 50710, backward nfe 23121, Train: 0.4950, Val: 0.2354, Test: 0.2653\n",
      "Epoch: 024, Runtime 2.530914, Loss 1.738581, forward nfe 54165, backward nfe 24159, Train: 0.4950, Val: 0.2408, Test: 0.2735\n",
      "Epoch: 025, Runtime 2.397610, Loss 1.698502, forward nfe 57115, backward nfe 25232, Train: 0.5000, Val: 0.2546, Test: 0.2863\n",
      "Epoch: 026, Runtime 2.600197, Loss 1.663268, forward nfe 60123, backward nfe 26312, Train: 0.5100, Val: 0.2600, Test: 0.2939\n",
      "Epoch: 027, Runtime 2.463934, Loss 1.642253, forward nfe 63227, backward nfe 27338, Train: 0.5300, Val: 0.2623, Test: 0.2992\n",
      "Epoch: 028, Runtime 2.277617, Loss 1.607437, forward nfe 66008, backward nfe 28395, Train: 0.5550, Val: 0.2631, Test: 0.3021\n",
      "Epoch: 029, Runtime 2.418182, Loss 1.572128, forward nfe 68796, backward nfe 29443, Train: 0.5250, Val: 0.2708, Test: 0.3028\n",
      "Epoch: 030, Runtime 2.434830, Loss 1.547100, forward nfe 71734, backward nfe 30562, Train: 0.5250, Val: 0.2569, Test: 0.2901\n",
      "Epoch: 031, Runtime 2.395290, Loss 1.520071, forward nfe 74738, backward nfe 31589, Train: 0.5500, Val: 0.2562, Test: 0.2943\n",
      "Epoch: 032, Runtime 2.398136, Loss 1.488329, forward nfe 77418, backward nfe 32691, Train: 0.5750, Val: 0.2646, Test: 0.3082\n",
      "Epoch: 033, Runtime 2.390673, Loss 1.453731, forward nfe 80086, backward nfe 33810, Train: 0.5650, Val: 0.2723, Test: 0.3136\n",
      "Epoch: 034, Runtime 2.240061, Loss 1.431717, forward nfe 82770, backward nfe 34929, Train: 0.5850, Val: 0.2785, Test: 0.3262\n",
      "Epoch: 035, Runtime 2.161027, Loss 1.393844, forward nfe 85099, backward nfe 36048, Train: 0.6450, Val: 0.2977, Test: 0.3431\n",
      "Epoch: 036, Runtime 2.072507, Loss 1.371431, forward nfe 87148, backward nfe 37127, Train: 0.6400, Val: 0.3031, Test: 0.3550\n",
      "Epoch: 037, Runtime 2.050320, Loss 1.340578, forward nfe 89264, backward nfe 38170, Train: 0.6400, Val: 0.3292, Test: 0.3818\n",
      "Epoch: 038, Runtime 2.127998, Loss 1.310862, forward nfe 91502, backward nfe 39209, Train: 0.6500, Val: 0.3677, Test: 0.4275\n",
      "Epoch: 039, Runtime 2.147500, Loss 1.280977, forward nfe 93820, backward nfe 40319, Train: 0.6800, Val: 0.4223, Test: 0.4777\n",
      "Epoch: 040, Runtime 2.055399, Loss 1.253206, forward nfe 95904, backward nfe 41348, Train: 0.7000, Val: 0.4254, Test: 0.4764\n",
      "Epoch: 041, Runtime 2.178857, Loss 1.222502, forward nfe 98237, backward nfe 42467, Train: 0.7100, Val: 0.4131, Test: 0.4693\n",
      "Epoch: 042, Runtime 1.960326, Loss 1.203344, forward nfe 100257, backward nfe 43557, Train: 0.7000, Val: 0.4385, Test: 0.4890\n",
      "Epoch: 043, Runtime 1.968650, Loss 1.170186, forward nfe 102250, backward nfe 44628, Train: 0.7450, Val: 0.5277, Test: 0.5738\n",
      "Epoch: 044, Runtime 1.966383, Loss 1.144855, forward nfe 104041, backward nfe 45679, Train: 0.7500, Val: 0.6377, Test: 0.6758\n",
      "Epoch: 045, Runtime 1.867117, Loss 1.106195, forward nfe 105902, backward nfe 46717, Train: 0.7700, Val: 0.6577, Test: 0.6908\n",
      "Epoch: 046, Runtime 1.847447, Loss 1.084709, forward nfe 107604, backward nfe 47743, Train: 0.7800, Val: 0.6438, Test: 0.6811\n",
      "Epoch: 047, Runtime 1.933152, Loss 1.057952, forward nfe 109435, backward nfe 48830, Train: 0.7900, Val: 0.6631, Test: 0.6932\n",
      "Epoch: 048, Runtime 1.852161, Loss 1.030916, forward nfe 111056, backward nfe 49914, Train: 0.7950, Val: 0.6808, Test: 0.7095\n",
      "Epoch: 049, Runtime 1.822707, Loss 0.999234, forward nfe 112749, backward nfe 50959, Train: 0.8000, Val: 0.6846, Test: 0.7162\n",
      "Epoch: 050, Runtime 1.773258, Loss 0.969133, forward nfe 114349, backward nfe 51994, Train: 0.8050, Val: 0.6931, Test: 0.7198\n",
      "Epoch: 051, Runtime 1.768429, Loss 0.942755, forward nfe 115838, backward nfe 53029, Train: 0.8050, Val: 0.6985, Test: 0.7264\n",
      "Epoch: 052, Runtime 1.735567, Loss 0.924058, forward nfe 117280, backward nfe 54056, Train: 0.8100, Val: 0.7008, Test: 0.7286\n",
      "Epoch: 053, Runtime 1.741756, Loss 0.889530, forward nfe 118778, backward nfe 55096, Train: 0.8200, Val: 0.6885, Test: 0.7245\n",
      "Epoch: 054, Runtime 1.710970, Loss 0.866914, forward nfe 120246, backward nfe 56127, Train: 0.8150, Val: 0.7100, Test: 0.7330\n",
      "Epoch: 055, Runtime 1.711162, Loss 0.844588, forward nfe 121652, backward nfe 57175, Train: 0.8150, Val: 0.7177, Test: 0.7398\n",
      "Epoch: 056, Runtime 1.654208, Loss 0.815517, forward nfe 122970, backward nfe 58201, Train: 0.8350, Val: 0.7108, Test: 0.7369\n",
      "Epoch: 057, Runtime 1.658280, Loss 0.799181, forward nfe 124248, backward nfe 59237, Train: 0.8400, Val: 0.7192, Test: 0.7428\n",
      "Epoch: 058, Runtime 1.667639, Loss 0.781410, forward nfe 125571, backward nfe 60264, Train: 0.8350, Val: 0.7215, Test: 0.7416\n",
      "Epoch: 059, Runtime 1.691400, Loss 0.770763, forward nfe 126871, backward nfe 61297, Train: 0.8350, Val: 0.7154, Test: 0.7359\n",
      "Epoch: 060, Runtime 1.668615, Loss 0.751336, forward nfe 128217, backward nfe 62339, Train: 0.8300, Val: 0.7269, Test: 0.7434\n",
      "Epoch: 061, Runtime 1.691720, Loss 0.729335, forward nfe 129524, backward nfe 63380, Train: 0.8350, Val: 0.7285, Test: 0.7487\n",
      "Epoch: 062, Runtime 1.625712, Loss 0.705762, forward nfe 130817, backward nfe 64406, Train: 0.8350, Val: 0.7277, Test: 0.7476\n",
      "Epoch: 063, Runtime 1.639050, Loss 0.692825, forward nfe 132048, backward nfe 65447, Train: 0.8350, Val: 0.7277, Test: 0.7492\n",
      "Epoch: 064, Runtime 1.609738, Loss 0.672919, forward nfe 133265, backward nfe 66471, Train: 0.8400, Val: 0.7331, Test: 0.7538\n",
      "Epoch: 065, Runtime 1.605934, Loss 0.655940, forward nfe 134461, backward nfe 67498, Train: 0.8350, Val: 0.7285, Test: 0.7506\n",
      "Epoch: 066, Runtime 1.599203, Loss 0.639691, forward nfe 135635, backward nfe 68528, Train: 0.8350, Val: 0.7308, Test: 0.7507\n",
      "Epoch: 067, Runtime 1.624051, Loss 0.626688, forward nfe 136846, backward nfe 69568, Train: 0.8400, Val: 0.7331, Test: 0.7516\n",
      "Epoch: 068, Runtime 1.605353, Loss 0.621854, forward nfe 138039, backward nfe 70592, Train: 0.8350, Val: 0.7300, Test: 0.7514\n",
      "Epoch: 069, Runtime 1.567989, Loss 0.607544, forward nfe 139207, backward nfe 71616, Train: 0.8500, Val: 0.7415, Test: 0.7596\n",
      "Epoch: 070, Runtime 1.594563, Loss 0.593618, forward nfe 140338, backward nfe 72640, Train: 0.8300, Val: 0.7462, Test: 0.7656\n",
      "Epoch: 071, Runtime 1.587143, Loss 0.589953, forward nfe 141485, backward nfe 73664, Train: 0.8500, Val: 0.7469, Test: 0.7591\n",
      "Epoch: 072, Runtime 1.557273, Loss 0.568595, forward nfe 142627, backward nfe 74688, Train: 0.8500, Val: 0.7415, Test: 0.7568\n",
      "Epoch: 073, Runtime 1.590488, Loss 0.555227, forward nfe 143734, backward nfe 75732, Train: 0.8500, Val: 0.7462, Test: 0.7603\n",
      "Epoch: 074, Runtime 1.581853, Loss 0.550793, forward nfe 144853, backward nfe 76763, Train: 0.8500, Val: 0.7492, Test: 0.7660\n",
      "Epoch: 075, Runtime 1.587100, Loss 0.523741, forward nfe 145976, backward nfe 77802, Train: 0.8500, Val: 0.7554, Test: 0.7696\n",
      "Epoch: 076, Runtime 1.569173, Loss 0.524254, forward nfe 147091, backward nfe 78829, Train: 0.8550, Val: 0.7569, Test: 0.7675\n",
      "Epoch: 077, Runtime 1.569346, Loss 0.518581, forward nfe 148234, backward nfe 79854, Train: 0.8550, Val: 0.7531, Test: 0.7660\n",
      "Epoch: 078, Runtime 1.557557, Loss 0.498261, forward nfe 149344, backward nfe 80880, Train: 0.8550, Val: 0.7515, Test: 0.7696\n",
      "Epoch: 079, Runtime 1.553396, Loss 0.503999, forward nfe 150443, backward nfe 81904, Train: 0.8550, Val: 0.7569, Test: 0.7688\n",
      "Epoch: 080, Runtime 1.556253, Loss 0.502467, forward nfe 151522, backward nfe 82933, Train: 0.8550, Val: 0.7569, Test: 0.7698\n",
      "Epoch: 081, Runtime 1.549180, Loss 0.478791, forward nfe 152605, backward nfe 83960, Train: 0.8550, Val: 0.7577, Test: 0.7698\n",
      "Epoch: 082, Runtime 1.559419, Loss 0.471313, forward nfe 153711, backward nfe 84990, Train: 0.8500, Val: 0.7585, Test: 0.7768\n",
      "Epoch: 083, Runtime 1.545516, Loss 0.462846, forward nfe 154805, backward nfe 86015, Train: 0.8500, Val: 0.7608, Test: 0.7802\n",
      "Epoch: 084, Runtime 1.544677, Loss 0.464111, forward nfe 155872, backward nfe 87039, Train: 0.8550, Val: 0.7554, Test: 0.7744\n",
      "Epoch: 085, Runtime 1.531629, Loss 0.453167, forward nfe 156935, backward nfe 88063, Train: 0.8550, Val: 0.7646, Test: 0.7754\n",
      "Epoch: 086, Runtime 1.535131, Loss 0.439181, forward nfe 157990, backward nfe 89087, Train: 0.8550, Val: 0.7662, Test: 0.7803\n",
      "Epoch: 087, Runtime 1.541668, Loss 0.440101, forward nfe 159045, backward nfe 90111, Train: 0.8550, Val: 0.7638, Test: 0.7860\n",
      "Epoch: 088, Runtime 1.534977, Loss 0.424416, forward nfe 160107, backward nfe 91135, Train: 0.8550, Val: 0.7615, Test: 0.7817\n",
      "Epoch: 089, Runtime 1.542059, Loss 0.428963, forward nfe 161151, backward nfe 92169, Train: 0.8650, Val: 0.7677, Test: 0.7804\n",
      "Epoch: 090, Runtime 1.519162, Loss 0.417022, forward nfe 162187, backward nfe 93193, Train: 0.8650, Val: 0.7638, Test: 0.7812\n",
      "Epoch: 091, Runtime 1.527222, Loss 0.408047, forward nfe 163224, backward nfe 94217, Train: 0.8550, Val: 0.7677, Test: 0.7867\n",
      "Epoch: 092, Runtime 1.538446, Loss 0.398527, forward nfe 164273, backward nfe 95241, Train: 0.8700, Val: 0.7600, Test: 0.7871\n",
      "Epoch: 093, Runtime 1.529891, Loss 0.404483, forward nfe 165340, backward nfe 96264, Train: 0.8600, Val: 0.7777, Test: 0.7927\n",
      "Epoch: 094, Runtime 1.520188, Loss 0.398264, forward nfe 166374, backward nfe 97288, Train: 0.8600, Val: 0.7754, Test: 0.7905\n",
      "Epoch: 095, Runtime 1.533678, Loss 0.386198, forward nfe 167407, backward nfe 98314, Train: 0.8800, Val: 0.7646, Test: 0.7852\n",
      "Epoch: 096, Runtime 1.529168, Loss 0.374861, forward nfe 168460, backward nfe 99339, Train: 0.8650, Val: 0.7754, Test: 0.7957\n",
      "Epoch: 097, Runtime 1.523791, Loss 0.379599, forward nfe 169494, backward nfe 100365, Train: 0.8700, Val: 0.7746, Test: 0.7964\n",
      "Epoch: 098, Runtime 1.527303, Loss 0.374119, forward nfe 170519, backward nfe 101392, Train: 0.8800, Val: 0.7662, Test: 0.7890\n",
      "Epoch: 099, Runtime 1.522152, Loss 0.360326, forward nfe 171550, backward nfe 102415, Train: 0.8700, Val: 0.7762, Test: 0.7968\n",
      "best val accuracy 0.777692 with test accuracy 0.792687 at epoch 93\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.595887, Loss 2.316411, forward nfe 448, backward nfe 69, Train: 0.1050, Val: 0.0277, Test: 0.0381\n",
      "Epoch: 002, Runtime 1.490878, Loss 3.716868, forward nfe 1408, backward nfe 1093, Train: 0.1150, Val: 0.0785, Test: 0.0753\n",
      "Epoch: 003, Runtime 1.503323, Loss 2.333451, forward nfe 2368, backward nfe 2117, Train: 0.0950, Val: 0.0115, Test: 0.0209\n",
      "Epoch: 004, Runtime 1.506961, Loss 2.308862, forward nfe 3354, backward nfe 3142, Train: 0.0800, Val: 0.0892, Test: 0.0856\n",
      "Epoch: 005, Runtime 1.516479, Loss 2.338597, forward nfe 4360, backward nfe 4166, Train: 0.0950, Val: 0.0115, Test: 0.0209\n",
      "Epoch: 006, Runtime 1.519202, Loss 2.302354, forward nfe 5383, backward nfe 5190, Train: 0.0950, Val: 0.0115, Test: 0.0210\n",
      "Epoch: 007, Runtime 1.591797, Loss 2.306302, forward nfe 6437, backward nfe 6220, Train: 0.1000, Val: 0.0115, Test: 0.0209\n",
      "Epoch: 008, Runtime 1.725271, Loss 2.304623, forward nfe 7706, backward nfe 7246, Train: 0.1000, Val: 0.0123, Test: 0.0209\n",
      "Epoch: 009, Runtime 1.902632, Loss 2.302647, forward nfe 9301, backward nfe 8277, Train: 0.1000, Val: 0.0115, Test: 0.0209\n",
      "Epoch: 010, Runtime 2.276339, Loss 2.302710, forward nfe 11465, backward nfe 9396, Train: 0.1300, Val: 0.0192, Test: 0.0269\n",
      "Epoch: 011, Runtime 2.523062, Loss 2.287247, forward nfe 14242, backward nfe 10433, Train: 0.1600, Val: 0.0400, Test: 0.0446\n",
      "Epoch: 012, Runtime 2.894918, Loss 2.242275, forward nfe 17683, backward nfe 11533, Train: 0.1500, Val: 0.1146, Test: 0.1139\n",
      "Epoch: 013, Runtime 3.847027, Loss 2.242903, forward nfe 22299, backward nfe 12652, Train: 0.1150, Val: 0.0523, Test: 0.0559\n",
      "Epoch: 014, Runtime 4.317196, Loss 2.230322, forward nfe 28888, backward nfe 13771, Train: 0.1600, Val: 0.0600, Test: 0.0678\n",
      "Epoch: 015, Runtime 5.585799, Loss 2.203089, forward nfe 36169, backward nfe 14890, Train: 0.2300, Val: 0.0885, Test: 0.0952\n",
      "Epoch: 016, Runtime 6.107759, Loss 2.219286, forward nfe 47256, backward nfe 16009, Train: 0.1900, Val: 0.0623, Test: 0.0705\n",
      "Epoch: 017, Runtime 7.475331, Loss 2.183111, forward nfe 58220, backward nfe 17128, Train: 0.1200, Val: 0.0523, Test: 0.0577\n",
      "Epoch: 018, Runtime 12.230549, Loss 2.187604, forward nfe 78442, backward nfe 18247, Train: 0.1250, Val: 0.0585, Test: 0.0643\n",
      "Epoch: 019, Runtime 10.043938, Loss 2.166598, forward nfe 97652, backward nfe 19366, Train: 0.2050, Val: 0.1169, Test: 0.1160\n",
      "Epoch: 020, Runtime 10.197318, Loss 2.153733, forward nfe 118943, backward nfe 20485, Train: 0.2650, Val: 0.1338, Test: 0.1412\n",
      "Epoch: 021, Runtime 11.915401, Loss 2.155083, forward nfe 141214, backward nfe 21604, Train: 0.2300, Val: 0.1046, Test: 0.1083\n",
      "Epoch: 022, Runtime 12.172706, Loss 2.127192, forward nfe 164360, backward nfe 22723, Train: 0.1550, Val: 0.0646, Test: 0.0742\n",
      "Epoch: 023, Runtime 11.187159, Loss 2.111426, forward nfe 188324, backward nfe 23842, Train: 0.1600, Val: 0.0638, Test: 0.0779\n",
      "Epoch: 024, Runtime 9.366900, Loss 2.105831, forward nfe 207134, backward nfe 24961, Train: 0.2000, Val: 0.0715, Test: 0.0890\n",
      "Epoch: 025, Runtime 12.157325, Loss 2.074253, forward nfe 227429, backward nfe 26080, Train: 0.2200, Val: 0.0815, Test: 0.1003\n",
      "Epoch: 026, Runtime 12.128324, Loss 2.068968, forward nfe 251349, backward nfe 27199, Train: 0.2150, Val: 0.0777, Test: 0.0973\n",
      "Epoch: 027, Runtime 10.785120, Loss 2.057105, forward nfe 271986, backward nfe 28318, Train: 0.1800, Val: 0.0646, Test: 0.0802\n",
      "Epoch: 028, Runtime 10.614550, Loss 2.041318, forward nfe 293789, backward nfe 29437, Train: 0.1950, Val: 0.0669, Test: 0.0828\n",
      "Epoch: 029, Runtime 12.267735, Loss 2.030394, forward nfe 316457, backward nfe 30556, Train: 0.2150, Val: 0.0662, Test: 0.0872\n",
      "Epoch: 030, Runtime 10.792541, Loss 2.014756, forward nfe 340402, backward nfe 31675, Train: 0.2300, Val: 0.0692, Test: 0.0910\n",
      "Epoch: 031, Runtime 12.232781, Loss 2.005949, forward nfe 361351, backward nfe 32794, Train: 0.2200, Val: 0.0700, Test: 0.0894\n",
      "Epoch: 032, Runtime 10.612471, Loss 1.993606, forward nfe 385310, backward nfe 33913, Train: 0.2250, Val: 0.0838, Test: 0.0973\n",
      "Epoch: 033, Runtime 10.299182, Loss 1.982874, forward nfe 405809, backward nfe 35032, Train: 0.2850, Val: 0.1831, Test: 0.1905\n",
      "Epoch: 034, Runtime 12.255734, Loss 1.966599, forward nfe 425570, backward nfe 36151, Train: 0.3150, Val: 0.2008, Test: 0.2134\n",
      "Epoch: 035, Runtime 9.812727, Loss 1.952449, forward nfe 449611, backward nfe 37270, Train: 0.3400, Val: 0.2085, Test: 0.2254\n",
      "Epoch: 036, Runtime 11.042389, Loss 1.944468, forward nfe 466655, backward nfe 38389, Train: 0.3400, Val: 0.2015, Test: 0.2193\n",
      "Epoch: 037, Runtime 10.070858, Loss 1.930328, forward nfe 487646, backward nfe 39508, Train: 0.3350, Val: 0.1823, Test: 0.1976\n",
      "Epoch: 038, Runtime 10.128199, Loss 1.920795, forward nfe 508888, backward nfe 40627, Train: 0.3500, Val: 0.2585, Test: 0.2556\n",
      "Epoch: 039, Runtime 8.644886, Loss 1.902302, forward nfe 523402, backward nfe 41746, Train: 0.3600, Val: 0.2392, Test: 0.2493\n",
      "Epoch: 040, Runtime 12.148385, Loss 1.887194, forward nfe 544385, backward nfe 42865, Train: 0.3600, Val: 0.1985, Test: 0.2242\n",
      "Epoch: 041, Runtime 9.360908, Loss 1.867784, forward nfe 563147, backward nfe 43984, Train: 0.3700, Val: 0.2408, Test: 0.2511\n",
      "Epoch: 042, Runtime 7.662980, Loss 1.855087, forward nfe 581146, backward nfe 45084, Train: 0.3550, Val: 0.2208, Test: 0.2342\n",
      "Epoch: 043, Runtime 9.320449, Loss 1.840647, forward nfe 600168, backward nfe 46203, Train: 0.3750, Val: 0.2031, Test: 0.2262\n",
      "Epoch: 044, Runtime 7.428910, Loss 1.811632, forward nfe 613775, backward nfe 47322, Train: 0.4400, Val: 0.2608, Test: 0.2795\n",
      "Epoch: 045, Runtime 6.473106, Loss 1.794228, forward nfe 625229, backward nfe 48441, Train: 0.5350, Val: 0.3162, Test: 0.3277\n",
      "Epoch: 046, Runtime 5.892570, Loss 1.781055, forward nfe 636824, backward nfe 49560, Train: 0.5000, Val: 0.2808, Test: 0.2976\n",
      "Epoch: 047, Runtime 5.375726, Loss 1.753108, forward nfe 645572, backward nfe 50679, Train: 0.4800, Val: 0.2500, Test: 0.2742\n",
      "Epoch: 048, Runtime 5.364885, Loss 1.736826, forward nfe 654967, backward nfe 51798, Train: 0.5550, Val: 0.3046, Test: 0.3241\n",
      "Epoch: 049, Runtime 5.568507, Loss 1.716017, forward nfe 664101, backward nfe 52917, Train: 0.5600, Val: 0.3123, Test: 0.3301\n",
      "Epoch: 050, Runtime 5.086413, Loss 1.685926, forward nfe 673309, backward nfe 54036, Train: 0.5250, Val: 0.2908, Test: 0.3086\n",
      "Epoch: 051, Runtime 5.276517, Loss 1.670969, forward nfe 681680, backward nfe 55155, Train: 0.5500, Val: 0.3062, Test: 0.3253\n",
      "Epoch: 052, Runtime 4.441082, Loss 1.650013, forward nfe 689870, backward nfe 56274, Train: 0.5600, Val: 0.3000, Test: 0.3233\n",
      "Epoch: 053, Runtime 4.771933, Loss 1.623151, forward nfe 697370, backward nfe 57393, Train: 0.5350, Val: 0.2862, Test: 0.3112\n",
      "Epoch: 054, Runtime 4.452340, Loss 1.604037, forward nfe 705094, backward nfe 58512, Train: 0.5600, Val: 0.3146, Test: 0.3343\n",
      "Epoch: 055, Runtime 3.947238, Loss 1.576245, forward nfe 711978, backward nfe 59631, Train: 0.5550, Val: 0.3131, Test: 0.3343\n",
      "Epoch: 056, Runtime 3.501248, Loss 1.559826, forward nfe 717376, backward nfe 60745, Train: 0.5600, Val: 0.3054, Test: 0.3308\n",
      "Epoch: 057, Runtime 3.211025, Loss 1.534122, forward nfe 722022, backward nfe 61813, Train: 0.5700, Val: 0.3085, Test: 0.3319\n",
      "Epoch: 058, Runtime 3.015265, Loss 1.509719, forward nfe 726349, backward nfe 62932, Train: 0.5650, Val: 0.3123, Test: 0.3352\n",
      "Epoch: 059, Runtime 3.060356, Loss 1.493463, forward nfe 730403, backward nfe 63974, Train: 0.5650, Val: 0.3177, Test: 0.3391\n",
      "Epoch: 060, Runtime 2.991540, Loss 1.472219, forward nfe 734654, backward nfe 65066, Train: 0.5650, Val: 0.3092, Test: 0.3374\n",
      "Epoch: 061, Runtime 2.942618, Loss 1.447034, forward nfe 738688, backward nfe 66185, Train: 0.5700, Val: 0.3077, Test: 0.3364\n",
      "Epoch: 062, Runtime 2.968195, Loss 1.434307, forward nfe 742758, backward nfe 67304, Train: 0.5650, Val: 0.3146, Test: 0.3390\n",
      "Epoch: 063, Runtime 2.390926, Loss 1.414019, forward nfe 745967, backward nfe 68402, Train: 0.5650, Val: 0.3138, Test: 0.3395\n",
      "Epoch: 064, Runtime 2.321456, Loss 1.399368, forward nfe 748465, backward nfe 69501, Train: 0.5700, Val: 0.3200, Test: 0.3446\n",
      "Epoch: 065, Runtime 2.607711, Loss 1.379593, forward nfe 751707, backward nfe 70620, Train: 0.5800, Val: 0.3254, Test: 0.3469\n",
      "Epoch: 066, Runtime 2.241966, Loss 1.363997, forward nfe 754365, backward nfe 71663, Train: 0.5900, Val: 0.3254, Test: 0.3549\n",
      "Epoch: 067, Runtime 2.263432, Loss 1.346236, forward nfe 757090, backward nfe 72710, Train: 0.5800, Val: 0.3323, Test: 0.3551\n",
      "Epoch: 068, Runtime 2.248310, Loss 1.334425, forward nfe 759316, backward nfe 73790, Train: 0.5950, Val: 0.3323, Test: 0.3659\n",
      "Epoch: 069, Runtime 2.034129, Loss 1.313805, forward nfe 761648, backward nfe 74825, Train: 0.6150, Val: 0.3469, Test: 0.3672\n",
      "Epoch: 070, Runtime 1.903930, Loss 1.307711, forward nfe 763551, backward nfe 75865, Train: 0.6100, Val: 0.3469, Test: 0.3726\n",
      "Epoch: 071, Runtime 1.954803, Loss 1.291933, forward nfe 765379, backward nfe 76979, Train: 0.5900, Val: 0.3369, Test: 0.3677\n",
      "Epoch: 072, Runtime 1.896754, Loss 1.273454, forward nfe 767215, backward nfe 78033, Train: 0.6050, Val: 0.3423, Test: 0.3732\n",
      "Epoch: 073, Runtime 1.883168, Loss 1.252474, forward nfe 768892, backward nfe 79070, Train: 0.6000, Val: 0.3569, Test: 0.3805\n",
      "Epoch: 074, Runtime 1.819737, Loss 1.245479, forward nfe 770689, backward nfe 80103, Train: 0.6200, Val: 0.3615, Test: 0.3857\n",
      "Epoch: 075, Runtime 1.781904, Loss 1.236641, forward nfe 772327, backward nfe 81129, Train: 0.6300, Val: 0.3600, Test: 0.3879\n",
      "Epoch: 076, Runtime 1.754502, Loss 1.227335, forward nfe 773924, backward nfe 82169, Train: 0.6200, Val: 0.3585, Test: 0.3881\n",
      "Epoch: 077, Runtime 1.763640, Loss 1.212361, forward nfe 775357, backward nfe 83206, Train: 0.6250, Val: 0.3646, Test: 0.3914\n",
      "Epoch: 078, Runtime 1.722440, Loss 1.198135, forward nfe 776869, backward nfe 84231, Train: 0.6300, Val: 0.3669, Test: 0.3931\n",
      "Epoch: 079, Runtime 1.706207, Loss 1.190144, forward nfe 778324, backward nfe 85269, Train: 0.6350, Val: 0.3685, Test: 0.3954\n",
      "Epoch: 080, Runtime 1.686460, Loss 1.179653, forward nfe 779701, backward nfe 86300, Train: 0.6400, Val: 0.3685, Test: 0.4008\n",
      "Epoch: 081, Runtime 1.653043, Loss 1.171954, forward nfe 781032, backward nfe 87325, Train: 0.6550, Val: 0.3715, Test: 0.4028\n",
      "Epoch: 082, Runtime 1.647062, Loss 1.160014, forward nfe 782350, backward nfe 88356, Train: 0.6500, Val: 0.3723, Test: 0.4047\n",
      "Epoch: 083, Runtime 1.683656, Loss 1.141909, forward nfe 783623, backward nfe 89400, Train: 0.6600, Val: 0.3715, Test: 0.3999\n",
      "Epoch: 084, Runtime 1.642566, Loss 1.142058, forward nfe 784920, backward nfe 90441, Train: 0.6650, Val: 0.3769, Test: 0.4015\n",
      "Epoch: 085, Runtime 1.627776, Loss 1.127946, forward nfe 786175, backward nfe 91483, Train: 0.6700, Val: 0.3792, Test: 0.4078\n",
      "Epoch: 086, Runtime 1.639061, Loss 1.121109, forward nfe 787364, backward nfe 92545, Train: 0.6700, Val: 0.3854, Test: 0.4114\n",
      "Epoch: 087, Runtime 1.604011, Loss 1.106097, forward nfe 788589, backward nfe 93576, Train: 0.6850, Val: 0.3854, Test: 0.4133\n",
      "Epoch: 088, Runtime 1.578227, Loss 1.105367, forward nfe 789733, backward nfe 94608, Train: 0.6700, Val: 0.3838, Test: 0.4114\n",
      "Epoch: 089, Runtime 1.589147, Loss 1.083677, forward nfe 790884, backward nfe 95637, Train: 0.6650, Val: 0.3800, Test: 0.4096\n",
      "Epoch: 090, Runtime 1.585757, Loss 1.080647, forward nfe 792039, backward nfe 96665, Train: 0.6700, Val: 0.3785, Test: 0.4118\n",
      "Epoch: 091, Runtime 1.559857, Loss 1.077496, forward nfe 793188, backward nfe 97689, Train: 0.6850, Val: 0.3969, Test: 0.4173\n",
      "Epoch: 092, Runtime 1.558941, Loss 1.062905, forward nfe 794274, backward nfe 98722, Train: 0.7000, Val: 0.3900, Test: 0.4176\n",
      "Epoch: 093, Runtime 1.559067, Loss 1.053715, forward nfe 795362, backward nfe 99751, Train: 0.6800, Val: 0.3946, Test: 0.4141\n",
      "Epoch: 094, Runtime 1.564952, Loss 1.042323, forward nfe 796458, backward nfe 100784, Train: 0.6600, Val: 0.3762, Test: 0.4106\n",
      "Epoch: 095, Runtime 1.547184, Loss 1.036151, forward nfe 797566, backward nfe 101812, Train: 0.6800, Val: 0.3885, Test: 0.4167\n",
      "Epoch: 096, Runtime 1.541267, Loss 1.011690, forward nfe 798628, backward nfe 102840, Train: 0.7050, Val: 0.4008, Test: 0.4172\n",
      "Epoch: 097, Runtime 1.530048, Loss 1.021226, forward nfe 799684, backward nfe 103864, Train: 0.7250, Val: 0.3938, Test: 0.4208\n",
      "Epoch: 098, Runtime 1.531559, Loss 1.022199, forward nfe 800725, backward nfe 104891, Train: 0.6950, Val: 0.4023, Test: 0.4214\n",
      "Epoch: 099, Runtime 1.516174, Loss 0.996201, forward nfe 801755, backward nfe 105916, Train: 0.6700, Val: 0.3731, Test: 0.4083\n",
      "best val accuracy 0.402308 with test accuracy 0.421401 at epoch 98\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.594282, Loss 2.313658, forward nfe 436, backward nfe 71, Train: 0.1000, Val: 0.0562, Test: 0.0596\n",
      "Epoch: 002, Runtime 1.490659, Loss 2.363779, forward nfe 1394, backward nfe 1095, Train: 0.1250, Val: 0.0538, Test: 0.0633\n",
      "Epoch: 003, Runtime 1.496542, Loss 2.360669, forward nfe 2354, backward nfe 2119, Train: 0.1050, Val: 0.0569, Test: 0.0597\n",
      "Epoch: 004, Runtime 1.506274, Loss 2.343094, forward nfe 3344, backward nfe 3143, Train: 0.1100, Val: 0.0631, Test: 0.0659\n",
      "Epoch: 005, Runtime 1.506654, Loss 2.248065, forward nfe 4336, backward nfe 4167, Train: 0.1400, Val: 0.0377, Test: 0.0464\n",
      "Epoch: 006, Runtime 1.527607, Loss 2.255704, forward nfe 5346, backward nfe 5192, Train: 0.1000, Val: 0.0169, Test: 0.0220\n",
      "Epoch: 007, Runtime 1.563107, Loss 2.251394, forward nfe 6403, backward nfe 6221, Train: 0.1150, Val: 0.0446, Test: 0.0519\n",
      "Epoch: 008, Runtime 1.635853, Loss 2.224659, forward nfe 7571, backward nfe 7253, Train: 0.2600, Val: 0.3308, Test: 0.3085\n",
      "Epoch: 009, Runtime 1.722891, Loss 2.226707, forward nfe 8942, backward nfe 8282, Train: 0.2550, Val: 0.3385, Test: 0.3179\n",
      "Epoch: 010, Runtime 1.942725, Loss 2.209683, forward nfe 10493, backward nfe 9345, Train: 0.1750, Val: 0.2215, Test: 0.2048\n",
      "Epoch: 011, Runtime 1.973257, Loss 2.185818, forward nfe 12525, backward nfe 10389, Train: 0.1550, Val: 0.1492, Test: 0.1325\n",
      "Epoch: 012, Runtime 2.269243, Loss 2.177020, forward nfe 14791, backward nfe 11454, Train: 0.1700, Val: 0.1938, Test: 0.1919\n",
      "Epoch: 013, Runtime 2.218140, Loss 2.158619, forward nfe 17334, backward nfe 12488, Train: 0.2350, Val: 0.3200, Test: 0.2940\n",
      "Epoch: 014, Runtime 2.438197, Loss 2.140334, forward nfe 19956, backward nfe 13580, Train: 0.2700, Val: 0.3585, Test: 0.3319\n",
      "Epoch: 015, Runtime 2.616905, Loss 2.126538, forward nfe 23057, backward nfe 14612, Train: 0.2750, Val: 0.3515, Test: 0.3355\n",
      "Epoch: 016, Runtime 2.812380, Loss 2.111411, forward nfe 26628, backward nfe 15641, Train: 0.2650, Val: 0.3277, Test: 0.3051\n",
      "Epoch: 017, Runtime 2.830883, Loss 2.087633, forward nfe 30700, backward nfe 16669, Train: 0.2300, Val: 0.2523, Test: 0.2423\n",
      "Epoch: 018, Runtime 2.904019, Loss 2.076790, forward nfe 34363, backward nfe 17788, Train: 0.2350, Val: 0.1762, Test: 0.1726\n",
      "Epoch: 019, Runtime 2.758970, Loss 2.058443, forward nfe 38220, backward nfe 18852, Train: 0.2400, Val: 0.1377, Test: 0.1432\n",
      "Epoch: 020, Runtime 2.863437, Loss 2.038153, forward nfe 41744, backward nfe 19971, Train: 0.2950, Val: 0.1785, Test: 0.1870\n",
      "Epoch: 021, Runtime 2.454968, Loss 2.017608, forward nfe 44982, backward nfe 21011, Train: 0.3750, Val: 0.1708, Test: 0.1908\n",
      "Epoch: 022, Runtime 2.639711, Loss 2.010231, forward nfe 48457, backward nfe 22036, Train: 0.3850, Val: 0.1800, Test: 0.1992\n",
      "Epoch: 023, Runtime 2.667296, Loss 1.977849, forward nfe 51619, backward nfe 23149, Train: 0.3250, Val: 0.1808, Test: 0.1857\n",
      "Epoch: 024, Runtime 2.869057, Loss 1.963002, forward nfe 55129, backward nfe 24268, Train: 0.3100, Val: 0.1738, Test: 0.1859\n",
      "Epoch: 025, Runtime 3.122287, Loss 1.932951, forward nfe 59398, backward nfe 25387, Train: 0.3400, Val: 0.1831, Test: 0.1964\n",
      "Epoch: 026, Runtime 2.908095, Loss 1.917030, forward nfe 63239, backward nfe 26506, Train: 0.4000, Val: 0.1923, Test: 0.2063\n",
      "Epoch: 027, Runtime 3.242316, Loss 1.887994, forward nfe 67259, backward nfe 27625, Train: 0.4150, Val: 0.1938, Test: 0.2034\n",
      "Epoch: 028, Runtime 3.072551, Loss 1.868829, forward nfe 71960, backward nfe 28713, Train: 0.4200, Val: 0.1823, Test: 0.1970\n",
      "Epoch: 029, Runtime 2.794395, Loss 1.839153, forward nfe 75809, backward nfe 29804, Train: 0.4300, Val: 0.1900, Test: 0.2120\n",
      "Epoch: 030, Runtime 2.552921, Loss 1.818162, forward nfe 79447, backward nfe 30868, Train: 0.4400, Val: 0.2138, Test: 0.2372\n",
      "Epoch: 031, Runtime 2.559366, Loss 1.791903, forward nfe 82288, backward nfe 31954, Train: 0.4700, Val: 0.2300, Test: 0.2604\n",
      "Epoch: 032, Runtime 2.724670, Loss 1.771662, forward nfe 85887, backward nfe 33060, Train: 0.5100, Val: 0.2285, Test: 0.2639\n",
      "Epoch: 033, Runtime 2.467435, Loss 1.731248, forward nfe 88831, backward nfe 34179, Train: 0.5300, Val: 0.2223, Test: 0.2531\n",
      "Epoch: 034, Runtime 2.459923, Loss 1.712935, forward nfe 91893, backward nfe 35211, Train: 0.5550, Val: 0.2354, Test: 0.2633\n",
      "Epoch: 035, Runtime 2.364881, Loss 1.678785, forward nfe 94793, backward nfe 36330, Train: 0.5750, Val: 0.2654, Test: 0.3075\n",
      "Epoch: 036, Runtime 2.321241, Loss 1.655947, forward nfe 97466, backward nfe 37355, Train: 0.5750, Val: 0.2931, Test: 0.3256\n",
      "Epoch: 037, Runtime 2.283912, Loss 1.628515, forward nfe 100052, backward nfe 38445, Train: 0.5850, Val: 0.3031, Test: 0.3368\n",
      "Epoch: 038, Runtime 2.177907, Loss 1.599595, forward nfe 102539, backward nfe 39471, Train: 0.5900, Val: 0.3054, Test: 0.3435\n",
      "Epoch: 039, Runtime 2.131197, Loss 1.569387, forward nfe 104902, backward nfe 40590, Train: 0.5950, Val: 0.3069, Test: 0.3428\n",
      "Epoch: 040, Runtime 2.020807, Loss 1.534305, forward nfe 106892, backward nfe 41638, Train: 0.6150, Val: 0.3138, Test: 0.3456\n",
      "Epoch: 041, Runtime 2.073124, Loss 1.510745, forward nfe 109018, backward nfe 42680, Train: 0.6450, Val: 0.3262, Test: 0.3596\n",
      "Epoch: 042, Runtime 1.994794, Loss 1.481386, forward nfe 111153, backward nfe 43751, Train: 0.6350, Val: 0.3308, Test: 0.3734\n",
      "Epoch: 043, Runtime 1.953522, Loss 1.448448, forward nfe 113101, backward nfe 44804, Train: 0.6100, Val: 0.3292, Test: 0.3681\n",
      "Epoch: 044, Runtime 1.932468, Loss 1.438704, forward nfe 114917, backward nfe 45850, Train: 0.6450, Val: 0.3400, Test: 0.3772\n",
      "Epoch: 045, Runtime 1.836913, Loss 1.396152, forward nfe 116716, backward nfe 46874, Train: 0.6700, Val: 0.3400, Test: 0.3713\n",
      "Epoch: 046, Runtime 1.797427, Loss 1.378767, forward nfe 118382, backward nfe 47901, Train: 0.6700, Val: 0.3369, Test: 0.3653\n",
      "Epoch: 047, Runtime 1.782273, Loss 1.347019, forward nfe 119953, backward nfe 48930, Train: 0.6700, Val: 0.3446, Test: 0.3746\n",
      "Epoch: 048, Runtime 1.790190, Loss 1.319521, forward nfe 121548, backward nfe 50007, Train: 0.6600, Val: 0.3515, Test: 0.3825\n",
      "Epoch: 049, Runtime 1.749939, Loss 1.296543, forward nfe 123034, backward nfe 51054, Train: 0.6750, Val: 0.3569, Test: 0.3855\n",
      "Epoch: 050, Runtime 1.732114, Loss 1.279746, forward nfe 124487, backward nfe 52084, Train: 0.6950, Val: 0.3608, Test: 0.3887\n",
      "Epoch: 051, Runtime 1.739689, Loss 1.260861, forward nfe 125939, backward nfe 53124, Train: 0.6950, Val: 0.3608, Test: 0.3996\n",
      "Epoch: 052, Runtime 1.703232, Loss 1.228385, forward nfe 127387, backward nfe 54157, Train: 0.6900, Val: 0.3654, Test: 0.4096\n",
      "Epoch: 053, Runtime 1.663516, Loss 1.205843, forward nfe 128802, backward nfe 55185, Train: 0.6950, Val: 0.3800, Test: 0.4218\n",
      "Epoch: 054, Runtime 1.680105, Loss 1.188025, forward nfe 130131, backward nfe 56218, Train: 0.7050, Val: 0.3962, Test: 0.4401\n",
      "Epoch: 055, Runtime 1.647584, Loss 1.165393, forward nfe 131439, backward nfe 57252, Train: 0.6950, Val: 0.4138, Test: 0.4524\n",
      "Epoch: 056, Runtime 1.648536, Loss 1.151271, forward nfe 132727, backward nfe 58286, Train: 0.7000, Val: 0.4138, Test: 0.4565\n",
      "Epoch: 057, Runtime 1.641613, Loss 1.120061, forward nfe 134023, backward nfe 59327, Train: 0.6900, Val: 0.4146, Test: 0.4592\n",
      "Epoch: 058, Runtime 1.615814, Loss 1.115715, forward nfe 135227, backward nfe 60351, Train: 0.6900, Val: 0.4285, Test: 0.4645\n",
      "Epoch: 059, Runtime 1.610622, Loss 1.082401, forward nfe 136449, backward nfe 61383, Train: 0.7250, Val: 0.4323, Test: 0.4714\n",
      "Epoch: 060, Runtime 1.597615, Loss 1.063464, forward nfe 137647, backward nfe 62407, Train: 0.7150, Val: 0.4346, Test: 0.4715\n",
      "Epoch: 061, Runtime 1.595039, Loss 1.046555, forward nfe 138850, backward nfe 63432, Train: 0.7050, Val: 0.4369, Test: 0.4678\n",
      "Epoch: 062, Runtime 1.565441, Loss 1.030962, forward nfe 139995, backward nfe 64457, Train: 0.7200, Val: 0.4400, Test: 0.4700\n",
      "Epoch: 063, Runtime 1.560326, Loss 1.021809, forward nfe 141110, backward nfe 65485, Train: 0.7250, Val: 0.4423, Test: 0.4721\n",
      "Epoch: 064, Runtime 1.545094, Loss 0.993283, forward nfe 142216, backward nfe 66509, Train: 0.7200, Val: 0.4562, Test: 0.4868\n",
      "Epoch: 065, Runtime 1.535905, Loss 0.978266, forward nfe 143270, backward nfe 67534, Train: 0.7250, Val: 0.4846, Test: 0.5114\n",
      "Epoch: 066, Runtime 1.551699, Loss 0.966620, forward nfe 144335, backward nfe 68558, Train: 0.7200, Val: 0.4954, Test: 0.5214\n",
      "Epoch: 067, Runtime 1.533040, Loss 0.940377, forward nfe 145402, backward nfe 69582, Train: 0.7500, Val: 0.4885, Test: 0.5170\n",
      "Epoch: 068, Runtime 1.542778, Loss 0.939048, forward nfe 146470, backward nfe 70607, Train: 0.7300, Val: 0.4846, Test: 0.5163\n",
      "Epoch: 069, Runtime 1.545347, Loss 0.918008, forward nfe 147536, backward nfe 71638, Train: 0.7250, Val: 0.5069, Test: 0.5380\n",
      "Epoch: 070, Runtime 1.520558, Loss 0.907925, forward nfe 148589, backward nfe 72662, Train: 0.7400, Val: 0.5223, Test: 0.5527\n",
      "Epoch: 071, Runtime 1.532968, Loss 0.888892, forward nfe 149610, backward nfe 73686, Train: 0.7500, Val: 0.5192, Test: 0.5495\n",
      "Epoch: 072, Runtime 1.524904, Loss 0.868531, forward nfe 150652, backward nfe 74715, Train: 0.7400, Val: 0.5123, Test: 0.5390\n",
      "Epoch: 073, Runtime 1.521663, Loss 0.862040, forward nfe 151687, backward nfe 75739, Train: 0.7550, Val: 0.5246, Test: 0.5519\n",
      "Epoch: 074, Runtime 1.516147, Loss 0.845011, forward nfe 152711, backward nfe 76764, Train: 0.7550, Val: 0.5754, Test: 0.5950\n",
      "Epoch: 075, Runtime 1.515048, Loss 0.823110, forward nfe 153714, backward nfe 77789, Train: 0.7550, Val: 0.6015, Test: 0.6214\n",
      "Epoch: 076, Runtime 1.516004, Loss 0.812974, forward nfe 154730, backward nfe 78814, Train: 0.7650, Val: 0.5838, Test: 0.6085\n",
      "Epoch: 077, Runtime 1.509598, Loss 0.791824, forward nfe 155740, backward nfe 79840, Train: 0.7750, Val: 0.5731, Test: 0.5904\n",
      "Epoch: 078, Runtime 1.504240, Loss 0.787872, forward nfe 156726, backward nfe 80865, Train: 0.7700, Val: 0.5938, Test: 0.6138\n",
      "Epoch: 079, Runtime 1.497742, Loss 0.770251, forward nfe 157711, backward nfe 81889, Train: 0.7750, Val: 0.6131, Test: 0.6384\n",
      "Epoch: 080, Runtime 1.507074, Loss 0.765302, forward nfe 158700, backward nfe 82915, Train: 0.7850, Val: 0.6223, Test: 0.6470\n",
      "Epoch: 081, Runtime 1.498329, Loss 0.761757, forward nfe 159693, backward nfe 83939, Train: 0.7800, Val: 0.6015, Test: 0.6260\n",
      "Epoch: 082, Runtime 1.498888, Loss 0.748484, forward nfe 160673, backward nfe 84963, Train: 0.7900, Val: 0.6354, Test: 0.6518\n",
      "Epoch: 083, Runtime 1.500996, Loss 0.724448, forward nfe 161647, backward nfe 85990, Train: 0.7950, Val: 0.6369, Test: 0.6653\n",
      "Epoch: 084, Runtime 1.500055, Loss 0.731790, forward nfe 162630, backward nfe 87013, Train: 0.7900, Val: 0.6385, Test: 0.6654\n",
      "Epoch: 085, Runtime 1.499758, Loss 0.716433, forward nfe 163606, backward nfe 88037, Train: 0.7900, Val: 0.6377, Test: 0.6636\n",
      "Epoch: 086, Runtime 1.501312, Loss 0.703659, forward nfe 164593, backward nfe 89061, Train: 0.8050, Val: 0.6431, Test: 0.6582\n",
      "Epoch: 087, Runtime 1.500169, Loss 0.697889, forward nfe 165571, backward nfe 90084, Train: 0.8050, Val: 0.6446, Test: 0.6633\n",
      "Epoch: 088, Runtime 1.497237, Loss 0.684873, forward nfe 166552, backward nfe 91107, Train: 0.8050, Val: 0.6508, Test: 0.6734\n",
      "Epoch: 089, Runtime 1.502679, Loss 0.692931, forward nfe 167533, backward nfe 92130, Train: 0.8100, Val: 0.6546, Test: 0.6746\n",
      "Epoch: 090, Runtime 1.494630, Loss 0.681996, forward nfe 168512, backward nfe 93153, Train: 0.8150, Val: 0.6500, Test: 0.6756\n",
      "Epoch: 091, Runtime 1.498682, Loss 0.663863, forward nfe 169485, backward nfe 94177, Train: 0.8200, Val: 0.6500, Test: 0.6698\n",
      "Epoch: 092, Runtime 1.496339, Loss 0.643438, forward nfe 170459, backward nfe 95201, Train: 0.8200, Val: 0.6431, Test: 0.6680\n",
      "Epoch: 093, Runtime 1.499870, Loss 0.645283, forward nfe 171434, backward nfe 96224, Train: 0.8200, Val: 0.6392, Test: 0.6636\n",
      "Epoch: 094, Runtime 1.493028, Loss 0.645343, forward nfe 172402, backward nfe 97247, Train: 0.8200, Val: 0.6431, Test: 0.6673\n",
      "Epoch: 095, Runtime 1.499675, Loss 0.639837, forward nfe 173382, backward nfe 98272, Train: 0.8250, Val: 0.6492, Test: 0.6720\n",
      "Epoch: 096, Runtime 1.495592, Loss 0.614790, forward nfe 174354, backward nfe 99295, Train: 0.8200, Val: 0.6531, Test: 0.6726\n",
      "Epoch: 097, Runtime 1.492878, Loss 0.624859, forward nfe 175320, backward nfe 100319, Train: 0.8300, Val: 0.6531, Test: 0.6761\n",
      "Epoch: 098, Runtime 1.497887, Loss 0.607577, forward nfe 176297, backward nfe 101342, Train: 0.8250, Val: 0.6546, Test: 0.6772\n",
      "Epoch: 099, Runtime 1.501978, Loss 0.602224, forward nfe 177273, backward nfe 102365, Train: 0.8250, Val: 0.6677, Test: 0.6838\n",
      "best val accuracy 0.667692 with test accuracy 0.683807 at epoch 99\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.596229, Loss 2.314051, forward nfe 444, backward nfe 71, Train: 0.1000, Val: 0.0223, Test: 0.0357\n",
      "Epoch: 002, Runtime 1.491484, Loss 2.545168, forward nfe 1402, backward nfe 1095, Train: 0.1050, Val: 0.0554, Test: 0.0621\n",
      "Epoch: 003, Runtime 1.490626, Loss 2.319571, forward nfe 2361, backward nfe 2119, Train: 0.1250, Val: 0.2762, Test: 0.2708\n",
      "Epoch: 004, Runtime 1.503613, Loss 2.382843, forward nfe 3332, backward nfe 3144, Train: 0.1150, Val: 0.0538, Test: 0.0632\n",
      "Epoch: 005, Runtime 1.508544, Loss 2.306698, forward nfe 4322, backward nfe 4168, Train: 0.1950, Val: 0.1277, Test: 0.1326\n",
      "Epoch: 006, Runtime 1.513880, Loss 2.216791, forward nfe 5322, backward nfe 5194, Train: 0.2150, Val: 0.1308, Test: 0.1433\n",
      "Epoch: 007, Runtime 1.516324, Loss 2.212255, forward nfe 6349, backward nfe 6219, Train: 0.2050, Val: 0.1262, Test: 0.1402\n",
      "Epoch: 008, Runtime 1.545602, Loss 2.219440, forward nfe 7384, backward nfe 7245, Train: 0.1900, Val: 0.1054, Test: 0.1214\n",
      "Epoch: 009, Runtime 1.584928, Loss 2.176368, forward nfe 8486, backward nfe 8274, Train: 0.2300, Val: 0.1131, Test: 0.1153\n",
      "Epoch: 010, Runtime 1.624827, Loss 2.139942, forward nfe 9691, backward nfe 9302, Train: 0.2800, Val: 0.1346, Test: 0.1632\n",
      "Epoch: 011, Runtime 1.697844, Loss 2.131481, forward nfe 11044, backward nfe 10327, Train: 0.3250, Val: 0.1446, Test: 0.1773\n",
      "Epoch: 012, Runtime 1.694018, Loss 2.104035, forward nfe 12404, backward nfe 11351, Train: 0.2350, Val: 0.1492, Test: 0.1608\n",
      "Epoch: 013, Runtime 1.771847, Loss 2.071465, forward nfe 13909, backward nfe 12386, Train: 0.3000, Val: 0.1685, Test: 0.1870\n",
      "Epoch: 014, Runtime 1.782124, Loss 2.042344, forward nfe 15491, backward nfe 13426, Train: 0.3200, Val: 0.2000, Test: 0.2182\n",
      "Epoch: 015, Runtime 1.714958, Loss 2.025836, forward nfe 16954, backward nfe 14453, Train: 0.3300, Val: 0.1723, Test: 0.1927\n",
      "Epoch: 016, Runtime 1.803074, Loss 1.999235, forward nfe 18494, backward nfe 15481, Train: 0.3200, Val: 0.1685, Test: 0.1839\n",
      "Epoch: 017, Runtime 1.857704, Loss 1.965544, forward nfe 20118, backward nfe 16508, Train: 0.3100, Val: 0.1992, Test: 0.2119\n",
      "Epoch: 018, Runtime 1.823791, Loss 1.943422, forward nfe 21902, backward nfe 17533, Train: 0.4000, Val: 0.2631, Test: 0.3004\n",
      "Epoch: 019, Runtime 1.952069, Loss 1.915578, forward nfe 23773, backward nfe 18560, Train: 0.4650, Val: 0.2923, Test: 0.3235\n",
      "Epoch: 020, Runtime 1.865183, Loss 1.881904, forward nfe 25533, backward nfe 19608, Train: 0.5300, Val: 0.2862, Test: 0.3244\n",
      "Epoch: 021, Runtime 1.939487, Loss 1.830797, forward nfe 27360, backward nfe 20639, Train: 0.5250, Val: 0.4854, Test: 0.5113\n",
      "Epoch: 022, Runtime 1.937020, Loss 1.777452, forward nfe 29316, backward nfe 21663, Train: 0.5200, Val: 0.5000, Test: 0.5091\n",
      "Epoch: 023, Runtime 2.012435, Loss 1.761172, forward nfe 31254, backward nfe 22703, Train: 0.5100, Val: 0.4985, Test: 0.5102\n",
      "Epoch: 024, Runtime 2.013952, Loss 1.722016, forward nfe 33353, backward nfe 23729, Train: 0.6650, Val: 0.6185, Test: 0.6240\n",
      "Epoch: 025, Runtime 1.982719, Loss 1.664119, forward nfe 35377, backward nfe 24766, Train: 0.7050, Val: 0.6662, Test: 0.6738\n",
      "Epoch: 026, Runtime 2.006789, Loss 1.639592, forward nfe 37390, backward nfe 25811, Train: 0.6950, Val: 0.7000, Test: 0.7002\n",
      "Epoch: 027, Runtime 1.914927, Loss 1.606103, forward nfe 39434, backward nfe 26836, Train: 0.6800, Val: 0.7100, Test: 0.7072\n",
      "Epoch: 028, Runtime 1.851688, Loss 1.544561, forward nfe 41104, backward nfe 27885, Train: 0.5100, Val: 0.7008, Test: 0.6715\n",
      "Epoch: 029, Runtime 1.822214, Loss 1.516751, forward nfe 42789, backward nfe 28924, Train: 0.6100, Val: 0.6669, Test: 0.6626\n",
      "Epoch: 030, Runtime 1.836661, Loss 1.464552, forward nfe 44452, backward nfe 29957, Train: 0.6750, Val: 0.6923, Test: 0.6859\n",
      "Epoch: 031, Runtime 1.868745, Loss 1.425221, forward nfe 46148, backward nfe 31009, Train: 0.7650, Val: 0.6969, Test: 0.7076\n",
      "Epoch: 032, Runtime 1.827023, Loss 1.382142, forward nfe 47851, backward nfe 32044, Train: 0.7500, Val: 0.5177, Test: 0.5624\n",
      "Epoch: 033, Runtime 1.847842, Loss 1.353223, forward nfe 49459, backward nfe 33072, Train: 0.7550, Val: 0.5577, Test: 0.5950\n",
      "Epoch: 034, Runtime 1.851662, Loss 1.319019, forward nfe 51220, backward nfe 34099, Train: 0.7750, Val: 0.6262, Test: 0.6583\n",
      "Epoch: 035, Runtime 1.923701, Loss 1.272705, forward nfe 52995, backward nfe 35161, Train: 0.7400, Val: 0.6192, Test: 0.6388\n",
      "Epoch: 036, Runtime 1.910779, Loss 1.239504, forward nfe 54895, backward nfe 36209, Train: 0.7450, Val: 0.6231, Test: 0.6425\n",
      "Epoch: 037, Runtime 1.902061, Loss 1.208359, forward nfe 56661, backward nfe 37250, Train: 0.7800, Val: 0.6454, Test: 0.6727\n",
      "Epoch: 038, Runtime 1.990918, Loss 1.170975, forward nfe 58608, backward nfe 38333, Train: 0.8350, Val: 0.6808, Test: 0.7080\n",
      "Epoch: 039, Runtime 1.847211, Loss 1.144359, forward nfe 60337, backward nfe 39359, Train: 0.8300, Val: 0.7185, Test: 0.7368\n",
      "Epoch: 040, Runtime 1.863104, Loss 1.110788, forward nfe 62084, backward nfe 40394, Train: 0.8400, Val: 0.7138, Test: 0.7393\n",
      "Epoch: 041, Runtime 1.875394, Loss 1.079574, forward nfe 63836, backward nfe 41437, Train: 0.8500, Val: 0.6931, Test: 0.7116\n",
      "Epoch: 042, Runtime 1.901475, Loss 1.045993, forward nfe 65673, backward nfe 42465, Train: 0.8300, Val: 0.6677, Test: 0.6869\n",
      "Epoch: 043, Runtime 1.839918, Loss 1.028627, forward nfe 67447, backward nfe 43490, Train: 0.8300, Val: 0.6746, Test: 0.6933\n",
      "Epoch: 044, Runtime 1.833192, Loss 0.992772, forward nfe 69086, backward nfe 44546, Train: 0.8400, Val: 0.6800, Test: 0.7015\n",
      "Epoch: 045, Runtime 1.882514, Loss 0.966568, forward nfe 70783, backward nfe 45625, Train: 0.8600, Val: 0.6977, Test: 0.7205\n",
      "Epoch: 046, Runtime 1.821447, Loss 0.938350, forward nfe 72465, backward nfe 46667, Train: 0.8600, Val: 0.7262, Test: 0.7455\n",
      "Epoch: 047, Runtime 1.762774, Loss 0.922695, forward nfe 74037, backward nfe 47708, Train: 0.8600, Val: 0.7338, Test: 0.7537\n",
      "Epoch: 048, Runtime 1.740365, Loss 0.887314, forward nfe 75491, backward nfe 48740, Train: 0.8600, Val: 0.7208, Test: 0.7401\n",
      "Epoch: 049, Runtime 1.760155, Loss 0.871541, forward nfe 77020, backward nfe 49770, Train: 0.8550, Val: 0.7077, Test: 0.7222\n",
      "Epoch: 050, Runtime 1.810107, Loss 0.855720, forward nfe 78607, backward nfe 50806, Train: 0.8500, Val: 0.7008, Test: 0.7181\n",
      "Epoch: 051, Runtime 1.768996, Loss 0.836189, forward nfe 80180, backward nfe 51833, Train: 0.8650, Val: 0.7054, Test: 0.7237\n",
      "Epoch: 052, Runtime 1.756696, Loss 0.805658, forward nfe 81753, backward nfe 52857, Train: 0.8750, Val: 0.7115, Test: 0.7314\n",
      "Epoch: 053, Runtime 1.736946, Loss 0.788482, forward nfe 83311, backward nfe 53888, Train: 0.8700, Val: 0.7231, Test: 0.7385\n",
      "Epoch: 054, Runtime 1.682445, Loss 0.776257, forward nfe 84658, backward nfe 54913, Train: 0.8700, Val: 0.7185, Test: 0.7376\n",
      "Epoch: 055, Runtime 1.672168, Loss 0.754598, forward nfe 86058, backward nfe 55941, Train: 0.8700, Val: 0.7215, Test: 0.7365\n",
      "Epoch: 056, Runtime 1.674632, Loss 0.737904, forward nfe 87347, backward nfe 56987, Train: 0.8600, Val: 0.7308, Test: 0.7448\n",
      "Epoch: 057, Runtime 1.657948, Loss 0.716654, forward nfe 88656, backward nfe 58015, Train: 0.8600, Val: 0.7369, Test: 0.7477\n",
      "Epoch: 058, Runtime 1.633853, Loss 0.698876, forward nfe 89938, backward nfe 59039, Train: 0.8600, Val: 0.7269, Test: 0.7420\n",
      "Epoch: 059, Runtime 1.636375, Loss 0.689244, forward nfe 91200, backward nfe 60083, Train: 0.8700, Val: 0.7269, Test: 0.7379\n",
      "Epoch: 060, Runtime 1.606128, Loss 0.669356, forward nfe 92412, backward nfe 61120, Train: 0.8800, Val: 0.7331, Test: 0.7464\n",
      "Epoch: 061, Runtime 1.585219, Loss 0.657806, forward nfe 93591, backward nfe 62146, Train: 0.8850, Val: 0.7446, Test: 0.7524\n",
      "Epoch: 062, Runtime 1.608212, Loss 0.644642, forward nfe 94750, backward nfe 63185, Train: 0.8800, Val: 0.7423, Test: 0.7533\n",
      "Epoch: 063, Runtime 1.592447, Loss 0.628392, forward nfe 95941, backward nfe 64215, Train: 0.8700, Val: 0.7323, Test: 0.7487\n",
      "Epoch: 064, Runtime 1.582573, Loss 0.608197, forward nfe 97095, backward nfe 65242, Train: 0.8700, Val: 0.7385, Test: 0.7479\n",
      "Epoch: 065, Runtime 1.567251, Loss 0.602324, forward nfe 98242, backward nfe 66267, Train: 0.8600, Val: 0.7415, Test: 0.7517\n",
      "Epoch: 066, Runtime 1.553335, Loss 0.595032, forward nfe 99337, backward nfe 67291, Train: 0.8750, Val: 0.7523, Test: 0.7599\n",
      "Epoch: 067, Runtime 1.571753, Loss 0.569937, forward nfe 100451, backward nfe 68315, Train: 0.8800, Val: 0.7492, Test: 0.7631\n",
      "Epoch: 068, Runtime 1.536168, Loss 0.567967, forward nfe 101565, backward nfe 69340, Train: 0.8800, Val: 0.7508, Test: 0.7616\n",
      "Epoch: 069, Runtime 1.572747, Loss 0.551824, forward nfe 102631, backward nfe 70385, Train: 0.8700, Val: 0.7500, Test: 0.7588\n",
      "Epoch: 070, Runtime 1.549117, Loss 0.537162, forward nfe 103730, backward nfe 71412, Train: 0.8750, Val: 0.7538, Test: 0.7612\n",
      "Epoch: 071, Runtime 1.538716, Loss 0.530413, forward nfe 104802, backward nfe 72436, Train: 0.8750, Val: 0.7623, Test: 0.7721\n",
      "Epoch: 072, Runtime 1.548886, Loss 0.526967, forward nfe 105867, backward nfe 73470, Train: 0.8800, Val: 0.7615, Test: 0.7729\n",
      "Epoch: 073, Runtime 1.538636, Loss 0.511377, forward nfe 106913, backward nfe 74496, Train: 0.8800, Val: 0.7577, Test: 0.7671\n",
      "Epoch: 074, Runtime 1.536472, Loss 0.501605, forward nfe 107964, backward nfe 75525, Train: 0.8800, Val: 0.7577, Test: 0.7685\n",
      "Epoch: 075, Runtime 1.534416, Loss 0.484781, forward nfe 109023, backward nfe 76549, Train: 0.8800, Val: 0.7685, Test: 0.7770\n",
      "Epoch: 076, Runtime 1.519635, Loss 0.476730, forward nfe 110062, backward nfe 77574, Train: 0.8900, Val: 0.7700, Test: 0.7839\n",
      "Epoch: 077, Runtime 1.529190, Loss 0.460153, forward nfe 111100, backward nfe 78598, Train: 0.8900, Val: 0.7723, Test: 0.7822\n",
      "Epoch: 078, Runtime 1.528634, Loss 0.457397, forward nfe 112136, backward nfe 79623, Train: 0.8850, Val: 0.7677, Test: 0.7773\n",
      "Epoch: 079, Runtime 1.523904, Loss 0.446723, forward nfe 113185, backward nfe 80648, Train: 0.8850, Val: 0.7662, Test: 0.7749\n",
      "Epoch: 080, Runtime 1.525648, Loss 0.438332, forward nfe 114208, backward nfe 81677, Train: 0.8900, Val: 0.7723, Test: 0.7827\n",
      "Epoch: 081, Runtime 1.531709, Loss 0.431239, forward nfe 115242, backward nfe 82702, Train: 0.8900, Val: 0.7738, Test: 0.7880\n",
      "Epoch: 082, Runtime 1.534240, Loss 0.423183, forward nfe 116301, backward nfe 83727, Train: 0.8900, Val: 0.7708, Test: 0.7876\n",
      "Epoch: 083, Runtime 1.531700, Loss 0.416395, forward nfe 117346, backward nfe 84752, Train: 0.8950, Val: 0.7692, Test: 0.7826\n",
      "Epoch: 084, Runtime 1.548373, Loss 0.406494, forward nfe 118405, backward nfe 85784, Train: 0.8950, Val: 0.7608, Test: 0.7794\n",
      "Epoch: 085, Runtime 1.534309, Loss 0.405870, forward nfe 119469, backward nfe 86814, Train: 0.8850, Val: 0.7808, Test: 0.7923\n",
      "Epoch: 086, Runtime 1.534302, Loss 0.404974, forward nfe 120510, backward nfe 87839, Train: 0.8950, Val: 0.7731, Test: 0.7920\n",
      "Epoch: 087, Runtime 1.547939, Loss 0.398961, forward nfe 121574, backward nfe 88863, Train: 0.9100, Val: 0.7692, Test: 0.7911\n",
      "Epoch: 088, Runtime 1.531962, Loss 0.387406, forward nfe 122643, backward nfe 89890, Train: 0.8850, Val: 0.7792, Test: 0.7991\n",
      "Epoch: 089, Runtime 1.537331, Loss 0.382282, forward nfe 123700, backward nfe 90914, Train: 0.8900, Val: 0.7823, Test: 0.7920\n",
      "Epoch: 090, Runtime 1.542531, Loss 0.379498, forward nfe 124757, backward nfe 91938, Train: 0.9100, Val: 0.7708, Test: 0.7870\n",
      "Epoch: 091, Runtime 1.545006, Loss 0.372899, forward nfe 125828, backward nfe 92964, Train: 0.9150, Val: 0.7754, Test: 0.7918\n",
      "Epoch: 092, Runtime 1.542174, Loss 0.360615, forward nfe 126902, backward nfe 93992, Train: 0.8850, Val: 0.7862, Test: 0.8052\n",
      "Epoch: 093, Runtime 1.541820, Loss 0.376544, forward nfe 127971, backward nfe 95018, Train: 0.8950, Val: 0.7846, Test: 0.8032\n",
      "Epoch: 094, Runtime 1.527750, Loss 0.354895, forward nfe 129020, backward nfe 96042, Train: 0.9100, Val: 0.7731, Test: 0.7899\n",
      "Epoch: 095, Runtime 1.546217, Loss 0.356877, forward nfe 130074, backward nfe 97067, Train: 0.9100, Val: 0.7746, Test: 0.7901\n",
      "Epoch: 096, Runtime 1.530419, Loss 0.341198, forward nfe 131126, backward nfe 98092, Train: 0.8950, Val: 0.7831, Test: 0.8007\n",
      "Epoch: 097, Runtime 1.531956, Loss 0.334855, forward nfe 132181, backward nfe 99115, Train: 0.8950, Val: 0.7823, Test: 0.8017\n",
      "Epoch: 098, Runtime 1.527456, Loss 0.332457, forward nfe 133218, backward nfe 100140, Train: 0.9150, Val: 0.7762, Test: 0.7986\n",
      "Epoch: 099, Runtime 1.546373, Loss 0.323815, forward nfe 134280, backward nfe 101165, Train: 0.9150, Val: 0.7792, Test: 0.8025\n",
      "best val accuracy 0.786154 with test accuracy 0.805175 at epoch 92\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.1, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.1, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 0.578389, Loss 2.322957, forward nfe 417, backward nfe 66, Train: 0.1050, Val: 0.3700, Test: 0.3429\n",
      "Epoch: 002, Runtime 1.489183, Loss 2.653935, forward nfe 1374, backward nfe 1090, Train: 0.1000, Val: 0.0562, Test: 0.0593\n",
      "Epoch: 003, Runtime 1.501467, Loss 2.557802, forward nfe 2337, backward nfe 2116, Train: 0.1000, Val: 0.0223, Test: 0.0404\n",
      "Epoch: 004, Runtime 1.515002, Loss 2.340688, forward nfe 3342, backward nfe 3140, Train: 0.1800, Val: 0.0308, Test: 0.0503\n",
      "Epoch: 005, Runtime 1.518864, Loss 2.358947, forward nfe 4359, backward nfe 4165, Train: 0.1950, Val: 0.0315, Test: 0.0520\n",
      "Epoch: 006, Runtime 1.533961, Loss 2.250401, forward nfe 5378, backward nfe 5189, Train: 0.2250, Val: 0.0700, Test: 0.0849\n",
      "Epoch: 007, Runtime 1.578587, Loss 2.180869, forward nfe 6451, backward nfe 6218, Train: 0.1200, Val: 0.0546, Test: 0.0613\n",
      "Epoch: 008, Runtime 1.633807, Loss 2.206161, forward nfe 7639, backward nfe 7245, Train: 0.1150, Val: 0.0585, Test: 0.0625\n",
      "Epoch: 009, Runtime 1.812127, Loss 2.186911, forward nfe 9025, backward nfe 8272, Train: 0.2300, Val: 0.1492, Test: 0.1588\n",
      "Epoch: 010, Runtime 1.981893, Loss 2.148816, forward nfe 10834, backward nfe 9312, Train: 0.2750, Val: 0.1446, Test: 0.1587\n",
      "Epoch: 011, Runtime 2.170184, Loss 2.126715, forward nfe 13100, backward nfe 10349, Train: 0.2550, Val: 0.0931, Test: 0.1137\n",
      "Epoch: 012, Runtime 2.302067, Loss 2.127982, forward nfe 15642, backward nfe 11393, Train: 0.2600, Val: 0.1423, Test: 0.1483\n",
      "Epoch: 013, Runtime 2.493113, Loss 2.092741, forward nfe 18647, backward nfe 12428, Train: 0.2850, Val: 0.1546, Test: 0.1756\n",
      "Epoch: 014, Runtime 2.736722, Loss 2.064942, forward nfe 21955, backward nfe 13506, Train: 0.3000, Val: 0.1754, Test: 0.1942\n",
      "Epoch: 015, Runtime 2.960501, Loss 2.048332, forward nfe 25861, backward nfe 14553, Train: 0.3300, Val: 0.1931, Test: 0.2032\n",
      "Epoch: 016, Runtime 3.090171, Loss 2.025043, forward nfe 29820, backward nfe 15625, Train: 0.3600, Val: 0.1762, Test: 0.1882\n",
      "Epoch: 017, Runtime 3.341669, Loss 2.007839, forward nfe 34650, backward nfe 16724, Train: 0.3150, Val: 0.1692, Test: 0.1731\n",
      "Epoch: 018, Runtime 3.308182, Loss 1.987502, forward nfe 39155, backward nfe 17809, Train: 0.3650, Val: 0.1946, Test: 0.2091\n",
      "Epoch: 019, Runtime 3.238881, Loss 1.957376, forward nfe 43880, backward nfe 18928, Train: 0.3550, Val: 0.2100, Test: 0.2163\n",
      "Epoch: 020, Runtime 3.366341, Loss 1.933548, forward nfe 48793, backward nfe 19957, Train: 0.3450, Val: 0.2077, Test: 0.2166\n",
      "Epoch: 021, Runtime 3.208616, Loss 1.909840, forward nfe 53282, backward nfe 20987, Train: 0.3450, Val: 0.1992, Test: 0.2060\n",
      "Epoch: 022, Runtime 3.486914, Loss 1.888232, forward nfe 58652, backward nfe 22106, Train: 0.3800, Val: 0.1946, Test: 0.2071\n",
      "Epoch: 023, Runtime 3.351959, Loss 1.867960, forward nfe 63382, backward nfe 23151, Train: 0.4350, Val: 0.2354, Test: 0.2480\n",
      "Epoch: 024, Runtime 3.413673, Loss 1.843900, forward nfe 68252, backward nfe 24180, Train: 0.4100, Val: 0.2200, Test: 0.2243\n",
      "Epoch: 025, Runtime 3.627156, Loss 1.815873, forward nfe 73607, backward nfe 25299, Train: 0.4100, Val: 0.2000, Test: 0.2118\n",
      "Epoch: 026, Runtime 3.305424, Loss 1.795734, forward nfe 78709, backward nfe 26418, Train: 0.4050, Val: 0.1969, Test: 0.2108\n",
      "Epoch: 027, Runtime 3.353696, Loss 1.781820, forward nfe 83207, backward nfe 27443, Train: 0.4200, Val: 0.1969, Test: 0.2151\n",
      "Epoch: 028, Runtime 3.396582, Loss 1.758904, forward nfe 88354, backward nfe 28551, Train: 0.4550, Val: 0.1992, Test: 0.2213\n",
      "Epoch: 029, Runtime 3.595341, Loss 1.735722, forward nfe 93846, backward nfe 29670, Train: 0.4450, Val: 0.2031, Test: 0.2214\n",
      "Epoch: 030, Runtime 3.664580, Loss 1.722551, forward nfe 98996, backward nfe 30789, Train: 0.4550, Val: 0.1938, Test: 0.2170\n",
      "Epoch: 031, Runtime 3.324694, Loss 1.705358, forward nfe 104489, backward nfe 31828, Train: 0.4450, Val: 0.1900, Test: 0.2141\n",
      "Epoch: 032, Runtime 3.361644, Loss 1.685410, forward nfe 109120, backward nfe 32880, Train: 0.4500, Val: 0.1900, Test: 0.2153\n",
      "Epoch: 033, Runtime 3.065392, Loss 1.663001, forward nfe 113709, backward nfe 33996, Train: 0.4600, Val: 0.1923, Test: 0.2194\n",
      "Epoch: 034, Runtime 2.848659, Loss 1.646175, forward nfe 117530, backward nfe 35115, Train: 0.4700, Val: 0.1969, Test: 0.2249\n",
      "Epoch: 035, Runtime 2.847833, Loss 1.631899, forward nfe 121312, backward nfe 36203, Train: 0.4750, Val: 0.1985, Test: 0.2305\n",
      "Epoch: 036, Runtime 2.700195, Loss 1.618649, forward nfe 124910, backward nfe 37277, Train: 0.5050, Val: 0.2046, Test: 0.2351\n",
      "Epoch: 037, Runtime 2.701805, Loss 1.598940, forward nfe 128441, backward nfe 38305, Train: 0.5450, Val: 0.2054, Test: 0.2367\n",
      "Epoch: 038, Runtime 2.637370, Loss 1.578775, forward nfe 131870, backward nfe 39343, Train: 0.5500, Val: 0.2038, Test: 0.2364\n",
      "Epoch: 039, Runtime 2.619913, Loss 1.566140, forward nfe 135118, backward nfe 40425, Train: 0.5450, Val: 0.2038, Test: 0.2384\n",
      "Epoch: 040, Runtime 2.553032, Loss 1.551137, forward nfe 138664, backward nfe 41495, Train: 0.5450, Val: 0.2177, Test: 0.2473\n",
      "Epoch: 041, Runtime 2.377262, Loss 1.527972, forward nfe 141493, backward nfe 42523, Train: 0.5600, Val: 0.2285, Test: 0.2578\n",
      "Epoch: 042, Runtime 2.350140, Loss 1.514032, forward nfe 144370, backward nfe 43587, Train: 0.5900, Val: 0.2315, Test: 0.2661\n",
      "Epoch: 043, Runtime 2.192668, Loss 1.484939, forward nfe 147051, backward nfe 44612, Train: 0.6000, Val: 0.2262, Test: 0.2673\n",
      "Epoch: 044, Runtime 2.119381, Loss 1.472830, forward nfe 149354, backward nfe 45637, Train: 0.6000, Val: 0.2254, Test: 0.2667\n",
      "Epoch: 045, Runtime 2.022892, Loss 1.450459, forward nfe 151469, backward nfe 46676, Train: 0.5900, Val: 0.2423, Test: 0.2710\n",
      "Epoch: 046, Runtime 1.999009, Loss 1.429854, forward nfe 153543, backward nfe 47720, Train: 0.5900, Val: 0.2515, Test: 0.2826\n",
      "Epoch: 047, Runtime 2.084255, Loss 1.401914, forward nfe 155614, backward nfe 48749, Train: 0.6050, Val: 0.2677, Test: 0.3010\n",
      "Epoch: 048, Runtime 2.061898, Loss 1.377303, forward nfe 157789, backward nfe 49778, Train: 0.6200, Val: 0.2762, Test: 0.3155\n",
      "Epoch: 049, Runtime 1.975652, Loss 1.352408, forward nfe 160001, backward nfe 50806, Train: 0.6250, Val: 0.2862, Test: 0.3242\n",
      "Epoch: 050, Runtime 1.929893, Loss 1.330372, forward nfe 161946, backward nfe 51841, Train: 0.6250, Val: 0.2946, Test: 0.3368\n",
      "Epoch: 051, Runtime 1.965620, Loss 1.309304, forward nfe 163849, backward nfe 52865, Train: 0.6500, Val: 0.3115, Test: 0.3623\n",
      "Epoch: 052, Runtime 1.921707, Loss 1.282132, forward nfe 165799, backward nfe 53921, Train: 0.6700, Val: 0.3338, Test: 0.3879\n",
      "Epoch: 053, Runtime 1.831807, Loss 1.247889, forward nfe 167483, backward nfe 54966, Train: 0.6900, Val: 0.3600, Test: 0.4073\n",
      "Epoch: 054, Runtime 1.912012, Loss 1.224421, forward nfe 169197, backward nfe 56009, Train: 0.7100, Val: 0.3777, Test: 0.4201\n",
      "Epoch: 055, Runtime 1.820018, Loss 1.201885, forward nfe 170969, backward nfe 57071, Train: 0.7150, Val: 0.3892, Test: 0.4363\n",
      "Epoch: 056, Runtime 1.879774, Loss 1.177623, forward nfe 172582, backward nfe 58108, Train: 0.7050, Val: 0.4223, Test: 0.4669\n",
      "Epoch: 057, Runtime 1.831216, Loss 1.149157, forward nfe 174369, backward nfe 59138, Train: 0.7100, Val: 0.4669, Test: 0.5049\n",
      "Epoch: 058, Runtime 1.745260, Loss 1.115921, forward nfe 176011, backward nfe 60169, Train: 0.7100, Val: 0.5015, Test: 0.5335\n",
      "Epoch: 059, Runtime 1.712484, Loss 1.100423, forward nfe 177444, backward nfe 61205, Train: 0.7400, Val: 0.5092, Test: 0.5463\n",
      "Epoch: 060, Runtime 1.719334, Loss 1.084072, forward nfe 178850, backward nfe 62231, Train: 0.7400, Val: 0.5223, Test: 0.5595\n",
      "Epoch: 061, Runtime 1.706605, Loss 1.053936, forward nfe 180275, backward nfe 63266, Train: 0.7450, Val: 0.5469, Test: 0.5790\n",
      "Epoch: 062, Runtime 1.658497, Loss 1.032280, forward nfe 181656, backward nfe 64291, Train: 0.7500, Val: 0.5592, Test: 0.5891\n",
      "Epoch: 063, Runtime 1.715861, Loss 1.008960, forward nfe 183059, backward nfe 65346, Train: 0.7400, Val: 0.5646, Test: 0.5890\n",
      "Epoch: 064, Runtime 1.654846, Loss 0.984595, forward nfe 184373, backward nfe 66378, Train: 0.7700, Val: 0.5754, Test: 0.6095\n",
      "Epoch: 065, Runtime 1.618826, Loss 0.958782, forward nfe 185637, backward nfe 67412, Train: 0.7650, Val: 0.5923, Test: 0.6287\n",
      "Epoch: 066, Runtime 1.640397, Loss 0.962755, forward nfe 186867, backward nfe 68447, Train: 0.7700, Val: 0.5962, Test: 0.6373\n",
      "Epoch: 067, Runtime 1.603431, Loss 0.930462, forward nfe 188107, backward nfe 69474, Train: 0.7600, Val: 0.5954, Test: 0.6370\n",
      "Epoch: 068, Runtime 1.621907, Loss 0.904433, forward nfe 189333, backward nfe 70500, Train: 0.7650, Val: 0.6008, Test: 0.6397\n",
      "Epoch: 069, Runtime 1.602506, Loss 0.904547, forward nfe 190540, backward nfe 71532, Train: 0.7750, Val: 0.6077, Test: 0.6469\n",
      "Epoch: 070, Runtime 1.593203, Loss 0.878091, forward nfe 191715, backward nfe 72569, Train: 0.7800, Val: 0.6177, Test: 0.6542\n",
      "Epoch: 071, Runtime 1.578134, Loss 0.857645, forward nfe 192862, backward nfe 73595, Train: 0.7800, Val: 0.6254, Test: 0.6580\n",
      "Epoch: 072, Runtime 1.578514, Loss 0.856014, forward nfe 194012, backward nfe 74620, Train: 0.7850, Val: 0.6231, Test: 0.6574\n",
      "Epoch: 073, Runtime 1.556759, Loss 0.826481, forward nfe 195156, backward nfe 75646, Train: 0.7900, Val: 0.6215, Test: 0.6576\n",
      "Epoch: 074, Runtime 1.564317, Loss 0.806607, forward nfe 196263, backward nfe 76675, Train: 0.7950, Val: 0.6331, Test: 0.6636\n",
      "Epoch: 075, Runtime 1.568357, Loss 0.792606, forward nfe 197372, backward nfe 77700, Train: 0.7950, Val: 0.6392, Test: 0.6667\n",
      "Epoch: 076, Runtime 1.534404, Loss 0.788835, forward nfe 198451, backward nfe 78726, Train: 0.7950, Val: 0.6392, Test: 0.6691\n",
      "Epoch: 077, Runtime 1.538414, Loss 0.772319, forward nfe 199503, backward nfe 79751, Train: 0.8000, Val: 0.6338, Test: 0.6657\n",
      "Epoch: 078, Runtime 1.533522, Loss 0.764691, forward nfe 200562, backward nfe 80775, Train: 0.8000, Val: 0.6369, Test: 0.6694\n",
      "Epoch: 079, Runtime 1.531172, Loss 0.745940, forward nfe 201606, backward nfe 81800, Train: 0.8000, Val: 0.6546, Test: 0.6780\n",
      "Epoch: 080, Runtime 1.542217, Loss 0.741089, forward nfe 202676, backward nfe 82825, Train: 0.7900, Val: 0.6615, Test: 0.6817\n",
      "Epoch: 081, Runtime 1.532944, Loss 0.732060, forward nfe 203721, backward nfe 83854, Train: 0.8000, Val: 0.6608, Test: 0.6816\n",
      "Epoch: 082, Runtime 1.539936, Loss 0.704945, forward nfe 204781, backward nfe 84879, Train: 0.8050, Val: 0.6485, Test: 0.6770\n",
      "Epoch: 083, Runtime 1.529821, Loss 0.712534, forward nfe 205843, backward nfe 85909, Train: 0.8100, Val: 0.6423, Test: 0.6750\n",
      "Epoch: 084, Runtime 1.535315, Loss 0.689529, forward nfe 206870, backward nfe 86940, Train: 0.8100, Val: 0.6585, Test: 0.6836\n",
      "Epoch: 085, Runtime 1.530218, Loss 0.679160, forward nfe 207913, backward nfe 87966, Train: 0.7950, Val: 0.6692, Test: 0.6921\n",
      "Epoch: 086, Runtime 1.527273, Loss 0.671523, forward nfe 208978, backward nfe 88990, Train: 0.8000, Val: 0.6846, Test: 0.6978\n",
      "Epoch: 087, Runtime 1.530774, Loss 0.649949, forward nfe 210018, backward nfe 90014, Train: 0.8000, Val: 0.6808, Test: 0.6964\n",
      "Epoch: 088, Runtime 1.529882, Loss 0.658466, forward nfe 211057, backward nfe 91040, Train: 0.8250, Val: 0.6700, Test: 0.6886\n",
      "Epoch: 089, Runtime 1.529763, Loss 0.639363, forward nfe 212105, backward nfe 92065, Train: 0.8200, Val: 0.6715, Test: 0.6897\n",
      "Epoch: 090, Runtime 1.524954, Loss 0.620771, forward nfe 213134, backward nfe 93090, Train: 0.8100, Val: 0.6831, Test: 0.7034\n",
      "Epoch: 091, Runtime 1.523938, Loss 0.621686, forward nfe 214179, backward nfe 94115, Train: 0.8100, Val: 0.6931, Test: 0.7085\n",
      "Epoch: 092, Runtime 1.540412, Loss 0.615734, forward nfe 215226, backward nfe 95140, Train: 0.8150, Val: 0.6915, Test: 0.7069\n",
      "Epoch: 093, Runtime 1.527175, Loss 0.605951, forward nfe 216266, backward nfe 96164, Train: 0.8200, Val: 0.6908, Test: 0.6998\n",
      "Epoch: 094, Runtime 1.543254, Loss 0.587927, forward nfe 217330, backward nfe 97196, Train: 0.8250, Val: 0.6892, Test: 0.7047\n",
      "Epoch: 095, Runtime 1.518104, Loss 0.572126, forward nfe 218367, backward nfe 98222, Train: 0.8250, Val: 0.6962, Test: 0.7138\n",
      "Epoch: 096, Runtime 1.532080, Loss 0.570440, forward nfe 219408, backward nfe 99248, Train: 0.8350, Val: 0.6985, Test: 0.7178\n",
      "Epoch: 097, Runtime 1.528529, Loss 0.546269, forward nfe 220457, backward nfe 100272, Train: 0.8400, Val: 0.6946, Test: 0.7141\n",
      "Epoch: 098, Runtime 1.532154, Loss 0.542231, forward nfe 221510, backward nfe 101298, Train: 0.8450, Val: 0.7008, Test: 0.7205\n",
      "Epoch: 099, Runtime 1.527375, Loss 0.542007, forward nfe 222552, backward nfe 102322, Train: 0.8300, Val: 0.7046, Test: 0.7207\n",
      "best val accuracy 0.704615 with test accuracy 0.720699 at epoch 99\n",
      "*** Doing stepsize 0.01 ***\n",
      "*** Doing run 0 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.229225, Loss 2.316312, forward nfe 3627, backward nfe 72, Train: 0.1100, Val: 0.1062, Test: 0.1045\n",
      "Epoch: 002, Runtime 5.107252, Loss 2.659009, forward nfe 10886, backward nfe 1096, Train: 0.1050, Val: 0.0462, Test: 0.0502\n",
      "Epoch: 003, Runtime 5.107551, Loss 2.313474, forward nfe 18146, backward nfe 2120, Train: 0.1000, Val: 0.0169, Test: 0.0291\n",
      "Epoch: 004, Runtime 5.105564, Loss 2.369763, forward nfe 25406, backward nfe 3144, Train: 0.0950, Val: 0.0192, Test: 0.0301\n",
      "Epoch: 005, Runtime 5.107463, Loss 2.294445, forward nfe 32668, backward nfe 4168, Train: 0.1150, Val: 0.1523, Test: 0.1436\n",
      "Epoch: 006, Runtime 5.116003, Loss 2.254738, forward nfe 39929, backward nfe 5203, Train: 0.1050, Val: 0.1662, Test: 0.1567\n",
      "Epoch: 007, Runtime 5.106475, Loss 2.287936, forward nfe 47191, backward nfe 6228, Train: 0.1050, Val: 0.1669, Test: 0.1565\n",
      "Epoch: 008, Runtime 5.113301, Loss 2.258919, forward nfe 54454, backward nfe 7254, Train: 0.1150, Val: 0.1577, Test: 0.1519\n",
      "Epoch: 009, Runtime 5.130603, Loss 2.211205, forward nfe 61716, backward nfe 8299, Train: 0.1600, Val: 0.1292, Test: 0.1316\n",
      "Epoch: 010, Runtime 5.200696, Loss 2.234190, forward nfe 68979, backward nfe 9418, Train: 0.2350, Val: 0.0823, Test: 0.0971\n",
      "Epoch: 011, Runtime 5.171049, Loss 2.234493, forward nfe 76243, backward nfe 10508, Train: 0.2100, Val: 0.1800, Test: 0.1819\n",
      "Epoch: 012, Runtime 5.123405, Loss 2.185426, forward nfe 83505, backward nfe 11548, Train: 0.1750, Val: 0.1869, Test: 0.1772\n",
      "Epoch: 013, Runtime 5.195583, Loss 2.171744, forward nfe 90769, backward nfe 12667, Train: 0.1500, Val: 0.1669, Test: 0.1623\n",
      "Epoch: 014, Runtime 5.197400, Loss 2.174771, forward nfe 98032, backward nfe 13786, Train: 0.1500, Val: 0.1492, Test: 0.1494\n",
      "Epoch: 015, Runtime 5.122517, Loss 2.155329, forward nfe 105294, backward nfe 14827, Train: 0.2000, Val: 0.1462, Test: 0.1514\n",
      "Epoch: 016, Runtime 5.194759, Loss 2.125554, forward nfe 112557, backward nfe 15946, Train: 0.2300, Val: 0.1362, Test: 0.1575\n",
      "Epoch: 017, Runtime 5.122253, Loss 2.121302, forward nfe 119821, backward nfe 16983, Train: 0.2650, Val: 0.1354, Test: 0.1607\n",
      "Epoch: 018, Runtime 5.195674, Loss 2.109499, forward nfe 127083, backward nfe 18102, Train: 0.2750, Val: 0.1431, Test: 0.1698\n",
      "Epoch: 019, Runtime 5.195553, Loss 2.095186, forward nfe 134347, backward nfe 19221, Train: 0.2900, Val: 0.1738, Test: 0.1983\n",
      "Epoch: 020, Runtime 5.200009, Loss 2.075192, forward nfe 141609, backward nfe 20340, Train: 0.2850, Val: 0.1862, Test: 0.2040\n",
      "Epoch: 021, Runtime 5.193450, Loss 2.054053, forward nfe 148873, backward nfe 21459, Train: 0.2750, Val: 0.1800, Test: 0.1934\n",
      "Epoch: 022, Runtime 5.193048, Loss 2.046524, forward nfe 156137, backward nfe 22578, Train: 0.2750, Val: 0.1815, Test: 0.1928\n",
      "Epoch: 023, Runtime 5.147314, Loss 2.029730, forward nfe 163400, backward nfe 23646, Train: 0.2850, Val: 0.1808, Test: 0.1969\n",
      "Epoch: 024, Runtime 5.194954, Loss 2.005431, forward nfe 170663, backward nfe 24765, Train: 0.3000, Val: 0.1731, Test: 0.1912\n",
      "Epoch: 025, Runtime 5.198208, Loss 1.991951, forward nfe 177927, backward nfe 25884, Train: 0.2850, Val: 0.1515, Test: 0.1755\n",
      "Epoch: 026, Runtime 5.197924, Loss 1.979978, forward nfe 185190, backward nfe 27003, Train: 0.2900, Val: 0.1546, Test: 0.1822\n",
      "Epoch: 027, Runtime 5.192509, Loss 1.963655, forward nfe 192453, backward nfe 28122, Train: 0.3150, Val: 0.1785, Test: 0.2028\n",
      "Epoch: 028, Runtime 5.194946, Loss 1.949285, forward nfe 199716, backward nfe 29241, Train: 0.3100, Val: 0.1923, Test: 0.2189\n",
      "Epoch: 029, Runtime 5.194043, Loss 1.929585, forward nfe 206979, backward nfe 30360, Train: 0.3250, Val: 0.2069, Test: 0.2316\n",
      "Epoch: 030, Runtime 5.195017, Loss 1.912294, forward nfe 214242, backward nfe 31479, Train: 0.3150, Val: 0.2200, Test: 0.2413\n",
      "Epoch: 031, Runtime 5.201408, Loss 1.900194, forward nfe 221506, backward nfe 32598, Train: 0.3250, Val: 0.2269, Test: 0.2503\n",
      "Epoch: 032, Runtime 5.195428, Loss 1.883801, forward nfe 228770, backward nfe 33717, Train: 0.3150, Val: 0.2362, Test: 0.2622\n",
      "Epoch: 033, Runtime 5.193971, Loss 1.870648, forward nfe 236032, backward nfe 34836, Train: 0.3300, Val: 0.2515, Test: 0.2769\n",
      "Epoch: 034, Runtime 5.131675, Loss 1.851742, forward nfe 243295, backward nfe 35878, Train: 0.3550, Val: 0.2677, Test: 0.2929\n",
      "Epoch: 035, Runtime 5.196676, Loss 1.840420, forward nfe 250559, backward nfe 36997, Train: 0.3450, Val: 0.2946, Test: 0.3071\n",
      "Epoch: 036, Runtime 5.194483, Loss 1.828646, forward nfe 257822, backward nfe 38116, Train: 0.3550, Val: 0.3085, Test: 0.3228\n",
      "Epoch: 037, Runtime 5.120479, Loss 1.810386, forward nfe 265086, backward nfe 39152, Train: 0.3400, Val: 0.3208, Test: 0.3298\n",
      "Epoch: 038, Runtime 5.195457, Loss 1.792909, forward nfe 272349, backward nfe 40271, Train: 0.3550, Val: 0.3277, Test: 0.3341\n",
      "Epoch: 039, Runtime 5.191871, Loss 1.783616, forward nfe 279612, backward nfe 41390, Train: 0.3650, Val: 0.3323, Test: 0.3380\n",
      "Epoch: 040, Runtime 5.111548, Loss 1.768288, forward nfe 286876, backward nfe 42417, Train: 0.3600, Val: 0.3369, Test: 0.3430\n",
      "Epoch: 041, Runtime 5.198460, Loss 1.758951, forward nfe 294139, backward nfe 43536, Train: 0.3600, Val: 0.3438, Test: 0.3468\n",
      "Epoch: 042, Runtime 5.200039, Loss 1.744975, forward nfe 301401, backward nfe 44655, Train: 0.3650, Val: 0.3477, Test: 0.3471\n",
      "Epoch: 043, Runtime 5.112276, Loss 1.728549, forward nfe 308664, backward nfe 45685, Train: 0.3800, Val: 0.3446, Test: 0.3484\n",
      "Epoch: 044, Runtime 5.196484, Loss 1.717534, forward nfe 315926, backward nfe 46804, Train: 0.3800, Val: 0.3431, Test: 0.3509\n",
      "Epoch: 045, Runtime 5.118005, Loss 1.709640, forward nfe 323190, backward nfe 47839, Train: 0.3800, Val: 0.3369, Test: 0.3467\n",
      "Epoch: 046, Runtime 5.136966, Loss 1.694764, forward nfe 330452, backward nfe 48892, Train: 0.3750, Val: 0.3354, Test: 0.3411\n",
      "Epoch: 047, Runtime 5.110584, Loss 1.678878, forward nfe 337715, backward nfe 49916, Train: 0.3900, Val: 0.3215, Test: 0.3339\n",
      "Epoch: 048, Runtime 5.119119, Loss 1.671432, forward nfe 344977, backward nfe 50954, Train: 0.3900, Val: 0.3162, Test: 0.3276\n",
      "Epoch: 049, Runtime 5.163634, Loss 1.665058, forward nfe 352239, backward nfe 52040, Train: 0.4000, Val: 0.3100, Test: 0.3218\n",
      "Epoch: 050, Runtime 5.127154, Loss 1.645565, forward nfe 359501, backward nfe 53084, Train: 0.4050, Val: 0.3031, Test: 0.3178\n",
      "Epoch: 051, Runtime 5.121401, Loss 1.633914, forward nfe 366764, backward nfe 54122, Train: 0.3750, Val: 0.2946, Test: 0.3126\n",
      "Epoch: 052, Runtime 5.109884, Loss 1.627652, forward nfe 374028, backward nfe 55147, Train: 0.3850, Val: 0.2992, Test: 0.3146\n",
      "Epoch: 053, Runtime 5.135795, Loss 1.612083, forward nfe 381290, backward nfe 56200, Train: 0.4200, Val: 0.3008, Test: 0.3199\n",
      "Epoch: 054, Runtime 5.123730, Loss 1.603571, forward nfe 388553, backward nfe 57244, Train: 0.4400, Val: 0.3138, Test: 0.3269\n",
      "Epoch: 055, Runtime 5.113264, Loss 1.600560, forward nfe 395815, backward nfe 58272, Train: 0.4300, Val: 0.3169, Test: 0.3326\n",
      "Epoch: 056, Runtime 5.125412, Loss 1.587900, forward nfe 403077, backward nfe 59318, Train: 0.4300, Val: 0.3269, Test: 0.3384\n",
      "Epoch: 057, Runtime 5.110809, Loss 1.576725, forward nfe 410339, backward nfe 60346, Train: 0.4450, Val: 0.3385, Test: 0.3499\n",
      "Epoch: 058, Runtime 5.113659, Loss 1.560624, forward nfe 417601, backward nfe 61377, Train: 0.4500, Val: 0.3608, Test: 0.3612\n",
      "Epoch: 059, Runtime 5.117947, Loss 1.548412, forward nfe 424864, backward nfe 62412, Train: 0.3950, Val: 0.2669, Test: 0.2884\n",
      "Epoch: 060, Runtime 5.156588, Loss 1.625682, forward nfe 432126, backward nfe 63486, Train: 0.4250, Val: 0.3685, Test: 0.3670\n",
      "Epoch: 061, Runtime 5.106703, Loss 1.553504, forward nfe 439388, backward nfe 64514, Train: 0.4400, Val: 0.4162, Test: 0.4115\n",
      "Epoch: 062, Runtime 5.120776, Loss 1.523306, forward nfe 446650, backward nfe 65553, Train: 0.4350, Val: 0.4177, Test: 0.4152\n",
      "Epoch: 063, Runtime 5.105145, Loss 1.527141, forward nfe 453912, backward nfe 66579, Train: 0.4250, Val: 0.4146, Test: 0.4161\n",
      "Epoch: 064, Runtime 5.109644, Loss 1.534395, forward nfe 461174, backward nfe 67608, Train: 0.4700, Val: 0.4462, Test: 0.4473\n",
      "Epoch: 065, Runtime 5.121116, Loss 1.505026, forward nfe 468438, backward nfe 68646, Train: 0.4750, Val: 0.4362, Test: 0.4332\n",
      "Epoch: 066, Runtime 5.112468, Loss 1.510181, forward nfe 475700, backward nfe 69676, Train: 0.4550, Val: 0.4400, Test: 0.4419\n",
      "Epoch: 067, Runtime 5.112787, Loss 1.500015, forward nfe 482963, backward nfe 70700, Train: 0.4800, Val: 0.4338, Test: 0.4384\n",
      "Epoch: 068, Runtime 5.105684, Loss 1.476233, forward nfe 490225, backward nfe 71725, Train: 0.4900, Val: 0.4146, Test: 0.4282\n",
      "Epoch: 069, Runtime 5.113132, Loss 1.476512, forward nfe 497486, backward nfe 72759, Train: 0.5100, Val: 0.4215, Test: 0.4292\n",
      "Epoch: 070, Runtime 5.105962, Loss 1.461082, forward nfe 504749, backward nfe 73786, Train: 0.4750, Val: 0.4069, Test: 0.4013\n",
      "Epoch: 071, Runtime 5.112552, Loss 1.459229, forward nfe 512011, backward nfe 74815, Train: 0.4900, Val: 0.3846, Test: 0.3909\n",
      "Epoch: 072, Runtime 5.124501, Loss 1.445344, forward nfe 519273, backward nfe 75855, Train: 0.5200, Val: 0.4062, Test: 0.4183\n",
      "Epoch: 073, Runtime 5.107853, Loss 1.426233, forward nfe 526535, backward nfe 76879, Train: 0.5200, Val: 0.4069, Test: 0.4234\n",
      "Epoch: 074, Runtime 5.106353, Loss 1.427903, forward nfe 533797, backward nfe 77903, Train: 0.5350, Val: 0.4108, Test: 0.4257\n",
      "Epoch: 075, Runtime 5.110494, Loss 1.423187, forward nfe 541059, backward nfe 78932, Train: 0.5200, Val: 0.4108, Test: 0.4136\n",
      "Epoch: 076, Runtime 5.108418, Loss 1.410683, forward nfe 548321, backward nfe 79956, Train: 0.5050, Val: 0.4054, Test: 0.4034\n",
      "Epoch: 077, Runtime 5.105394, Loss 1.411511, forward nfe 555584, backward nfe 80980, Train: 0.5450, Val: 0.4231, Test: 0.4293\n",
      "Epoch: 078, Runtime 5.121896, Loss 1.388533, forward nfe 562845, backward nfe 82016, Train: 0.5400, Val: 0.4438, Test: 0.4492\n",
      "Epoch: 079, Runtime 5.108189, Loss 1.405238, forward nfe 570106, backward nfe 83042, Train: 0.5650, Val: 0.4654, Test: 0.4700\n",
      "Epoch: 080, Runtime 5.107830, Loss 1.384640, forward nfe 577368, backward nfe 84066, Train: 0.5250, Val: 0.4708, Test: 0.4659\n",
      "Epoch: 081, Runtime 5.110557, Loss 1.372783, forward nfe 584629, backward nfe 85098, Train: 0.5100, Val: 0.4777, Test: 0.4746\n",
      "Epoch: 082, Runtime 5.106054, Loss 1.370319, forward nfe 591890, backward nfe 86125, Train: 0.5450, Val: 0.5085, Test: 0.4944\n",
      "Epoch: 083, Runtime 5.105554, Loss 1.349637, forward nfe 599151, backward nfe 87149, Train: 0.5350, Val: 0.4946, Test: 0.4929\n",
      "Epoch: 084, Runtime 5.105578, Loss 1.352438, forward nfe 606413, backward nfe 88173, Train: 0.5250, Val: 0.4946, Test: 0.4932\n",
      "Epoch: 085, Runtime 5.105172, Loss 1.342369, forward nfe 613674, backward nfe 89198, Train: 0.5400, Val: 0.5000, Test: 0.4912\n",
      "Epoch: 086, Runtime 5.108815, Loss 1.330481, forward nfe 620936, backward nfe 90225, Train: 0.5150, Val: 0.4815, Test: 0.4804\n",
      "Epoch: 087, Runtime 5.106332, Loss 1.333672, forward nfe 628197, backward nfe 91249, Train: 0.5450, Val: 0.4915, Test: 0.4905\n",
      "Epoch: 088, Runtime 5.106159, Loss 1.323697, forward nfe 635458, backward nfe 92273, Train: 0.5700, Val: 0.4954, Test: 0.4960\n",
      "Epoch: 089, Runtime 5.111677, Loss 1.315506, forward nfe 642720, backward nfe 93301, Train: 0.5600, Val: 0.4985, Test: 0.4962\n",
      "Epoch: 090, Runtime 5.107934, Loss 1.312120, forward nfe 649982, backward nfe 94327, Train: 0.5650, Val: 0.5008, Test: 0.4947\n",
      "Epoch: 091, Runtime 5.103626, Loss 1.300393, forward nfe 657243, backward nfe 95351, Train: 0.5500, Val: 0.4854, Test: 0.4917\n",
      "Epoch: 092, Runtime 5.104835, Loss 1.290938, forward nfe 664503, backward nfe 96375, Train: 0.5500, Val: 0.4885, Test: 0.4938\n",
      "Epoch: 093, Runtime 5.109206, Loss 1.281729, forward nfe 671764, backward nfe 97400, Train: 0.5800, Val: 0.5015, Test: 0.4990\n",
      "Epoch: 094, Runtime 5.103535, Loss 1.275503, forward nfe 679026, backward nfe 98424, Train: 0.5850, Val: 0.5046, Test: 0.5021\n",
      "Epoch: 095, Runtime 5.108967, Loss 1.285831, forward nfe 686287, backward nfe 99451, Train: 0.5800, Val: 0.5177, Test: 0.5087\n",
      "Epoch: 096, Runtime 5.107314, Loss 1.264594, forward nfe 693547, backward nfe 100476, Train: 0.5550, Val: 0.4592, Test: 0.4623\n",
      "Epoch: 097, Runtime 5.106173, Loss 1.251733, forward nfe 700807, backward nfe 101500, Train: 0.5450, Val: 0.5300, Test: 0.5227\n",
      "Epoch: 098, Runtime 5.104718, Loss 1.239966, forward nfe 708067, backward nfe 102524, Train: 0.5600, Val: 0.5523, Test: 0.5337\n",
      "Epoch: 099, Runtime 5.106690, Loss 1.248543, forward nfe 715328, backward nfe 103548, Train: 0.5550, Val: 0.5469, Test: 0.5374\n",
      "best val accuracy 0.552308 with test accuracy 0.533709 at epoch 98\n",
      "*** Doing run 1 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.239343, Loss 2.336881, forward nfe 3626, backward nfe 76, Train: 0.1000, Val: 0.1646, Test: 0.1569\n",
      "Epoch: 002, Runtime 5.111248, Loss 2.570657, forward nfe 10884, backward nfe 1100, Train: 0.1000, Val: 0.1646, Test: 0.1569\n",
      "Epoch: 003, Runtime 5.108257, Loss 2.323345, forward nfe 18144, backward nfe 2124, Train: 0.1050, Val: 0.1646, Test: 0.1572\n",
      "Epoch: 004, Runtime 5.102923, Loss 2.307152, forward nfe 25404, backward nfe 3148, Train: 0.1050, Val: 0.1646, Test: 0.1570\n",
      "Epoch: 005, Runtime 5.106910, Loss 2.303966, forward nfe 32665, backward nfe 4173, Train: 0.1050, Val: 0.1646, Test: 0.1562\n",
      "Epoch: 006, Runtime 5.116977, Loss 2.301806, forward nfe 39925, backward nfe 5208, Train: 0.1000, Val: 0.0562, Test: 0.0612\n",
      "Epoch: 007, Runtime 5.108705, Loss 2.319030, forward nfe 47187, backward nfe 6234, Train: 0.1050, Val: 0.0638, Test: 0.0664\n",
      "Epoch: 008, Runtime 5.163943, Loss 2.296615, forward nfe 54449, backward nfe 7314, Train: 0.1450, Val: 0.0431, Test: 0.0461\n",
      "Epoch: 009, Runtime 5.116196, Loss 2.285711, forward nfe 61713, backward nfe 8348, Train: 0.1000, Val: 0.0462, Test: 0.0522\n",
      "Epoch: 010, Runtime 5.152418, Loss 2.253335, forward nfe 68976, backward nfe 9418, Train: 0.1000, Val: 0.0569, Test: 0.0592\n",
      "Epoch: 011, Runtime 5.195504, Loss 2.259002, forward nfe 76238, backward nfe 10537, Train: 0.1000, Val: 0.0562, Test: 0.0593\n",
      "Epoch: 012, Runtime 5.194780, Loss 2.242757, forward nfe 83501, backward nfe 11656, Train: 0.1000, Val: 0.0615, Test: 0.0628\n",
      "Epoch: 013, Runtime 5.198287, Loss 2.222603, forward nfe 90765, backward nfe 12775, Train: 0.1150, Val: 0.0723, Test: 0.0762\n",
      "Epoch: 014, Runtime 5.196895, Loss 2.218041, forward nfe 98029, backward nfe 13894, Train: 0.2400, Val: 0.2092, Test: 0.2071\n",
      "Epoch: 015, Runtime 5.192432, Loss 2.198896, forward nfe 105292, backward nfe 15013, Train: 0.1650, Val: 0.1392, Test: 0.1350\n",
      "Epoch: 016, Runtime 5.192034, Loss 2.182466, forward nfe 112554, backward nfe 16132, Train: 0.1400, Val: 0.0662, Test: 0.0744\n",
      "Epoch: 017, Runtime 5.192830, Loss 2.172884, forward nfe 119817, backward nfe 17251, Train: 0.1300, Val: 0.0731, Test: 0.0757\n",
      "Epoch: 018, Runtime 5.191806, Loss 2.161814, forward nfe 127080, backward nfe 18370, Train: 0.1450, Val: 0.0900, Test: 0.0867\n",
      "Epoch: 019, Runtime 5.196053, Loss 2.141811, forward nfe 134343, backward nfe 19489, Train: 0.2250, Val: 0.1654, Test: 0.1661\n",
      "Epoch: 020, Runtime 5.193909, Loss 2.126116, forward nfe 141607, backward nfe 20608, Train: 0.2650, Val: 0.1946, Test: 0.1934\n",
      "Epoch: 021, Runtime 5.194600, Loss 2.114291, forward nfe 148870, backward nfe 21727, Train: 0.2150, Val: 0.0808, Test: 0.0913\n",
      "Epoch: 022, Runtime 5.196197, Loss 2.095969, forward nfe 156134, backward nfe 22846, Train: 0.2200, Val: 0.0800, Test: 0.0860\n",
      "Epoch: 023, Runtime 5.198549, Loss 2.088712, forward nfe 163398, backward nfe 23965, Train: 0.2450, Val: 0.0838, Test: 0.0931\n",
      "Epoch: 024, Runtime 5.195440, Loss 2.065377, forward nfe 170661, backward nfe 25084, Train: 0.2600, Val: 0.1200, Test: 0.1287\n",
      "Epoch: 025, Runtime 5.196280, Loss 2.046231, forward nfe 177924, backward nfe 26203, Train: 0.2500, Val: 0.0923, Test: 0.1030\n",
      "Epoch: 026, Runtime 5.200569, Loss 2.037415, forward nfe 185188, backward nfe 27322, Train: 0.2500, Val: 0.0985, Test: 0.1010\n",
      "Epoch: 027, Runtime 5.193174, Loss 2.014136, forward nfe 192452, backward nfe 28441, Train: 0.2450, Val: 0.0854, Test: 0.0976\n",
      "Epoch: 028, Runtime 5.193979, Loss 2.000012, forward nfe 199715, backward nfe 29560, Train: 0.2650, Val: 0.0892, Test: 0.1041\n",
      "Epoch: 029, Runtime 5.194782, Loss 1.982966, forward nfe 206979, backward nfe 30679, Train: 0.2950, Val: 0.1208, Test: 0.1364\n",
      "Epoch: 030, Runtime 5.193910, Loss 1.961831, forward nfe 214243, backward nfe 31798, Train: 0.3250, Val: 0.1292, Test: 0.1482\n",
      "Epoch: 031, Runtime 5.194385, Loss 1.946845, forward nfe 221507, backward nfe 32917, Train: 0.3500, Val: 0.1523, Test: 0.1665\n",
      "Epoch: 032, Runtime 5.195388, Loss 1.930189, forward nfe 228771, backward nfe 34036, Train: 0.3600, Val: 0.1469, Test: 0.1752\n",
      "Epoch: 033, Runtime 5.191950, Loss 1.912283, forward nfe 236034, backward nfe 35155, Train: 0.3800, Val: 0.1562, Test: 0.1828\n",
      "Epoch: 034, Runtime 5.193727, Loss 1.893593, forward nfe 243297, backward nfe 36274, Train: 0.3750, Val: 0.1654, Test: 0.1970\n",
      "Epoch: 035, Runtime 5.195771, Loss 1.871719, forward nfe 250561, backward nfe 37393, Train: 0.3750, Val: 0.1692, Test: 0.2035\n",
      "Epoch: 036, Runtime 5.113555, Loss 1.854708, forward nfe 257824, backward nfe 38423, Train: 0.3750, Val: 0.1823, Test: 0.2107\n",
      "Epoch: 037, Runtime 5.196118, Loss 1.844337, forward nfe 265087, backward nfe 39542, Train: 0.3850, Val: 0.1915, Test: 0.2171\n",
      "Epoch: 038, Runtime 5.196994, Loss 1.816394, forward nfe 272351, backward nfe 40661, Train: 0.4150, Val: 0.1831, Test: 0.2208\n",
      "Epoch: 039, Runtime 5.182049, Loss 1.804368, forward nfe 279615, backward nfe 41765, Train: 0.4300, Val: 0.1946, Test: 0.2267\n",
      "Epoch: 040, Runtime 5.112209, Loss 1.784378, forward nfe 286878, backward nfe 42793, Train: 0.3950, Val: 0.2031, Test: 0.2364\n",
      "Epoch: 041, Runtime 5.194722, Loss 1.763912, forward nfe 294141, backward nfe 43912, Train: 0.4400, Val: 0.3338, Test: 0.3563\n",
      "Epoch: 042, Runtime 5.181471, Loss 1.744597, forward nfe 301404, backward nfe 45005, Train: 0.4550, Val: 0.3231, Test: 0.3477\n",
      "Epoch: 043, Runtime 5.132701, Loss 1.710552, forward nfe 308667, backward nfe 46059, Train: 0.4350, Val: 0.3100, Test: 0.3337\n",
      "Epoch: 044, Runtime 5.139572, Loss 1.662578, forward nfe 315931, backward nfe 47116, Train: 0.4600, Val: 0.3308, Test: 0.3621\n",
      "Epoch: 045, Runtime 5.151843, Loss 1.626367, forward nfe 323195, backward nfe 48190, Train: 0.5300, Val: 0.4585, Test: 0.4765\n",
      "Epoch: 046, Runtime 5.198105, Loss 1.594467, forward nfe 330457, backward nfe 49309, Train: 0.5150, Val: 0.4538, Test: 0.4625\n",
      "Epoch: 047, Runtime 5.117734, Loss 1.569207, forward nfe 337720, backward nfe 50343, Train: 0.5400, Val: 0.4469, Test: 0.4554\n",
      "Epoch: 048, Runtime 5.192967, Loss 1.529253, forward nfe 344982, backward nfe 51462, Train: 0.5700, Val: 0.5346, Test: 0.5497\n",
      "Epoch: 049, Runtime 5.129455, Loss 1.499175, forward nfe 352245, backward nfe 52506, Train: 0.5650, Val: 0.4092, Test: 0.4275\n",
      "Epoch: 050, Runtime 5.122660, Loss 1.478351, forward nfe 359508, backward nfe 53530, Train: 0.5550, Val: 0.3246, Test: 0.3457\n",
      "Epoch: 051, Runtime 5.134221, Loss 1.452130, forward nfe 366772, backward nfe 54582, Train: 0.5450, Val: 0.3115, Test: 0.3315\n",
      "Epoch: 052, Runtime 5.113207, Loss 1.429466, forward nfe 374036, backward nfe 55614, Train: 0.5650, Val: 0.3338, Test: 0.3503\n",
      "Epoch: 053, Runtime 5.172949, Loss 1.394119, forward nfe 381298, backward nfe 56708, Train: 0.5700, Val: 0.3631, Test: 0.3963\n",
      "Epoch: 054, Runtime 5.111336, Loss 1.374474, forward nfe 388560, backward nfe 57737, Train: 0.5350, Val: 0.4623, Test: 0.4909\n",
      "Epoch: 055, Runtime 5.144474, Loss 1.346679, forward nfe 395822, backward nfe 58801, Train: 0.6000, Val: 0.5662, Test: 0.5756\n",
      "Epoch: 056, Runtime 5.136218, Loss 1.320523, forward nfe 403085, backward nfe 59852, Train: 0.5950, Val: 0.5546, Test: 0.5740\n",
      "Epoch: 057, Runtime 5.165962, Loss 1.296143, forward nfe 410347, backward nfe 60937, Train: 0.6300, Val: 0.5315, Test: 0.5508\n",
      "Epoch: 058, Runtime 5.164912, Loss 1.275474, forward nfe 417609, backward nfe 62023, Train: 0.6050, Val: 0.3785, Test: 0.4228\n",
      "Epoch: 059, Runtime 5.114125, Loss 1.249596, forward nfe 424873, backward nfe 63051, Train: 0.5950, Val: 0.3731, Test: 0.4045\n",
      "Epoch: 060, Runtime 5.114136, Loss 1.224522, forward nfe 432136, backward nfe 64083, Train: 0.6150, Val: 0.3846, Test: 0.4085\n",
      "Epoch: 061, Runtime 5.192266, Loss 1.209407, forward nfe 439397, backward nfe 65202, Train: 0.6100, Val: 0.3808, Test: 0.4154\n",
      "Epoch: 062, Runtime 5.128804, Loss 1.198253, forward nfe 446660, backward nfe 66244, Train: 0.6400, Val: 0.4092, Test: 0.4487\n",
      "Epoch: 063, Runtime 5.116756, Loss 1.156003, forward nfe 453923, backward nfe 67275, Train: 0.6000, Val: 0.4854, Test: 0.4953\n",
      "Epoch: 064, Runtime 5.133368, Loss 1.155352, forward nfe 461185, backward nfe 68326, Train: 0.6300, Val: 0.5569, Test: 0.5667\n",
      "Epoch: 065, Runtime 5.135338, Loss 1.126933, forward nfe 468447, backward nfe 69378, Train: 0.6500, Val: 0.5385, Test: 0.5647\n",
      "Epoch: 066, Runtime 5.132355, Loss 1.128384, forward nfe 475709, backward nfe 70430, Train: 0.6450, Val: 0.4938, Test: 0.5087\n",
      "Epoch: 067, Runtime 5.115596, Loss 1.097445, forward nfe 482972, backward nfe 71464, Train: 0.6950, Val: 0.4369, Test: 0.4681\n",
      "Epoch: 068, Runtime 5.151759, Loss 1.076523, forward nfe 490235, backward nfe 72534, Train: 0.6900, Val: 0.4223, Test: 0.4596\n",
      "Epoch: 069, Runtime 5.110547, Loss 1.066247, forward nfe 497498, backward nfe 73560, Train: 0.6800, Val: 0.4115, Test: 0.4526\n",
      "Epoch: 070, Runtime 5.121181, Loss 1.056095, forward nfe 504761, backward nfe 74597, Train: 0.6650, Val: 0.4238, Test: 0.4579\n",
      "Epoch: 071, Runtime 5.125050, Loss 1.039070, forward nfe 512024, backward nfe 75632, Train: 0.6550, Val: 0.4492, Test: 0.4900\n",
      "Epoch: 072, Runtime 5.122272, Loss 1.018498, forward nfe 519288, backward nfe 76668, Train: 0.6800, Val: 0.5238, Test: 0.5368\n",
      "Epoch: 073, Runtime 5.120194, Loss 1.009389, forward nfe 526551, backward nfe 77700, Train: 0.7100, Val: 0.5369, Test: 0.5648\n",
      "Epoch: 074, Runtime 5.116215, Loss 0.994251, forward nfe 533815, backward nfe 78731, Train: 0.6950, Val: 0.5146, Test: 0.5356\n",
      "Epoch: 075, Runtime 5.118084, Loss 0.968538, forward nfe 541078, backward nfe 79766, Train: 0.6950, Val: 0.4600, Test: 0.5001\n",
      "Epoch: 076, Runtime 5.136501, Loss 0.967698, forward nfe 548341, backward nfe 80820, Train: 0.6950, Val: 0.4346, Test: 0.4732\n",
      "Epoch: 077, Runtime 5.108677, Loss 0.945389, forward nfe 555602, backward nfe 81846, Train: 0.6950, Val: 0.4331, Test: 0.4755\n",
      "Epoch: 078, Runtime 5.117866, Loss 0.938444, forward nfe 562864, backward nfe 82878, Train: 0.7100, Val: 0.4477, Test: 0.4829\n",
      "Epoch: 079, Runtime 5.110866, Loss 0.927235, forward nfe 570127, backward nfe 83904, Train: 0.7150, Val: 0.4677, Test: 0.5122\n",
      "Epoch: 080, Runtime 5.115602, Loss 0.933034, forward nfe 577390, backward nfe 84932, Train: 0.7250, Val: 0.5200, Test: 0.5477\n",
      "Epoch: 081, Runtime 5.110168, Loss 0.915981, forward nfe 584652, backward nfe 85956, Train: 0.7250, Val: 0.5185, Test: 0.5506\n",
      "Epoch: 082, Runtime 5.112912, Loss 0.898639, forward nfe 591915, backward nfe 86982, Train: 0.7100, Val: 0.4692, Test: 0.5137\n",
      "Epoch: 083, Runtime 5.114017, Loss 0.870486, forward nfe 599178, backward nfe 88009, Train: 0.7200, Val: 0.4577, Test: 0.4968\n",
      "Epoch: 084, Runtime 5.110602, Loss 0.879336, forward nfe 606440, backward nfe 89037, Train: 0.7200, Val: 0.4623, Test: 0.4976\n",
      "Epoch: 085, Runtime 5.115144, Loss 0.858396, forward nfe 613704, backward nfe 90066, Train: 0.7400, Val: 0.4585, Test: 0.5024\n",
      "Epoch: 086, Runtime 5.111165, Loss 0.865012, forward nfe 620966, backward nfe 91090, Train: 0.7500, Val: 0.4815, Test: 0.5190\n",
      "Epoch: 087, Runtime 5.110436, Loss 0.842001, forward nfe 628230, backward nfe 92114, Train: 0.7400, Val: 0.5192, Test: 0.5548\n",
      "Epoch: 088, Runtime 5.110144, Loss 0.839706, forward nfe 635493, backward nfe 93138, Train: 0.7350, Val: 0.5177, Test: 0.5473\n",
      "Epoch: 089, Runtime 5.114846, Loss 0.819514, forward nfe 642757, backward nfe 94166, Train: 0.7250, Val: 0.4977, Test: 0.5353\n",
      "Epoch: 090, Runtime 5.117596, Loss 0.808213, forward nfe 650019, backward nfe 95199, Train: 0.7550, Val: 0.4692, Test: 0.5114\n",
      "Epoch: 091, Runtime 5.110346, Loss 0.818915, forward nfe 657283, backward nfe 96227, Train: 0.7500, Val: 0.4592, Test: 0.5029\n",
      "Epoch: 092, Runtime 5.123066, Loss 0.789015, forward nfe 664544, backward nfe 97261, Train: 0.7300, Val: 0.4554, Test: 0.5009\n",
      "Epoch: 093, Runtime 5.114573, Loss 0.803025, forward nfe 671807, backward nfe 98287, Train: 0.7600, Val: 0.4885, Test: 0.5329\n",
      "Epoch: 094, Runtime 5.110253, Loss 0.781749, forward nfe 679070, backward nfe 99312, Train: 0.7850, Val: 0.5538, Test: 0.5908\n",
      "Epoch: 095, Runtime 5.115535, Loss 0.782317, forward nfe 686331, backward nfe 100341, Train: 0.7850, Val: 0.5731, Test: 0.6008\n",
      "Epoch: 096, Runtime 5.110031, Loss 0.763476, forward nfe 693595, backward nfe 101367, Train: 0.7650, Val: 0.5231, Test: 0.5500\n",
      "Epoch: 097, Runtime 5.115216, Loss 0.750992, forward nfe 700856, backward nfe 102403, Train: 0.7550, Val: 0.4792, Test: 0.5150\n",
      "Epoch: 098, Runtime 5.115493, Loss 0.747054, forward nfe 708119, backward nfe 103432, Train: 0.7550, Val: 0.4638, Test: 0.5035\n",
      "Epoch: 099, Runtime 5.108363, Loss 0.743556, forward nfe 715383, backward nfe 104457, Train: 0.7700, Val: 0.5054, Test: 0.5380\n",
      "best val accuracy 0.573077 with test accuracy 0.600800 at epoch 95\n",
      "*** Doing run 2 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.230402, Loss 2.330452, forward nfe 3625, backward nfe 65, Train: 0.0950, Val: 0.0808, Test: 0.0907\n",
      "Epoch: 002, Runtime 5.104370, Loss 2.676410, forward nfe 10882, backward nfe 1089, Train: 0.1000, Val: 0.1031, Test: 0.1126\n",
      "Epoch: 003, Runtime 5.110903, Loss 2.321634, forward nfe 18142, backward nfe 2113, Train: 0.1500, Val: 0.0608, Test: 0.0648\n",
      "Epoch: 004, Runtime 5.104733, Loss 2.281218, forward nfe 25403, backward nfe 3137, Train: 0.1000, Val: 0.0185, Test: 0.0328\n",
      "Epoch: 005, Runtime 5.108989, Loss 2.294698, forward nfe 32663, backward nfe 4161, Train: 0.1350, Val: 0.0246, Test: 0.0348\n",
      "Epoch: 006, Runtime 5.112647, Loss 2.272092, forward nfe 39925, backward nfe 5186, Train: 0.2150, Val: 0.1177, Test: 0.1174\n",
      "Epoch: 007, Runtime 5.106735, Loss 2.238566, forward nfe 47188, backward nfe 6210, Train: 0.1500, Val: 0.0877, Test: 0.0878\n",
      "Epoch: 008, Runtime 5.110409, Loss 2.244422, forward nfe 54449, backward nfe 7235, Train: 0.1950, Val: 0.1008, Test: 0.1081\n",
      "Epoch: 009, Runtime 5.121770, Loss 2.210598, forward nfe 61712, backward nfe 8267, Train: 0.2800, Val: 0.1085, Test: 0.1195\n",
      "Epoch: 010, Runtime 5.115623, Loss 2.186742, forward nfe 68974, backward nfe 9297, Train: 0.2250, Val: 0.0600, Test: 0.0729\n",
      "Epoch: 011, Runtime 5.156359, Loss 2.172096, forward nfe 76237, backward nfe 10373, Train: 0.2400, Val: 0.1062, Test: 0.1139\n",
      "Epoch: 012, Runtime 5.115224, Loss 2.148481, forward nfe 83500, backward nfe 11399, Train: 0.3150, Val: 0.2077, Test: 0.2232\n",
      "Epoch: 013, Runtime 5.113770, Loss 2.122562, forward nfe 90762, backward nfe 12428, Train: 0.3750, Val: 0.2462, Test: 0.2648\n",
      "Epoch: 014, Runtime 5.195077, Loss 2.099684, forward nfe 98023, backward nfe 13547, Train: 0.3500, Val: 0.2154, Test: 0.2354\n",
      "Epoch: 015, Runtime 5.199570, Loss 2.079463, forward nfe 105283, backward nfe 14666, Train: 0.3500, Val: 0.2115, Test: 0.2378\n",
      "Epoch: 016, Runtime 5.196485, Loss 2.054572, forward nfe 112547, backward nfe 15785, Train: 0.3700, Val: 0.2500, Test: 0.2770\n",
      "Epoch: 017, Runtime 5.150322, Loss 2.025430, forward nfe 119810, backward nfe 16854, Train: 0.3800, Val: 0.2885, Test: 0.3053\n",
      "Epoch: 018, Runtime 5.198253, Loss 2.002748, forward nfe 127074, backward nfe 17973, Train: 0.3750, Val: 0.2931, Test: 0.3070\n",
      "Epoch: 019, Runtime 5.183593, Loss 1.977422, forward nfe 134336, backward nfe 19073, Train: 0.3800, Val: 0.2869, Test: 0.3061\n",
      "Epoch: 020, Runtime 5.200091, Loss 1.951639, forward nfe 141600, backward nfe 20192, Train: 0.3950, Val: 0.2892, Test: 0.3066\n",
      "Epoch: 021, Runtime 5.197258, Loss 1.926815, forward nfe 148863, backward nfe 21311, Train: 0.4050, Val: 0.2685, Test: 0.2954\n",
      "Epoch: 022, Runtime 5.195254, Loss 1.899023, forward nfe 156126, backward nfe 22430, Train: 0.4050, Val: 0.2623, Test: 0.3015\n",
      "Epoch: 023, Runtime 5.195139, Loss 1.872665, forward nfe 163389, backward nfe 23549, Train: 0.4050, Val: 0.2592, Test: 0.2971\n",
      "Epoch: 024, Runtime 5.194679, Loss 1.847863, forward nfe 170651, backward nfe 24668, Train: 0.4300, Val: 0.3038, Test: 0.3364\n",
      "Epoch: 025, Runtime 5.196541, Loss 1.817942, forward nfe 177914, backward nfe 25787, Train: 0.4850, Val: 0.3469, Test: 0.3760\n",
      "Epoch: 026, Runtime 5.197694, Loss 1.792304, forward nfe 185177, backward nfe 26906, Train: 0.4750, Val: 0.3569, Test: 0.3823\n",
      "Epoch: 027, Runtime 5.197815, Loss 1.763973, forward nfe 192441, backward nfe 28025, Train: 0.4300, Val: 0.3731, Test: 0.3875\n",
      "Epoch: 028, Runtime 5.174536, Loss 1.739038, forward nfe 199703, backward nfe 29119, Train: 0.4250, Val: 0.3692, Test: 0.3790\n",
      "Epoch: 029, Runtime 5.195957, Loss 1.708845, forward nfe 206966, backward nfe 30238, Train: 0.4300, Val: 0.3731, Test: 0.3803\n",
      "Epoch: 030, Runtime 5.197753, Loss 1.681795, forward nfe 214228, backward nfe 31357, Train: 0.4150, Val: 0.3692, Test: 0.3829\n",
      "Epoch: 031, Runtime 5.174251, Loss 1.656313, forward nfe 221491, backward nfe 32452, Train: 0.4550, Val: 0.3762, Test: 0.3865\n",
      "Epoch: 032, Runtime 5.197069, Loss 1.623740, forward nfe 228754, backward nfe 33571, Train: 0.4900, Val: 0.3815, Test: 0.3994\n",
      "Epoch: 033, Runtime 5.159756, Loss 1.599547, forward nfe 236016, backward nfe 34651, Train: 0.5000, Val: 0.4008, Test: 0.4151\n",
      "Epoch: 034, Runtime 5.110103, Loss 1.568334, forward nfe 243279, backward nfe 35676, Train: 0.5300, Val: 0.4085, Test: 0.4216\n",
      "Epoch: 035, Runtime 5.196053, Loss 1.544804, forward nfe 250541, backward nfe 36795, Train: 0.5250, Val: 0.4077, Test: 0.4215\n",
      "Epoch: 036, Runtime 5.199231, Loss 1.528586, forward nfe 257805, backward nfe 37914, Train: 0.5500, Val: 0.4077, Test: 0.4237\n",
      "Epoch: 037, Runtime 5.198954, Loss 1.488557, forward nfe 265067, backward nfe 39033, Train: 0.5450, Val: 0.4192, Test: 0.4315\n",
      "Epoch: 038, Runtime 5.194539, Loss 1.457076, forward nfe 272329, backward nfe 40152, Train: 0.5600, Val: 0.4362, Test: 0.4555\n",
      "Epoch: 039, Runtime 5.195015, Loss 1.436409, forward nfe 279592, backward nfe 41268, Train: 0.5500, Val: 0.4462, Test: 0.4678\n",
      "Epoch: 040, Runtime 5.133193, Loss 1.404095, forward nfe 286855, backward nfe 42316, Train: 0.5600, Val: 0.4592, Test: 0.4700\n",
      "Epoch: 041, Runtime 5.198524, Loss 1.387378, forward nfe 294118, backward nfe 43435, Train: 0.5800, Val: 0.4908, Test: 0.5024\n",
      "Epoch: 042, Runtime 5.116803, Loss 1.360238, forward nfe 301381, backward nfe 44464, Train: 0.5700, Val: 0.5200, Test: 0.5282\n",
      "Epoch: 043, Runtime 5.151246, Loss 1.342168, forward nfe 308644, backward nfe 45533, Train: 0.5550, Val: 0.5323, Test: 0.5427\n",
      "Epoch: 044, Runtime 5.114057, Loss 1.316557, forward nfe 315906, backward nfe 46565, Train: 0.5750, Val: 0.5400, Test: 0.5436\n",
      "Epoch: 045, Runtime 5.118505, Loss 1.300483, forward nfe 323168, backward nfe 47599, Train: 0.5900, Val: 0.5354, Test: 0.5452\n",
      "Epoch: 046, Runtime 5.128038, Loss 1.275951, forward nfe 330431, backward nfe 48639, Train: 0.6000, Val: 0.5700, Test: 0.5725\n",
      "Epoch: 047, Runtime 5.125083, Loss 1.245065, forward nfe 337693, backward nfe 49678, Train: 0.6000, Val: 0.5785, Test: 0.5831\n",
      "Epoch: 048, Runtime 5.124412, Loss 1.237305, forward nfe 344955, backward nfe 50720, Train: 0.6150, Val: 0.5785, Test: 0.5851\n",
      "Epoch: 049, Runtime 5.106807, Loss 1.203117, forward nfe 352217, backward nfe 51745, Train: 0.6250, Val: 0.5731, Test: 0.5839\n",
      "Epoch: 050, Runtime 5.112980, Loss 1.199453, forward nfe 359480, backward nfe 52774, Train: 0.6350, Val: 0.6046, Test: 0.6007\n",
      "Epoch: 051, Runtime 5.112515, Loss 1.168274, forward nfe 366742, backward nfe 53804, Train: 0.6150, Val: 0.6131, Test: 0.6063\n",
      "Epoch: 052, Runtime 5.110619, Loss 1.155558, forward nfe 374004, backward nfe 54831, Train: 0.6250, Val: 0.6169, Test: 0.6099\n",
      "Epoch: 053, Runtime 5.132881, Loss 1.129543, forward nfe 381267, backward nfe 55876, Train: 0.6450, Val: 0.6138, Test: 0.6135\n",
      "Epoch: 054, Runtime 5.137157, Loss 1.106443, forward nfe 388529, backward nfe 56928, Train: 0.6450, Val: 0.6177, Test: 0.6161\n",
      "Epoch: 055, Runtime 5.107153, Loss 1.089946, forward nfe 395792, backward nfe 57952, Train: 0.6300, Val: 0.6131, Test: 0.6125\n",
      "Epoch: 056, Runtime 5.117565, Loss 1.090475, forward nfe 403055, backward nfe 58983, Train: 0.6300, Val: 0.6123, Test: 0.6124\n",
      "Epoch: 057, Runtime 5.115525, Loss 1.059983, forward nfe 410317, backward nfe 60012, Train: 0.6400, Val: 0.6146, Test: 0.6165\n",
      "Epoch: 058, Runtime 5.154919, Loss 1.049888, forward nfe 417579, backward nfe 61083, Train: 0.6700, Val: 0.6208, Test: 0.6179\n",
      "Epoch: 059, Runtime 5.116017, Loss 1.031328, forward nfe 424842, backward nfe 62114, Train: 0.6700, Val: 0.6223, Test: 0.6236\n",
      "Epoch: 060, Runtime 5.114678, Loss 1.016512, forward nfe 432104, backward nfe 63145, Train: 0.6650, Val: 0.6223, Test: 0.6256\n",
      "Epoch: 061, Runtime 5.106310, Loss 0.994567, forward nfe 439366, backward nfe 64170, Train: 0.6800, Val: 0.6254, Test: 0.6289\n",
      "Epoch: 062, Runtime 5.119595, Loss 0.977901, forward nfe 446629, backward nfe 65202, Train: 0.6900, Val: 0.6285, Test: 0.6247\n",
      "Epoch: 063, Runtime 5.113510, Loss 0.974917, forward nfe 453891, backward nfe 66229, Train: 0.6800, Val: 0.6238, Test: 0.6266\n",
      "Epoch: 064, Runtime 5.109634, Loss 0.958889, forward nfe 461153, backward nfe 67254, Train: 0.6800, Val: 0.6269, Test: 0.6338\n",
      "Epoch: 065, Runtime 5.119628, Loss 0.949829, forward nfe 468415, backward nfe 68284, Train: 0.7000, Val: 0.6215, Test: 0.6318\n",
      "Epoch: 066, Runtime 5.113922, Loss 0.934020, forward nfe 475676, backward nfe 69308, Train: 0.7300, Val: 0.6208, Test: 0.6313\n",
      "Epoch: 067, Runtime 5.106958, Loss 0.912941, forward nfe 482936, backward nfe 70333, Train: 0.7400, Val: 0.6362, Test: 0.6453\n",
      "Epoch: 068, Runtime 5.109149, Loss 0.881541, forward nfe 490198, backward nfe 71357, Train: 0.7500, Val: 0.6385, Test: 0.6485\n",
      "Epoch: 069, Runtime 5.113511, Loss 0.865013, forward nfe 497460, backward nfe 72384, Train: 0.7550, Val: 0.6408, Test: 0.6504\n",
      "Epoch: 070, Runtime 5.109329, Loss 0.857693, forward nfe 504722, backward nfe 73410, Train: 0.7750, Val: 0.6431, Test: 0.6571\n",
      "Epoch: 071, Runtime 5.115561, Loss 0.816273, forward nfe 511984, backward nfe 74439, Train: 0.7750, Val: 0.6454, Test: 0.6583\n",
      "Epoch: 072, Runtime 5.112883, Loss 0.801176, forward nfe 519246, backward nfe 75464, Train: 0.7950, Val: 0.6477, Test: 0.6673\n",
      "Epoch: 073, Runtime 5.111887, Loss 0.772774, forward nfe 526508, backward nfe 76491, Train: 0.8050, Val: 0.6677, Test: 0.6886\n",
      "Epoch: 074, Runtime 5.109894, Loss 0.753820, forward nfe 533769, backward nfe 77515, Train: 0.8000, Val: 0.6738, Test: 0.6943\n",
      "Epoch: 075, Runtime 5.114911, Loss 0.731441, forward nfe 541031, backward nfe 78545, Train: 0.7950, Val: 0.6715, Test: 0.6866\n",
      "Epoch: 076, Runtime 5.126386, Loss 0.701097, forward nfe 548293, backward nfe 79585, Train: 0.8200, Val: 0.6854, Test: 0.7032\n",
      "Epoch: 077, Runtime 5.116945, Loss 0.678325, forward nfe 555553, backward nfe 80614, Train: 0.8400, Val: 0.6900, Test: 0.7131\n",
      "Epoch: 078, Runtime 5.109743, Loss 0.669201, forward nfe 562815, backward nfe 81639, Train: 0.8450, Val: 0.6892, Test: 0.7126\n",
      "Epoch: 079, Runtime 5.112172, Loss 0.658665, forward nfe 570077, backward nfe 82664, Train: 0.8600, Val: 0.6885, Test: 0.7045\n",
      "Epoch: 080, Runtime 5.113765, Loss 0.618757, forward nfe 577339, backward nfe 83691, Train: 0.8550, Val: 0.6931, Test: 0.7143\n",
      "Epoch: 081, Runtime 5.113750, Loss 0.609500, forward nfe 584600, backward nfe 84719, Train: 0.8600, Val: 0.6977, Test: 0.7196\n",
      "Epoch: 082, Runtime 5.105588, Loss 0.602900, forward nfe 591861, backward nfe 85743, Train: 0.8600, Val: 0.7108, Test: 0.7271\n",
      "Epoch: 083, Runtime 5.112664, Loss 0.573374, forward nfe 599122, backward nfe 86769, Train: 0.8600, Val: 0.7131, Test: 0.7335\n",
      "Epoch: 084, Runtime 5.109017, Loss 0.569519, forward nfe 606384, backward nfe 87793, Train: 0.8600, Val: 0.7123, Test: 0.7300\n",
      "Epoch: 085, Runtime 5.112151, Loss 0.564098, forward nfe 613647, backward nfe 88817, Train: 0.8750, Val: 0.7031, Test: 0.7234\n",
      "Epoch: 086, Runtime 5.111116, Loss 0.542223, forward nfe 620909, backward nfe 89841, Train: 0.8650, Val: 0.7146, Test: 0.7338\n",
      "Epoch: 087, Runtime 5.114943, Loss 0.523008, forward nfe 628169, backward nfe 90869, Train: 0.8650, Val: 0.7169, Test: 0.7421\n",
      "Epoch: 088, Runtime 5.116512, Loss 0.512966, forward nfe 635431, backward nfe 91898, Train: 0.8750, Val: 0.7169, Test: 0.7405\n",
      "Epoch: 089, Runtime 5.111827, Loss 0.493133, forward nfe 642693, backward nfe 92922, Train: 0.8800, Val: 0.7177, Test: 0.7389\n",
      "Epoch: 090, Runtime 5.110676, Loss 0.483903, forward nfe 649955, backward nfe 93947, Train: 0.8650, Val: 0.7169, Test: 0.7385\n",
      "Epoch: 091, Runtime 5.115500, Loss 0.482165, forward nfe 657217, backward nfe 94976, Train: 0.8750, Val: 0.7254, Test: 0.7464\n",
      "Epoch: 092, Runtime 5.109645, Loss 0.459017, forward nfe 664479, backward nfe 96000, Train: 0.8750, Val: 0.7246, Test: 0.7483\n",
      "Epoch: 093, Runtime 5.107748, Loss 0.464146, forward nfe 671739, backward nfe 97025, Train: 0.8750, Val: 0.7292, Test: 0.7503\n",
      "Epoch: 094, Runtime 5.110957, Loss 0.444877, forward nfe 679000, backward nfe 98050, Train: 0.8750, Val: 0.7292, Test: 0.7491\n",
      "Epoch: 095, Runtime 5.110611, Loss 0.434396, forward nfe 686263, backward nfe 99073, Train: 0.8850, Val: 0.7223, Test: 0.7484\n",
      "Epoch: 096, Runtime 5.106403, Loss 0.430300, forward nfe 693525, backward nfe 100096, Train: 0.8750, Val: 0.7277, Test: 0.7538\n",
      "Epoch: 097, Runtime 5.107441, Loss 0.422419, forward nfe 700787, backward nfe 101121, Train: 0.8700, Val: 0.7423, Test: 0.7591\n",
      "Epoch: 098, Runtime 5.114671, Loss 0.421057, forward nfe 708048, backward nfe 102146, Train: 0.8700, Val: 0.7346, Test: 0.7606\n",
      "Epoch: 099, Runtime 5.111763, Loss 0.391179, forward nfe 715308, backward nfe 103171, Train: 0.8850, Val: 0.7192, Test: 0.7504\n",
      "best val accuracy 0.742308 with test accuracy 0.759060 at epoch 97\n",
      "*** Doing run 3 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.230159, Loss 2.307330, forward nfe 3626, backward nfe 69, Train: 0.1000, Val: 0.0977, Test: 0.1008\n",
      "Epoch: 002, Runtime 5.112599, Loss 2.449961, forward nfe 10884, backward nfe 1093, Train: 0.0800, Val: 0.1362, Test: 0.1336\n",
      "Epoch: 003, Runtime 5.110579, Loss 2.430367, forward nfe 18142, backward nfe 2117, Train: 0.1450, Val: 0.0731, Test: 0.0868\n",
      "Epoch: 004, Runtime 5.107726, Loss 2.222100, forward nfe 25402, backward nfe 3141, Train: 0.1350, Val: 0.0562, Test: 0.0735\n",
      "Epoch: 005, Runtime 5.109698, Loss 2.270181, forward nfe 32662, backward nfe 4165, Train: 0.1750, Val: 0.1338, Test: 0.1392\n",
      "Epoch: 006, Runtime 5.112549, Loss 2.217719, forward nfe 39924, backward nfe 5192, Train: 0.2850, Val: 0.4323, Test: 0.4202\n",
      "Epoch: 007, Runtime 5.112012, Loss 2.162230, forward nfe 47185, backward nfe 6217, Train: 0.2350, Val: 0.4362, Test: 0.4226\n",
      "Epoch: 008, Runtime 5.109819, Loss 2.172585, forward nfe 54447, backward nfe 7241, Train: 0.2700, Val: 0.3877, Test: 0.3784\n",
      "Epoch: 009, Runtime 5.109274, Loss 2.146189, forward nfe 61709, backward nfe 8267, Train: 0.3300, Val: 0.3869, Test: 0.3874\n",
      "Epoch: 010, Runtime 5.112594, Loss 2.081246, forward nfe 68971, backward nfe 9292, Train: 0.3900, Val: 0.2546, Test: 0.2681\n",
      "Epoch: 011, Runtime 5.121029, Loss 2.063766, forward nfe 76233, backward nfe 10326, Train: 0.4000, Val: 0.2215, Test: 0.2470\n",
      "Epoch: 012, Runtime 5.134453, Loss 2.044326, forward nfe 83496, backward nfe 11379, Train: 0.4300, Val: 0.2638, Test: 0.2844\n",
      "Epoch: 013, Runtime 5.144838, Loss 2.006194, forward nfe 90757, backward nfe 12441, Train: 0.3700, Val: 0.2738, Test: 0.2865\n",
      "Epoch: 014, Runtime 5.153717, Loss 1.970414, forward nfe 98019, backward nfe 13507, Train: 0.3700, Val: 0.2415, Test: 0.2622\n",
      "Epoch: 015, Runtime 5.180240, Loss 1.974725, forward nfe 105281, backward nfe 14603, Train: 0.4100, Val: 0.3215, Test: 0.3405\n",
      "Epoch: 016, Runtime 5.195061, Loss 1.921963, forward nfe 112543, backward nfe 15722, Train: 0.4250, Val: 0.3715, Test: 0.3914\n",
      "Epoch: 017, Runtime 5.117283, Loss 1.886584, forward nfe 119806, backward nfe 16749, Train: 0.3800, Val: 0.3962, Test: 0.4101\n",
      "Epoch: 018, Runtime 5.124906, Loss 1.844335, forward nfe 127070, backward nfe 17788, Train: 0.4150, Val: 0.3685, Test: 0.3886\n",
      "Epoch: 019, Runtime 5.186061, Loss 1.811824, forward nfe 134333, backward nfe 18898, Train: 0.4200, Val: 0.3677, Test: 0.3891\n",
      "Epoch: 020, Runtime 5.143061, Loss 1.776633, forward nfe 141595, backward nfe 19954, Train: 0.4650, Val: 0.4715, Test: 0.4858\n",
      "Epoch: 021, Runtime 5.153173, Loss 1.736366, forward nfe 148856, backward nfe 21028, Train: 0.5050, Val: 0.5738, Test: 0.5841\n",
      "Epoch: 022, Runtime 5.119886, Loss 1.703994, forward nfe 156118, backward nfe 22060, Train: 0.5150, Val: 0.5954, Test: 0.5964\n",
      "Epoch: 023, Runtime 5.148982, Loss 1.670769, forward nfe 163381, backward nfe 23126, Train: 0.5500, Val: 0.5831, Test: 0.5898\n",
      "Epoch: 024, Runtime 5.179335, Loss 1.630111, forward nfe 170644, backward nfe 24226, Train: 0.6150, Val: 0.5500, Test: 0.5604\n",
      "Epoch: 025, Runtime 5.187639, Loss 1.596537, forward nfe 177907, backward nfe 25324, Train: 0.6350, Val: 0.5354, Test: 0.5526\n",
      "Epoch: 026, Runtime 5.199337, Loss 1.561110, forward nfe 185169, backward nfe 26443, Train: 0.6000, Val: 0.5338, Test: 0.5528\n",
      "Epoch: 027, Runtime 5.111163, Loss 1.527806, forward nfe 192432, backward nfe 27467, Train: 0.5750, Val: 0.5500, Test: 0.5687\n",
      "Epoch: 028, Runtime 5.120505, Loss 1.494477, forward nfe 199695, backward nfe 28505, Train: 0.5950, Val: 0.5485, Test: 0.5702\n",
      "Epoch: 029, Runtime 5.168292, Loss 1.453608, forward nfe 206958, backward nfe 29589, Train: 0.6150, Val: 0.5415, Test: 0.5600\n",
      "Epoch: 030, Runtime 5.196660, Loss 1.414492, forward nfe 214221, backward nfe 30708, Train: 0.6350, Val: 0.5369, Test: 0.5611\n",
      "Epoch: 031, Runtime 5.134000, Loss 1.390082, forward nfe 221484, backward nfe 31758, Train: 0.6500, Val: 0.5500, Test: 0.5739\n",
      "Epoch: 032, Runtime 5.160082, Loss 1.351567, forward nfe 228747, backward nfe 32837, Train: 0.6800, Val: 0.5854, Test: 0.6121\n",
      "Epoch: 033, Runtime 5.109554, Loss 1.316028, forward nfe 236009, backward nfe 33862, Train: 0.6400, Val: 0.5985, Test: 0.6250\n",
      "Epoch: 034, Runtime 5.114167, Loss 1.287855, forward nfe 243273, backward nfe 34889, Train: 0.6650, Val: 0.6054, Test: 0.6349\n",
      "Epoch: 035, Runtime 5.123716, Loss 1.252650, forward nfe 250536, backward nfe 35926, Train: 0.7200, Val: 0.6123, Test: 0.6483\n",
      "Epoch: 036, Runtime 5.140027, Loss 1.228197, forward nfe 257798, backward nfe 36987, Train: 0.7400, Val: 0.6269, Test: 0.6495\n",
      "Epoch: 037, Runtime 5.130477, Loss 1.192659, forward nfe 265061, backward nfe 38034, Train: 0.7250, Val: 0.6238, Test: 0.6451\n",
      "Epoch: 038, Runtime 5.120429, Loss 1.160470, forward nfe 272325, backward nfe 39069, Train: 0.7300, Val: 0.6215, Test: 0.6492\n",
      "Epoch: 039, Runtime 5.131557, Loss 1.139059, forward nfe 279587, backward nfe 40116, Train: 0.7650, Val: 0.6315, Test: 0.6574\n",
      "Epoch: 040, Runtime 5.115707, Loss 1.095282, forward nfe 286850, backward nfe 41141, Train: 0.7850, Val: 0.6385, Test: 0.6627\n",
      "Epoch: 041, Runtime 5.120420, Loss 1.072841, forward nfe 294113, backward nfe 42178, Train: 0.7900, Val: 0.6508, Test: 0.6727\n",
      "Epoch: 042, Runtime 5.197223, Loss 1.043910, forward nfe 301375, backward nfe 43297, Train: 0.8000, Val: 0.6562, Test: 0.6795\n",
      "Epoch: 043, Runtime 5.118866, Loss 1.017015, forward nfe 308637, backward nfe 44328, Train: 0.8100, Val: 0.6554, Test: 0.6775\n",
      "Epoch: 044, Runtime 5.167930, Loss 0.985598, forward nfe 315900, backward nfe 45412, Train: 0.8000, Val: 0.6577, Test: 0.6820\n",
      "Epoch: 045, Runtime 5.108571, Loss 0.971844, forward nfe 323163, backward nfe 46436, Train: 0.8100, Val: 0.6738, Test: 0.6970\n",
      "Epoch: 046, Runtime 5.110622, Loss 0.929468, forward nfe 330426, backward nfe 47461, Train: 0.8100, Val: 0.6785, Test: 0.7023\n",
      "Epoch: 047, Runtime 5.120168, Loss 0.919406, forward nfe 337687, backward nfe 48493, Train: 0.8000, Val: 0.6685, Test: 0.6942\n",
      "Epoch: 048, Runtime 5.130896, Loss 0.887469, forward nfe 344949, backward nfe 49536, Train: 0.8050, Val: 0.6715, Test: 0.6952\n",
      "Epoch: 049, Runtime 5.116781, Loss 0.864473, forward nfe 352211, backward nfe 50570, Train: 0.8150, Val: 0.6785, Test: 0.6979\n",
      "Epoch: 050, Runtime 5.123915, Loss 0.848616, forward nfe 359473, backward nfe 51601, Train: 0.8050, Val: 0.6746, Test: 0.7005\n",
      "Epoch: 051, Runtime 5.112728, Loss 0.819004, forward nfe 366736, backward nfe 52628, Train: 0.8250, Val: 0.6746, Test: 0.7077\n",
      "Epoch: 052, Runtime 5.114177, Loss 0.804097, forward nfe 373998, backward nfe 53654, Train: 0.8200, Val: 0.6938, Test: 0.7151\n",
      "Epoch: 053, Runtime 5.149332, Loss 0.789098, forward nfe 381261, backward nfe 54716, Train: 0.8150, Val: 0.7008, Test: 0.7195\n",
      "Epoch: 054, Runtime 5.114794, Loss 0.771930, forward nfe 388523, backward nfe 55743, Train: 0.8250, Val: 0.6908, Test: 0.7168\n",
      "Epoch: 055, Runtime 5.111033, Loss 0.739779, forward nfe 395784, backward nfe 56769, Train: 0.8400, Val: 0.6869, Test: 0.7182\n",
      "Epoch: 056, Runtime 5.121590, Loss 0.729449, forward nfe 403046, backward nfe 57806, Train: 0.8300, Val: 0.6908, Test: 0.7172\n",
      "Epoch: 057, Runtime 5.130222, Loss 0.708687, forward nfe 410309, backward nfe 58847, Train: 0.8400, Val: 0.6892, Test: 0.7190\n",
      "Epoch: 058, Runtime 5.117726, Loss 0.695149, forward nfe 417571, backward nfe 59877, Train: 0.8400, Val: 0.6946, Test: 0.7222\n",
      "Epoch: 059, Runtime 5.117005, Loss 0.670283, forward nfe 424832, backward nfe 60904, Train: 0.8400, Val: 0.7023, Test: 0.7246\n",
      "Epoch: 060, Runtime 5.113887, Loss 0.662435, forward nfe 432094, backward nfe 61932, Train: 0.8350, Val: 0.7069, Test: 0.7258\n",
      "Epoch: 061, Runtime 5.113321, Loss 0.650445, forward nfe 439355, backward nfe 62960, Train: 0.8400, Val: 0.7023, Test: 0.7249\n",
      "Epoch: 062, Runtime 5.113462, Loss 0.630846, forward nfe 446616, backward nfe 63984, Train: 0.8550, Val: 0.6969, Test: 0.7231\n",
      "Epoch: 063, Runtime 5.110338, Loss 0.621347, forward nfe 453878, backward nfe 65009, Train: 0.8550, Val: 0.7069, Test: 0.7285\n",
      "Epoch: 064, Runtime 5.113539, Loss 0.604501, forward nfe 461139, backward nfe 66038, Train: 0.8450, Val: 0.7138, Test: 0.7316\n",
      "Epoch: 065, Runtime 5.117010, Loss 0.596963, forward nfe 468400, backward nfe 67070, Train: 0.8550, Val: 0.7138, Test: 0.7319\n",
      "Epoch: 066, Runtime 5.113132, Loss 0.591649, forward nfe 475662, backward nfe 68102, Train: 0.8550, Val: 0.7069, Test: 0.7304\n",
      "Epoch: 067, Runtime 5.113975, Loss 0.581627, forward nfe 482923, backward nfe 69130, Train: 0.8600, Val: 0.7108, Test: 0.7315\n",
      "Epoch: 068, Runtime 5.160402, Loss 0.565994, forward nfe 490186, backward nfe 70155, Train: 0.8500, Val: 0.7108, Test: 0.7311\n",
      "Epoch: 069, Runtime 5.115833, Loss 0.553043, forward nfe 497449, backward nfe 71183, Train: 0.8600, Val: 0.7108, Test: 0.7341\n",
      "Epoch: 070, Runtime 5.115927, Loss 0.539961, forward nfe 504710, backward nfe 72210, Train: 0.8600, Val: 0.7131, Test: 0.7354\n",
      "Epoch: 071, Runtime 5.117315, Loss 0.543676, forward nfe 511972, backward nfe 73234, Train: 0.8650, Val: 0.7185, Test: 0.7373\n",
      "Epoch: 072, Runtime 5.110245, Loss 0.525930, forward nfe 519233, backward nfe 74260, Train: 0.8650, Val: 0.7200, Test: 0.7380\n",
      "Epoch: 073, Runtime 5.112963, Loss 0.510060, forward nfe 526495, backward nfe 75286, Train: 0.8750, Val: 0.7177, Test: 0.7396\n",
      "Epoch: 074, Runtime 5.110738, Loss 0.504620, forward nfe 533755, backward nfe 76311, Train: 0.8550, Val: 0.7131, Test: 0.7333\n",
      "Epoch: 075, Runtime 5.112701, Loss 0.508512, forward nfe 541016, backward nfe 77337, Train: 0.8700, Val: 0.7185, Test: 0.7410\n",
      "Epoch: 076, Runtime 5.109960, Loss 0.484569, forward nfe 548277, backward nfe 78362, Train: 0.8550, Val: 0.7238, Test: 0.7421\n",
      "Epoch: 077, Runtime 5.112947, Loss 0.485079, forward nfe 555539, backward nfe 79386, Train: 0.8800, Val: 0.7223, Test: 0.7431\n",
      "Epoch: 078, Runtime 5.113968, Loss 0.480764, forward nfe 562801, backward nfe 80410, Train: 0.8750, Val: 0.7154, Test: 0.7405\n",
      "Epoch: 079, Runtime 5.106153, Loss 0.472286, forward nfe 570062, backward nfe 81434, Train: 0.8800, Val: 0.7177, Test: 0.7436\n",
      "Epoch: 080, Runtime 5.116616, Loss 0.454284, forward nfe 577323, backward nfe 82458, Train: 0.8700, Val: 0.7262, Test: 0.7461\n",
      "Epoch: 081, Runtime 5.110755, Loss 0.459205, forward nfe 584583, backward nfe 83482, Train: 0.8650, Val: 0.7277, Test: 0.7498\n",
      "Epoch: 082, Runtime 5.114378, Loss 0.443500, forward nfe 591844, backward nfe 84506, Train: 0.8700, Val: 0.7277, Test: 0.7507\n",
      "Epoch: 083, Runtime 5.111997, Loss 0.446371, forward nfe 599106, backward nfe 85530, Train: 0.8750, Val: 0.7223, Test: 0.7471\n",
      "Epoch: 084, Runtime 5.112321, Loss 0.446682, forward nfe 606368, backward nfe 86554, Train: 0.8750, Val: 0.7246, Test: 0.7453\n",
      "Epoch: 085, Runtime 5.111630, Loss 0.433955, forward nfe 613629, backward nfe 87579, Train: 0.8800, Val: 0.7246, Test: 0.7476\n",
      "Epoch: 086, Runtime 5.116757, Loss 0.424817, forward nfe 620890, backward nfe 88606, Train: 0.8800, Val: 0.7323, Test: 0.7542\n",
      "Epoch: 087, Runtime 5.110045, Loss 0.411510, forward nfe 628151, backward nfe 89629, Train: 0.8800, Val: 0.7362, Test: 0.7566\n",
      "Epoch: 088, Runtime 5.111870, Loss 0.418006, forward nfe 635411, backward nfe 90655, Train: 0.8800, Val: 0.7362, Test: 0.7557\n",
      "Epoch: 089, Runtime 5.108114, Loss 0.402814, forward nfe 642672, backward nfe 91678, Train: 0.8850, Val: 0.7277, Test: 0.7517\n",
      "Epoch: 090, Runtime 5.110208, Loss 0.406109, forward nfe 649933, backward nfe 92703, Train: 0.8700, Val: 0.7308, Test: 0.7567\n",
      "Epoch: 091, Runtime 5.111177, Loss 0.395484, forward nfe 657193, backward nfe 93726, Train: 0.8750, Val: 0.7415, Test: 0.7562\n",
      "Epoch: 092, Runtime 5.113864, Loss 0.395979, forward nfe 664454, backward nfe 94749, Train: 0.8800, Val: 0.7354, Test: 0.7564\n",
      "Epoch: 093, Runtime 5.109103, Loss 0.384052, forward nfe 671714, backward nfe 95772, Train: 0.8950, Val: 0.7315, Test: 0.7539\n",
      "Epoch: 094, Runtime 5.113235, Loss 0.374706, forward nfe 678974, backward nfe 96797, Train: 0.8950, Val: 0.7308, Test: 0.7559\n",
      "Epoch: 095, Runtime 5.108489, Loss 0.361266, forward nfe 686235, backward nfe 97821, Train: 0.8850, Val: 0.7492, Test: 0.7616\n",
      "Epoch: 096, Runtime 5.114055, Loss 0.364993, forward nfe 693496, backward nfe 98849, Train: 0.8800, Val: 0.7508, Test: 0.7624\n",
      "Epoch: 097, Runtime 5.111779, Loss 0.377224, forward nfe 700756, backward nfe 99875, Train: 0.8750, Val: 0.7392, Test: 0.7635\n",
      "Epoch: 098, Runtime 5.115251, Loss 0.352798, forward nfe 708017, backward nfe 100900, Train: 0.8950, Val: 0.7408, Test: 0.7616\n",
      "Epoch: 099, Runtime 5.106506, Loss 0.352067, forward nfe 715277, backward nfe 101923, Train: 0.8900, Val: 0.7377, Test: 0.7621\n",
      "best val accuracy 0.750769 with test accuracy 0.762406 at epoch 96\n",
      "*** Doing run 4 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.240807, Loss 2.345677, forward nfe 3626, backward nfe 77, Train: 0.1000, Val: 0.4023, Test: 0.3767\n",
      "Epoch: 002, Runtime 5.104807, Loss 2.392374, forward nfe 10885, backward nfe 1101, Train: 0.1000, Val: 0.3992, Test: 0.3729\n",
      "Epoch: 003, Runtime 5.109269, Loss 2.288765, forward nfe 18144, backward nfe 2127, Train: 0.1250, Val: 0.1877, Test: 0.1939\n",
      "Epoch: 004, Runtime 5.111207, Loss 2.281535, forward nfe 25405, backward nfe 3153, Train: 0.1300, Val: 0.1369, Test: 0.1419\n",
      "Epoch: 005, Runtime 5.107113, Loss 2.278946, forward nfe 32665, backward nfe 4177, Train: 0.1350, Val: 0.1762, Test: 0.1796\n",
      "Epoch: 006, Runtime 5.110765, Loss 2.266777, forward nfe 39927, backward nfe 5203, Train: 0.1350, Val: 0.1723, Test: 0.1769\n",
      "Epoch: 007, Runtime 5.113895, Loss 2.261046, forward nfe 47189, backward nfe 6231, Train: 0.1350, Val: 0.1123, Test: 0.1208\n",
      "Epoch: 008, Runtime 5.185395, Loss 2.254391, forward nfe 54451, backward nfe 7340, Train: 0.1400, Val: 0.1215, Test: 0.1294\n",
      "Epoch: 009, Runtime 5.145238, Loss 2.244033, forward nfe 61713, backward nfe 8403, Train: 0.1350, Val: 0.1577, Test: 0.1654\n",
      "Epoch: 010, Runtime 5.194448, Loss 2.240436, forward nfe 68976, backward nfe 9522, Train: 0.2350, Val: 0.1262, Test: 0.1471\n",
      "Epoch: 011, Runtime 5.194143, Loss 2.229447, forward nfe 76239, backward nfe 10641, Train: 0.1500, Val: 0.0869, Test: 0.0975\n",
      "Epoch: 012, Runtime 5.192456, Loss 2.222485, forward nfe 83502, backward nfe 11760, Train: 0.1950, Val: 0.0969, Test: 0.1130\n",
      "Epoch: 013, Runtime 5.193281, Loss 2.212621, forward nfe 90765, backward nfe 12879, Train: 0.2350, Val: 0.0946, Test: 0.1281\n",
      "Epoch: 014, Runtime 5.195923, Loss 2.206055, forward nfe 98029, backward nfe 13998, Train: 0.2200, Val: 0.0923, Test: 0.1185\n",
      "Epoch: 015, Runtime 5.195526, Loss 2.199363, forward nfe 105293, backward nfe 15117, Train: 0.2350, Val: 0.0777, Test: 0.1107\n",
      "Epoch: 016, Runtime 5.130670, Loss 2.184600, forward nfe 112555, backward nfe 16166, Train: 0.2400, Val: 0.0954, Test: 0.1174\n",
      "Epoch: 017, Runtime 5.175877, Loss 2.184854, forward nfe 119818, backward nfe 17262, Train: 0.2350, Val: 0.1038, Test: 0.1294\n",
      "Epoch: 018, Runtime 5.202089, Loss 2.169248, forward nfe 127081, backward nfe 18381, Train: 0.2250, Val: 0.1085, Test: 0.1303\n",
      "Epoch: 019, Runtime 5.195008, Loss 2.165588, forward nfe 134344, backward nfe 19500, Train: 0.2400, Val: 0.1154, Test: 0.1308\n",
      "Epoch: 020, Runtime 5.193461, Loss 2.157789, forward nfe 141608, backward nfe 20619, Train: 0.2400, Val: 0.1208, Test: 0.1345\n",
      "Epoch: 021, Runtime 5.194153, Loss 2.147783, forward nfe 148871, backward nfe 21738, Train: 0.2450, Val: 0.1300, Test: 0.1384\n",
      "Epoch: 022, Runtime 5.194723, Loss 2.139185, forward nfe 156134, backward nfe 22857, Train: 0.2450, Val: 0.1323, Test: 0.1401\n",
      "Epoch: 023, Runtime 5.149548, Loss 2.130410, forward nfe 163397, backward nfe 23928, Train: 0.2450, Val: 0.1331, Test: 0.1424\n",
      "Epoch: 024, Runtime 5.128453, Loss 2.120747, forward nfe 170660, backward nfe 24974, Train: 0.2450, Val: 0.1308, Test: 0.1435\n",
      "Epoch: 025, Runtime 5.192754, Loss 2.111203, forward nfe 177924, backward nfe 26093, Train: 0.2500, Val: 0.1362, Test: 0.1453\n",
      "Epoch: 026, Runtime 5.196356, Loss 2.104474, forward nfe 185188, backward nfe 27212, Train: 0.2350, Val: 0.1400, Test: 0.1486\n",
      "Epoch: 027, Runtime 5.108603, Loss 2.093939, forward nfe 192451, backward nfe 28237, Train: 0.2250, Val: 0.1415, Test: 0.1508\n",
      "Epoch: 028, Runtime 5.190872, Loss 2.085859, forward nfe 199714, backward nfe 29351, Train: 0.2200, Val: 0.1446, Test: 0.1532\n",
      "Epoch: 029, Runtime 5.196749, Loss 2.078394, forward nfe 206977, backward nfe 30470, Train: 0.2300, Val: 0.1469, Test: 0.1562\n",
      "Epoch: 030, Runtime 5.197790, Loss 2.066763, forward nfe 214241, backward nfe 31589, Train: 0.2200, Val: 0.1462, Test: 0.1563\n",
      "Epoch: 031, Runtime 5.193857, Loss 2.061871, forward nfe 221504, backward nfe 32708, Train: 0.2200, Val: 0.1469, Test: 0.1586\n",
      "Epoch: 032, Runtime 5.193702, Loss 2.053850, forward nfe 228767, backward nfe 33827, Train: 0.2200, Val: 0.1492, Test: 0.1608\n",
      "Epoch: 033, Runtime 5.194512, Loss 2.044385, forward nfe 236030, backward nfe 34946, Train: 0.2200, Val: 0.1538, Test: 0.1648\n",
      "Epoch: 034, Runtime 5.147056, Loss 2.036397, forward nfe 243294, backward nfe 36015, Train: 0.2200, Val: 0.1554, Test: 0.1655\n",
      "Epoch: 035, Runtime 5.197288, Loss 2.028281, forward nfe 250556, backward nfe 37134, Train: 0.2150, Val: 0.1500, Test: 0.1691\n",
      "Epoch: 036, Runtime 5.178671, Loss 2.017741, forward nfe 257818, backward nfe 38232, Train: 0.2200, Val: 0.1523, Test: 0.1703\n",
      "Epoch: 037, Runtime 5.200001, Loss 2.011163, forward nfe 265082, backward nfe 39351, Train: 0.2200, Val: 0.1531, Test: 0.1734\n",
      "Epoch: 038, Runtime 5.111260, Loss 2.004431, forward nfe 272345, backward nfe 40379, Train: 0.2200, Val: 0.1508, Test: 0.1691\n",
      "Epoch: 039, Runtime 5.195794, Loss 1.999098, forward nfe 279608, backward nfe 41498, Train: 0.2200, Val: 0.1400, Test: 0.1558\n",
      "Epoch: 040, Runtime 5.195811, Loss 1.989409, forward nfe 286872, backward nfe 42617, Train: 0.2200, Val: 0.1300, Test: 0.1448\n",
      "Epoch: 041, Runtime 5.121829, Loss 1.983887, forward nfe 294135, backward nfe 43650, Train: 0.2600, Val: 0.1192, Test: 0.1347\n",
      "Epoch: 042, Runtime 5.196627, Loss 1.976016, forward nfe 301397, backward nfe 44769, Train: 0.2550, Val: 0.1146, Test: 0.1283\n",
      "Epoch: 043, Runtime 5.117111, Loss 1.970608, forward nfe 308660, backward nfe 45804, Train: 0.2400, Val: 0.1177, Test: 0.1290\n",
      "Epoch: 044, Runtime 5.196688, Loss 1.963750, forward nfe 315923, backward nfe 46923, Train: 0.2500, Val: 0.1146, Test: 0.1279\n",
      "Epoch: 045, Runtime 5.113503, Loss 1.957662, forward nfe 323187, backward nfe 47947, Train: 0.2450, Val: 0.1177, Test: 0.1290\n",
      "Epoch: 046, Runtime 5.107915, Loss 1.949498, forward nfe 330449, backward nfe 48974, Train: 0.2600, Val: 0.1185, Test: 0.1302\n",
      "Epoch: 047, Runtime 5.129571, Loss 1.945909, forward nfe 337712, backward nfe 50020, Train: 0.2400, Val: 0.1077, Test: 0.1179\n",
      "Epoch: 048, Runtime 5.168524, Loss 1.942124, forward nfe 344974, backward nfe 51110, Train: 0.2400, Val: 0.1192, Test: 0.1312\n",
      "Epoch: 049, Runtime 5.111293, Loss 1.947630, forward nfe 352236, backward nfe 52137, Train: 0.2550, Val: 0.1231, Test: 0.1330\n",
      "Epoch: 050, Runtime 5.121201, Loss 1.923706, forward nfe 359498, backward nfe 53172, Train: 0.2750, Val: 0.1108, Test: 0.1238\n",
      "Epoch: 051, Runtime 5.124398, Loss 1.934592, forward nfe 366760, backward nfe 54211, Train: 0.2500, Val: 0.1162, Test: 0.1256\n",
      "Epoch: 052, Runtime 5.115978, Loss 1.917907, forward nfe 374023, backward nfe 55245, Train: 0.2400, Val: 0.1262, Test: 0.1369\n",
      "Epoch: 053, Runtime 5.109549, Loss 1.913555, forward nfe 381287, backward nfe 56269, Train: 0.2350, Val: 0.1292, Test: 0.1373\n",
      "Epoch: 054, Runtime 5.110262, Loss 1.885450, forward nfe 388549, backward nfe 57297, Train: 0.2550, Val: 0.1092, Test: 0.1270\n",
      "Epoch: 055, Runtime 5.121284, Loss 1.873048, forward nfe 395811, backward nfe 58336, Train: 0.3150, Val: 0.1523, Test: 0.1611\n",
      "Epoch: 056, Runtime 5.110157, Loss 1.846589, forward nfe 403074, backward nfe 59360, Train: 0.3150, Val: 0.1446, Test: 0.1596\n",
      "Epoch: 057, Runtime 5.117153, Loss 1.816819, forward nfe 410336, backward nfe 60388, Train: 0.2950, Val: 0.1400, Test: 0.1539\n",
      "Epoch: 058, Runtime 5.112134, Loss 1.814553, forward nfe 417599, backward nfe 61418, Train: 0.3250, Val: 0.1562, Test: 0.1734\n",
      "Epoch: 059, Runtime 5.118406, Loss 1.779142, forward nfe 424861, backward nfe 62451, Train: 0.3650, Val: 0.1692, Test: 0.1886\n",
      "Epoch: 060, Runtime 5.106649, Loss 1.776172, forward nfe 432123, backward nfe 63477, Train: 0.3300, Val: 0.1662, Test: 0.1806\n",
      "Epoch: 061, Runtime 5.109429, Loss 1.744568, forward nfe 439385, backward nfe 64505, Train: 0.3150, Val: 0.1608, Test: 0.1778\n",
      "Epoch: 062, Runtime 5.107717, Loss 1.742643, forward nfe 446647, backward nfe 65530, Train: 0.3900, Val: 0.1838, Test: 0.2070\n",
      "Epoch: 063, Runtime 5.121308, Loss 1.710747, forward nfe 453908, backward nfe 66567, Train: 0.4150, Val: 0.2085, Test: 0.2311\n",
      "Epoch: 064, Runtime 5.108472, Loss 1.711180, forward nfe 461170, backward nfe 67593, Train: 0.3550, Val: 0.1846, Test: 0.2013\n",
      "Epoch: 065, Runtime 5.108617, Loss 1.683690, forward nfe 468433, backward nfe 68617, Train: 0.3800, Val: 0.1954, Test: 0.2133\n",
      "Epoch: 066, Runtime 5.112584, Loss 1.668279, forward nfe 475695, backward nfe 69646, Train: 0.4250, Val: 0.2269, Test: 0.2561\n",
      "Epoch: 067, Runtime 5.109436, Loss 1.647808, forward nfe 482956, backward nfe 70671, Train: 0.4300, Val: 0.2377, Test: 0.2657\n",
      "Epoch: 068, Runtime 5.112297, Loss 1.649293, forward nfe 490219, backward nfe 71698, Train: 0.4400, Val: 0.2323, Test: 0.2600\n",
      "Epoch: 069, Runtime 5.112501, Loss 1.613545, forward nfe 497481, backward nfe 72724, Train: 0.4250, Val: 0.2285, Test: 0.2537\n",
      "Epoch: 070, Runtime 5.112496, Loss 1.598371, forward nfe 504743, backward nfe 73749, Train: 0.4500, Val: 0.2446, Test: 0.2742\n",
      "Epoch: 071, Runtime 5.109056, Loss 1.581039, forward nfe 512004, backward nfe 74776, Train: 0.4350, Val: 0.2554, Test: 0.2878\n",
      "Epoch: 072, Runtime 5.112387, Loss 1.572531, forward nfe 519266, backward nfe 75800, Train: 0.4400, Val: 0.2608, Test: 0.2998\n",
      "Epoch: 073, Runtime 5.127129, Loss 1.565549, forward nfe 526528, backward nfe 76824, Train: 0.4500, Val: 0.2708, Test: 0.3094\n",
      "Epoch: 074, Runtime 5.150103, Loss 1.541240, forward nfe 533788, backward nfe 77848, Train: 0.4500, Val: 0.2785, Test: 0.3166\n",
      "Epoch: 075, Runtime 5.127662, Loss 1.529026, forward nfe 541049, backward nfe 78873, Train: 0.4450, Val: 0.2808, Test: 0.3216\n",
      "Epoch: 076, Runtime 5.133628, Loss 1.527086, forward nfe 548311, backward nfe 79897, Train: 0.4550, Val: 0.2892, Test: 0.3244\n",
      "Epoch: 077, Runtime 5.137466, Loss 1.505608, forward nfe 555572, backward nfe 80923, Train: 0.4500, Val: 0.2946, Test: 0.3283\n",
      "Epoch: 078, Runtime 5.141083, Loss 1.492818, forward nfe 562834, backward nfe 81947, Train: 0.4600, Val: 0.2992, Test: 0.3321\n",
      "Epoch: 079, Runtime 5.190091, Loss 1.483937, forward nfe 570094, backward nfe 82975, Train: 0.4550, Val: 0.3023, Test: 0.3345\n",
      "Epoch: 080, Runtime 5.181299, Loss 1.474489, forward nfe 577354, backward nfe 84000, Train: 0.4550, Val: 0.2985, Test: 0.3340\n",
      "Epoch: 081, Runtime 5.141581, Loss 1.467024, forward nfe 584615, backward nfe 85025, Train: 0.4550, Val: 0.3054, Test: 0.3359\n",
      "Epoch: 082, Runtime 5.151844, Loss 1.464992, forward nfe 591876, backward nfe 86049, Train: 0.4600, Val: 0.3123, Test: 0.3383\n",
      "Epoch: 083, Runtime 5.114000, Loss 1.453804, forward nfe 599136, backward nfe 87073, Train: 0.4650, Val: 0.3138, Test: 0.3437\n",
      "Epoch: 084, Runtime 5.127307, Loss 1.437592, forward nfe 606396, backward nfe 88099, Train: 0.4650, Val: 0.3146, Test: 0.3480\n",
      "Epoch: 085, Runtime 5.107439, Loss 1.436453, forward nfe 613657, backward nfe 89122, Train: 0.4650, Val: 0.3254, Test: 0.3496\n",
      "Epoch: 086, Runtime 5.106513, Loss 1.425431, forward nfe 620918, backward nfe 90146, Train: 0.4700, Val: 0.3277, Test: 0.3488\n",
      "Epoch: 087, Runtime 5.112877, Loss 1.421484, forward nfe 628180, backward nfe 91170, Train: 0.4750, Val: 0.3285, Test: 0.3530\n",
      "Epoch: 088, Runtime 5.108297, Loss 1.409855, forward nfe 635440, backward nfe 92195, Train: 0.4850, Val: 0.3362, Test: 0.3587\n",
      "Epoch: 089, Runtime 5.109906, Loss 1.393420, forward nfe 642702, backward nfe 93221, Train: 0.4750, Val: 0.3385, Test: 0.3624\n",
      "Epoch: 090, Runtime 5.108435, Loss 1.399650, forward nfe 649962, backward nfe 94244, Train: 0.4900, Val: 0.3423, Test: 0.3644\n",
      "Epoch: 091, Runtime 5.110428, Loss 1.382972, forward nfe 657223, backward nfe 95268, Train: 0.4750, Val: 0.3392, Test: 0.3603\n",
      "Epoch: 092, Runtime 5.103420, Loss 1.377874, forward nfe 664484, backward nfe 96292, Train: 0.4650, Val: 0.3323, Test: 0.3550\n",
      "Epoch: 093, Runtime 5.108059, Loss 1.371756, forward nfe 671745, backward nfe 97316, Train: 0.4850, Val: 0.3400, Test: 0.3612\n",
      "Epoch: 094, Runtime 5.107490, Loss 1.369396, forward nfe 679005, backward nfe 98339, Train: 0.4950, Val: 0.3446, Test: 0.3675\n",
      "Epoch: 095, Runtime 5.107123, Loss 1.358248, forward nfe 686265, backward nfe 99362, Train: 0.4950, Val: 0.3438, Test: 0.3685\n",
      "Epoch: 096, Runtime 5.109142, Loss 1.347653, forward nfe 693525, backward nfe 100387, Train: 0.5000, Val: 0.3415, Test: 0.3686\n",
      "Epoch: 097, Runtime 5.109256, Loss 1.343416, forward nfe 700786, backward nfe 101410, Train: 0.5000, Val: 0.3392, Test: 0.3699\n",
      "Epoch: 098, Runtime 5.108261, Loss 1.343895, forward nfe 708046, backward nfe 102434, Train: 0.4850, Val: 0.3454, Test: 0.3718\n",
      "Epoch: 099, Runtime 5.108968, Loss 1.329966, forward nfe 715306, backward nfe 103457, Train: 0.4800, Val: 0.3515, Test: 0.3732\n",
      "best val accuracy 0.402308 with test accuracy 0.376673 at epoch 1\n",
      "*** Doing run 5 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.232182, Loss 2.323740, forward nfe 3625, backward nfe 69, Train: 0.1000, Val: 0.0231, Test: 0.0358\n",
      "Epoch: 002, Runtime 5.110696, Loss 3.162744, forward nfe 10883, backward nfe 1093, Train: 0.1300, Val: 0.0931, Test: 0.0954\n",
      "Epoch: 003, Runtime 5.109085, Loss 2.369628, forward nfe 18141, backward nfe 2117, Train: 0.1900, Val: 0.1323, Test: 0.1409\n",
      "Epoch: 004, Runtime 5.108940, Loss 2.302293, forward nfe 25401, backward nfe 3141, Train: 0.1400, Val: 0.0569, Test: 0.0658\n",
      "Epoch: 005, Runtime 5.106845, Loss 2.250670, forward nfe 32661, backward nfe 4165, Train: 0.1750, Val: 0.0492, Test: 0.0676\n",
      "Epoch: 006, Runtime 5.105233, Loss 2.251981, forward nfe 39923, backward nfe 5190, Train: 0.2000, Val: 0.0677, Test: 0.0876\n",
      "Epoch: 007, Runtime 5.110588, Loss 2.233865, forward nfe 47184, backward nfe 6219, Train: 0.2250, Val: 0.1254, Test: 0.1410\n",
      "Epoch: 008, Runtime 5.114535, Loss 2.183798, forward nfe 54447, backward nfe 7245, Train: 0.2050, Val: 0.1346, Test: 0.1434\n",
      "Epoch: 009, Runtime 5.125500, Loss 2.183486, forward nfe 61709, backward nfe 8288, Train: 0.2150, Val: 0.1385, Test: 0.1489\n",
      "Epoch: 010, Runtime 5.112862, Loss 2.167047, forward nfe 68973, backward nfe 9312, Train: 0.2200, Val: 0.1385, Test: 0.1526\n",
      "Epoch: 011, Runtime 5.172300, Loss 2.140913, forward nfe 76235, backward nfe 10403, Train: 0.2400, Val: 0.1377, Test: 0.1495\n",
      "Epoch: 012, Runtime 5.122374, Loss 2.104017, forward nfe 83498, backward nfe 11437, Train: 0.2550, Val: 0.1423, Test: 0.1525\n",
      "Epoch: 013, Runtime 5.178336, Loss 2.094043, forward nfe 90762, backward nfe 12534, Train: 0.2500, Val: 0.1400, Test: 0.1543\n",
      "Epoch: 014, Runtime 5.142237, Loss 2.081714, forward nfe 98025, backward nfe 13593, Train: 0.2350, Val: 0.1400, Test: 0.1535\n",
      "Epoch: 015, Runtime 5.119742, Loss 2.044899, forward nfe 105289, backward nfe 14626, Train: 0.2200, Val: 0.1408, Test: 0.1544\n",
      "Epoch: 016, Runtime 5.193164, Loss 2.023039, forward nfe 112553, backward nfe 15743, Train: 0.2150, Val: 0.1400, Test: 0.1569\n",
      "Epoch: 017, Runtime 5.162636, Loss 2.013252, forward nfe 119816, backward nfe 16824, Train: 0.2350, Val: 0.1431, Test: 0.1567\n",
      "Epoch: 018, Runtime 5.162350, Loss 1.983749, forward nfe 127080, backward nfe 17906, Train: 0.2700, Val: 0.1631, Test: 0.1734\n",
      "Epoch: 019, Runtime 5.166095, Loss 1.950758, forward nfe 134344, backward nfe 18994, Train: 0.3350, Val: 0.2038, Test: 0.2115\n",
      "Epoch: 020, Runtime 5.175251, Loss 1.929460, forward nfe 141606, backward nfe 20087, Train: 0.3750, Val: 0.2177, Test: 0.2245\n",
      "Epoch: 021, Runtime 5.193537, Loss 1.907097, forward nfe 148869, backward nfe 21206, Train: 0.3700, Val: 0.2092, Test: 0.2187\n",
      "Epoch: 022, Runtime 5.199073, Loss 1.876231, forward nfe 156132, backward nfe 22325, Train: 0.3600, Val: 0.1969, Test: 0.2098\n",
      "Epoch: 023, Runtime 5.197531, Loss 1.852382, forward nfe 163396, backward nfe 23444, Train: 0.3650, Val: 0.2238, Test: 0.2370\n",
      "Epoch: 024, Runtime 5.199373, Loss 1.834420, forward nfe 170659, backward nfe 24563, Train: 0.3650, Val: 0.2546, Test: 0.2640\n",
      "Epoch: 025, Runtime 5.196318, Loss 1.805799, forward nfe 177922, backward nfe 25682, Train: 0.4700, Val: 0.3315, Test: 0.3353\n",
      "Epoch: 026, Runtime 5.196816, Loss 1.776966, forward nfe 185185, backward nfe 26801, Train: 0.5300, Val: 0.3762, Test: 0.3937\n",
      "Epoch: 027, Runtime 5.196140, Loss 1.761056, forward nfe 192448, backward nfe 27920, Train: 0.5550, Val: 0.4100, Test: 0.4244\n",
      "Epoch: 028, Runtime 5.195848, Loss 1.728586, forward nfe 199711, backward nfe 29039, Train: 0.5000, Val: 0.4108, Test: 0.4161\n",
      "Epoch: 029, Runtime 5.136796, Loss 1.697492, forward nfe 206975, backward nfe 30089, Train: 0.4950, Val: 0.3969, Test: 0.4025\n",
      "Epoch: 030, Runtime 5.197476, Loss 1.670536, forward nfe 214238, backward nfe 31208, Train: 0.5150, Val: 0.4069, Test: 0.4178\n",
      "Epoch: 031, Runtime 5.197047, Loss 1.652961, forward nfe 221501, backward nfe 32327, Train: 0.5800, Val: 0.4846, Test: 0.4978\n",
      "Epoch: 032, Runtime 5.200534, Loss 1.611825, forward nfe 228765, backward nfe 33446, Train: 0.5900, Val: 0.5392, Test: 0.5472\n",
      "Epoch: 033, Runtime 5.161702, Loss 1.571934, forward nfe 236028, backward nfe 34526, Train: 0.6450, Val: 0.5700, Test: 0.5777\n",
      "Epoch: 034, Runtime 5.184993, Loss 1.519648, forward nfe 243291, backward nfe 35630, Train: 0.6450, Val: 0.5877, Test: 0.5994\n",
      "Epoch: 035, Runtime 5.196125, Loss 1.462100, forward nfe 250555, backward nfe 36749, Train: 0.6300, Val: 0.5885, Test: 0.5992\n",
      "Epoch: 036, Runtime 5.155569, Loss 1.411393, forward nfe 257818, backward nfe 37826, Train: 0.6200, Val: 0.5638, Test: 0.5810\n",
      "Epoch: 037, Runtime 5.200207, Loss 1.364661, forward nfe 265082, backward nfe 38945, Train: 0.6550, Val: 0.5415, Test: 0.5657\n",
      "Epoch: 038, Runtime 5.185611, Loss 1.350751, forward nfe 272345, backward nfe 40049, Train: 0.6600, Val: 0.5600, Test: 0.5790\n",
      "Epoch: 039, Runtime 5.196020, Loss 1.310533, forward nfe 279609, backward nfe 41168, Train: 0.6500, Val: 0.5831, Test: 0.6095\n",
      "Epoch: 040, Runtime 5.198991, Loss 1.278281, forward nfe 286871, backward nfe 42287, Train: 0.6900, Val: 0.5977, Test: 0.6376\n",
      "Epoch: 041, Runtime 5.197298, Loss 1.246070, forward nfe 294135, backward nfe 43406, Train: 0.7350, Val: 0.6362, Test: 0.6548\n",
      "Epoch: 042, Runtime 5.200531, Loss 1.209045, forward nfe 301399, backward nfe 44525, Train: 0.7450, Val: 0.6238, Test: 0.6415\n",
      "Epoch: 043, Runtime 5.147742, Loss 1.182138, forward nfe 308663, backward nfe 45592, Train: 0.7250, Val: 0.6162, Test: 0.6389\n",
      "Epoch: 044, Runtime 5.202742, Loss 1.146077, forward nfe 315927, backward nfe 46711, Train: 0.7300, Val: 0.6138, Test: 0.6410\n",
      "Epoch: 045, Runtime 5.200919, Loss 1.119697, forward nfe 323190, backward nfe 47830, Train: 0.7250, Val: 0.6315, Test: 0.6648\n",
      "Epoch: 046, Runtime 5.181962, Loss 1.090705, forward nfe 330453, backward nfe 48933, Train: 0.7650, Val: 0.6408, Test: 0.6781\n",
      "Epoch: 047, Runtime 5.197191, Loss 1.061259, forward nfe 337717, backward nfe 50052, Train: 0.7700, Val: 0.6562, Test: 0.6926\n",
      "Epoch: 048, Runtime 5.163865, Loss 1.037063, forward nfe 344980, backward nfe 51138, Train: 0.7800, Val: 0.6600, Test: 0.6831\n",
      "Epoch: 049, Runtime 5.121336, Loss 1.000773, forward nfe 352242, backward nfe 52178, Train: 0.7700, Val: 0.6400, Test: 0.6652\n",
      "Epoch: 050, Runtime 5.196460, Loss 0.984388, forward nfe 359506, backward nfe 53297, Train: 0.7700, Val: 0.6492, Test: 0.6816\n",
      "Epoch: 051, Runtime 5.196891, Loss 0.957905, forward nfe 366769, backward nfe 54416, Train: 0.7850, Val: 0.6823, Test: 0.7043\n",
      "Epoch: 052, Runtime 5.120173, Loss 0.937375, forward nfe 374032, backward nfe 55453, Train: 0.7750, Val: 0.6862, Test: 0.7160\n",
      "Epoch: 053, Runtime 5.117207, Loss 0.911129, forward nfe 381296, backward nfe 56484, Train: 0.7850, Val: 0.6869, Test: 0.7152\n",
      "Epoch: 054, Runtime 5.161330, Loss 0.894528, forward nfe 388559, backward nfe 57565, Train: 0.8050, Val: 0.6715, Test: 0.7077\n",
      "Epoch: 055, Runtime 5.196809, Loss 0.880721, forward nfe 395823, backward nfe 58684, Train: 0.8050, Val: 0.6677, Test: 0.7009\n",
      "Epoch: 056, Runtime 5.201123, Loss 0.855686, forward nfe 403086, backward nfe 59803, Train: 0.8100, Val: 0.6846, Test: 0.7125\n",
      "Epoch: 057, Runtime 5.198487, Loss 0.838075, forward nfe 410350, backward nfe 60922, Train: 0.8100, Val: 0.7015, Test: 0.7274\n",
      "Epoch: 058, Runtime 5.196506, Loss 0.812726, forward nfe 417613, backward nfe 62041, Train: 0.8150, Val: 0.7031, Test: 0.7298\n",
      "Epoch: 059, Runtime 5.166287, Loss 0.799308, forward nfe 424877, backward nfe 63124, Train: 0.8200, Val: 0.7046, Test: 0.7304\n",
      "Epoch: 060, Runtime 5.162734, Loss 0.763022, forward nfe 432141, backward nfe 64207, Train: 0.8250, Val: 0.6992, Test: 0.7197\n",
      "Epoch: 061, Runtime 5.123137, Loss 0.760042, forward nfe 439404, backward nfe 65244, Train: 0.8200, Val: 0.6823, Test: 0.7109\n",
      "Epoch: 062, Runtime 5.141759, Loss 0.747043, forward nfe 446666, backward nfe 66300, Train: 0.8200, Val: 0.6985, Test: 0.7238\n",
      "Epoch: 063, Runtime 5.197912, Loss 0.732005, forward nfe 453930, backward nfe 67419, Train: 0.8200, Val: 0.7069, Test: 0.7364\n",
      "Epoch: 064, Runtime 5.182787, Loss 0.708712, forward nfe 461193, backward nfe 68504, Train: 0.8400, Val: 0.7146, Test: 0.7373\n",
      "Epoch: 065, Runtime 5.198098, Loss 0.698791, forward nfe 468457, backward nfe 69619, Train: 0.8400, Val: 0.7000, Test: 0.7227\n",
      "Epoch: 066, Runtime 5.195044, Loss 0.673225, forward nfe 475720, backward nfe 70738, Train: 0.8400, Val: 0.7069, Test: 0.7338\n",
      "Epoch: 067, Runtime 5.200436, Loss 0.655761, forward nfe 482983, backward nfe 71857, Train: 0.8450, Val: 0.7185, Test: 0.7462\n",
      "Epoch: 068, Runtime 5.164950, Loss 0.647751, forward nfe 490246, backward nfe 72940, Train: 0.8500, Val: 0.7131, Test: 0.7314\n",
      "Epoch: 069, Runtime 5.116422, Loss 0.644978, forward nfe 497508, backward nfe 73972, Train: 0.8550, Val: 0.7154, Test: 0.7404\n",
      "Epoch: 070, Runtime 5.194250, Loss 0.616305, forward nfe 504770, backward nfe 75091, Train: 0.8450, Val: 0.7246, Test: 0.7529\n",
      "Epoch: 071, Runtime 5.132513, Loss 0.605437, forward nfe 512034, backward nfe 76141, Train: 0.8500, Val: 0.7208, Test: 0.7466\n",
      "Epoch: 072, Runtime 5.117785, Loss 0.587898, forward nfe 519298, backward nfe 77173, Train: 0.8450, Val: 0.7169, Test: 0.7418\n",
      "Epoch: 073, Runtime 5.150937, Loss 0.574561, forward nfe 526562, backward nfe 78240, Train: 0.8500, Val: 0.7208, Test: 0.7481\n",
      "Epoch: 074, Runtime 5.127526, Loss 0.560775, forward nfe 533825, backward nfe 79286, Train: 0.8600, Val: 0.7331, Test: 0.7543\n",
      "Epoch: 075, Runtime 5.150712, Loss 0.547663, forward nfe 541089, backward nfe 80354, Train: 0.8750, Val: 0.7354, Test: 0.7531\n",
      "Epoch: 076, Runtime 5.137514, Loss 0.528176, forward nfe 548353, backward nfe 81404, Train: 0.8750, Val: 0.7415, Test: 0.7567\n",
      "Epoch: 077, Runtime 5.127470, Loss 0.517731, forward nfe 555615, backward nfe 82445, Train: 0.8850, Val: 0.7446, Test: 0.7608\n",
      "Epoch: 078, Runtime 5.131860, Loss 0.510521, forward nfe 562878, backward nfe 83490, Train: 0.8850, Val: 0.7415, Test: 0.7619\n",
      "Epoch: 079, Runtime 5.136124, Loss 0.493227, forward nfe 570142, backward nfe 84538, Train: 0.8650, Val: 0.7569, Test: 0.7751\n",
      "Epoch: 080, Runtime 5.120790, Loss 0.495721, forward nfe 577405, backward nfe 85571, Train: 0.8800, Val: 0.7462, Test: 0.7617\n",
      "Epoch: 081, Runtime 5.126104, Loss 0.473546, forward nfe 584668, backward nfe 86617, Train: 0.8800, Val: 0.7262, Test: 0.7467\n",
      "Epoch: 082, Runtime 5.125034, Loss 0.478713, forward nfe 591932, backward nfe 87655, Train: 0.8900, Val: 0.7446, Test: 0.7746\n",
      "Epoch: 083, Runtime 5.113787, Loss 0.463131, forward nfe 599196, backward nfe 88684, Train: 0.8800, Val: 0.7608, Test: 0.7800\n",
      "Epoch: 084, Runtime 5.122912, Loss 0.448294, forward nfe 606459, backward nfe 89725, Train: 0.8900, Val: 0.7523, Test: 0.7725\n",
      "Epoch: 085, Runtime 5.121080, Loss 0.446025, forward nfe 613722, backward nfe 90762, Train: 0.8900, Val: 0.7585, Test: 0.7786\n",
      "Epoch: 086, Runtime 5.115846, Loss 0.425385, forward nfe 620985, backward nfe 91795, Train: 0.8900, Val: 0.7623, Test: 0.7875\n",
      "Epoch: 087, Runtime 5.134657, Loss 0.434114, forward nfe 628247, backward nfe 92845, Train: 0.8950, Val: 0.7538, Test: 0.7830\n",
      "Epoch: 088, Runtime 5.115641, Loss 0.410037, forward nfe 635511, backward nfe 93873, Train: 0.8900, Val: 0.7515, Test: 0.7752\n",
      "Epoch: 089, Runtime 5.112171, Loss 0.404212, forward nfe 642773, backward nfe 94897, Train: 0.8850, Val: 0.7577, Test: 0.7849\n",
      "Epoch: 090, Runtime 5.122149, Loss 0.397065, forward nfe 650036, backward nfe 95928, Train: 0.8850, Val: 0.7731, Test: 0.7964\n",
      "Epoch: 091, Runtime 5.111968, Loss 0.386903, forward nfe 657298, backward nfe 96957, Train: 0.8900, Val: 0.7692, Test: 0.7893\n",
      "Epoch: 092, Runtime 5.115030, Loss 0.389805, forward nfe 664562, backward nfe 97988, Train: 0.8900, Val: 0.7715, Test: 0.7863\n",
      "Epoch: 093, Runtime 5.118041, Loss 0.375670, forward nfe 671825, backward nfe 99018, Train: 0.8900, Val: 0.7900, Test: 0.8029\n",
      "Epoch: 094, Runtime 5.106973, Loss 0.362584, forward nfe 679087, backward nfe 100043, Train: 0.8900, Val: 0.7838, Test: 0.8037\n",
      "Epoch: 095, Runtime 5.112147, Loss 0.360421, forward nfe 686350, backward nfe 101070, Train: 0.9050, Val: 0.7662, Test: 0.7875\n",
      "Epoch: 096, Runtime 5.115406, Loss 0.354989, forward nfe 693613, backward nfe 102103, Train: 0.9050, Val: 0.7769, Test: 0.7943\n",
      "Epoch: 097, Runtime 5.110315, Loss 0.355594, forward nfe 700877, backward nfe 103128, Train: 0.8950, Val: 0.7785, Test: 0.8031\n",
      "Epoch: 098, Runtime 5.113647, Loss 0.347614, forward nfe 708141, backward nfe 104156, Train: 0.8950, Val: 0.7923, Test: 0.8075\n",
      "Epoch: 099, Runtime 5.129423, Loss 0.340491, forward nfe 715405, backward nfe 105205, Train: 0.9000, Val: 0.7854, Test: 0.8035\n",
      "best val accuracy 0.792308 with test accuracy 0.807542 at epoch 98\n",
      "*** Doing run 6 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.227193, Loss 2.312374, forward nfe 3626, backward nfe 71, Train: 0.1200, Val: 0.1638, Test: 0.1583\n",
      "Epoch: 002, Runtime 5.103549, Loss 2.900358, forward nfe 10883, backward nfe 1095, Train: 0.1050, Val: 0.0715, Test: 0.0720\n",
      "Epoch: 003, Runtime 5.107422, Loss 2.541941, forward nfe 18143, backward nfe 2119, Train: 0.1100, Val: 0.0238, Test: 0.0317\n",
      "Epoch: 004, Runtime 5.107916, Loss 2.385755, forward nfe 25402, backward nfe 3143, Train: 0.2000, Val: 0.0331, Test: 0.0461\n",
      "Epoch: 005, Runtime 5.105578, Loss 2.326597, forward nfe 32661, backward nfe 4168, Train: 0.1100, Val: 0.0400, Test: 0.0452\n",
      "Epoch: 006, Runtime 5.109226, Loss 2.326952, forward nfe 39921, backward nfe 5193, Train: 0.1650, Val: 0.1115, Test: 0.1162\n",
      "Epoch: 007, Runtime 5.108171, Loss 2.271877, forward nfe 47182, backward nfe 6218, Train: 0.1550, Val: 0.2023, Test: 0.1941\n",
      "Epoch: 008, Runtime 5.109611, Loss 2.230827, forward nfe 54443, backward nfe 7243, Train: 0.1400, Val: 0.1500, Test: 0.1460\n",
      "Epoch: 009, Runtime 5.114638, Loss 2.214862, forward nfe 61704, backward nfe 8272, Train: 0.1400, Val: 0.1062, Test: 0.1077\n",
      "Epoch: 010, Runtime 5.109505, Loss 2.212177, forward nfe 68965, backward nfe 9296, Train: 0.1650, Val: 0.1131, Test: 0.1161\n",
      "Epoch: 011, Runtime 5.111647, Loss 2.198281, forward nfe 76227, backward nfe 10325, Train: 0.2150, Val: 0.1123, Test: 0.1249\n",
      "Epoch: 012, Runtime 5.108503, Loss 2.190507, forward nfe 83489, backward nfe 11349, Train: 0.2650, Val: 0.1231, Test: 0.1386\n",
      "Epoch: 013, Runtime 5.120836, Loss 2.159890, forward nfe 90752, backward nfe 12385, Train: 0.3300, Val: 0.1054, Test: 0.1348\n",
      "Epoch: 014, Runtime 5.125242, Loss 2.142406, forward nfe 98015, backward nfe 13429, Train: 0.2600, Val: 0.0423, Test: 0.0684\n",
      "Epoch: 015, Runtime 5.179862, Loss 2.141620, forward nfe 105278, backward nfe 14525, Train: 0.2350, Val: 0.1223, Test: 0.1312\n",
      "Epoch: 016, Runtime 5.148244, Loss 2.121804, forward nfe 112540, backward nfe 15594, Train: 0.2900, Val: 0.1415, Test: 0.1505\n",
      "Epoch: 017, Runtime 5.193921, Loss 2.098780, forward nfe 119802, backward nfe 16713, Train: 0.3700, Val: 0.1646, Test: 0.1858\n",
      "Epoch: 018, Runtime 5.110974, Loss 2.075849, forward nfe 127065, backward nfe 17741, Train: 0.3650, Val: 0.1700, Test: 0.1941\n",
      "Epoch: 019, Runtime 5.165051, Loss 2.067384, forward nfe 134327, backward nfe 18828, Train: 0.3800, Val: 0.1546, Test: 0.1875\n",
      "Epoch: 020, Runtime 5.177190, Loss 2.028157, forward nfe 141590, backward nfe 19930, Train: 0.4050, Val: 0.0985, Test: 0.1347\n",
      "Epoch: 021, Runtime 5.195291, Loss 2.017508, forward nfe 148853, backward nfe 21049, Train: 0.4650, Val: 0.1908, Test: 0.2244\n",
      "Epoch: 022, Runtime 5.133624, Loss 1.974174, forward nfe 156115, backward nfe 22102, Train: 0.4150, Val: 0.2369, Test: 0.2600\n",
      "Epoch: 023, Runtime 5.141382, Loss 1.970803, forward nfe 163378, backward nfe 23160, Train: 0.4600, Val: 0.2669, Test: 0.2954\n",
      "Epoch: 024, Runtime 5.111444, Loss 1.923917, forward nfe 170642, backward nfe 24189, Train: 0.4800, Val: 0.2562, Test: 0.2898\n",
      "Epoch: 025, Runtime 5.113226, Loss 1.899219, forward nfe 177906, backward nfe 25216, Train: 0.4900, Val: 0.2900, Test: 0.3315\n",
      "Epoch: 026, Runtime 5.112862, Loss 1.877112, forward nfe 185169, backward nfe 26243, Train: 0.5000, Val: 0.4023, Test: 0.4236\n",
      "Epoch: 027, Runtime 5.117777, Loss 1.837110, forward nfe 192433, backward nfe 27279, Train: 0.4900, Val: 0.3792, Test: 0.4101\n",
      "Epoch: 028, Runtime 5.170191, Loss 1.815948, forward nfe 199695, backward nfe 28373, Train: 0.5000, Val: 0.3738, Test: 0.4012\n",
      "Epoch: 029, Runtime 5.137216, Loss 1.769433, forward nfe 206957, backward nfe 29427, Train: 0.5250, Val: 0.3769, Test: 0.4076\n",
      "Epoch: 030, Runtime 5.112968, Loss 1.719838, forward nfe 214220, backward nfe 30458, Train: 0.5750, Val: 0.4262, Test: 0.4518\n",
      "Epoch: 031, Runtime 5.126359, Loss 1.673885, forward nfe 221482, backward nfe 31501, Train: 0.6050, Val: 0.6077, Test: 0.6197\n",
      "Epoch: 032, Runtime 5.181955, Loss 1.626439, forward nfe 228744, backward nfe 32608, Train: 0.5500, Val: 0.6223, Test: 0.6244\n",
      "Epoch: 033, Runtime 5.196604, Loss 1.588768, forward nfe 236006, backward nfe 33727, Train: 0.5600, Val: 0.6169, Test: 0.6242\n",
      "Epoch: 034, Runtime 5.122180, Loss 1.534195, forward nfe 243269, backward nfe 34766, Train: 0.6450, Val: 0.6200, Test: 0.6382\n",
      "Epoch: 035, Runtime 5.174914, Loss 1.481302, forward nfe 250532, backward nfe 35858, Train: 0.6600, Val: 0.5892, Test: 0.6171\n",
      "Epoch: 036, Runtime 5.152372, Loss 1.461440, forward nfe 257794, backward nfe 36929, Train: 0.6750, Val: 0.5423, Test: 0.5793\n",
      "Epoch: 037, Runtime 5.113922, Loss 1.419189, forward nfe 265057, backward nfe 37959, Train: 0.6650, Val: 0.6008, Test: 0.6335\n",
      "Epoch: 038, Runtime 5.121672, Loss 1.379771, forward nfe 272319, backward nfe 38995, Train: 0.6550, Val: 0.6438, Test: 0.6622\n",
      "Epoch: 039, Runtime 5.118886, Loss 1.343468, forward nfe 279583, backward nfe 40029, Train: 0.7050, Val: 0.6600, Test: 0.6722\n",
      "Epoch: 040, Runtime 5.115880, Loss 1.312978, forward nfe 286845, backward nfe 41061, Train: 0.7200, Val: 0.6315, Test: 0.6578\n",
      "Epoch: 041, Runtime 5.118226, Loss 1.270807, forward nfe 294109, backward nfe 42092, Train: 0.7550, Val: 0.6100, Test: 0.6451\n",
      "Epoch: 042, Runtime 5.112998, Loss 1.233070, forward nfe 301371, backward nfe 43120, Train: 0.7550, Val: 0.6385, Test: 0.6610\n",
      "Epoch: 043, Runtime 5.125531, Loss 1.203126, forward nfe 308633, backward nfe 44163, Train: 0.7150, Val: 0.6462, Test: 0.6665\n",
      "Epoch: 044, Runtime 5.195692, Loss 1.182728, forward nfe 315895, backward nfe 45282, Train: 0.6900, Val: 0.6485, Test: 0.6672\n",
      "Epoch: 045, Runtime 5.194933, Loss 1.153136, forward nfe 323158, backward nfe 46401, Train: 0.7050, Val: 0.6538, Test: 0.6723\n",
      "Epoch: 046, Runtime 5.110227, Loss 1.129702, forward nfe 330421, backward nfe 47428, Train: 0.7600, Val: 0.6585, Test: 0.6761\n",
      "Epoch: 047, Runtime 5.144012, Loss 1.098929, forward nfe 337684, backward nfe 48493, Train: 0.7850, Val: 0.6654, Test: 0.6735\n",
      "Epoch: 048, Runtime 5.113718, Loss 1.065679, forward nfe 344947, backward nfe 49523, Train: 0.7950, Val: 0.6685, Test: 0.6823\n",
      "Epoch: 049, Runtime 5.131906, Loss 1.037849, forward nfe 352210, backward nfe 50574, Train: 0.8100, Val: 0.6800, Test: 0.7000\n",
      "Epoch: 050, Runtime 5.196071, Loss 1.010282, forward nfe 359473, backward nfe 51693, Train: 0.8100, Val: 0.6838, Test: 0.7056\n",
      "Epoch: 051, Runtime 5.156397, Loss 0.999304, forward nfe 366736, backward nfe 52773, Train: 0.8100, Val: 0.6862, Test: 0.7064\n",
      "Epoch: 052, Runtime 5.132717, Loss 0.972872, forward nfe 373998, backward nfe 53824, Train: 0.8050, Val: 0.6769, Test: 0.6955\n",
      "Epoch: 053, Runtime 5.181361, Loss 0.953940, forward nfe 381260, backward nfe 54927, Train: 0.8150, Val: 0.6692, Test: 0.6868\n",
      "Epoch: 054, Runtime 5.154827, Loss 0.932401, forward nfe 388523, backward nfe 56001, Train: 0.8150, Val: 0.6738, Test: 0.6955\n",
      "Epoch: 055, Runtime 5.124364, Loss 0.906963, forward nfe 395787, backward nfe 57046, Train: 0.8250, Val: 0.6854, Test: 0.7085\n",
      "Epoch: 056, Runtime 5.160694, Loss 0.891850, forward nfe 403050, backward nfe 58126, Train: 0.8250, Val: 0.7023, Test: 0.7158\n",
      "Epoch: 057, Runtime 5.135493, Loss 0.869827, forward nfe 410314, backward nfe 59178, Train: 0.8450, Val: 0.7015, Test: 0.7164\n",
      "Epoch: 058, Runtime 5.177019, Loss 0.843326, forward nfe 417576, backward nfe 60279, Train: 0.8250, Val: 0.6992, Test: 0.7114\n",
      "Epoch: 059, Runtime 5.111054, Loss 0.833528, forward nfe 424838, backward nfe 61305, Train: 0.8250, Val: 0.6938, Test: 0.7087\n",
      "Epoch: 060, Runtime 5.116990, Loss 0.818782, forward nfe 432100, backward nfe 62339, Train: 0.8400, Val: 0.6900, Test: 0.7048\n",
      "Epoch: 061, Runtime 5.117732, Loss 0.798420, forward nfe 439361, backward nfe 63369, Train: 0.8300, Val: 0.6908, Test: 0.7075\n",
      "Epoch: 062, Runtime 5.121444, Loss 0.788412, forward nfe 446624, backward nfe 64408, Train: 0.8400, Val: 0.6946, Test: 0.7144\n",
      "Epoch: 063, Runtime 5.128257, Loss 0.770499, forward nfe 453886, backward nfe 65456, Train: 0.8300, Val: 0.7100, Test: 0.7213\n",
      "Epoch: 064, Runtime 5.117761, Loss 0.759651, forward nfe 461149, backward nfe 66491, Train: 0.8500, Val: 0.7108, Test: 0.7247\n",
      "Epoch: 065, Runtime 5.119700, Loss 0.739598, forward nfe 468412, backward nfe 67527, Train: 0.8500, Val: 0.7077, Test: 0.7280\n",
      "Epoch: 066, Runtime 5.132364, Loss 0.728187, forward nfe 475674, backward nfe 68576, Train: 0.8500, Val: 0.7069, Test: 0.7294\n",
      "Epoch: 067, Runtime 5.127187, Loss 0.716702, forward nfe 482938, backward nfe 69620, Train: 0.8650, Val: 0.7108, Test: 0.7289\n",
      "Epoch: 068, Runtime 5.122108, Loss 0.705755, forward nfe 490200, backward nfe 70661, Train: 0.8400, Val: 0.7108, Test: 0.7272\n",
      "Epoch: 069, Runtime 5.126290, Loss 0.695444, forward nfe 497462, backward nfe 71702, Train: 0.8550, Val: 0.7169, Test: 0.7335\n",
      "Epoch: 070, Runtime 5.113916, Loss 0.674980, forward nfe 504724, backward nfe 72734, Train: 0.8500, Val: 0.7192, Test: 0.7341\n",
      "Epoch: 071, Runtime 5.108579, Loss 0.672871, forward nfe 511987, backward nfe 73759, Train: 0.8500, Val: 0.7177, Test: 0.7320\n",
      "Epoch: 072, Runtime 5.127093, Loss 0.653690, forward nfe 519249, backward nfe 74800, Train: 0.8650, Val: 0.7215, Test: 0.7300\n",
      "Epoch: 073, Runtime 5.108123, Loss 0.636123, forward nfe 526513, backward nfe 75825, Train: 0.8800, Val: 0.7223, Test: 0.7334\n",
      "Epoch: 074, Runtime 5.108941, Loss 0.635731, forward nfe 533776, backward nfe 76849, Train: 0.8750, Val: 0.7231, Test: 0.7391\n",
      "Epoch: 075, Runtime 5.110490, Loss 0.620312, forward nfe 541037, backward nfe 77876, Train: 0.8700, Val: 0.7231, Test: 0.7378\n",
      "Epoch: 076, Runtime 5.120367, Loss 0.619354, forward nfe 548299, backward nfe 78914, Train: 0.8700, Val: 0.7231, Test: 0.7367\n",
      "Epoch: 077, Runtime 5.109784, Loss 0.598750, forward nfe 555562, backward nfe 79939, Train: 0.8700, Val: 0.7262, Test: 0.7366\n",
      "Epoch: 078, Runtime 5.112606, Loss 0.603591, forward nfe 562825, backward nfe 80966, Train: 0.8800, Val: 0.7285, Test: 0.7413\n",
      "Epoch: 079, Runtime 5.108282, Loss 0.579383, forward nfe 570087, backward nfe 81992, Train: 0.8750, Val: 0.7277, Test: 0.7440\n",
      "Epoch: 080, Runtime 5.114766, Loss 0.577249, forward nfe 577348, backward nfe 83025, Train: 0.8800, Val: 0.7269, Test: 0.7422\n",
      "Epoch: 081, Runtime 5.117390, Loss 0.574930, forward nfe 584611, backward nfe 84061, Train: 0.8800, Val: 0.7246, Test: 0.7400\n",
      "Epoch: 082, Runtime 5.117297, Loss 0.555858, forward nfe 591873, backward nfe 85098, Train: 0.8850, Val: 0.7262, Test: 0.7391\n",
      "Epoch: 083, Runtime 5.109039, Loss 0.544227, forward nfe 599135, backward nfe 86122, Train: 0.8750, Val: 0.7300, Test: 0.7452\n",
      "Epoch: 084, Runtime 5.110536, Loss 0.545997, forward nfe 606398, backward nfe 87146, Train: 0.8750, Val: 0.7346, Test: 0.7517\n",
      "Epoch: 085, Runtime 5.106737, Loss 0.528693, forward nfe 613661, backward nfe 88170, Train: 0.8750, Val: 0.7323, Test: 0.7502\n",
      "Epoch: 086, Runtime 5.105134, Loss 0.532254, forward nfe 620923, backward nfe 89194, Train: 0.8750, Val: 0.7254, Test: 0.7428\n",
      "Epoch: 087, Runtime 5.109695, Loss 0.514414, forward nfe 628185, backward nfe 90218, Train: 0.8900, Val: 0.7238, Test: 0.7411\n",
      "Epoch: 088, Runtime 5.111259, Loss 0.521536, forward nfe 635447, backward nfe 91246, Train: 0.8900, Val: 0.7285, Test: 0.7495\n",
      "Epoch: 089, Runtime 5.116103, Loss 0.502782, forward nfe 642707, backward nfe 92277, Train: 0.8800, Val: 0.7392, Test: 0.7538\n",
      "Epoch: 090, Runtime 5.135170, Loss 0.503995, forward nfe 649970, backward nfe 93331, Train: 0.8900, Val: 0.7315, Test: 0.7509\n",
      "Epoch: 091, Runtime 5.106949, Loss 0.479499, forward nfe 657231, backward nfe 94357, Train: 0.8900, Val: 0.7223, Test: 0.7453\n",
      "Epoch: 092, Runtime 5.112762, Loss 0.499038, forward nfe 664493, backward nfe 95385, Train: 0.8900, Val: 0.7285, Test: 0.7477\n",
      "Epoch: 093, Runtime 5.112472, Loss 0.472005, forward nfe 671755, backward nfe 96411, Train: 0.8800, Val: 0.7385, Test: 0.7549\n",
      "Epoch: 094, Runtime 5.122547, Loss 0.469390, forward nfe 679016, backward nfe 97451, Train: 0.8800, Val: 0.7400, Test: 0.7564\n",
      "Epoch: 095, Runtime 5.110137, Loss 0.456796, forward nfe 686278, backward nfe 98475, Train: 0.8850, Val: 0.7369, Test: 0.7554\n",
      "Epoch: 096, Runtime 5.106632, Loss 0.457347, forward nfe 693540, backward nfe 99499, Train: 0.8950, Val: 0.7385, Test: 0.7538\n",
      "Epoch: 097, Runtime 5.107086, Loss 0.459886, forward nfe 700802, backward nfe 100526, Train: 0.8900, Val: 0.7400, Test: 0.7566\n",
      "Epoch: 098, Runtime 5.111206, Loss 0.446768, forward nfe 708063, backward nfe 101555, Train: 0.8850, Val: 0.7423, Test: 0.7605\n",
      "Epoch: 099, Runtime 5.111385, Loss 0.436230, forward nfe 715325, backward nfe 102584, Train: 0.8800, Val: 0.7438, Test: 0.7598\n",
      "best val accuracy 0.743846 with test accuracy 0.759794 at epoch 99\n",
      "*** Doing run 7 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.228160, Loss 2.314286, forward nfe 3626, backward nfe 67, Train: 0.1050, Val: 0.1854, Test: 0.1761\n",
      "Epoch: 002, Runtime 5.108690, Loss 2.468031, forward nfe 10884, backward nfe 1091, Train: 0.1200, Val: 0.1469, Test: 0.1477\n",
      "Epoch: 003, Runtime 5.107733, Loss 2.291057, forward nfe 18143, backward nfe 2115, Train: 0.1000, Val: 0.4023, Test: 0.3754\n",
      "Epoch: 004, Runtime 5.105973, Loss 2.306795, forward nfe 25403, backward nfe 3139, Train: 0.1300, Val: 0.1900, Test: 0.1861\n",
      "Epoch: 005, Runtime 5.111623, Loss 2.267894, forward nfe 32665, backward nfe 4163, Train: 0.1100, Val: 0.1208, Test: 0.1221\n",
      "Epoch: 006, Runtime 5.104142, Loss 2.281637, forward nfe 39927, backward nfe 5187, Train: 0.1200, Val: 0.1377, Test: 0.1396\n",
      "Epoch: 007, Runtime 5.111496, Loss 2.264419, forward nfe 47188, backward nfe 6212, Train: 0.1900, Val: 0.2223, Test: 0.2293\n",
      "Epoch: 008, Runtime 5.114663, Loss 2.240454, forward nfe 54450, backward nfe 7239, Train: 0.2650, Val: 0.4600, Test: 0.4371\n",
      "Epoch: 009, Runtime 5.107349, Loss 2.229518, forward nfe 61712, backward nfe 8264, Train: 0.1700, Val: 0.2131, Test: 0.1899\n",
      "Epoch: 010, Runtime 5.120678, Loss 2.231369, forward nfe 68974, backward nfe 9300, Train: 0.1750, Val: 0.2169, Test: 0.1986\n",
      "Epoch: 011, Runtime 5.128477, Loss 2.205950, forward nfe 76238, backward nfe 10344, Train: 0.2750, Val: 0.2469, Test: 0.2349\n",
      "Epoch: 012, Runtime 5.124704, Loss 2.180341, forward nfe 83500, backward nfe 11386, Train: 0.2600, Val: 0.2062, Test: 0.2142\n",
      "Epoch: 013, Runtime 5.124543, Loss 2.171150, forward nfe 90762, backward nfe 12425, Train: 0.1700, Val: 0.1200, Test: 0.1209\n",
      "Epoch: 014, Runtime 5.184078, Loss 2.164271, forward nfe 98024, backward nfe 13531, Train: 0.1700, Val: 0.1346, Test: 0.1330\n",
      "Epoch: 015, Runtime 5.116789, Loss 2.144495, forward nfe 105286, backward nfe 14566, Train: 0.2000, Val: 0.1931, Test: 0.1774\n",
      "Epoch: 016, Runtime 5.179808, Loss 2.115897, forward nfe 112549, backward nfe 15667, Train: 0.3050, Val: 0.2438, Test: 0.2330\n",
      "Epoch: 017, Runtime 5.199128, Loss 2.091186, forward nfe 119812, backward nfe 16786, Train: 0.3300, Val: 0.2438, Test: 0.2420\n",
      "Epoch: 018, Runtime 5.197231, Loss 2.076825, forward nfe 127075, backward nfe 17905, Train: 0.3500, Val: 0.2900, Test: 0.2783\n",
      "Epoch: 019, Runtime 5.197219, Loss 2.051586, forward nfe 134337, backward nfe 19024, Train: 0.2800, Val: 0.2654, Test: 0.2541\n",
      "Epoch: 020, Runtime 5.193655, Loss 2.032507, forward nfe 141599, backward nfe 20143, Train: 0.3500, Val: 0.3515, Test: 0.3239\n",
      "Epoch: 021, Runtime 5.194021, Loss 1.998732, forward nfe 148862, backward nfe 21262, Train: 0.4200, Val: 0.4123, Test: 0.4048\n",
      "Epoch: 022, Runtime 5.193454, Loss 1.978244, forward nfe 156125, backward nfe 22381, Train: 0.4300, Val: 0.4092, Test: 0.4154\n",
      "Epoch: 023, Runtime 5.153821, Loss 1.950517, forward nfe 163389, backward nfe 23447, Train: 0.4050, Val: 0.3900, Test: 0.3941\n",
      "Epoch: 024, Runtime 5.195628, Loss 1.928648, forward nfe 170652, backward nfe 24566, Train: 0.3900, Val: 0.4062, Test: 0.3999\n",
      "Epoch: 025, Runtime 5.197233, Loss 1.894623, forward nfe 177915, backward nfe 25685, Train: 0.4300, Val: 0.4715, Test: 0.4643\n",
      "Epoch: 026, Runtime 5.116218, Loss 1.862817, forward nfe 185178, backward nfe 26715, Train: 0.4900, Val: 0.5492, Test: 0.5481\n",
      "Epoch: 027, Runtime 5.192913, Loss 1.837242, forward nfe 192441, backward nfe 27830, Train: 0.4500, Val: 0.5508, Test: 0.5479\n",
      "Epoch: 028, Runtime 5.109338, Loss 1.806710, forward nfe 199703, backward nfe 28857, Train: 0.4550, Val: 0.5562, Test: 0.5460\n",
      "Epoch: 029, Runtime 5.199763, Loss 1.773346, forward nfe 206966, backward nfe 29976, Train: 0.4800, Val: 0.5846, Test: 0.5817\n",
      "Epoch: 030, Runtime 5.195754, Loss 1.744971, forward nfe 214228, backward nfe 31095, Train: 0.4650, Val: 0.5631, Test: 0.5698\n",
      "Epoch: 031, Runtime 5.131933, Loss 1.711761, forward nfe 221492, backward nfe 32144, Train: 0.4650, Val: 0.5715, Test: 0.5693\n",
      "Epoch: 032, Runtime 5.167404, Loss 1.681893, forward nfe 228755, backward nfe 33231, Train: 0.4750, Val: 0.5823, Test: 0.5823\n",
      "Epoch: 033, Runtime 5.197092, Loss 1.651103, forward nfe 236018, backward nfe 34350, Train: 0.5100, Val: 0.6046, Test: 0.5979\n",
      "Epoch: 034, Runtime 5.159943, Loss 1.617236, forward nfe 243282, backward nfe 35432, Train: 0.5650, Val: 0.6038, Test: 0.6146\n",
      "Epoch: 035, Runtime 5.138400, Loss 1.584931, forward nfe 250545, backward nfe 36486, Train: 0.5700, Val: 0.6169, Test: 0.6266\n",
      "Epoch: 036, Runtime 5.195620, Loss 1.557497, forward nfe 257808, backward nfe 37605, Train: 0.5700, Val: 0.6300, Test: 0.6373\n",
      "Epoch: 037, Runtime 5.199661, Loss 1.529090, forward nfe 265072, backward nfe 38724, Train: 0.5650, Val: 0.6300, Test: 0.6385\n",
      "Epoch: 038, Runtime 5.196843, Loss 1.504533, forward nfe 272334, backward nfe 39843, Train: 0.5800, Val: 0.6069, Test: 0.6186\n",
      "Epoch: 039, Runtime 5.196101, Loss 1.469339, forward nfe 279597, backward nfe 40962, Train: 0.5800, Val: 0.5923, Test: 0.6020\n",
      "Epoch: 040, Runtime 5.160057, Loss 1.438111, forward nfe 286860, backward nfe 42039, Train: 0.5600, Val: 0.5838, Test: 0.5917\n",
      "Epoch: 041, Runtime 5.143999, Loss 1.423629, forward nfe 294123, backward nfe 43102, Train: 0.5500, Val: 0.5854, Test: 0.5905\n",
      "Epoch: 042, Runtime 5.199995, Loss 1.397343, forward nfe 301385, backward nfe 44221, Train: 0.5650, Val: 0.5900, Test: 0.5948\n",
      "Epoch: 043, Runtime 5.117860, Loss 1.368690, forward nfe 308647, backward nfe 45259, Train: 0.6000, Val: 0.5962, Test: 0.6057\n",
      "Epoch: 044, Runtime 5.199723, Loss 1.334693, forward nfe 315909, backward nfe 46378, Train: 0.5750, Val: 0.6077, Test: 0.6169\n",
      "Epoch: 045, Runtime 5.147075, Loss 1.307609, forward nfe 323171, backward nfe 47447, Train: 0.5800, Val: 0.6138, Test: 0.6281\n",
      "Epoch: 046, Runtime 5.178806, Loss 1.297364, forward nfe 330434, backward nfe 48546, Train: 0.6300, Val: 0.5885, Test: 0.6098\n",
      "Epoch: 047, Runtime 5.119708, Loss 1.267566, forward nfe 337696, backward nfe 49583, Train: 0.5950, Val: 0.6477, Test: 0.6634\n",
      "Epoch: 048, Runtime 5.140595, Loss 1.243412, forward nfe 344957, backward nfe 50641, Train: 0.6750, Val: 0.6231, Test: 0.6438\n",
      "Epoch: 049, Runtime 5.123702, Loss 1.220101, forward nfe 352219, backward nfe 51680, Train: 0.7250, Val: 0.6069, Test: 0.6344\n",
      "Epoch: 050, Runtime 5.136518, Loss 1.189630, forward nfe 359483, backward nfe 52730, Train: 0.6750, Val: 0.6615, Test: 0.6801\n",
      "Epoch: 051, Runtime 5.114301, Loss 1.171275, forward nfe 366745, backward nfe 53759, Train: 0.7350, Val: 0.6569, Test: 0.6881\n",
      "Epoch: 052, Runtime 5.119985, Loss 1.144848, forward nfe 374007, backward nfe 54793, Train: 0.7200, Val: 0.6292, Test: 0.6548\n",
      "Epoch: 053, Runtime 5.130207, Loss 1.116701, forward nfe 381270, backward nfe 55838, Train: 0.7350, Val: 0.6638, Test: 0.6931\n",
      "Epoch: 054, Runtime 5.107823, Loss 1.100446, forward nfe 388533, backward nfe 56862, Train: 0.7450, Val: 0.6715, Test: 0.6989\n",
      "Epoch: 055, Runtime 5.111995, Loss 1.065834, forward nfe 395796, backward nfe 57888, Train: 0.7400, Val: 0.6562, Test: 0.6832\n",
      "Epoch: 056, Runtime 5.118240, Loss 1.046393, forward nfe 403058, backward nfe 58923, Train: 0.7450, Val: 0.6715, Test: 0.7036\n",
      "Epoch: 057, Runtime 5.119899, Loss 1.020740, forward nfe 410319, backward nfe 59956, Train: 0.7550, Val: 0.6800, Test: 0.7126\n",
      "Epoch: 058, Runtime 5.113491, Loss 0.992437, forward nfe 417582, backward nfe 60985, Train: 0.7600, Val: 0.6769, Test: 0.7049\n",
      "Epoch: 059, Runtime 5.136853, Loss 0.974548, forward nfe 424843, backward nfe 62036, Train: 0.7700, Val: 0.6708, Test: 0.6965\n",
      "Epoch: 060, Runtime 5.113443, Loss 0.950269, forward nfe 432105, backward nfe 63066, Train: 0.7650, Val: 0.6838, Test: 0.7138\n",
      "Epoch: 061, Runtime 5.114390, Loss 0.921617, forward nfe 439367, backward nfe 64097, Train: 0.7650, Val: 0.6892, Test: 0.7209\n",
      "Epoch: 062, Runtime 5.109850, Loss 0.895762, forward nfe 446628, backward nfe 65123, Train: 0.7550, Val: 0.6869, Test: 0.7136\n",
      "Epoch: 063, Runtime 5.112643, Loss 0.883054, forward nfe 453890, backward nfe 66150, Train: 0.7600, Val: 0.6846, Test: 0.7160\n",
      "Epoch: 064, Runtime 5.110015, Loss 0.870372, forward nfe 461152, backward nfe 67177, Train: 0.7800, Val: 0.6946, Test: 0.7218\n",
      "Epoch: 065, Runtime 5.109684, Loss 0.848315, forward nfe 468413, backward nfe 68201, Train: 0.7850, Val: 0.6877, Test: 0.7208\n",
      "Epoch: 066, Runtime 5.126448, Loss 0.831314, forward nfe 475674, backward nfe 69243, Train: 0.7800, Val: 0.6885, Test: 0.7167\n",
      "Epoch: 067, Runtime 5.113205, Loss 0.794681, forward nfe 482935, backward nfe 70273, Train: 0.7850, Val: 0.6908, Test: 0.7196\n",
      "Epoch: 068, Runtime 5.113959, Loss 0.788143, forward nfe 490195, backward nfe 71303, Train: 0.7900, Val: 0.6969, Test: 0.7196\n",
      "Epoch: 069, Runtime 5.110344, Loss 0.784418, forward nfe 497457, backward nfe 72330, Train: 0.7900, Val: 0.6923, Test: 0.7179\n",
      "Epoch: 070, Runtime 5.112052, Loss 0.748051, forward nfe 504719, backward nfe 73361, Train: 0.7850, Val: 0.6962, Test: 0.7204\n",
      "Epoch: 071, Runtime 5.115758, Loss 0.741232, forward nfe 511980, backward nfe 74390, Train: 0.7950, Val: 0.7054, Test: 0.7300\n",
      "Epoch: 072, Runtime 5.111655, Loss 0.727907, forward nfe 519241, backward nfe 75417, Train: 0.8050, Val: 0.7254, Test: 0.7383\n",
      "Epoch: 073, Runtime 5.109394, Loss 0.702887, forward nfe 526503, backward nfe 76441, Train: 0.8100, Val: 0.7192, Test: 0.7351\n",
      "Epoch: 074, Runtime 5.109938, Loss 0.685494, forward nfe 533763, backward nfe 77465, Train: 0.8250, Val: 0.7077, Test: 0.7293\n",
      "Epoch: 075, Runtime 5.109441, Loss 0.677637, forward nfe 541024, backward nfe 78489, Train: 0.8300, Val: 0.7085, Test: 0.7315\n",
      "Epoch: 076, Runtime 5.106485, Loss 0.668451, forward nfe 548285, backward nfe 79513, Train: 0.8200, Val: 0.7254, Test: 0.7384\n",
      "Epoch: 077, Runtime 5.114539, Loss 0.651315, forward nfe 555546, backward nfe 80537, Train: 0.8200, Val: 0.7246, Test: 0.7433\n",
      "Epoch: 078, Runtime 5.105414, Loss 0.628651, forward nfe 562807, backward nfe 81561, Train: 0.8350, Val: 0.7223, Test: 0.7407\n",
      "Epoch: 079, Runtime 5.110333, Loss 0.630862, forward nfe 570068, backward nfe 82586, Train: 0.8450, Val: 0.7338, Test: 0.7475\n",
      "Epoch: 080, Runtime 5.109987, Loss 0.616244, forward nfe 577330, backward nfe 83610, Train: 0.8350, Val: 0.7385, Test: 0.7494\n",
      "Epoch: 081, Runtime 5.110288, Loss 0.589622, forward nfe 584592, backward nfe 84634, Train: 0.8400, Val: 0.7354, Test: 0.7457\n",
      "Epoch: 082, Runtime 5.110762, Loss 0.595339, forward nfe 591853, backward nfe 85658, Train: 0.8600, Val: 0.7254, Test: 0.7431\n",
      "Epoch: 083, Runtime 5.112431, Loss 0.573395, forward nfe 599115, backward nfe 86682, Train: 0.8500, Val: 0.7362, Test: 0.7536\n",
      "Epoch: 084, Runtime 5.106466, Loss 0.562328, forward nfe 606376, backward nfe 87706, Train: 0.8550, Val: 0.7469, Test: 0.7600\n",
      "Epoch: 085, Runtime 5.111216, Loss 0.561667, forward nfe 613638, backward nfe 88731, Train: 0.8650, Val: 0.7538, Test: 0.7620\n",
      "Epoch: 086, Runtime 5.107343, Loss 0.561757, forward nfe 620899, backward nfe 89754, Train: 0.8700, Val: 0.7454, Test: 0.7567\n",
      "Epoch: 087, Runtime 5.108469, Loss 0.554652, forward nfe 628160, backward nfe 90778, Train: 0.8650, Val: 0.7492, Test: 0.7588\n",
      "Epoch: 088, Runtime 5.114602, Loss 0.531447, forward nfe 635422, backward nfe 91802, Train: 0.8600, Val: 0.7515, Test: 0.7608\n",
      "Epoch: 089, Runtime 5.110483, Loss 0.545121, forward nfe 642684, backward nfe 92826, Train: 0.8600, Val: 0.7354, Test: 0.7587\n",
      "Epoch: 090, Runtime 5.111308, Loss 0.519262, forward nfe 649944, backward nfe 93850, Train: 0.8700, Val: 0.7508, Test: 0.7657\n",
      "Epoch: 091, Runtime 5.111185, Loss 0.505536, forward nfe 657204, backward nfe 94875, Train: 0.8600, Val: 0.7692, Test: 0.7738\n",
      "Epoch: 092, Runtime 5.109657, Loss 0.495652, forward nfe 664465, backward nfe 95900, Train: 0.8600, Val: 0.7685, Test: 0.7721\n",
      "Epoch: 093, Runtime 5.108464, Loss 0.487540, forward nfe 671725, backward nfe 96924, Train: 0.8600, Val: 0.7523, Test: 0.7663\n",
      "Epoch: 094, Runtime 5.109653, Loss 0.471018, forward nfe 678985, backward nfe 97948, Train: 0.8700, Val: 0.7400, Test: 0.7626\n",
      "Epoch: 095, Runtime 5.106789, Loss 0.463711, forward nfe 686245, backward nfe 98972, Train: 0.8750, Val: 0.7508, Test: 0.7698\n",
      "Epoch: 096, Runtime 5.110768, Loss 0.463811, forward nfe 693505, backward nfe 99995, Train: 0.8650, Val: 0.7677, Test: 0.7756\n",
      "Epoch: 097, Runtime 5.105190, Loss 0.454924, forward nfe 700765, backward nfe 101019, Train: 0.8600, Val: 0.7615, Test: 0.7754\n",
      "Epoch: 098, Runtime 5.110945, Loss 0.465395, forward nfe 708025, backward nfe 102043, Train: 0.8750, Val: 0.7669, Test: 0.7759\n",
      "Epoch: 099, Runtime 5.109871, Loss 0.449121, forward nfe 715286, backward nfe 103070, Train: 0.8800, Val: 0.7592, Test: 0.7755\n",
      "best val accuracy 0.769231 with test accuracy 0.773751 at epoch 91\n",
      "*** Doing run 8 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.235740, Loss 2.323266, forward nfe 3627, backward nfe 70, Train: 0.0950, Val: 0.0269, Test: 0.0375\n",
      "Epoch: 002, Runtime 5.104985, Loss 2.524817, forward nfe 10886, backward nfe 1094, Train: 0.1750, Val: 0.1008, Test: 0.1152\n",
      "Epoch: 003, Runtime 5.108839, Loss 2.898710, forward nfe 18145, backward nfe 2118, Train: 0.1150, Val: 0.1569, Test: 0.1596\n",
      "Epoch: 004, Runtime 5.103689, Loss 2.287510, forward nfe 25405, backward nfe 3142, Train: 0.1000, Val: 0.0223, Test: 0.0350\n",
      "Epoch: 005, Runtime 5.106408, Loss 2.363780, forward nfe 32666, backward nfe 4166, Train: 0.0800, Val: 0.0869, Test: 0.0886\n",
      "Epoch: 006, Runtime 5.109613, Loss 2.356420, forward nfe 39927, backward nfe 5195, Train: 0.1200, Val: 0.1108, Test: 0.1133\n",
      "Epoch: 007, Runtime 5.117871, Loss 2.332740, forward nfe 47189, backward nfe 6232, Train: 0.1250, Val: 0.0954, Test: 0.1000\n",
      "Epoch: 008, Runtime 5.110165, Loss 2.306191, forward nfe 54451, backward nfe 7259, Train: 0.1700, Val: 0.0646, Test: 0.0699\n",
      "Epoch: 009, Runtime 5.153600, Loss 2.268288, forward nfe 61713, backward nfe 8333, Train: 0.1950, Val: 0.0400, Test: 0.0595\n",
      "Epoch: 010, Runtime 5.114618, Loss 2.239949, forward nfe 68976, backward nfe 9365, Train: 0.1950, Val: 0.0377, Test: 0.0555\n",
      "Epoch: 011, Runtime 5.122286, Loss 2.254646, forward nfe 76238, backward nfe 10402, Train: 0.1900, Val: 0.0369, Test: 0.0548\n",
      "Epoch: 012, Runtime 5.197904, Loss 2.273702, forward nfe 83500, backward nfe 11521, Train: 0.1950, Val: 0.0377, Test: 0.0556\n",
      "Epoch: 013, Runtime 5.160881, Loss 2.254958, forward nfe 90763, backward nfe 12597, Train: 0.2450, Val: 0.0508, Test: 0.0714\n",
      "Epoch: 014, Runtime 5.196094, Loss 2.189137, forward nfe 98026, backward nfe 13716, Train: 0.1500, Val: 0.0262, Test: 0.0366\n",
      "Epoch: 015, Runtime 5.195719, Loss 2.242511, forward nfe 105289, backward nfe 14835, Train: 0.1900, Val: 0.0354, Test: 0.0511\n",
      "Epoch: 016, Runtime 5.173141, Loss 2.182957, forward nfe 112552, backward nfe 15929, Train: 0.2200, Val: 0.0631, Test: 0.0848\n",
      "Epoch: 017, Runtime 5.158529, Loss 2.156675, forward nfe 119815, backward nfe 17005, Train: 0.2800, Val: 0.0785, Test: 0.0983\n",
      "Epoch: 018, Runtime 5.197919, Loss 2.156357, forward nfe 127079, backward nfe 18124, Train: 0.2750, Val: 0.0738, Test: 0.0932\n",
      "Epoch: 019, Runtime 5.114728, Loss 2.153220, forward nfe 134342, backward nfe 19159, Train: 0.2750, Val: 0.0769, Test: 0.0958\n",
      "Epoch: 020, Runtime 5.154649, Loss 2.133385, forward nfe 141606, backward nfe 20235, Train: 0.2850, Val: 0.0838, Test: 0.1063\n",
      "Epoch: 021, Runtime 5.196322, Loss 2.102234, forward nfe 148870, backward nfe 21354, Train: 0.2900, Val: 0.0862, Test: 0.1081\n",
      "Epoch: 022, Runtime 5.198077, Loss 2.091763, forward nfe 156133, backward nfe 22473, Train: 0.2800, Val: 0.0877, Test: 0.1099\n",
      "Epoch: 023, Runtime 5.194660, Loss 2.062821, forward nfe 163397, backward nfe 23592, Train: 0.3800, Val: 0.1731, Test: 0.1968\n",
      "Epoch: 024, Runtime 5.200626, Loss 2.032609, forward nfe 170660, backward nfe 24711, Train: 0.3900, Val: 0.1615, Test: 0.1832\n",
      "Epoch: 025, Runtime 5.168552, Loss 2.026608, forward nfe 177923, backward nfe 25830, Train: 0.3850, Val: 0.1600, Test: 0.1867\n",
      "Epoch: 026, Runtime 5.170186, Loss 2.005729, forward nfe 185186, backward nfe 26949, Train: 0.3750, Val: 0.1831, Test: 0.2058\n",
      "Epoch: 027, Runtime 5.168573, Loss 1.974932, forward nfe 192450, backward nfe 28068, Train: 0.3550, Val: 0.1677, Test: 0.1860\n",
      "Epoch: 028, Runtime 5.167028, Loss 1.966032, forward nfe 199713, backward nfe 29187, Train: 0.3600, Val: 0.1885, Test: 0.2085\n",
      "Epoch: 029, Runtime 5.171170, Loss 1.948092, forward nfe 206976, backward nfe 30306, Train: 0.3800, Val: 0.2315, Test: 0.2593\n",
      "Epoch: 030, Runtime 5.169821, Loss 1.921716, forward nfe 214239, backward nfe 31425, Train: 0.4050, Val: 0.2577, Test: 0.2848\n",
      "Epoch: 031, Runtime 5.171381, Loss 1.897617, forward nfe 221503, backward nfe 32544, Train: 0.3950, Val: 0.2323, Test: 0.2582\n",
      "Epoch: 032, Runtime 5.167550, Loss 1.863827, forward nfe 228765, backward nfe 33663, Train: 0.3750, Val: 0.1946, Test: 0.2169\n",
      "Epoch: 033, Runtime 5.173530, Loss 1.858894, forward nfe 236028, backward nfe 34782, Train: 0.4000, Val: 0.2262, Test: 0.2449\n",
      "Epoch: 034, Runtime 5.171650, Loss 1.802088, forward nfe 243292, backward nfe 35901, Train: 0.4000, Val: 0.2377, Test: 0.2640\n",
      "Epoch: 035, Runtime 5.171351, Loss 1.800060, forward nfe 250555, backward nfe 37020, Train: 0.4050, Val: 0.2485, Test: 0.2755\n",
      "Epoch: 036, Runtime 5.171951, Loss 1.776198, forward nfe 257819, backward nfe 38139, Train: 0.4150, Val: 0.2592, Test: 0.2866\n",
      "Epoch: 037, Runtime 5.169037, Loss 1.732698, forward nfe 265082, backward nfe 39258, Train: 0.3950, Val: 0.2692, Test: 0.2918\n",
      "Epoch: 038, Runtime 5.118854, Loss 1.723044, forward nfe 272344, backward nfe 40321, Train: 0.3950, Val: 0.2969, Test: 0.3173\n",
      "Epoch: 039, Runtime 5.168855, Loss 1.699456, forward nfe 279607, backward nfe 41440, Train: 0.4450, Val: 0.3308, Test: 0.3605\n",
      "Epoch: 040, Runtime 5.169274, Loss 1.663067, forward nfe 286869, backward nfe 42559, Train: 0.4650, Val: 0.3500, Test: 0.3749\n",
      "Epoch: 041, Runtime 5.171025, Loss 1.655768, forward nfe 294132, backward nfe 43678, Train: 0.4600, Val: 0.3577, Test: 0.3849\n",
      "Epoch: 042, Runtime 5.110988, Loss 1.625596, forward nfe 301395, backward nfe 44734, Train: 0.4750, Val: 0.3723, Test: 0.3951\n",
      "Epoch: 043, Runtime 5.143476, Loss 1.600786, forward nfe 308658, backward nfe 45826, Train: 0.4450, Val: 0.3954, Test: 0.4172\n",
      "Epoch: 044, Runtime 5.102053, Loss 1.584381, forward nfe 315920, backward nfe 46873, Train: 0.4700, Val: 0.4215, Test: 0.4393\n",
      "Epoch: 045, Runtime 5.173924, Loss 1.555509, forward nfe 323183, backward nfe 47992, Train: 0.4850, Val: 0.4369, Test: 0.4517\n",
      "Epoch: 046, Runtime 5.151420, Loss 1.527916, forward nfe 330445, backward nfe 49092, Train: 0.5200, Val: 0.4323, Test: 0.4518\n",
      "Epoch: 047, Runtime 5.085258, Loss 1.515574, forward nfe 337707, backward nfe 50117, Train: 0.5350, Val: 0.4477, Test: 0.4602\n",
      "Epoch: 048, Runtime 5.144346, Loss 1.487492, forward nfe 344969, backward nfe 51204, Train: 0.5100, Val: 0.4554, Test: 0.4683\n",
      "Epoch: 049, Runtime 5.108143, Loss 1.465639, forward nfe 352233, backward nfe 52253, Train: 0.5150, Val: 0.4615, Test: 0.4825\n",
      "Epoch: 050, Runtime 5.111824, Loss 1.448005, forward nfe 359495, backward nfe 53309, Train: 0.5250, Val: 0.4731, Test: 0.4904\n",
      "Epoch: 051, Runtime 5.089752, Loss 1.421259, forward nfe 366759, backward nfe 54338, Train: 0.5750, Val: 0.4762, Test: 0.4934\n",
      "Epoch: 052, Runtime 5.087471, Loss 1.401336, forward nfe 374023, backward nfe 55367, Train: 0.5900, Val: 0.4923, Test: 0.5031\n",
      "Epoch: 053, Runtime 5.082584, Loss 1.394015, forward nfe 381286, backward nfe 56391, Train: 0.6000, Val: 0.5115, Test: 0.5249\n",
      "Epoch: 054, Runtime 5.096244, Loss 1.363896, forward nfe 388548, backward nfe 57431, Train: 0.5950, Val: 0.5292, Test: 0.5382\n",
      "Epoch: 055, Runtime 5.090848, Loss 1.344363, forward nfe 395810, backward nfe 58465, Train: 0.6000, Val: 0.5292, Test: 0.5477\n",
      "Epoch: 056, Runtime 5.109665, Loss 1.323611, forward nfe 403072, backward nfe 59518, Train: 0.6300, Val: 0.5392, Test: 0.5583\n",
      "Epoch: 057, Runtime 5.090483, Loss 1.288418, forward nfe 410334, backward nfe 60550, Train: 0.6350, Val: 0.5500, Test: 0.5624\n",
      "Epoch: 058, Runtime 5.090323, Loss 1.265740, forward nfe 417597, backward nfe 61582, Train: 0.6500, Val: 0.5577, Test: 0.5736\n",
      "Epoch: 059, Runtime 5.086450, Loss 1.255503, forward nfe 424860, backward nfe 62608, Train: 0.6450, Val: 0.5746, Test: 0.5866\n",
      "Epoch: 060, Runtime 5.092728, Loss 1.236470, forward nfe 432123, backward nfe 63640, Train: 0.6550, Val: 0.5877, Test: 0.6013\n",
      "Epoch: 061, Runtime 5.090264, Loss 1.212703, forward nfe 439385, backward nfe 64671, Train: 0.6500, Val: 0.5962, Test: 0.6095\n",
      "Epoch: 062, Runtime 5.093550, Loss 1.189545, forward nfe 446647, backward nfe 65702, Train: 0.6650, Val: 0.5962, Test: 0.6130\n",
      "Epoch: 063, Runtime 5.093507, Loss 1.168802, forward nfe 453910, backward nfe 66737, Train: 0.6950, Val: 0.5962, Test: 0.6201\n",
      "Epoch: 064, Runtime 5.095193, Loss 1.154086, forward nfe 461172, backward nfe 67773, Train: 0.7100, Val: 0.6031, Test: 0.6261\n",
      "Epoch: 065, Runtime 5.084909, Loss 1.138712, forward nfe 468433, backward nfe 68800, Train: 0.7000, Val: 0.6108, Test: 0.6336\n",
      "Epoch: 066, Runtime 5.079478, Loss 1.110116, forward nfe 475695, backward nfe 69824, Train: 0.6950, Val: 0.6200, Test: 0.6410\n",
      "Epoch: 067, Runtime 5.081721, Loss 1.091463, forward nfe 482957, backward nfe 70849, Train: 0.6950, Val: 0.6185, Test: 0.6400\n",
      "Epoch: 068, Runtime 5.089954, Loss 1.071164, forward nfe 490220, backward nfe 71877, Train: 0.7050, Val: 0.6123, Test: 0.6385\n",
      "Epoch: 069, Runtime 5.084566, Loss 1.047062, forward nfe 497481, backward nfe 72902, Train: 0.7100, Val: 0.6162, Test: 0.6410\n",
      "Epoch: 070, Runtime 5.088441, Loss 1.034855, forward nfe 504744, backward nfe 73930, Train: 0.7300, Val: 0.6200, Test: 0.6450\n",
      "Epoch: 071, Runtime 5.085147, Loss 0.999795, forward nfe 512005, backward nfe 74960, Train: 0.7150, Val: 0.6262, Test: 0.6534\n",
      "Epoch: 072, Runtime 5.085487, Loss 0.989072, forward nfe 519267, backward nfe 75987, Train: 0.7150, Val: 0.6323, Test: 0.6600\n",
      "Epoch: 073, Runtime 5.083054, Loss 0.971957, forward nfe 526528, backward nfe 77012, Train: 0.7250, Val: 0.6308, Test: 0.6653\n",
      "Epoch: 074, Runtime 5.084462, Loss 0.934573, forward nfe 533789, backward nfe 78037, Train: 0.7550, Val: 0.6331, Test: 0.6672\n",
      "Epoch: 075, Runtime 5.085296, Loss 0.936498, forward nfe 541051, backward nfe 79062, Train: 0.7500, Val: 0.6285, Test: 0.6704\n",
      "Epoch: 076, Runtime 5.079430, Loss 0.918625, forward nfe 548313, backward nfe 80086, Train: 0.7600, Val: 0.6400, Test: 0.6738\n",
      "Epoch: 077, Runtime 5.082206, Loss 0.895317, forward nfe 555575, backward nfe 81110, Train: 0.7550, Val: 0.6431, Test: 0.6803\n",
      "Epoch: 078, Runtime 5.086722, Loss 0.885196, forward nfe 562837, backward nfe 82136, Train: 0.7650, Val: 0.6454, Test: 0.6836\n",
      "Epoch: 079, Runtime 5.083285, Loss 0.862475, forward nfe 570099, backward nfe 83162, Train: 0.7650, Val: 0.6523, Test: 0.6832\n",
      "Epoch: 080, Runtime 5.081325, Loss 0.849598, forward nfe 577359, backward nfe 84187, Train: 0.7800, Val: 0.6446, Test: 0.6822\n",
      "Epoch: 081, Runtime 5.085903, Loss 0.847624, forward nfe 584620, backward nfe 85213, Train: 0.7850, Val: 0.6377, Test: 0.6814\n",
      "Epoch: 082, Runtime 5.085375, Loss 0.809258, forward nfe 591881, backward nfe 86241, Train: 0.7950, Val: 0.6408, Test: 0.6832\n",
      "Epoch: 083, Runtime 5.083810, Loss 0.799087, forward nfe 599142, backward nfe 87265, Train: 0.8000, Val: 0.6523, Test: 0.6897\n",
      "Epoch: 084, Runtime 5.085940, Loss 0.773568, forward nfe 606402, backward nfe 88290, Train: 0.7900, Val: 0.6615, Test: 0.7004\n",
      "Epoch: 085, Runtime 5.081383, Loss 0.764899, forward nfe 613663, backward nfe 89315, Train: 0.7850, Val: 0.6638, Test: 0.7032\n",
      "Epoch: 086, Runtime 5.080948, Loss 0.750555, forward nfe 620924, backward nfe 90339, Train: 0.7950, Val: 0.6654, Test: 0.7040\n",
      "Epoch: 087, Runtime 5.084405, Loss 0.726034, forward nfe 628186, backward nfe 91366, Train: 0.8150, Val: 0.6623, Test: 0.7014\n",
      "Epoch: 088, Runtime 5.081199, Loss 0.716351, forward nfe 635446, backward nfe 92389, Train: 0.8300, Val: 0.6662, Test: 0.7016\n",
      "Epoch: 089, Runtime 5.080971, Loss 0.704910, forward nfe 642706, backward nfe 93412, Train: 0.8300, Val: 0.6677, Test: 0.7042\n",
      "Epoch: 090, Runtime 5.080887, Loss 0.699559, forward nfe 649967, backward nfe 94436, Train: 0.8250, Val: 0.6685, Test: 0.7091\n",
      "Epoch: 091, Runtime 5.080214, Loss 0.668243, forward nfe 657228, backward nfe 95459, Train: 0.8350, Val: 0.6769, Test: 0.7147\n",
      "Epoch: 092, Runtime 5.081196, Loss 0.682479, forward nfe 664488, backward nfe 96482, Train: 0.8350, Val: 0.6746, Test: 0.7138\n",
      "Epoch: 093, Runtime 5.081067, Loss 0.639194, forward nfe 671750, backward nfe 97505, Train: 0.8300, Val: 0.6708, Test: 0.7122\n",
      "Epoch: 094, Runtime 5.080779, Loss 0.645048, forward nfe 679012, backward nfe 98529, Train: 0.8350, Val: 0.6708, Test: 0.7093\n",
      "Epoch: 095, Runtime 5.083244, Loss 0.623170, forward nfe 686273, backward nfe 99553, Train: 0.8500, Val: 0.6762, Test: 0.7109\n",
      "Epoch: 096, Runtime 5.084239, Loss 0.626872, forward nfe 693534, backward nfe 100578, Train: 0.8450, Val: 0.6838, Test: 0.7178\n",
      "Epoch: 097, Runtime 5.076719, Loss 0.608209, forward nfe 700795, backward nfe 101601, Train: 0.8450, Val: 0.6931, Test: 0.7276\n",
      "Epoch: 098, Runtime 5.079113, Loss 0.588510, forward nfe 708056, backward nfe 102624, Train: 0.8500, Val: 0.7038, Test: 0.7329\n",
      "Epoch: 099, Runtime 5.079458, Loss 0.583765, forward nfe 715316, backward nfe 103647, Train: 0.8500, Val: 0.6992, Test: 0.7302\n",
      "best val accuracy 0.703846 with test accuracy 0.732860 at epoch 98\n",
      "*** Doing run 9 ***\n",
      "{'use_cora_defaults': False, 'dataset': 'Computers', 'data_norm': 'rw', 'hidden_dim': 16, 'input_dropout': 0.5, 'dropout': 0, 'optimizer': 'adam', 'lr': 0.01, 'decay': 0.0005, 'self_loop_weight': 0.555, 'epoch': 100, 'alpha': 0.918, 'time': 12.1, 'augment': False, 'alpha_dim': 'sc', 'no_alpha_sigmoid': False, 'beta_dim': 'sc', 'block': 'constant', 'function': 'laplacian', 'geom_gcn_splits': False, 'method': 'implicit_adams', 'step_size': 0.01, 'adjoint_method': 'implicit_adams', 'adjoint_step_size': 1, 'adjoint': True, 'tol_scale': 100.0, 'tol_scale_adjoint': 100.0, 'ode_blocks': 1, 'add_source': False, 'dt_min': 0.01, 'dt': 0.001, 'adaptive': False, 'leaky_relu_slope': 0.2, 'attention_dropout': 0, 'heads': 4, 'attention_norm_idx': 0, 'attention_dim': 64, 'mix_features': False, 'max_nfe': 100000, 'reweight_attention': False, 'jacobian_norm2': None, 'total_deriv': None, 'kinetic_energy': None, 'directional_penalty': None, 'rewiring': None, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_k': 64, 'gdc_threshold': 0.0001, 'gdc_avg_degree': 64, 'ppr_alpha': 0.05, 'heat_time': 3.0, 'not_lcc': True, 'att_samp_pct': 1, 'use_flux': False, 'exact': False, 'M_nodes': 64, 'new_edges': 'random', 'sparsify': 'S_hat', 'threshold_type': 'topk_adj', 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', 'rewire_KNN_epoch': 5, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'KNN_online': False, 'KNN_online_reps': 4, 'KNN_space': 'pos_distance', 'count_runs': 10, 'beltrami': False, 'fa_layer': False, 'pos_enc_type': 'DW64', 'pos_enc_orientation': 'row', 'feat_hidden_dim': 64, 'pos_enc_hidden_dim': 32, 'edge_sampling': False, 'edge_sampling_T': 'T0', 'edge_sampling_epoch': 5, 'edge_sampling_add': 0.64, 'edge_sampling_add_type': 'importance', 'edge_sampling_rmv': 0.32, 'edge_sampling_sym': False, 'edge_sampling_online': False, 'edge_sampling_online_reps': 4, 'edge_sampling_space': 'attention', 'symmetric_attention': False, 'fa_layer_edge_sampling_rmv': 0.8, 'gpu': 0, 'pos_enc_csv': False, 'pos_dist_quantile': 0.001, 'use_mlp': False, 'use_labels': False, 'fc_out': False, 'batch_norm': False, 'num_feature': 1433, 'num_class': 7, 'num_nodes': 2708, 'ode': 'ode', 'max_iters': 100}\n",
      "GNN\n",
      "m1.weight\n",
      "torch.Size([16, 767])\n",
      "m1.bias\n",
      "torch.Size([16])\n",
      "m2.weight\n",
      "torch.Size([10, 16])\n",
      "m2.bias\n",
      "torch.Size([10])\n",
      "odeblock.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.odefunc.d\n",
      "torch.Size([16])\n",
      "odeblock.reg_odefunc.odefunc.alpha_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.beta_train\n",
      "torch.Size([])\n",
      "odeblock.reg_odefunc.odefunc.alpha_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.beta_sc\n",
      "torch.Size([1])\n",
      "odeblock.reg_odefunc.odefunc.w\n",
      "torch.Size([16, 16])\n",
      "odeblock.reg_odefunc.odefunc.d\n",
      "torch.Size([16])\n",
      "Epoch: 001, Runtime 4.206305, Loss 2.317721, forward nfe 3627, backward nfe 71, Train: 0.1350, Val: 0.0931, Test: 0.0899\n",
      "Epoch: 002, Runtime 5.083633, Loss 2.625843, forward nfe 10886, backward nfe 1095, Train: 0.0950, Val: 0.0262, Test: 0.0375\n",
      "Epoch: 003, Runtime 5.085582, Loss 2.823226, forward nfe 18146, backward nfe 2120, Train: 0.1000, Val: 0.0223, Test: 0.0362\n",
      "Epoch: 004, Runtime 5.083934, Loss 2.368410, forward nfe 25405, backward nfe 3144, Train: 0.1050, Val: 0.0515, Test: 0.0579\n",
      "Epoch: 005, Runtime 5.088912, Loss 2.339871, forward nfe 32666, backward nfe 4168, Train: 0.1650, Val: 0.0692, Test: 0.0739\n",
      "Epoch: 006, Runtime 5.085317, Loss 2.418784, forward nfe 39926, backward nfe 5193, Train: 0.1950, Val: 0.0731, Test: 0.0793\n",
      "Epoch: 007, Runtime 5.086807, Loss 2.350095, forward nfe 47186, backward nfe 6217, Train: 0.2700, Val: 0.1092, Test: 0.1227\n",
      "Epoch: 008, Runtime 5.092566, Loss 2.263374, forward nfe 54446, backward nfe 7247, Train: 0.2150, Val: 0.0569, Test: 0.0738\n",
      "Epoch: 009, Runtime 5.085597, Loss 2.224288, forward nfe 61707, backward nfe 8274, Train: 0.2950, Val: 0.1423, Test: 0.1598\n",
      "Epoch: 010, Runtime 5.087876, Loss 2.169622, forward nfe 68969, backward nfe 9300, Train: 0.2550, Val: 0.1431, Test: 0.1622\n",
      "Epoch: 011, Runtime 5.106679, Loss 2.144758, forward nfe 76231, backward nfe 10350, Train: 0.2250, Val: 0.1346, Test: 0.1463\n",
      "Epoch: 012, Runtime 5.088335, Loss 2.142231, forward nfe 83493, backward nfe 11379, Train: 0.2350, Val: 0.1308, Test: 0.1439\n",
      "Epoch: 013, Runtime 5.092270, Loss 2.147224, forward nfe 90755, backward nfe 12413, Train: 0.2000, Val: 0.1269, Test: 0.1388\n",
      "Epoch: 014, Runtime 5.086905, Loss 2.138405, forward nfe 98017, backward nfe 13438, Train: 0.2250, Val: 0.1300, Test: 0.1442\n",
      "Epoch: 015, Runtime 5.092418, Loss 2.104886, forward nfe 105279, backward nfe 14471, Train: 0.2700, Val: 0.1408, Test: 0.1565\n",
      "Epoch: 016, Runtime 5.087447, Loss 2.070807, forward nfe 112541, backward nfe 15496, Train: 0.2800, Val: 0.1431, Test: 0.1580\n",
      "Epoch: 017, Runtime 5.147989, Loss 2.036374, forward nfe 119803, backward nfe 16585, Train: 0.2800, Val: 0.1415, Test: 0.1566\n",
      "Epoch: 018, Runtime 5.088364, Loss 2.004797, forward nfe 127066, backward nfe 17614, Train: 0.2800, Val: 0.1415, Test: 0.1569\n",
      "Epoch: 019, Runtime 5.086219, Loss 1.985579, forward nfe 134329, backward nfe 18641, Train: 0.2900, Val: 0.1415, Test: 0.1578\n",
      "Epoch: 020, Runtime 5.145650, Loss 1.966693, forward nfe 141591, backward nfe 19730, Train: 0.2950, Val: 0.1415, Test: 0.1584\n",
      "Epoch: 021, Runtime 5.087905, Loss 1.940685, forward nfe 148854, backward nfe 20757, Train: 0.3000, Val: 0.1415, Test: 0.1590\n",
      "Epoch: 022, Runtime 5.094059, Loss 1.909407, forward nfe 156117, backward nfe 21793, Train: 0.3250, Val: 0.1515, Test: 0.1694\n",
      "Epoch: 023, Runtime 5.154972, Loss 1.881028, forward nfe 163379, backward nfe 22892, Train: 0.3500, Val: 0.1592, Test: 0.1829\n",
      "Epoch: 024, Runtime 5.115844, Loss 1.848088, forward nfe 170641, backward nfe 23949, Train: 0.3650, Val: 0.1685, Test: 0.1912\n",
      "Epoch: 025, Runtime 5.104209, Loss 1.816989, forward nfe 177903, backward nfe 24994, Train: 0.3800, Val: 0.1792, Test: 0.2000\n",
      "Epoch: 026, Runtime 5.099340, Loss 1.788159, forward nfe 185166, backward nfe 26032, Train: 0.4300, Val: 0.1908, Test: 0.2176\n",
      "Epoch: 027, Runtime 5.110044, Loss 1.752241, forward nfe 192428, backward nfe 27082, Train: 0.4500, Val: 0.1954, Test: 0.2273\n",
      "Epoch: 028, Runtime 5.097549, Loss 1.713361, forward nfe 199690, backward nfe 28122, Train: 0.4700, Val: 0.2038, Test: 0.2348\n",
      "Epoch: 029, Runtime 5.174084, Loss 1.680849, forward nfe 206952, backward nfe 29241, Train: 0.4800, Val: 0.2069, Test: 0.2390\n",
      "Epoch: 030, Runtime 5.093724, Loss 1.654429, forward nfe 214215, backward nfe 30274, Train: 0.4900, Val: 0.2077, Test: 0.2407\n",
      "Epoch: 031, Runtime 5.104386, Loss 1.634225, forward nfe 221478, backward nfe 31320, Train: 0.5050, Val: 0.2146, Test: 0.2498\n",
      "Epoch: 032, Runtime 5.151518, Loss 1.602797, forward nfe 228740, backward nfe 32414, Train: 0.5100, Val: 0.2269, Test: 0.2664\n",
      "Epoch: 033, Runtime 5.136157, Loss 1.573820, forward nfe 236002, backward nfe 33492, Train: 0.5150, Val: 0.2477, Test: 0.2911\n",
      "Epoch: 034, Runtime 5.091952, Loss 1.549445, forward nfe 243265, backward nfe 34523, Train: 0.5100, Val: 0.2908, Test: 0.3374\n",
      "Epoch: 035, Runtime 5.088855, Loss 1.522040, forward nfe 250529, backward nfe 35550, Train: 0.5250, Val: 0.3008, Test: 0.3571\n",
      "Epoch: 036, Runtime 5.111083, Loss 1.492100, forward nfe 257791, backward nfe 36600, Train: 0.5300, Val: 0.3000, Test: 0.3501\n",
      "Epoch: 037, Runtime 5.089157, Loss 1.462122, forward nfe 265053, backward nfe 37628, Train: 0.5300, Val: 0.2923, Test: 0.3421\n",
      "Epoch: 038, Runtime 5.114134, Loss 1.426727, forward nfe 272316, backward nfe 38685, Train: 0.5500, Val: 0.3092, Test: 0.3559\n",
      "Epoch: 039, Runtime 5.087714, Loss 1.417238, forward nfe 279578, backward nfe 39713, Train: 0.5850, Val: 0.3392, Test: 0.3888\n",
      "Epoch: 040, Runtime 5.085582, Loss 1.384092, forward nfe 286838, backward nfe 40737, Train: 0.6050, Val: 0.3815, Test: 0.4281\n",
      "Epoch: 041, Runtime 5.097119, Loss 1.362105, forward nfe 294100, backward nfe 41771, Train: 0.5950, Val: 0.4323, Test: 0.4652\n",
      "Epoch: 042, Runtime 5.087088, Loss 1.337671, forward nfe 301362, backward nfe 42800, Train: 0.6150, Val: 0.4800, Test: 0.4988\n",
      "Epoch: 043, Runtime 5.087069, Loss 1.312161, forward nfe 308624, backward nfe 43827, Train: 0.6300, Val: 0.4923, Test: 0.5156\n",
      "Epoch: 044, Runtime 5.089318, Loss 1.288624, forward nfe 315887, backward nfe 44854, Train: 0.6600, Val: 0.5046, Test: 0.5304\n",
      "Epoch: 045, Runtime 5.080914, Loss 1.271756, forward nfe 323148, backward nfe 45879, Train: 0.7000, Val: 0.5092, Test: 0.5404\n",
      "Epoch: 046, Runtime 5.085927, Loss 1.242341, forward nfe 330410, backward nfe 46905, Train: 0.6950, Val: 0.5223, Test: 0.5483\n",
      "Epoch: 047, Runtime 5.085793, Loss 1.228464, forward nfe 337672, backward nfe 47929, Train: 0.7150, Val: 0.5385, Test: 0.5606\n",
      "Epoch: 048, Runtime 5.084127, Loss 1.203125, forward nfe 344936, backward nfe 48954, Train: 0.7100, Val: 0.5546, Test: 0.5797\n",
      "Epoch: 049, Runtime 5.086954, Loss 1.194961, forward nfe 352198, backward nfe 49978, Train: 0.7200, Val: 0.5738, Test: 0.5939\n",
      "Epoch: 050, Runtime 5.086243, Loss 1.175408, forward nfe 359460, backward nfe 51003, Train: 0.7200, Val: 0.5808, Test: 0.6050\n",
      "Epoch: 051, Runtime 5.087690, Loss 1.148044, forward nfe 366723, backward nfe 52029, Train: 0.7300, Val: 0.5838, Test: 0.6060\n",
      "Epoch: 052, Runtime 5.091668, Loss 1.130781, forward nfe 373985, backward nfe 53055, Train: 0.7500, Val: 0.5769, Test: 0.5998\n",
      "Epoch: 053, Runtime 5.122660, Loss 1.106396, forward nfe 381247, backward nfe 54079, Train: 0.7400, Val: 0.5669, Test: 0.5928\n",
      "Epoch: 054, Runtime 5.113656, Loss 1.098548, forward nfe 388510, backward nfe 55108, Train: 0.7350, Val: 0.5715, Test: 0.5946\n",
      "Epoch: 055, Runtime 5.111889, Loss 1.075773, forward nfe 395772, backward nfe 56135, Train: 0.7400, Val: 0.5908, Test: 0.6161\n",
      "Epoch: 056, Runtime 5.109811, Loss 1.067995, forward nfe 403034, backward nfe 57162, Train: 0.7350, Val: 0.6131, Test: 0.6339\n",
      "Epoch: 057, Runtime 5.109260, Loss 1.045562, forward nfe 410296, backward nfe 58190, Train: 0.7350, Val: 0.6331, Test: 0.6488\n",
      "Epoch: 058, Runtime 5.112643, Loss 1.029436, forward nfe 417559, backward nfe 59220, Train: 0.7350, Val: 0.6338, Test: 0.6516\n",
      "Epoch: 059, Runtime 5.109039, Loss 1.023221, forward nfe 424821, backward nfe 60244, Train: 0.7650, Val: 0.6208, Test: 0.6516\n",
      "Epoch: 060, Runtime 5.105873, Loss 0.999457, forward nfe 432082, backward nfe 61268, Train: 0.7750, Val: 0.6246, Test: 0.6508\n",
      "Epoch: 061, Runtime 5.111861, Loss 0.986361, forward nfe 439343, backward nfe 62294, Train: 0.7750, Val: 0.6331, Test: 0.6590\n",
      "Epoch: 062, Runtime 5.114293, Loss 0.963908, forward nfe 446604, backward nfe 63323, Train: 0.7550, Val: 0.6438, Test: 0.6633\n",
      "Epoch: 063, Runtime 5.107966, Loss 0.956820, forward nfe 453866, backward nfe 64347, Train: 0.7600, Val: 0.6477, Test: 0.6678\n",
      "Epoch: 064, Runtime 5.112051, Loss 0.932677, forward nfe 461128, backward nfe 65372, Train: 0.7650, Val: 0.6500, Test: 0.6707\n",
      "Epoch: 065, Runtime 5.113093, Loss 0.929035, forward nfe 468390, backward nfe 66396, Train: 0.7900, Val: 0.6469, Test: 0.6719\n",
      "Epoch: 066, Runtime 5.114784, Loss 0.919118, forward nfe 475653, backward nfe 67420, Train: 0.7750, Val: 0.6438, Test: 0.6703\n",
      "Epoch: 067, Runtime 5.115545, Loss 0.898433, forward nfe 482914, backward nfe 68452, Train: 0.7900, Val: 0.6538, Test: 0.6728\n",
      "Epoch: 068, Runtime 5.111797, Loss 0.882865, forward nfe 490177, backward nfe 69480, Train: 0.8000, Val: 0.6615, Test: 0.6817\n",
      "Epoch: 069, Runtime 5.108801, Loss 0.860980, forward nfe 497439, backward nfe 70504, Train: 0.8000, Val: 0.6692, Test: 0.6817\n",
      "Epoch: 070, Runtime 5.105755, Loss 0.856690, forward nfe 504700, backward nfe 71528, Train: 0.7900, Val: 0.6708, Test: 0.6919\n",
      "Epoch: 071, Runtime 5.108337, Loss 0.833733, forward nfe 511962, backward nfe 72552, Train: 0.7950, Val: 0.6862, Test: 0.6997\n",
      "Epoch: 072, Runtime 5.106046, Loss 0.817182, forward nfe 519224, backward nfe 73577, Train: 0.8100, Val: 0.6731, Test: 0.6930\n",
      "Epoch: 073, Runtime 5.110724, Loss 0.790788, forward nfe 526485, backward nfe 74601, Train: 0.8100, Val: 0.6731, Test: 0.6901\n",
      "Epoch: 074, Runtime 5.105860, Loss 0.775824, forward nfe 533746, backward nfe 75625, Train: 0.8050, Val: 0.6815, Test: 0.7010\n",
      "Epoch: 075, Runtime 5.109620, Loss 0.762738, forward nfe 541006, backward nfe 76651, Train: 0.8050, Val: 0.6862, Test: 0.7055\n",
      "Epoch: 076, Runtime 5.113496, Loss 0.745703, forward nfe 548266, backward nfe 77680, Train: 0.8050, Val: 0.6838, Test: 0.7054\n",
      "Epoch: 077, Runtime 5.109568, Loss 0.726871, forward nfe 555528, backward nfe 78704, Train: 0.8150, Val: 0.6931, Test: 0.7142\n",
      "Epoch: 078, Runtime 5.106816, Loss 0.705958, forward nfe 562788, backward nfe 79729, Train: 0.8200, Val: 0.7023, Test: 0.7228\n",
      "Epoch: 079, Runtime 5.107424, Loss 0.687799, forward nfe 570050, backward nfe 80753, Train: 0.8100, Val: 0.6946, Test: 0.7158\n",
      "Epoch: 080, Runtime 5.111034, Loss 0.677532, forward nfe 577311, backward nfe 81778, Train: 0.8350, Val: 0.6908, Test: 0.7148\n",
      "Epoch: 081, Runtime 5.104974, Loss 0.644795, forward nfe 584572, backward nfe 82802, Train: 0.8350, Val: 0.7023, Test: 0.7270\n",
      "Epoch: 082, Runtime 5.109272, Loss 0.638641, forward nfe 591833, backward nfe 83827, Train: 0.8300, Val: 0.7154, Test: 0.7341\n",
      "Epoch: 083, Runtime 5.103938, Loss 0.619460, forward nfe 599093, backward nfe 84851, Train: 0.8400, Val: 0.7085, Test: 0.7310\n",
      "Epoch: 084, Runtime 5.105046, Loss 0.591987, forward nfe 606354, backward nfe 85875, Train: 0.8550, Val: 0.7100, Test: 0.7356\n",
      "Epoch: 085, Runtime 5.108139, Loss 0.595566, forward nfe 613615, backward nfe 86899, Train: 0.8550, Val: 0.7185, Test: 0.7429\n",
      "Epoch: 086, Runtime 5.110798, Loss 0.577569, forward nfe 620876, backward nfe 87924, Train: 0.8500, Val: 0.7238, Test: 0.7420\n",
      "Epoch: 087, Runtime 5.108185, Loss 0.567686, forward nfe 628136, backward nfe 88948, Train: 0.8600, Val: 0.7308, Test: 0.7444\n",
      "Epoch: 088, Runtime 5.104420, Loss 0.542389, forward nfe 635397, backward nfe 89973, Train: 0.8600, Val: 0.7308, Test: 0.7484\n",
      "Epoch: 089, Runtime 5.104750, Loss 0.554520, forward nfe 642658, backward nfe 90997, Train: 0.8650, Val: 0.7262, Test: 0.7447\n",
      "Epoch: 090, Runtime 5.108192, Loss 0.520209, forward nfe 649919, backward nfe 92021, Train: 0.8700, Val: 0.7223, Test: 0.7436\n",
      "Epoch: 091, Runtime 5.106376, Loss 0.504995, forward nfe 657180, backward nfe 93045, Train: 0.8550, Val: 0.7400, Test: 0.7634\n",
      "Epoch: 092, Runtime 5.109527, Loss 0.505921, forward nfe 664441, backward nfe 94071, Train: 0.8600, Val: 0.7315, Test: 0.7516\n",
      "Epoch: 093, Runtime 5.107132, Loss 0.493791, forward nfe 671701, backward nfe 95094, Train: 0.8650, Val: 0.7492, Test: 0.7642\n",
      "Epoch: 094, Runtime 5.110695, Loss 0.476501, forward nfe 678961, backward nfe 96118, Train: 0.8700, Val: 0.7531, Test: 0.7659\n",
      "Epoch: 095, Runtime 5.109766, Loss 0.484561, forward nfe 686220, backward nfe 97142, Train: 0.8750, Val: 0.7377, Test: 0.7518\n",
      "Epoch: 096, Runtime 5.108086, Loss 0.453786, forward nfe 693479, backward nfe 98166, Train: 0.8650, Val: 0.7469, Test: 0.7561\n",
      "Epoch: 097, Runtime 5.109499, Loss 0.439618, forward nfe 700739, backward nfe 99189, Train: 0.8650, Val: 0.7546, Test: 0.7697\n",
      "Epoch: 098, Runtime 5.111223, Loss 0.431251, forward nfe 707999, backward nfe 100216, Train: 0.8800, Val: 0.7531, Test: 0.7675\n",
      "Epoch: 099, Runtime 5.107674, Loss 0.433834, forward nfe 715259, backward nfe 101240, Train: 0.8900, Val: 0.7592, Test: 0.7666\n",
      "best val accuracy 0.759231 with test accuracy 0.766569 at epoch 99\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_cora_defaults', action='store_true',\n",
    "                  help='Whether to run with best params for cora. Overrides the choice of dataset')\n",
    "parser.add_argument('--dataset', type=str, default='Cora',\n",
    "                  help='Cora, Citeseer, Pubmed, Computers, Photo, CoauthorCS')\n",
    "parser.add_argument('--data_norm', type=str, default='rw',\n",
    "                  help='rw for random walk, gcn for symmetric gcn norm')\n",
    "parser.add_argument('--hidden_dim', type=int, default=16, help='Hidden dimension.')\n",
    "parser.add_argument('--input_dropout', type=float, default=0.5, help='Input dropout rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='One from sgd, rmsprop, adam, adagrad, adamax.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Learning rate.')\n",
    "parser.add_argument('--decay', type=float, default=5e-4, help='Weight decay for optimization')\n",
    "parser.add_argument('--self_loop_weight', type=float, default=1.0, help='Weight of self-loops.')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='Number of training epochs per iteration.')\n",
    "parser.add_argument('--alpha', type=float, default=1.0, help='Factor in front matrix A.')\n",
    "parser.add_argument('--time', type=float, default=1.0, help='End time of ODE integrator.')\n",
    "parser.add_argument('--augment', action='store_true',\n",
    "                  help='double the length of the feature vector by appending zeros to stabilist ODE learning')\n",
    "parser.add_argument('--alpha_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) alpha')\n",
    "parser.add_argument('--no_alpha_sigmoid', dest='no_alpha_sigmoid', action='store_true', help='apply sigmoid before multiplying by alpha')\n",
    "parser.add_argument('--beta_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) beta')\n",
    "parser.add_argument('--block', type=str, default='constant', help='constant, mixed, attention, SDE')\n",
    "parser.add_argument('--function', type=str, default='laplacian', help='laplacian, transformer, dorsey, GAT, SDE')\n",
    "parser.add_argument('--geom_gcn_splits', dest='geom_gcn_splits', action='store_true',\n",
    "                      help='use the 10 fixed splits from '\n",
    "                           'https://arxiv.org/abs/2002.05287')\n",
    "# ODE args\n",
    "parser.add_argument('--method', type=str, default='dopri5',\n",
    "                  help=\"set the numerical solver: dopri5, euler, rk4, midpoint\")\n",
    "parser.add_argument('--step_size', type=float, default=1, help='fixed step size when using fixed step solvers e.g. rk4')\n",
    "parser.add_argument(\n",
    "    \"--adjoint_method\", type=str, default=\"adaptive_heun\",\n",
    "    help=\"set the numerical solver for the backward pass: dopri5, euler, rk4, midpoint\"\n",
    ")\n",
    "parser.add_argument('--adjoint_step_size', type=float, default=1, help='fixed step size when using fixed step adjoint solvers e.g. rk4')\n",
    "parser.add_argument('--adjoint', default=False, help='use the adjoint ODE method to reduce memory footprint')\n",
    "parser.add_argument('--tol_scale', type=float, default=1., help='multiplier for atol and rtol')\n",
    "parser.add_argument(\"--tol_scale_adjoint\", type=float, default=1.0,\n",
    "                  help=\"multiplier for adjoint_atol and adjoint_rtol\")\n",
    "parser.add_argument('--ode_blocks', type=int, default=1, help='number of ode blocks to run')\n",
    "parser.add_argument('--add_source', dest='add_source', action='store_true',\n",
    "                  help='If try get rid of alpha param and the beta*x0 source term')\n",
    "# SDE args\n",
    "parser.add_argument('--dt_min', type=float, default=1e-5, help='minimum timestep for the SDE solver')\n",
    "parser.add_argument('--dt', type=float, default=1e-3, help='fixed step size')\n",
    "parser.add_argument('--adaptive', dest='adaptive', action='store_true', help='use adaptive step sizes')\n",
    "# Attention args\n",
    "parser.add_argument('--leaky_relu_slope', type=float, default=0.2,\n",
    "                  help='slope of the negative part of the leaky relu used in attention')\n",
    "parser.add_argument('--attention_dropout', type=float, default=0., help='dropout of attention weights')\n",
    "parser.add_argument('--heads', type=int, default=4, help='number of attention heads')\n",
    "parser.add_argument('--attention_norm_idx', type=int, default=0, help='0 = normalise rows, 1 = normalise cols')\n",
    "parser.add_argument('--attention_dim', type=int, default=64,\n",
    "                  help='the size to project x to before calculating att scores')\n",
    "parser.add_argument('--mix_features', dest='mix_features', action='store_true',\n",
    "                  help='apply a feature transformation xW to the ODE')\n",
    "parser.add_argument(\"--max_nfe\", type=int, default=1000, help=\"Maximum number of function evaluations allowed.\")\n",
    "parser.add_argument('--reweight_attention', dest='reweight_attention', action='store_true', help=\"multiply attention scores by edge weights before softmax\")\n",
    "# regularisation args\n",
    "parser.add_argument('--jacobian_norm2', type=float, default=None, help=\"int_t ||df/dx||_F^2\")\n",
    "parser.add_argument('--total_deriv', type=float, default=None, help=\"int_t ||df/dt||^2\")\n",
    "\n",
    "parser.add_argument('--kinetic_energy', type=float, default=None, help=\"int_t ||f||_2^2\")\n",
    "parser.add_argument('--directional_penalty', type=float, default=None, help=\"int_t ||(df/dx)^T f||^2\")\n",
    "\n",
    "# rewiring args\n",
    "parser.add_argument('--rewiring', type=str, default=None, help=\"two_hop, gdc\")\n",
    "parser.add_argument('--gdc_method', type=str, default='ppr', help=\"ppr, heat, coeff\")\n",
    "parser.add_argument('--gdc_sparsification', type=str, default='topk', help=\"threshold, topk\")\n",
    "parser.add_argument('--gdc_k', type=int, default=64, help=\"number of neighbours to sparsify to when using topk\")\n",
    "parser.add_argument('--gdc_threshold', type=float, default=0.0001, help=\"obove this edge weight, keep edges when using threshold\")\n",
    "parser.add_argument('--gdc_avg_degree', type=int, default=64,\n",
    "                  help=\"if gdc_threshold is not given can be calculated by specifying avg degree\")\n",
    "parser.add_argument('--ppr_alpha', type=float, default=0.05, help=\"teleport probability\")\n",
    "parser.add_argument('--heat_time', type=float, default=3., help=\"time to run gdc heat kernal diffusion for\")\n",
    "\n",
    "parser.add_argument(\"--not_lcc\", action=\"store_false\", help=\"don't use the largest connected component\")\n",
    "parser.add_argument('--att_samp_pct', type=float, default=1,\n",
    "                  help=\"float in [0,1). The percentage of edges to retain based on attention scores\")\n",
    "parser.add_argument('--use_flux', dest='use_flux', action='store_true',\n",
    "                  help='incorporate the feature grad in attention based edge dropout')\n",
    "parser.add_argument(\"--exact\", action=\"store_true\",\n",
    "                  help=\"for small datasets can do exact diffusion. If dataset is too big for matrix inversion then you can't\")\n",
    "parser.add_argument('--M_nodes', type=int, default=64, help=\"new number of nodes to add\")\n",
    "parser.add_argument('--new_edges', type=str, default=\"random\", help=\"random, random_walk, k_hop\")\n",
    "parser.add_argument('--sparsify', type=str, default=\"S_hat\", help=\"S_hat, recalc_att\")\n",
    "parser.add_argument('--threshold_type', type=str, default=\"topk_adj\", help=\"topk_adj, addD_rvR\")\n",
    "parser.add_argument('--rw_addD', type=float, default=0.02, help=\"percentage of new edges to add\")\n",
    "parser.add_argument('--rw_rmvR', type=float, default=0.02, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--rewire_KNN', action='store_true', help='perform KNN rewiring every few epochs')\n",
    "parser.add_argument('--rewire_KNN_T', type=str, default=\"T0\", help=\"T0, TN\")\n",
    "parser.add_argument('--rewire_KNN_epoch', type=int, default=5, help=\"frequency of epochs to rewire\")\n",
    "parser.add_argument('--rewire_KNN_k', type=int, default=64, help=\"target degree for KNN rewire\")\n",
    "parser.add_argument('--rewire_KNN_sym', action='store_true', help='make KNN symmetric')\n",
    "parser.add_argument('--KNN_online', action='store_true', help='perform rewiring online')\n",
    "parser.add_argument('--KNN_online_reps', type=int, default=4, help=\"how many online KNN its\")\n",
    "parser.add_argument('--KNN_space', type=str, default=\"pos_distance\", help=\"Z,P,QKZ,QKp\")\n",
    "\n",
    "# Stefan's experiment args\n",
    "parser.add_argument('--count_runs', type=int, default=10,\n",
    "                  help=\"number of runs to average results over per parameter settings for each experiment\")\n",
    "\n",
    "# beltrami\n",
    "parser.add_argument('--beltrami', action='store_true', help='perform diffusion beltrami style')\n",
    "parser.add_argument('--fa_layer', action='store_true', help='add a bottleneck paper style layer with more edges')\n",
    "parser.add_argument('--pos_enc_type', type=str, default=\"DW64\",\n",
    "                  help='positional encoder either GDC, DW64, DW128, DW256')\n",
    "parser.add_argument('--pos_enc_orientation', type=str, default=\"row\", help=\"row, col\")\n",
    "parser.add_argument('--feat_hidden_dim', type=int, default=64, help=\"dimension of features in beltrami\")\n",
    "parser.add_argument('--pos_enc_hidden_dim', type=int, default=32, help=\"dimension of position in beltrami\")\n",
    "parser.add_argument('--edge_sampling', action='store_true', help='perform edge sampling rewiring')\n",
    "parser.add_argument('--edge_sampling_T', type=str, default=\"T0\", help=\"T0, TN\")\n",
    "parser.add_argument('--edge_sampling_epoch', type=int, default=5, help=\"frequency of epochs to rewire\")\n",
    "parser.add_argument('--edge_sampling_add', type=float, default=0.64, help=\"percentage of new edges to add\")\n",
    "parser.add_argument('--edge_sampling_add_type', type=str, default=\"importance\",\n",
    "                  help=\"random, ,anchored, importance, degree\")\n",
    "parser.add_argument('--edge_sampling_rmv', type=float, default=0.32, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--edge_sampling_sym', action='store_true', help='make KNN symmetric')\n",
    "parser.add_argument('--edge_sampling_online', action='store_true', help='perform rewiring online')\n",
    "parser.add_argument('--edge_sampling_online_reps', type=int, default=4, help=\"how many online KNN its\")\n",
    "parser.add_argument('--edge_sampling_space', type=str, default=\"attention\",\n",
    "                  help=\"attention,pos_distance, z_distance, pos_distance_QK, z_distance_QK\")\n",
    "parser.add_argument('--symmetric_attention', action='store_true',\n",
    "                  help='maks the attention symmetric for rewring in QK space')\n",
    "\n",
    "parser.add_argument('--fa_layer_edge_sampling_rmv', type=float, default=0.8, help=\"percentage of edges to remove\")\n",
    "parser.add_argument('--gpu', type=int, default=0, help=\"GPU to run on (default 0)\")\n",
    "parser.add_argument('--pos_enc_csv', action='store_true', help=\"Generate pos encoding as a sparse CSV\")\n",
    "\n",
    "parser.add_argument('--pos_dist_quantile', type=float, default=0.001, help=\"percentage of N**2 edges to keep\")\n",
    "\n",
    "#added\n",
    "parser.add_argument('--use_mlp', dest='use_mlp', action='store_true',\n",
    "                  help='Add a fully connected layer to the encoder.')\n",
    "parser.add_argument('--use_labels', dest='use_labels', action='store_true', help='Also diffuse labels')\n",
    "parser.add_argument('--fc_out', dest='fc_out', action='store_true',\n",
    "                  help='Add a fully connected layer to the decoder.')\n",
    "parser.add_argument(\"--batch_norm\", dest='batch_norm', action='store_true', help='search over reg params')\n",
    "\n",
    "args = parser.parse_args(customArgs)\n",
    "opt = vars(args)\n",
    "\n",
    "#'Cora' #'Flickr' #'Computer'\n",
    "opt['dataset'] = 'Computer' \n",
    "\n",
    "if opt['dataset'] == 'Cora':\n",
    "    opt = get_cora_opt(opt)\n",
    "elif opt['dataset'] == 'Computer':\n",
    "    opt = get_computers_opt(opt)\n",
    "elif opt['dataset'] == 'Flickr':\n",
    "    opt = get_flickr_opt(opt)\n",
    "\n",
    "opt['adjoint'] = True\n",
    "#opt['method'] = 'explicit_adams'\n",
    "opt['method'] = 'implicit_adams'\n",
    "#opt['method'] = 'dopri5'\n",
    "opt['adjoint_method'] = opt['method']\n",
    "opt['max_iters'] = 100\n",
    "opt['step_size'] = opt['dt_min'] = 0.01\n",
    "opt['tol_scale'] = 100.0\n",
    "opt['tol_scale_adjoint'] = 100.0\n",
    "#added\n",
    "opt['max_nfe'] = 100000\n",
    "if opt['dataset'] == 'Flickr':    \n",
    "    opt['rewiring'] == 'gdc'\n",
    "\n",
    "# DEBUG\n",
    "#for k in ['dataset', 'epoch', 'adjoint', 'rewiring', 'adaptive', 'dt', 'dt_min', 'method', 'adjoint_method']:\n",
    "#  print(k, opt[k])\n",
    "#main(opt, 0)\n",
    "\n",
    "# Run combination of experiments\n",
    "for stepsize in [1.0,0.5,0.1,0.01]: #[0.5, 0.25, 0.1, 0.01]: # 2.0, 1.0\n",
    "    print(f'*** Doing stepsize {stepsize} ***')\n",
    "    for idx in range(opt['count_runs']):\n",
    "        print(f'*** Doing run {idx} ***')\n",
    "        # NOTE: I think setting dt_min may not be necessary, doing it just to be safe!\n",
    "        opt['step_size'] = opt['dt_min'] = stepsize\n",
    "        run(opt, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ff871-6d10-462c-b9f4-c0d2d6dbc161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedf3b1-af5d-49d0-8ddb-834f9e6dea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
